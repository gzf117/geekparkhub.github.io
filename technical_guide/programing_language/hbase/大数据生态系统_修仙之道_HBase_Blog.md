# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ HBase Blog

@(2019-04-10)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Language:HBase|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

##  ğŸ˜ HBase Technology ä¿®ä»™ä¹‹é“ åˆ»è‹¦ä¿®æŒ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/hbase.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------


[TOC]



## 1. HBase æ¦‚è¿°

### 1.1 HBase ç®€ä»‹
> HBaseæ˜¯åŸºäºè°·æ­Œå…¬å¸BigTableå¼€å‘çš„å¼€æºåˆ†å¸ƒå¼æ•°æ®åº“,å…·æœ‰`é«˜å¯é `/`é«˜æ€§èƒ½`/`é¢å‘åˆ—`/`å¯ä¼¸ç¼©ç­‰ç‰¹ç‚¹`.
> 
> HBaseè¿è¡Œåœ¨HDFSä¹‹ä¸Š,ä¸»è¦ç”¨æ¥å­˜å‚¨`éç»“æ„åŒ–æ•°æ®`å’Œ`åŠç»“æ„åŒ–æ•°æ®`.
> 
> HBaseé€šè¿‡æ°´å¹³æ‰©å±•çš„æ–¹å¼å®ç°äºå­˜å‚¨å¤§è¡¨æ•°æ®(è¡¨è§„æ¨¡å¯ä»¥è¾¾åˆ°æ•°åäº¿è¡Œä»¥åŠæ•°ç™¾åˆ—),å¹¶ä¸”å¯¹å¤§è¡¨æ•°æ®è¯»å†™è®¿é—®å¯è¾¾åˆ°å®æ—¶çº§åˆ«.

- HBaseç”±Powersetå…¬å¸åœ¨2007å¹´å‘èµ·,æœ€åˆæ˜¯Hadoopçš„ä¸€éƒ¨åˆ†,åœ¨æ­¤ä¹‹å,HBaseé¡¹ç›®é€æ¸å‘å±•æˆä¸ºApacheè½¯ä»¶åŸºé‡‘ä¼šæ——ä¸‹é¡¶çº§é¡¹ç›®.
- HBaseå‘å±•é‡Œç¨‹ç¢‘ç®€è¿° : 
- 2006å¹´11æœˆ Googleå‘å¸ƒBigTableè®ºæ–‡.
- 2007å¹´2æœˆ HBaseä½œä¸ºHadoopé¡¹ç›®çš„å­é¡¹ç›®æˆç«‹.
- 2007å¹´10æœˆ HBaseç¬¬ä¸€ä¸ªå¯ç”¨ç‰ˆæœ¬å‘å¸ƒ(Hadoop 0.15.0).
- 2010å¹´5æœˆ HBaseæˆä¸ºApacheè½¯ä»¶åŸºé‡‘ä¼šé¡¶çº§é¡¹ç›®.
- 2011å¹´1æœˆ HBase0.90.0å‘å¸ƒ,è¯¥ç‰ˆæœ¬ä¸ºé¢å‘æ‰€æœ‰ç”¨æˆ·çš„ç¨³å®šç‰ˆæœ¬.
- 2017å¹´2æœˆ HBase 1.2.xç‰ˆæœ¬å‘å¸ƒ.


### 1.2 HBase ç‰¹ç‚¹
#### 1.2.1 æµ·é‡å­˜å‚¨
> HBaseé€‚åˆå­˜å‚¨PBçº§åˆ«æµ·é‡æ•°æ®,åœ¨PBçº§åˆ«æ•°æ®ä»¥åŠé‡‡ç”¨å»‰ä»·PCå­˜å‚¨æƒ…å†µä¸‹,èƒ½åœ¨å‡ ååˆ°ç™¾æ¯«ç§’å†…è¿”å›æ•°æ®,è¿™ä¸HBaseææ˜“æ‰©å±•æ€§æ¯æ¯ç›¸å…³,æ­£å¼å› ä¸ºHBaseè‰¯å¥½çš„æ‰©å±•æ€§,æ‰ä¸ºæµ·é‡æ•°æ®å­˜å‚¨æä¾›äº†ä¾¿åˆ©.

#### 1.2.2 åˆ—å¼å­˜å‚¨
> åˆ—å¼å­˜å‚¨ä¹Ÿå°±æ˜¯åˆ—æ—å­˜å‚¨,HBaseæ˜¯æ ¹æ®åˆ—æ—æ¥å­˜å‚¨æ•°æ®,åˆ—æ—ä¸‹é¢å¯ä»¥æœ‰éå¸¸ä¹‹å¤šçš„åˆ—,åˆ—æ—åœ¨åˆ›å»ºè¡¨ç¤ºå°±å¿…é¡»æŒ‡å®š.

#### 1.2.3 ææ˜“æ‰©å±•
> HBaseæ‰©å±•æ€§ä¸»è¦ä½“ç°åœ¨ : 
> åŸºäºä¸Šå±‚å¤„ç†èƒ½åŠ›(RegionServer)çš„æ‰©å±•
> åŸºäºå­˜å‚¨æ‰©å±•(HDFS)
> é€šè¿‡æ¨ªå‘æ·»åŠ RegionServeræœºå™¨,è¿›è¡Œæ°´å¹³æ‰©å±•,æå‡HBaseä¸Šå±‚å¤„ç†èƒ½åŠ›,æå‡HBaseæœåŠ¡æ›´å¤šRegionèƒ½åŠ›.

#### 1.2.4 é«˜å¹¶å‘
> ç”±äºå¤§éƒ¨åˆ†ä½¿ç”¨HBaseæ¶æ„,éƒ½æ˜¯é‡‡ç”¨å»‰ä»·PC,å› æ­¤å•ä¸ªI/Oå»¶è¿Ÿå…¶å®å¹¶ä¸å°,ä¸€èˆ¬åœ¨å‡ ååˆ°å‡ ç™¾æ¯«ç§’ä¹‹é—´,é«˜å¹¶å‘å…¶å®æ˜¯åœ¨å¹¶å‘æƒ…å†µä¸‹,HBaseå•ä¸ªI/Oå»¶è¿Ÿä¸‹é™å¹¶ä¸å¤š,èƒ½è·å¾—é«˜å¹¶å‘,ä½å»¶æ—¶æœåŠ¡.

#### 1.2.5 ç¨€ç–
> ç¨€ç–ä¸»è¦é’ˆå¯¹HBaseåˆ—çš„çµæ´»æ€§,åœ¨åˆ—æ—ä¸­,å¯ä»¥æŒ‡å®šä»»æ„åˆ—,åœ¨åˆ—æ•°æ®ä¸ºç©ºçš„æƒ…å†µä¸‹,æ˜¯ä¸ä¼šå ç”¨å­˜å‚¨ç©ºé—´.

### 1.3 HBase åº”ç”¨åœºæ™¯
- é«˜ååé‡.
- æ€§èƒ½å¯ä¼¸ç¼©.
- æµ·é‡æ•°æ®(TB/PB).
- èƒ½å¤ŸåŒæ—¶å¤„ç†ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®.
- éœ€è¦åœ¨æµ·é‡æ•°æ®ä¸­å®ç°é«˜æ•ˆéšæœºè¯»å–.
- ä¸éœ€è¦å®Œå…¨æ‹¥æœ‰ä¼ ç»Ÿå…³ç³»å‹æ•°æ®åº“æ‰€å…·å¤‡(åŸå­æ€§/ä¸€è‡´æ€§/ç‹¬ç«‹æ€§/æŒä¹…æ€§)ACIDç‰¹æ€§.

### 1.4 HBase æ¶æ„
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_001.jpg)
- HBaseä¸€ç§æ˜¯ä½œä¸ºå­˜å‚¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,å¦ä¸€ç§æ˜¯ä½œä¸ºæ•°æ®å¤„ç†æ¨¡å‹çš„MRæ¡†æ¶,å› ä¸ºæ—¥å¸¸å¼€å‘äººå‘˜æ¯”è¾ƒç†Ÿç»ƒçš„æ˜¯ç»“æ„åŒ–çš„æ•°æ®è¿›è¡Œå¤„ç†,ä½†æ˜¯åœ¨HDFSç›´æ¥å­˜å‚¨çš„æ–‡ä»¶å¾€å¾€ä¸å…·æœ‰ç»“æ„åŒ–,æ‰€ä»¥å‚¬ç”Ÿå‡ºäº†HBaseåœ¨HDFSä¸Šæ“ä½œ,å¦‚æœéœ€è¦æŸ¥è¯¢æ•°æ®,åªéœ€è¦é€šè¿‡é”®å€¼ä¾¿å¯ä»¥æˆåŠŸè®¿é—®.
- HBaseé‡‡ç”¨Master/Slaveæ¶æ„,ä¸»è¦è§’è‰²åŒ…æ‹¬(HMaster,ç®¡ç†èŠ‚ç‚¹)MasteræœåŠ¡å™¨,(HRegionServer,æ•°æ®èŠ‚ç‚¹)RegionæœåŠ¡å™¨,ZookeeperæœåŠ¡å™¨ä»¥åŠå®¢æˆ·ç«¯,HBaseä»¥HDFSä½œä¸ºå…¶åº•å±‚å­˜å‚¨æ¥å®ç°æ•°æ®é«˜å¯ç”¨,å…¶æœ¬èº«ä¸å…·å¤‡æ•°æ®å¤åˆ¶åŠŸèƒ½.

### 1.5 HBase è§’è‰²

#### 1.5.1 HMaster
> `HMaster ä¸»èŠ‚ç‚¹`, HMasterä¸»è¦è´Ÿè´£ : HRegionServerç®¡ç†ä»¥åŠå…ƒæ•°æ®æ›´æ”¹,åŒ…æ‹¬ä»¥ä¸‹å†…å®¹(æ–°HRegionServeræ³¨å†Œ / è¡¨çš„å¢åˆ æ”¹æŸ¥ / HRegionServerè´Ÿè½½å‡è¡¡ / Regionè¡¨åˆ†åŒº / åˆ†å¸ƒè°ƒæ•´ / Regionåˆ†è£‚ / è£‚å˜ååˆ†é…ä»¥åŠè¿ç§»).
> 
> HMasteré‡‡ç”¨ä¸»å¤‡æ¨¡å¼éƒ¨ç½²,é›†ç¾¤å¯åŠ¨æ—¶,é€šè¿‡ç«äº‰è·å¾—ä¸»HMasterè§’è‰²,ä¸»HMasteråªèƒ½æœ‰ä¸€ä¸ª,å¤‡HMasterè¿›ç¨‹åœ¨é›†ç¾¤è¿è¡ŒæœŸé—´å¤„äºä¼‘çœ çŠ¶æ€,ä¸å¹²æ¶‰ä»»ä½•é›†ç¾¤äº‹åŠ¡,å½“ä¸»HMasteræ•…éšœæ—¶,å¤‡ç”¨HMasterå°†å–ä»£ä¸»ç”¨HMasterå¯¹å¤–æä¾›æœåŠ¡.
- `HMaster åŠŸèƒ½`
- ç›‘æ§ RegionServer
- å¤„ç† RegionServeræ•…éšœè½¬ç§»
- ç»´æŠ¤é›†ç¾¤è´Ÿè½½å‡è¡¡
- ç»´æŠ¤é›†ç¾¤å…ƒæ•°æ®ä¿¡æ¯
- å¤„ç†Regionåˆ†é…æˆ–ç§»é™¤
- é€šè¿‡Zookeeperå‘å¸ƒè‡ªèº«ä½ç½®ç»™å®¢æˆ·ç«¯

#### 1.5.2 HRegionServer
> HRegionServeræ˜¯HBaseä»èŠ‚ç‚¹,HRegionServerè´Ÿè´£æä¾›è¡¨æ•°æ®è¯»å†™æœåŠ¡,æ˜¯æ•°æ®å­˜å‚¨å’Œå’Œè®¡ç®—å•å…ƒ,HRegionServerä¸HDFSé›†ç¾¤çš„DataNodeéƒ¨ç½²åœ¨ä¸€èµ·,å®ç°æ•°æ®å­˜å‚¨åŠŸèƒ½.
- `HRegionServer åŠŸèƒ½`
- å¤„ç†å®¢æˆ·ç«¯è¯»å†™è¯·æ±‚
- è´Ÿè´£å­˜å‚¨HBaseå®é™…æ•°æ®
- å¤„ç†åˆ†é…ç»™Region
- åˆ·æ–°ç¼“å­˜åˆ°HDFS
- ç»´æŠ¤HLog
- æ‰§è¡Œå‹ç¼©

#### 1.5.3 HDFS
> HDFSä¸ºHBaseæä¾›æœ€ç»ˆåº•å±‚æ•°æ®å­˜å‚¨æœåŠ¡,åŒæ—¶ä¸ºHBaseæä¾›é«˜å¯ç”¨.
> HDFSæä¾›å…ƒæ•°æ®å’Œè¡¨æ•°æ®çš„åº•å±‚åˆ†å¸ƒå¼å­˜å‚¨æœåŠ¡.
> HDFSæ•°æ®å‰¯æœ¬,ä¿è¯é«˜å¯é æ€§å’Œé«˜å¯ç”¨æ€§.

#### 1.5.4 Zookeeper
> Zookeeperä¸ºHBaseé›†ç¾¤å„è¿›ç¨‹æä¾›åˆ†å¸ƒå¼åä½œæœåŠ¡,å„HRegionServerå°†è‡ªèº«ä¿¡æ¯æ³¨å†Œåˆ°Zookeeperä¸­,HMasteræ®æ­¤æ„ŸçŸ¥å„ä¸ªHRegionServerå¥åº·çŠ¶æ€.
> 
> HBaseé€šè¿‡Zookeeperæ¥ä½œä¸ºmasteré«˜å¯ç”¨ / HRegionServerç›‘æ§ / å…ƒæ•°æ®å…¥å£ / é›†ç¾¤é…ç½®ç»´æŠ¤ç­‰.
> 
> é€šè¿‡Zookeeperæ¥ä¿è¯é›†ç¾¤ä¸­åªæœ‰1ä¸ªmasterè¿è¡Œ,å¦‚masterå¼‚å¸¸,ä¼šé€šè¿‡ç«äº‰æœºåˆ¶äº§ç”Ÿæ–°çš„masterå¯¹å¤–æä¾›æœåŠ¡.
> 
> é€šè¿‡Zookeeperæ¥ç›‘æ§HRegionServerçŠ¶æ€,å½“HRegionServeræœ‰å¼‚å¸¸æ—¶,é€šè¿‡å›è°ƒå½¢å¼é€šçŸ¥MasterHRegionServerä¸Šä¸‹çº¿ä¿¡æ¯.
> 
> é€šè¿‡Zookeeperå­˜å‚¨å…ƒæ•°æ®ä¿¡æ¯ç»Ÿä¸€å…¥å£åœ°å€.

#### 1.5.5 Client
> ClientåŒ…æ‹¬è®¿é—®HBaseæ¥å£,å¦å¤–Clientè¿˜ç»´æŠ¤å¯¹åº”çš„Cacheæ¥åŠ é€ŸHBaseè®¿é—®.

#### 1.5.6 å…¶ä»–ç»„ä»¶
##### 1.5.6.1 Write-Ahead logs
> HBaseä¿®æ”¹è®°å½•,å½“å¯¹HBaseè¯»å†™æ•°æ®æ—¶,æ•°æ®ä¸æ˜¯ç›´æ¥å†™è¿›ç£ç›˜,å®ƒä¼šåœ¨å†…å­˜ä¸­ä¿ç•™ä¸€æ®µæ—¶é—´(æ—¶é—´ä»¥åŠæ•°æ®é‡é˜ˆå€¼å¯ä»¥è®¾å®š),ä½†æŠŠæ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­å¯èƒ½æœ‰æ›´é«˜çš„æ¦‚ç‡å¼•èµ·æ•°æ®ä¸¢å¤±,ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜,æ•°æ®ä¼šå…ˆå†™åœ¨ä¸€ä¸ªå«åšWrite-Ahead logfileæ–‡ä»¶ä¸­,ç„¶åå†å†™å…¥å†…å­˜ä¸­,æ‰€ä»¥åœ¨ç³»ç»Ÿå‡ºç°æ•…éšœæ—¶,æ•°æ®å¯ä»¥é€šè¿‡è¿™ä¸ªæ—¥å¿—æ–‡ä»¶é‡å»º.
##### 1.5.6.2 HFile
> åœ¨ç£ç›˜ä¸Šä¿å­˜åŸå§‹æ•°æ®å®é™…çš„ç‰©ç†æ–‡ä»¶,æ˜¯å®é™…å­˜å‚¨æ–‡ä»¶.
##### 1.5.6.3 Store
> HFileå­˜å‚¨åœ¨Storeä¸­,ä¸€ä¸ªStoreå¯¹åº”HBaseè¡¨ä¸­ä¸€ä¸ªåˆ—æ—.
##### 1.5.6.4 MemStore
> å†…å­˜å­˜å‚¨ä½äºå†…å­˜ä¸­,ç”¨æ¥ä¿å­˜å½“å‰æ•°æ®æ“ä½œ,æ‰€ä»¥å½“æ•°æ®ä¿å­˜åœ¨WALä¸­ä¹‹å,RegsionServerä¼šåœ¨å†…å­˜ä¸­å­˜å‚¨é”®å€¼å¯¹.
##### 1.5.6.5 Region
> Hbaseè¡¨åˆ†ç‰‡,HBaseè¡¨ä¼šæ ¹æ®RowKeyå€¼è¢«åˆ‡åˆ†æˆä¸åŒRegionå­˜å‚¨åœ¨RegionServerä¸­,åœ¨ä¸€ä¸ªRegionServerä¸­å¯ä»¥æœ‰å¤šä¸ªä¸åŒçš„Region.

## 2. HBase éƒ¨ç½²
### 2.1 Zookeeper æœåŠ¡
- ä¿è¯Zookeeperé›†ç¾¤æœåŠ¡æ­£å¸¸è¿è¡Œ
```
[root@systemhub511 zookeeper]$ bin/zkServer.sh start
```
```
[root@systemhub611 zookeeper]$ bin/zkServer.sh start
```
```
[root@systemhub711 zookeeper]$ bin/zkServer.sh start
```
### 2.2 Hadoop æœåŠ¡
- ä¿è¯Hadoopé›†ç¾¤æœåŠ¡æ­£å¸¸è¿è¡Œ
```
[root@systemhub511 hadoop]$ sbin/start-dfs.sh
```
```
[root@systemhub611 hadoop]$ sbin/start-yarn.sh
```
### 2.3 HBase æœåŠ¡
- è§£å‹HBaseåˆ°æŒ‡å®šç›®å½•
```
[root@systemhub511 software]# tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module/
```
- é‡å‘½ååŒ…åç§°
```
[root@systemhub511 module]# mv hbase-1.3.1 hbase
```
- æŸ¥çœ‹HBaseç›®å½•ç»“æ„
```
[root@systemhub511 hbase]# ll
total 348
drwxr-xr-x.  4 root root   4096 Apr  5  2017 bin
-rw-r--r--.  1 root root 148959 Apr  7  2017 CHANGES.txt
drwxr-xr-x.  2 root root   4096 Apr  5  2017 conf
drwxr-xr-x. 12 root root   4096 Apr  7  2017 docs
drwxr-xr-x.  7 root root   4096 Apr  7  2017 hbase-webapps
-rw-r--r--.  1 root root    261 Apr  7  2017 LEGAL
drwxr-xr-x.  3 root root   4096 Apr 26 14:43 lib
-rw-r--r--.  1 root root 130696 Apr  7  2017 LICENSE.txt
-rw-r--r--.  1 root root  43258 Apr  7  2017 NOTICE.txt
-rw-r--r--.  1 root root   1477 Sep 21  2016 README.txt
[root@systemhub511 hbase]# 
```
- é…ç½®HBase
- vim `hbase-env.sh`é…ç½®æ–‡ä»¶
```
[root@systemhub511 hbase]# echo $JAVA_HOME
/opt/module/jdk1.8.0_162
[root@systemhub511 hbase]# cd conf/
[root@systemhub511 conf]# vim hbase-env.sh
```
```
# The java implementation to use.  Java 1.7+ required.
export JAVA_HOME=/opt/module/jdk1.8.0_162
export HBASE_MANAGES_ZK=false
```
- vim `hbase-site.xml`é…ç½®æ–‡ä»¶
```
<configuration>
  <property>
   <name>hbase.rootdir</name>   
   <value>hdfs://systemhub511:9000/hbase</value>     
  </property>
  <property>             
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>
  <!--0.98åçš„æ–°å˜åŠ¨,ä¹‹å‰ç‰ˆæœ¬æ²¡æœ‰.port,é»˜è®¤ç«¯å£ä¸º60000 -->
  <property>
    <name>hbase.master.port</name>
    <value>16000</value>
  </property>          
  <property>            
    <name>hbase.zookeeper.quorum</name>
    <value>systemhub511:2181,systemhub611:2181,systemhub711:2181</value>
  </property>
  <property>                        
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/opt/module/zookeeper/zkData</value>
  </property>
</configuration>
```

- vim `regionservers`é…ç½®æ–‡ä»¶
```
systemhub511
systemhub611
systemhub711
```
- åœ¨/usr/local/binç›®å½•ä¸‹åˆ›å»ºè„šæœ¬
- å¯åŠ¨æ‰€æœ‰é›†ç¾¤èŠ‚ç‚¹
- vim `start-cluster.sh`
```
#!/bin/bash
echo "================          Start All Node Services         ==========="
echo "================================================================"
echo "================          Starting Zookeeper              ==========="
echo "================================================================"

for i in root@systemhub511 root@systemhub611 root@systemhub711
do
    ssh $i 'source /etc/profile;/opt/module/zookeeper/bin/zkServer.sh start'
done

echo "================          Starting HDFS           ==========="
ssh root@systemhub511 '/opt/module/hadoop/sbin/start-dfs.sh'

echo "================          Starting YARN           ==========="
ssh root@systemhub611 '/opt/module/hadoop/sbin/start-yarn.sh'

echo "================          Starting JobHistoryServer       ==========="
ssh root@systemhub511 '/opt/module/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver'
```
- å…³é—­æ‰€æœ‰é›†ç¾¤èŠ‚ç‚¹
- vim `stop-cluster.sh`
```
#!/bin/bash
echo "================          Stopping All Node Services      ==========="
echo "================          Stopping JobHistoryServer       ==========="
ssh root@systemhub511 '/opt/module/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver'

echo "================          Stopping YARN           ==========="
ssh root@systemhub611 '/opt/module/hadoop/sbin/stop-yarn.sh'

echo "================          Stopping HDFS           ==========="
ssh root@systemhub511 '/opt/module/hadoop/sbin/stop-dfs.sh'

echo "================          Stopping Zookeeper      ==========="
for i in root@systemhub511 root@systemhub611 root@systemhub711
do
    ssh $i 'source /etc/profile;/opt/module/zookeeper/bin/zkServer.sh stop'
done
```
- æŸ¥çœ‹æ‰€æœ‰é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
- vim `jps.sh`
```
 #!/bin/bash
for host in root@systemhub511 root@systemhub611 root@systemhub711
do
    echo "================      $host All Processes             ==========="
    ssh $host '/opt/module/jdk1.8.0_162/bin/jps'
done
```
- HBaseè½¯è¿æ¥Hadoopé…ç½®
```
[root@systemhub511 ~]# ln -s /opt/module/hadoop/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml
[root@systemhub511 ~]#
[root@systemhub511 ~]# ln -s /opt/module/hadoop/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml
[root@systemhub511 ~]#
```
- HBaseè¿œç¨‹åˆ†å‘å…¶ä»–é›†ç¾¤
```
[root@systemhub511 ~]# scp -r /opt/module/hbase/ systemhub611:/opt/module/hbase/
[root@systemhub511 ~]# scp -r /opt/module/hbase/ systemhub711:/opt/module/hbase/
```
- å¯åŠ¨HBaseæœåŠ¡
- æ–¹å¼ä¸€
```
[root@systemhub511 hbase]# bin/hbase-daemon.sh start master
[root@systemhub511 hbase]# bin/hbase-daemon.sh start regionserver
```
- æ–¹å¼äºŒ
```
[root@systemhub511 hbase]# bin/start-hbase.sh
starting master, logging to /opt/module/hbase/bin/../logs/hbase-root-master-systemhub511.out
systemhub711: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub711.out
systemhub611: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub611.out
systemhub511: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub511.out
```
- å¦‚æœé›†ç¾¤ä¹‹é—´çš„èŠ‚ç‚¹æ—¶é—´ä¸åŒæ­¥,ä¼šå¯¼è‡´regionserveræ— æ³•å¯åŠ¨,æŠ›å‡º`ClockOutOfSyncException`å¼‚å¸¸
- æ–¹å¼ä¸€:ä¿®æ”¹é›†ç¾¤åŒæ­¥æ—¶é—´æœåŠ¡ - [å¿«é€Ÿå›é¡¾é€šé“](https://geekparkhub.github.io/technical_guide/programing_language/hadoop/hadoop.html#é›†ç¾¤æ—¶é—´åŒæ­¥)
- æ–¹å¼äºŒ:åœ¨é…ç½®æ–‡ä»¶ä¸­è¿½åŠ  `hbase.master.maxclockskew`å±æ€§å€¼ä¸ºæœ€å¤§å€¼å³å¯
```
<property>
  <name>hbase.master.maxclockskew</name>
  <value>180000</value>
  <description>Time difference of regionserver from master</description>
</property>
```
- å¯åŠ¨æœåŠ¡å,æŸ¥çœ‹è¿è¡Œç»“æœ
- å¯ä»¥é€šè¿‡`host:port`æ–¹å¼è®¿é—®HBaseç®¡ç†é¡µ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_002.jpg)

## 3. HBase Shell
### 3.1 åŸºæœ¬æ“ä½œ
#### 3.1.1 è¿›å…¥HBaseå®¢æˆ·ç«¯å‘½ä»¤è¡Œçª—å£
- `bin/hbase shell`
``` powershell
[root@systemhub511 hbase]# bin/hbase shell
hbase(main):001:0> 
```
#### 3.1.2 æŸ¥çœ‹å¸®åŠ©å‘½ä»¤
- `help`
```
hbase(main):001:0> help
HBase Shell, version 1.3.1, r930b9a55528fe45d8edce7af42fef2d35e77677a, Thu Apr  6 19:36:54 PDT 2017
Type 'help "COMMAND"', (e.g. 'help "get"' -- the quotes are necessary) for help on a specific command.
Commands are grouped. Type 'help "COMMAND_GROUP"', (e.g. 'help "general"') for help on a command group.

COMMAND GROUPS:
  Group name: general
  Commands: status, table_help, version, whoami

  Group name: ddl
  Commands: alter, alter_async, alter_status, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, locate_region, show_filters

The HBase shell is the (J)Ruby IRB with the above HBase-specific commands added.
For more on the HBase Shell, see http://hbase.apache.org/book.html
hbase(main):002:0> 
```
#### 3.1.3 æŸ¥çœ‹å½“å‰æ•°æ®åº“ä¸­æœ‰å“ªäº›è¡¨
- `list`
```
hbase(main):002:0> list
TABLE        
0 row(s) in 0.4020 seconds
=> []
hbase(main):003:0>
```

### 3.2 è¡¨æ“ä½œ
#### 3.2.1 åˆ›å»ºè¡¨
- `create 'è¡¨å','åˆ—æ—å'`
```
hbase(main):003:0> create 'test','info'
0 row(s) in 2.8000 seconds

=> Hbase::Table - test
hbase(main):004:0>
```
#### 3.2.2 æ’å…¥æ•°æ®åˆ°è¡¨
- `put 'è¡¨å','ä¸»é”®ID','åˆ—æ—å:åˆ—åç§°','value'`
```
hbase(main):004:0> put 'test','0001','info:name','testuser001'
0 row(s) in 0.3590 seconds

hbase(main):005:0>
```
#### 3.2.3 æ‰«ææŸ¥çœ‹è¡¨æ•°æ®
- `scan 'è¡¨å'`
```
hbase(main):008:0> scan 'test'
ROW                        COLUMN+CELL                                                                 
 0001                      column=info:age, timestamp=1556459945452, value=60                          
 0001                      column=info:name, timestamp=1556459442613, value=testuser001                
 0002                      column=info:age, timestamp=1556459962770, value=70                          
 0002                      column=info:name, timestamp=1556459836540, value=testuser002                
2 row(s) in 0.0450 seconds

hbase(main):009:0> 
hbase(main):012:0> scan 'test',{STARTROW => '0001',STOPROW => '0002'}
ROW                        COLUMN+CELL
 0001                      column=info:age, timestamp=1556459945452, value=60                          
 0001                      column=info:name, timestamp=1556459442613, value=testuser001                
1 row(s) in 0.0310 seconds

hbase(main):013:0> 
```
#### 3.2.4 æŸ¥çœ‹ æŒ‡å®šè¡Œæˆ–æŒ‡å®šåˆ—æ—:åˆ—æ•°æ®
- `get 'è¡¨å','ä¸»é”®ID'`
```
hbase(main):009:0> get 'test','0001'
COLUMN                     CELL         
 info:age                  timestamp=1556459945452, value=60   
 info:name                 timestamp=1556459442613, value=testuser001
1 row(s) in 0.0460 seconds
hbase(main):010:0> get 'test','0001','info:name'
COLUMN                     CELL                                                                        
 info:name                 timestamp=1556459442613, value=testuser001
1 row(s) in 0.0260 seconds
hbase(main):011:0> 
```
#### 3.2.5 æŸ¥çœ‹è¡¨ç»“æ„
```
hbase(main):011:0> describe 'test'
Table test is ENABLED
test                                                                                                   
COLUMN FAMILIES DESCRIPTION
{NAME => 'info', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FA
LSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOC
KCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                      
1 row(s) in 0.1010 seconds
hbase(main):012:0> 
```
#### 3.2.6 å˜æ›´è¡¨ä¿¡æ¯
- å°†infoåˆ—æ—ä¸­çš„æ•°æ®å­˜æ”¾3ä¸ªç‰ˆæœ¬
```
hbase(main):014:0> alter 'test',{NAME=>'info',VERSIONS=>3}
Updating all regions with the new schema...
0/1 regions updated.
1/1 regions updated.
Done.
0 row(s) in 3.0960 seconds
hbase(main):015:0> 
hbase(main):015:0> get 'test','0001',{COLUMN=>'info:name',VERSIONS=>3}
COLUMN                     CELL
 info:name                 timestamp=1556459442613, value=testuser001                                  
1 row(s) in 0.0260 seconds
hbase(main):016:0>
```
#### 3.2.7 æ›´æ–°æŒ‡å®šå­—æ®µæ•°æ®
```
hbase(main):004:0> put 'test','0001','info:name','testuser003'
0 row(s) in 0.3590 seconds

hbase(main):005:0>
```
#### 3.2.8 ç»Ÿè®¡è¡¨æ•°æ®è¡Œæ•°
```
hbase(main):016:0> count 'test'
2 row(s) in 0.1140 seconds
=> 2
hbase(main):017:0> 
```
#### 3.2.9 åˆ é™¤æ•°æ®
- åˆ é™¤æŸrowkeyæŸä¸€åˆ—æ•°æ®
```
hbase(main):017:0> delete 'test','0002','info:age'
```
- åˆ é™¤æŸrowkeyå…¨éƒ¨æ•°æ®
```
hbase(main):016:0> deleteall 'test','0001'
```
#### 3.2.10 æ¸…ç©ºè¡¨æ•°æ®
- æ¸…ç©ºè¡¨æ“ä½œé¡ºåºä¸ºå…ˆdisable,ç„¶åå†truncate
```
hbase(main):018:0> truncate 'test'
```
#### 3.2.11 åˆ é™¤è¡¨
- éœ€è¦å…ˆè®©è¯¥è¡¨ä¸ºdisableçŠ¶æ€,ç„¶åæ‰èƒ½åˆ é™¤è¡¨
- å¦‚æœç›´æ¥dropè¡¨,ä¼šæŠ›å‡ºå¼‚å¸¸ï¼š`Drop the named table. Table must first be disabledERROR: Table test is enabled. Disable it first.`
```
hbase(main):019:0> disable 'test'
```
```
hbase(main):020:0> drop 'test'
```

### 3.3 å¸¸ç”¨Shellæ“ä½œ
#### 3.3.1 satus
- æ˜¾ç¤ºæœåŠ¡å™¨çŠ¶æ€
```
hbase> status 'test'
```

#### 3.3.2 whoami
- æ˜¾ç¤ºHBaseå½“å‰ç”¨æˆ·
```
hbase> whoami
```

#### 3.3.3 list
- æ˜¾ç¤ºå½“å‰æ‰€æœ‰æ•°æ®è¡¨
```
hbase> list
```

#### 3.3.4 count
- ç»Ÿè®¡æŒ‡å®šæ•°æ®è¡¨è®°å½•æ•°
```
hbase> count 'hbase_book'
```

#### 3.3.5 describe
- å±•ç¤ºæ•°æ®è¡¨ç»“æ„ä¿¡æ¯
```
hbase> describe 'hbase_book'
```

#### 3.3.6 exist
- æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨,é€‚ç”¨äºæ•°æ®è¡¨é‡ç‰¹åˆ«å¤šçš„æƒ…å†µ
```
hbase> exist 'hbase_book'
```

#### 3.3.7  is_enabled / is_disabled
- æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦å¯ç”¨æˆ–ç¦ç”¨
```
hbase> is_enabled 'hbase_book'
hbase> is_disabled 'hbase_book'
```

#### 3.3.8 alter
- è¯¥å‘½ä»¤å¯ä»¥æ”¹å˜è¡¨å’Œåˆ—æ—æ¨¡å¼
- ä¸ºå½“å‰è¡¨å¢åŠ åˆ—æ—
```
hbase> alter 'hbase_book',NAME => 'CF2',VERSIONS => 2
```
- ä¸ºå½“å‰è¡¨åˆ é™¤åˆ—æ—
```
hbase> alter 'hbase_book','delete' => 'CF2'
```

#### 3.3.9 disable
- ç¦ç”¨æ•°æ®è¡¨
```
hbase> disable 'hbase_book'
```

#### 3.3.10 drop
- åˆ é™¤æ•°æ®è¡¨,åœ¨åˆ é™¤è¡¨ä¹‹å‰å¿…é¡»å…ˆç¦ç”¨æ•°æ®è¡¨
```
hbase> disable 'hbase_book'
```

#### 3.3.11 delete
- åˆ é™¤ä¸€è¡Œä¸­ä¸€ä¸ªå•å…ƒæ ¼å€¼
```
hbase> delete 'hbase_book','rowKey','CF:C'
```

#### 3.3.12 truncate
- æ¸…ç©ºè¡¨æ•°æ®,å³ç¦ç”¨è¡¨-åˆ é™¤è¡¨-åˆ›å»ºè¡¨
```
hbase> truncate 'hbase_book'
```

#### 3.3.13 create
- åˆ›å»ºæ•°æ®è¡¨
```
hbase> create 'table','cf'
```
- åˆ›å»ºå¤šä¸ªåˆ—æ—
```
hbase> create 't1',{NAME => 'f1'},{NAME => 'f2'},{NAME => 'f3'}
```


## 4. HBase æ•°æ®ç»“æ„
### 4.1 Row Key
> ä¸nosqlæ•°æ®åº“ä»¬ä¸€æ ·,`row key`æ˜¯ç”¨æ¥æ£€ç´¢è®°å½•ä¸»é”®,è®¿é—®HBASEè¡¨ä¸­çš„è¡Œ,åªæœ‰ä¸‰ç§æ–¹å¼ :
- 1.é€šè¿‡å•ä¸ª`row key`è®¿é—®
- 2.é€šè¿‡`row key`çš„range(æ­£åˆ™)
- 3.å…¨è¡¨æ‰«æ
> `Row key`è¡Œé”®(Row key)å¯ä»¥æ˜¯ä»»æ„å­—ç¬¦ä¸²(æœ€å¤§é•¿åº¦æ˜¯`64KB`,å®é™…åº”ç”¨ä¸­é•¿åº¦ä¸€èˆ¬ä¸º10-100bytes),åœ¨HBASEå†…éƒ¨,`row key`ä¿å­˜ä¸ºå­—èŠ‚æ•°ç»„, å­˜å‚¨æ—¶æ•°æ®æŒ‰ç…§`Row key`çš„å­—å…¸åº(byte order)æ’åºå­˜å‚¨,è®¾è®¡keyæ—¶,è¦å……åˆ†æ’åºå­˜å‚¨è¿™ä¸ªç‰¹æ€§,å°†ç»å¸¸ä¸€èµ·è¯»å–çš„è¡Œå­˜å‚¨æ”¾åˆ°ä¸€èµ·(ä½ç½®ç›¸å…³æ€§).

### 4.2 Columns Family
> åˆ—æ— : HBASEè¡¨ä¸­æ¯ä¸ªåˆ—éƒ½å½’å±äºæŸä¸ªåˆ—æ—,åˆ—æ—æ˜¯è¡¨çš„schemaçš„ä¸€éƒ¨åˆ†(è€Œåˆ—ä¸æ˜¯),å¿…é¡»åœ¨ä½¿ç”¨è¡¨ä¹‹å‰å®šä¹‰,åˆ—åéƒ½ä»¥åˆ—æ—ä½œä¸ºå‰ç¼€,ä¾‹å¦‚`courses:history,courses:math`éƒ½å±äº`courses`è¿™ä¸ªåˆ—æ—.

### 4.3 Cell
> ç”±`{row key, columnFamily, version}` å”¯ä¸€ç¡®å®šå•å…ƒ,cellä¸­æ•°æ®æ˜¯æ²¡æœ‰ç±»å‹çš„,å…¨éƒ¨æ˜¯å­—èŠ‚ç å½¢å¼å­˜è´®,å…³é”®å­— : æ— ç±»å‹/å­—èŠ‚ç .

### 4.4 Time Stamp
> HBASEä¸­é€šè¿‡`rowkey`å’Œ`columns`ç¡®å®šä¸ºä¸€ä¸ªå­˜è´®å•å…ƒç§°ä¸ºcell,æ¯ä¸ªcelléƒ½ä¿å­˜ç€åŒä¸€ä»½æ•°æ®çš„å¤šä¸ªç‰ˆæœ¬,ç‰ˆæœ¬é€šè¿‡æ—¶é—´æˆ³æ¥ç´¢å¼•,`æ—¶é—´æˆ³ç±»å‹`æ˜¯`64ä½æ•´å‹`,æ—¶é—´æˆ³å¯ä»¥ç”±HBASE(åœ¨æ•°æ®å†™å…¥æ—¶è‡ªåŠ¨)èµ‹å€¼,æ­¤æ—¶æ—¶é—´æˆ³æ˜¯ç²¾ç¡®åˆ°æ¯«ç§’çš„å½“å‰ç³»ç»Ÿæ—¶é—´,æ—¶é—´æˆ³ä¹Ÿå¯ä»¥ç”±å¼€å‘è€…æ˜¾å¼èµ‹å€¼,å¦‚æœåº”ç”¨ç¨‹åºè¦é¿å…æ•°æ®ç‰ˆæœ¬å†²çª,å°±å¿…é¡»è‡ªå·±ç”Ÿæˆå…·æœ‰å”¯ä¸€æ€§çš„æ—¶é—´æˆ³,æ¯ä¸ªcellä¸­,ä¸åŒç‰ˆæœ¬çš„æ•°æ®æŒ‰ç…§æ—¶é—´å€’åºæ’åº,å³æœ€æ–°çš„æ•°æ®æ’åœ¨æœ€å‰é¢.
> 
> ä¸ºäº†é¿å…æ•°æ®å­˜åœ¨è¿‡å¤šç‰ˆæœ¬é€ æˆç®¡ç†(åŒ…æ‹¬å­˜è´®å’Œç´¢å¼•)è´Ÿæ‹…,HBASEæä¾›äº†ä¸¤ç§æ•°æ®ç‰ˆæœ¬å›æ”¶æ–¹å¼.
> ä¸€æ˜¯ä¿å­˜æ•°æ®çš„æœ€ånä¸ªç‰ˆæœ¬.
> äºŒæ˜¯ä¿å­˜æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„ç‰ˆæœ¬(æ¯”å¦‚æœ€è¿‘ä¸ƒå¤©),ç”¨æˆ·å¯ä»¥é’ˆå¯¹æ¯ä¸ªåˆ—æ—è¿›è¡Œè®¾ç½®.

### 4.5 å‘½åç©ºé—´
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_003.jpg)

> 1.`Table` : è¡¨ , æ‰€ä»¥çš„è¡¨éƒ½æ˜¯å‘½åç©ºé—´çš„æˆå‘˜,æ—¢è¡¨å¿…å±äºæŸä¸ªå‘½åç©ºé—´,å¦‚æœæ²¡æœ‰æŒ‡å®š,åˆ™åœ¨defaulté»˜è®¤å‘½åç©ºé—´ä¸­.
> 
> 2.`RegionServerGroup` : ä¸€ä¸ªå‘½åç©ºé—´åŒ…å«äº†é»˜è®¤çš„RegionServerGroup.
> 
> 3.`Permission` : æƒé™ , å‘½åç©ºé—´èƒ½å¤Ÿè®©å¼€å‘è€…æ¥å®šä¹‰è®¿é—®æ§åˆ¶è¡¨ACL(Access Control List). ä¾‹å¦‚ : åˆ›å»ºè¡¨ / è¯»å–è¡¨ / åˆ é™¤ / æ›´æ–°ç­‰æ“ä½œ. 
> 
> 4.`Quota` : é™é¢ , å¯ä»¥å¼ºåˆ¶ä¸€ä¸ªå‘½åç©ºé—´å¯åŒ…å«çš„regionæ•°æ®.


## 5. HBase åŸç†

### 5.1 HBase è¯»æ•°æ®æµç¨‹

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_005.jpg)
- 1.Clientå…ˆè®¿é—®Zookeeper,ä»metaè¡¨è¯»å–regionä½ç½®,ç„¶åè¯»å–metaè¡¨ä¸­æ•°æ®,metaä¸­åˆå­˜å‚¨äº†ç”¨æˆ·è¡¨çš„regionä¿¡æ¯.
- 2.æ ¹æ®namespaceã€è¡¨åå’Œrowkeyåœ¨metaè¡¨ä¸­æ‰¾åˆ°å¯¹åº”çš„regionä¿¡æ¯.
- 3.æ‰¾åˆ°regionå¯¹åº”çš„RegionServer.
- 4.æŸ¥æ‰¾å¯¹åº”çš„region.
- 5.å…ˆä»MemStoreæ‰¾æ•°æ®,å¦‚æœæ²¡æœ‰,å†åˆ°StoreFileä¸Šè¯»(ä¸ºäº†è¯»å–æ•ˆç‡).
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_008.jpg)

- æŸ¥çœ‹HBaseå…ƒæ•°æ®å†…å®¹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_004.jpg)
- æŸ¥çœ‹Zookeeperå­˜å‚¨æ•°æ®å†…å®¹
```
[zk: localhost:2181(CONNECTED) 0] ls /
[cluster, controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, latest_producer_id_block, config, hbase]
```
- æŸ¥çœ‹Zookeeperå­˜å‚¨HBaseæ•°æ®å†…å®¹
```
[zk: localhost:2181(CONNECTED) 1] ls /hbase
[replication, meta-region-server, rs, splitWAL, backup-masters, table-lock, flush-table-proc, region-in-transition, online-snapshot, switch, master, running, recovering-regions, draining, namespace, hbaseid, table]
[zk: localhost:2181(CONNECTED) 2] 
```
- æŸ¥çœ‹Zookeeperå­˜å‚¨HBase meta-region-server
```
[zk: localhost:2181(CONNECTED) 4] get /hbase/meta-region-server
ï¿½regionserver:16020eï¿½ï¿½;0Zï¿½PBUF
â½â‰¤â½â”œâŠâ””â¤â”¤â‰711ï¿½Â£ï¿½ï¿½ï¿½ï¿½ï¿½-
âŒZâ”‚â‹â = 0â”‚1âŠ00000051
âŒâ”œâ‹â””âŠ = Mâºâ”¼ Aâ»â¼ 29 01:37:37 CST 2019
â””Zâ”‚â‹â = 0â”‚1âŠ00000051
â””â”œâ‹â””âŠ = Mâºâ”¼ Aâ»â¼ 29 01:37:37 CST 2019
â»Zâ”‚â‹â = 0â”‚1âŠ00000051
âŒâ”´âŠâ¼â½â‹âºâ”¼ = 0
ââ–’â”œâ–’VâŠâ¼â½â‹âºâ”¼ = 0
â–’âŒâ”ŒVâŠâ¼â½â‹âºâ”¼ = 0
âŠâ»â¤âŠâ””âŠâ¼â–’â”ŒOâ”¬â”¼âŠâ¼ = 0â”‚0
ââ–’â”œâ–’LâŠâ”¼Â±â”œâ¤ = 65
â”¼â”¤â””Câ¤â‹â”Œââ¼âŠâ”¼ = 0
[â‰¥â”: â”ŒâºâŒâ–’â”Œâ¤âºâ½â”œ:2181(CONNECTED) 5] 
```
- æŸ¥çœ‹hbase:metaå…ƒæ•°æ®ä¿¡æ¯è¡¨
```
hbase(main):002:0> scan 'hbase:meta'
ROW                        COLUMN+CELL                                                                 
 hbase:namespace,,15562836 column=info:regioninfo, timestamp=1556473360414, value={ENCODED => d89184e0b
 52968.d89184e0b0e8bf9782b 0e8bf9782b86edf991f56d2, NAME => 'hbase:namespace,,1556283652968.d89184e0b0e
 86edf991f56d2.            8bf9782b86edf991f56d2.', STARTKEY => '', ENDKEY => ''}                      
 hbase:namespace,,15562836 column=info:seqnumDuringOpen, timestamp=1556473360414, value=\x00\x00\x00\x0
 52968.d89184e0b0e8bf9782b 0\x00\x00\x00\x14                                                           
 86edf991f56d2.                                                                                        
 hbase:namespace,,15562836 column=info:server, timestamp=1556473360414, value=systemhub511:16020       
 52968.d89184e0b0e8bf9782b                                                                             
 86edf991f56d2.                                                                                        
 hbase:namespace,,15562836 column=info:serverstartcode, timestamp=1556473360414, value=1556473045363   
 52968.d89184e0b0e8bf9782b                                                                             
 86edf991f56d2.                                                                                        
 test,,1556459101215.6b00f column=info:regioninfo, timestamp=1556473359938, value={ENCODED => 6b00fc62a
 c62ac627c675022a7fafb6fbf c627c675022a7fafb6fbfa0, NAME => 'test,,1556459101215.6b00fc62ac627c675022a7
 a0.                       fafb6fbfa0.', STARTKEY => '', ENDKEY => ''}                                 
 test,,1556459101215.6b00f column=info:seqnumDuringOpen, timestamp=1556473359938, value=\x00\x00\x00\x0
 c62ac627c675022a7fafb6fbf 0\x00\x00\x00\x13                                                           
 a0.                                                                                                   
 test,,1556459101215.6b00f column=info:server, timestamp=1556473359938, value=systemhub611:16020       
 c62ac627c675022a7fafb6fbf                                                                             
 a0.                                                                                                   
 test,,1556459101215.6b00f column=info:serverstartcode, timestamp=1556473359938, value=1556473042292   
 c62ac627c675022a7fafb6fbf                                                                             
 a0.                                                                                                   
2 row(s) in 0.0530 seconds

hbase(main):003:0> 
```

### 5.2 HBase å†™æ•°æ®æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_007.jpg)
- 1.Clientå‘HregionServerå‘é€å†™è¯·æ±‚
- 2.HregionServerå°†æ•°æ®å†™åˆ°HLog(write ahead log),ä¸ºäº†æ•°æ®æŒä¹…åŒ–å’Œæ¢å¤.
- 3.HregionServerå°†æ•°æ®å†™åˆ°å†…å­˜(MemStore)
- 4.åé¦ˆClientå†™å…¥æˆåŠŸ.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hbase/start_006.jpg)

### 5.3 HBase æ•°æ®Flushè¿‡ç¨‹
- 1.å½“MemStoreæ•°æ®è¾¾åˆ°é˜ˆå€¼(é»˜è®¤æ˜¯128M,è€ç‰ˆæœ¬æ˜¯64M),å°†æ•°æ®åˆ·åˆ°ç¡¬ç›˜,å°†å†…å­˜ä¸­æ•°æ®åˆ é™¤,åŒæ—¶åˆ é™¤HLogä¸­å†å²æ•°æ®.
- 2.å¹¶å°†æ•°æ®å­˜å‚¨åˆ°HDFSä¸­.
- 3.åœ¨HLogä¸­åšæ ‡è®°ç‚¹.

### 5.4 HBase æ•°æ®Compactè¿‡ç¨‹
- 1.å½“æ•°æ®å—è¾¾åˆ°4å—,Hmasterå°†æ•°æ®å—åŠ è½½åˆ°æœ¬åœ°è¿›è¡Œåˆå¹¶.
- 2.å½“åˆå¹¶æ•°æ®è¶…è¿‡256M,è¿›è¡Œæ‹†åˆ†,å°†æ‹†åˆ†åçš„Regionåˆ†é…ç»™ä¸åŒçš„HregionServerç®¡ç†.
- 3.å½“HregionServerå®•æœºå,å°†HregionServerä¸Šçš„hlogæ‹†åˆ†,ç„¶ååˆ†é…ç»™ä¸åŒHregionServeråŠ è½½,ä¿®æ”¹.META
- 4.æ³¨æ„ : HLogä¼šåŒæ­¥åˆ°HDFS.

### 5.5 Hmaster èŒè´£
- 1.ç®¡ç†ç”¨æˆ·å¯¹Tableå¢ã€åˆ ã€æ”¹ã€æŸ¥æ“ä½œ.
- 2.è®°å½•Regionåœ¨å“ªä¸€å°HRegion ServeræœåŠ¡å™¨ä¸Š.
- 3.åœ¨Region Splitå,è´Ÿè´£æ–°Regionåˆ†é….
- 4.æ–°æœºå™¨åŠ å…¥æ—¶,ç®¡ç†HRegion Serverè´Ÿè½½å‡è¡¡,è°ƒæ•´Regionåˆ†å¸ƒ.
- 5.åœ¨HRegion Serverå®•æœºå,è´Ÿè´£å¤±æ•ˆHRegion Serverä¸Šçš„Regionsè¿ç§».

### 5.6 HRegionServer èŒè´£
- 1.HRegion Serverä¸»è¦è´Ÿè´£å“åº”ç”¨æˆ·I/Oè¯·æ±‚.å‘HDFSæ–‡ä»¶ç³»ç»Ÿä¸­è¯»å†™æ•°æ®.æ˜¯HBASEä¸­æœ€æ ¸å¿ƒæ¨¡å—.
- 2.HRegion Serverç®¡ç†äº†å¾ˆå¤štableåˆ†åŒº,ä¹Ÿå°±æ˜¯region.

### 5.7 Client èŒè´£
- 1.HBASE Clientä½¿ç”¨HBASEçš„RPCæœºåˆ¶ä¸HMasterå’ŒRegionServerè¿›è¡Œé€šä¿¡.
- 2.ç®¡ç†ç±»æ“ä½œ : Clientä¸HMasterè¿›è¡ŒRPC.
- 3.æ•°æ®è¯»å†™ç±»æ“ä½œ : Clientä¸HRegionServerè¿›è¡ŒRPC.

## 6. HBase New API
### 6.1 ç¯å¢ƒå‡†å¤‡
- JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
- Add pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.geekparkhub.core.hbase.api</groupId>
    <artifactId>hbase_server</artifactId>
    <version>1.0-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-server</artifactId>
            <version>1.3.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>1.3.1</version>
        </dependency>
        <dependency>
            <groupId>jdk.tools</groupId>
            <artifactId>jdk.tools</artifactId>
            <version>1.8</version>
            <scope>system</scope>
            <systemPath>/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home/lib/tools.jar</systemPath>
        </dependency>
    </dependencies>
</project>
```

### 6.2 åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {
        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {
            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    public static void main(String[] args) throws IOException {

        /**
         * isTableExist Result
         */
        System.out.printf("test isTableExist Result is = " + String.valueOf(isTableExist("test")) + "\n");
        System.out.printf("test001 isTableExist Result is = " + String.valueOf(isTableExist("test001")) + "\n");
        System.out.printf("test002 isTableExist Result is = " + String.valueOf(isTableExist("test002")) + "\n");
        System.out.printf("test003 isTableExist Result is = " + String.valueOf(isTableExist("test003")) + "\n");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
``` prolog
test isTableExist Result is = true
test001 isTableExist Result is = true
test002 isTableExist Result is = false
test003 isTableExist Result is = false
```


### 6.3 åˆ›å»ºæ•°æ®è¡¨
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {
            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Create DataTable
     * åˆ›å»ºæ•°æ®è¡¨
     *
     * @param tableName
     * @param columnFamily
     * @throws IOException
     */
    public static void createTable(String tableName, String... columnFamily) throws IOException {

        /**
         * Verify that the DataTable Exists Before Creating the DataTable
         * åˆ›å»ºæ•°æ®è¡¨å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (isTableExist(tableName)) {
            System.out.printf(tableName + " Data table creation failed , " + tableName + " Data table already exists!" + "\n");
            return;
        }

        /**
         * Instantiation Table Description information
         * å®ä¾‹åŒ– è¡¨æè¿°ä¿¡æ¯
         */
        HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName));

        /**
         * Add Multiple Column Families
         * æ·»åŠ å¤šä¸ªåˆ—æ—
         */
        for (String cn : columnFamily) {
            /**
             * Instantiation Column Name Description information
             * å®ä¾‹åŒ– åˆ—åæè¿°ä¿¡æ¯
             */
            HColumnDescriptor columnDescriptor = new HColumnDescriptor(String.valueOf(cn));
            tableDescriptor.addFamily(columnDescriptor);
        }
        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        admin.createTable(tableDescriptor);

        /**
         * Logger INFO
         */
        System.out.printf(tableName + " Data Sheet Was Created Successfully!" + "\n");
    }

    public static void main(String[] args) throws IOException {

        /**
         * CreateTable Result
         */
        createTable("test001", "info");
        createTable("test_factory", "factorymode", "factoryinfo");
        System.out.printf("factory TableExist Result is = " + String.valueOf(isTableExist("factory")) + "\n");
        System.out.printf("test_factory TableExist Result is = " + String.valueOf(isTableExist("test_factory")) + "\n");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
``` prolog
test001 Data sheet was created successfully!
test_factory Data sheet was created successfully!
test_factory TableExist Result is = true
factory TableExist Result is = false
```


### 6.4 åˆ é™¤æ•°æ®è¡¨
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {
        
            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Delete Data Table
     * åˆ é™¤æ•°æ®è¡¨
     *
     * @param tableName
     * @throws IOException
     */
    public static void deleteTable(String tableName) throws IOException {

        /**
         * Before Deleting the Data Table, Verify that the Data Table Exists.
         */
        if (!isTableExist(tableName)) {
            System.out.println("Data Table Deletion Failed , Reason : The Data Table Does Not Exist !");
            return;
        }

        /**
         * Data Table Offline
         * æ•°æ®è¡¨ä¸‹çº¿
         */
        admin.disableTable(TableName.valueOf(tableName));

        /**
         * Delete Data Table
         * åˆ é™¤æ•°æ®è¡¨
         */
        admin.deleteTable(TableName.valueOf(tableName));

        /**
         * Logger INFO
         */
        System.out.println("Data Table Has Been Deleted Successfully!");
    }

    public static void main(String[] args) throws IOException {

        /**
         * Delete Data Table
         */
        deleteTable("test_factory");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
```
INFO [org.apache.hadoop.hbase.client.HBaseAdmin] - Disabled test_factory
INFO [org.apache.hadoop.hbase.client.HBaseAdmin] - Deleted test_factory
Data Table Has Been Deleted Successfully!
```


### 6.5 æ·»åŠ æ•°æ®
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {
            
            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
    
    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }
    
    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }
    
    /**
     * Adding Data
     * æ·»åŠ æ•°æ®
     *
     * @param tableName
     * @param rowKey
     * @param columnFamily
     * @param columnName
     * @param value
     * @throws IOException
     */
    public static void addData(String tableName, String rowKey, String columnFamily, String columnName, String value) throws IOException {

        /**
         * Instantiated Table Object
         * å®ä¾‹åŒ– è¡¨å¯¹è±¡
         */
        Table table = connection.getTable(TableName.valueOf(tableName));

        /**
         * Verify that the data table exists before adding data
         * æ·»åŠ æ•°æ®å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (!isTableExist(String.valueOf(TableName.valueOf(tableName)))) {
            System.out.println("Adding Data Failed, Reason: " + table + "DataTable Does Not Exist !");
            return;
        }

        /**
         * Instantiate Put Object
         * å®ä¾‹åŒ– Putå¯¹è±¡
         *
         * Convert String type primary key ID to byte data
         * å°†Stringç±»å‹ä¸»é”®IDè½¬æ¢å­—èŠ‚æ•°æ®
         */
        Put put = new Put(Bytes.toBytes(rowKey));

        /**
         * Add data (column family/column name/numeric) and convert the String type to a byte array
         * æ·»åŠ æ•°æ® (åˆ—æ—/ åˆ—å / æ•°å€¼)å¹¶å°†Stringç±»å‹è½¬æ¢ä¸ºå­—èŠ‚æ•°ç»„
         */
        put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(value));

        /**
         * Execution method
         * æ‰§è¡Œæ–¹æ³•
         */
        table.put(put);

        System.out.println(table.toString() + " Data added Successfully !");

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        table.close();
    }
    
    public static void main(String[] args) throws IOException {

        /**
         * Adding Data
         */
        addData("test001", "0003", "info", "name", "testUser003");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
```
test001;hconnection-0x281e3708 Data added Successfully
```

### 6.6 ä¿®æ”¹æ•°æ®
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {

            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Change Data
     * ä¿®æ”¹æ•°æ®
     *
     * @param tableName
     * @param rowKey
     * @param columnFamily
     * @param columnName
     * @param value
     */
    public static void changeData(String tableName, String rowKey, String columnFamily, String columnName, String value) throws IOException {

        /**
         * Instantiated Table Object
         * å®ä¾‹åŒ– è¡¨å¯¹è±¡
         */
        Table table = connection.getTable(TableName.valueOf(tableName));

        /**
         * Verify that the data table exists before modifying the data
         * ä¿®æ”¹æ•°æ®å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (!isTableExist(String.valueOf(TableName.valueOf(tableName)))) {
            System.out.println("Change Data Failed, Reason: " + table + "DataTable Does Not Exist !");
            return;
        }

        /**
         * Instantiate Put Object
         * å®ä¾‹åŒ– Putå¯¹è±¡
         *
         * Convert String type primary key ID to byte data
         * å°†Stringç±»å‹ä¸»é”®IDè½¬æ¢å­—èŠ‚æ•°æ®
         */
        Put put = new Put(Bytes.toBytes(rowKey));

        /**
         * Change data (column family/column name/numeric) and convert the String type to a byte array
         * ä¿®æ”¹æ•°æ® (åˆ—æ—/ åˆ—å / æ•°å€¼)å¹¶å°†Stringç±»å‹è½¬æ¢ä¸ºå­—èŠ‚æ•°ç»„
         */
        put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(value));

        /**
         * Execution method
         * æ‰§è¡Œæ–¹æ³•
         */
        table.put(put);

        System.out.println(table.toString() + " Data Change Successfully !");

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        table.close();
    }

    public static void main(String[] args) throws IOException {

        /**
         * Change Data
         */
        changeData("test001", "0003", "info", "name", "testUser004");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
```
test001;hconnection-0x281e3708 Data Change Successfully!
```


### 6.7 æŸ¥è¯¢æ•°æ®
#### 6.7.1 å…¨è¡¨æ‰«æ
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {

            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Full Table Scan - Query Data
     * å…¨è¡¨æ‰«æ - æŸ¥è¯¢æ•°æ®
     *
     * @param tableName
     */
    public static void scanTable(String tableName) throws IOException {

        /**
         * Instantiated Table Object
         * å®ä¾‹åŒ– è¡¨å¯¹è±¡
         */
        Table table = connection.getTable(TableName.valueOf(tableName));

        /**
         * Verify that the data table exists before modifying the data
         * æŸ¥è¯¢æ•°æ®å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (!isTableExist(String.valueOf(TableName.valueOf(tableName)))) {
            System.out.println("Inquire Data Failed, Reason: " + table + "DataTable Does Not Exist !");
            return;
        }

        /**
         * Instantiation Scanner
         * å®ä¾‹åŒ– æ‰«æå™¨
         */
        Scan scan = new Scan();

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        ResultScanner results = table.getScanner(scan);

        /**
         * Traversing RowKey data
         * éå†RowKeyæ•°æ®
         */
        for (Result result : results) {
            Cell[] cells = result.rawCells();
            /**
             * Traversing the Row Key collection data
             * éå†RowKeyé›†åˆæ•°æ®
             */
            for (Cell cell : cells) {
                System.out.println("RowKey is = " + Bytes.toString(CellUtil.cloneRow(cell))
                        + " & " + "ColumnFamily is = " + Bytes.toString(CellUtil.cloneFamily(cell))
                        + " & " + "ColumnName is = " + Bytes.toString(CellUtil.cloneQualifier(cell))
                        + " & " + "Value is = " + Bytes.toString(CellUtil.cloneValue(cell))
                        + "\n" + ("===========================================================================")
                );
            }
        }

        /**
         * Logger INFO
         */
        System.out.println("FullTable Data Query Success !");

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        table.close();
    }

    public static void main(String[] args) throws IOException {

        /**
         * Full Table Scan - Query Data
         */
        scanTable("test");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- å…¨è¡¨æ‰«æ æ‰§è¡Œè¿”å›ç»“æœ
```
RowKey is = 0001 & ColumnFamily is = info & ColumnName is = age & Value is = 60
===========================================================================
RowKey is = 0001 & ColumnFamily is = info & ColumnName is = name & Value is = testuser001
===========================================================================
RowKey is = 0002 & ColumnFamily is = info & ColumnName is = age & Value is = 70
===========================================================================
RowKey is = 0002 & ColumnFamily is = info & ColumnName is = name & Value is = testuser002
===========================================================================
FullTable Data Query Success !
```

#### 6.7.2 æŒ‡å®šå‚æ•°æŸ¥è¯¢æ•°æ®
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {

            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Specified Parameter - Query Data
     * æŒ‡å®šå‚æ•° - æŸ¥è¯¢æ•°æ®
     *
     * @param tableName
     * @param rowKey
     * @param columnFamily
     * @param columnName
     * @throws IOException
     */
    public static void getData(String tableName, String rowKey, String columnFamily, String columnName) throws IOException {

        /**
         * Instantiated Table Object
         * å®ä¾‹åŒ– è¡¨å¯¹è±¡
         */
        Table table = connection.getTable(TableName.valueOf(tableName));

        /**
         * Verify that the data table exists before modifying the data
         * æŸ¥è¯¢æ•°æ®å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (!isTableExist(String.valueOf(TableName.valueOf(tableName)))) {
            System.out.println("Inquire Data Failed, Reason: " + table + "DataTable Does Not Exist !");
            return;
        }

        /**
         * Instantiate the Get object
         * å®ä¾‹åŒ–Getå¯¹è±¡
         */
        Get get = new Get(Bytes.toBytes(rowKey));

        /**
         * Specify column family & column name
         * æŒ‡å®šåˆ—æ—&åˆ—å
         */
        get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName));

        /**
         * Execution method
         * æ‰§è¡Œæ–¹æ³•
         */
        Result result = table.get(get);

        Cell[] cells = result.rawCells();

        /**
         * Traversing the Row Key collection data
         * éå†RowKeyé›†åˆæ•°æ®
         */
        for (Cell cell : cells) {
            System.out.println("RowKey is = " + Bytes.toString(CellUtil.cloneRow(cell))
                    + " & " + "ColumnFamily is = " + Bytes.toString(CellUtil.cloneFamily(cell))
                    + " & " + "ColumnName is = " + Bytes.toString(CellUtil.cloneQualifier(cell))
                    + " & " + "Value is = " + Bytes.toString(CellUtil.cloneValue(cell))
                    + "\n" + ("===========================================================================")
            );
        }

        /**
         * Logger INFO
         */
        System.out.println("Specified Parameter Data Query Success !");

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        table.close();
    }

    public static void main(String[] args) throws IOException {

        /**
         * Specified Parameter - Query Data
         */
        getData("test", "0001", "info", "name");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æŒ‡å®šå‚æ•°æŸ¥è¯¢æ•°æ® æ‰§è¡Œè¿”å›ç»“æœ
```
RowKey is = 0001 & ColumnFamily is = info & ColumnName is = name & Value is = testuser001
===========================================================================
Specified Parameter Data Query Success !
```

### 6.8 åˆ é™¤æ•°æ®
``` java
package com.geekparkhub.core.hbase.api.producehub;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HbaseHub
 * <p>
 */

public class HbaseHub {

    /**
     * Public Configuration information
     * å…¬å…±é…ç½®ä¿¡æ¯
     */
    private static Admin admin = null;
    private static Connection connection = null;
    private static Configuration configuration;

    static {

        /**
         * HBase ConfigurationFile
         * HBase é…ç½®æ–‡ä»¶
         */
        configuration = HBaseConfiguration.create();
        configuration.set("hbase.zookeeper.quorum", "172.16.168.130");
        configuration.set("hbase.zookeeper.property.clientPort", "2181");
        try {

            /**
             * Get Connected
             * è·å–è¿æ¥
             */
            connection = ConnectionFactory.createConnection(configuration);

            /**
             * Get HBase Administrator Object
             * è·å–HBaseç®¡ç†å‘˜å¯¹è±¡
             */
            admin = connection.getAdmin();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Close Resource
     * å…³é—­èµ„æº
     *
     * @param conf
     * @param admin
     */
    private static void close(Connection conf, Admin admin) {
        if (conf != null) {
            try {
                conf.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        if (admin != null) {
            try {
                admin.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    /**
     * Determine if DataTable Exists
     * åˆ¤æ–­æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
     *
     * @param tableName
     */
    public static boolean isTableExist(String tableName) throws IOException {

        /**
         * Execution Method
         * æ‰§è¡Œæ–¹æ³•
         */
        boolean tableExists = admin.tableExists(TableName.valueOf(tableName));

        return tableExists;
    }

    /**
     * Delete Data
     * åˆ é™¤æ•°æ®
     *
     * @param tableName
     * @param rowKey
     * @param columnFamily
     * @param columnName
     * @throws IOException
     */
    public static void deleteData(String tableName, String rowKey, String columnFamily, String columnName) throws IOException {

        /**
         * Instantiated Table Object
         * å®ä¾‹åŒ– è¡¨å¯¹è±¡
         */
        Table table = connection.getTable(TableName.valueOf(tableName));

        /**
         * Verify that the data table exists before modifying the data
         * åˆ é™¤æ•°æ®å‰,éªŒè¯æ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
         */
        if (!isTableExist(String.valueOf(TableName.valueOf(tableName)))) {
            System.out.println("Delete Data Failed, Reason: " + table + "DataTable Does Not Exist !");
            return;
        }

        /**
         * Instantiate the delete object
         * å®ä¾‹åŒ– deleteå¯¹è±¡
         */
        Delete delete = new Delete(Bytes.toBytes(rowKey));

        /**
         * åˆ é™¤æŒ‡å®šæ•°æ®
         */
        delete.addColumns(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName));

        /**
         * Execution method
         * æ‰§è¡Œæ–¹æ³•
         */
        table.delete(delete);

        /**
         * Logger INFO
         */
        System.out.println("Data Deletion is Successful!");

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        table.close();
    }

    public static void main(String[] args) throws IOException {

        /**
         * Delete Data
         */
        deleteData("test001", "0003", "name", "testUser004");

        /**
         * Close Resource
         * å…³é—­èµ„æº
         */
        close(connection, admin);
    }
}
```
- æ‰§è¡Œè¿”å›ç»“æœ
```
Data Deletion is Successful!
```

### 6.9 MapReduce
- ä½¿ç”¨MapReduceå°†æ•°æ®ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå¯¼å…¥åˆ°HBaseçš„è¡¨ä¸­,ä»HBaseä¸­è¯»å–åŸå§‹æ•°æ®åä½¿ç”¨MapReduceæ•°æ®åˆ†æ.

#### 6.9.1 å®˜æ–¹HBase-MapReduce
- æŸ¥çœ‹HBaseçš„MapReduceä»»åŠ¡æ‰§è¡Œ
```
[root@systemhub511 hbase]# bin/hbase mapredcp
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
/opt/module/hbase/lib/netty-all-4.0.23.Final.jar:/opt/module/hbase/lib/hbase-client-1.3.1.jar:/opt/module/hbase/lib/metrics-core-2.2.0.jar:/opt/module/hbase/lib/zookeeper-3.4.10.jar:/opt/module/hbase/lib/hbase-prefix-tree-1.3.1.jar:/opt/module/hbase/lib/hbase-common-1.3.1.jar:/opt/module/hbase/lib/protobuf-java-2.5.0.jar:/opt/module/hbase/lib/guava-12.0.1.jar:/opt/module/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/module/hbase/lib/hbase-protocol-1.3.1.jar:/opt/module/hbase/lib/hbase-hadoop-compat-1.3.1.jar:/opt/module/hbase/lib/hbase-server-1.3.1.jar
[root@systemhub511 hbase]#
```
#### 6.9.2 æ‰§è¡Œç¯å¢ƒå˜é‡å¯¼å…¥
- å…³é—­æ‰€æœ‰Hadoopç›¸å…³æœåŠ¡
- vim `/etc/profile`
```
## SET HBASE_HOME
export HBASE_HOME=/opt/module/hbase
```
```
[root@systemhub511 hadoop]# source /etc/profile
```
- vim `hadoop-env.sh` / æ³¨æ„ : åœ¨forè¯­å¥åè¿½åŠ æ­¤é…ç½®å‚æ•°.
```
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*
```
- é…ç½®æ–‡ä»¶åˆ†å‘é›†ç¾¤
- é€ä¸€å¼€å¯ç›¸å…³æœåŠ¡

#### 6.9.3 è¿è¡Œå®˜æ–¹MapReduceä»»åŠ¡
- `Demo1` : ç»Ÿè®¡testè¡¨ä¸­æœ‰å¤šå°‘è¡Œæ•°æ®
```
[root@systemhub511 hbase]# /opt/module/hadoop/bin/yarn jar lib/hbase-server-1.3.1.jar rowcounter test
HBase Counters
                BYTES_IN_REMOTE_RESULTS=0
                BYTES_IN_RESULTS=74
                MILLIS_BETWEEN_NEXTS=549
                NOT_SERVING_REGION_EXCEPTION=0
                NUM_SCANNER_RESTARTS=0
                NUM_SCAN_RESULTS_STALE=0
                REGIONS_SCANNED=1
                REMOTE_RPC_CALLS=0
                REMOTE_RPC_RETRIES=0
                ROWS_FILTERED=0
                ROWS_SCANNED=2
                RPC_CALLS=3
                RPC_RETRIES=0
        org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters
                ROWS=2
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=0
```

- `Demo2` : ä½¿ç”¨MapReduceå°†æœ¬åœ°æ•°æ®å¯¼å…¥åˆ°HBase
- åˆ›å»ºåç¼€ä¸º.tsvæ ¼å¼æ–‡ä»¶
```
[root@systemhub511 hbase]# touch test002.tsv
[root@systemhub511 hbase]# vim test002.tsv
```
```
1001    Test001 Red
1002    Test002 Yellow
1003    Test003 Yellow
```
- å°†æ–‡ä»¶ä¸Šä¼ åˆ°HDFS
```
[root@systemhub511 hbase]# hadoop fs -put test002.tsv /core_flow/test/
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[root@systemhub511 hbase]# 
```
- åˆ›å»ºHBaseè¡¨
```
hbase(main):001:0> create 'test002','info'
0 row(s) in 3.1610 seconds

=> Hbase::Table - test002
hbase(main):002:0> 
```
- æ‰§è¡ŒMapReduce
```
[root@systemhub511 hbase]# /opt/module/hadoop/bin/yarn jar ./lib/hbase-server-1.3.1.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color test002 hdfs://systemhub511:9000/core_flow/test/test002.tsv
Map-Reduce Framework
                Map input records=3
                Map output records=3
                Input split bytes=116
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=131
                CPU time spent (ms)=1690
                Physical memory (bytes) snapshot=108429312
                Virtual memory (bytes) snapshot=2071105536
                Total committed heap usage (bytes)=16318464

```
- ä½¿ç”¨scanæŒ‡ä»¤æŸ¥çœ‹å¯¼å…¥åç»“æœ
```
hbase(main):001:0> scan 'test002'
ROW                        COLUMN+CELL                                                                 
 1001                      column=info:color, timestamp=1556886127536, value=Red                       
 1001                      column=info:name, timestamp=1556886127536, value=Test001                    
 1002                      column=info:color, timestamp=1556886127536, value=Yellow                    
 1002                      column=info:name, timestamp=1556886127536, value=Test002                    
 1003                      column=info:color, timestamp=1556886127536, value=Yellow                    
 1003                      column=info:name, timestamp=1556886127536, value=Test003                    
3 row(s) in 0.5090 seconds
hbase(main):002:0> 
```

#### 6.9.4 è‡ªå®šä¹‰ HBase-MapReduce (1)
- å°†test002è¡¨ä¸­ä¸€éƒ¨åˆ†æ•°æ®,é€šè¿‡MapReduceè¿å…¥åˆ°test002_mrè¡¨ä¸­.
- Create `TestMapper.class` / ç”¨äºè¯»å–test002è¡¨ä¸­æ•°æ®.
``` java
package com.geekparkhub.core.hbase.api.mapreduce;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * TestMapper
 * <p>
 */

public class TestMapper extends TableMapper<ImmutableBytesWritable, Put> {

    @Override
    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {

        /**
         * Instantiate Put object
         * å®ä¾‹åŒ– Putå¯¹è±¡
         */
        Put put = new Put(key.get());

        /**
         * Traversing rowkey data
         * éå†rowkeyæ•°æ®
         */
        Cell[] cells = value.rawCells();
        for (Cell cell : cells) {
            if ("name".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))) {
                put.add(cell);
            }
        }
        context.write(key, put);
    }
}
```
- Create `TestReducer.class` / ç”¨äºå°†è¯»å–test002è¡¨æ•°æ®å†™å…¥åˆ°test002_mrè¡¨ä¸­.
``` java
package com.geekparkhub.core.hbase.api.mapreduce;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.io.NullWritable;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * TestReducer
 * <p>
 */

public class TestReducer extends TableReducer<ImmutableBytesWritable, Put, NullWritable> {

    @Override
    protected void reduce(ImmutableBytesWritable key, Iterable<Put> values, Context context) throws IOException, InterruptedException {
        for (Put value : values) {
            context.write(NullWritable.get(), value);
        }
    }
}
```
- Create `TestDriver.class` / ç”¨äºç»„è£…è¿è¡ŒJobä»»åŠ¡
``` java
package com.geekparkhub.core.hbase.api.mapreduce;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * TestDriver
 * <p>
 */

public class TestDriver extends Configuration implements Tool {

    Configuration configuration = null;

    public int run(String[] args) throws Exception {

        /**
         * å®ä¾‹åŒ– Jobå¯¹è±¡
         */
        Job job = Job.getInstance(configuration);
        job.setJarByClass(TestDriver.class);

        TableMapReduceUtil.initTableMapperJob("test002", new Scan(), TestMapper.class, ImmutableBytesWritable.class, Put.class, job);

        TableMapReduceUtil.initTableReducerJob("test002_mr", TestReducer.class, job);

        boolean completion = job.waitForCompletion(true);

        return completion ? 0 : 1;
    }

    public void setConf(Configuration conf) {
        this.configuration = conf;
    }

    public Configuration getConf() {
        return configuration;
    }

    public static void main(String[] args) throws Exception {
        Configuration configuration = HBaseConfiguration.create();
        int run = ToolRunner.run(configuration, new TestDriver(), args);
    }
}
```
- è¿è¡Œä»»åŠ¡å‰,å¦‚æœå¾…æ•°æ®å¯¼å…¥è¡¨ä¸å­˜åœ¨,åˆ™éœ€è¦æå‰åˆ›å»º
```
[root@systemhub511 hbase]# bin/hbase shell
hbase(main):001:0> create 'test002_mr','info'
0 row(s) in 2.8570 seconds
=> Hbase::Table - test002_mr
hbase(main):002:0> 
```
- æ‰“åŒ…è¿è¡Œä»»åŠ¡
```
[root@systemhub511 hbase]# /opt/module/hadoop/bin/yarn jar ./hbase_server-1.0.jar com.geekparkhub.core.hbase.api.mapreduce.TestDriver
 INFO mapreduce.Job: Counters: 62
        File System Counters
                FILE: Number of bytes read=195
                FILE: Number of bytes written=298305
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=109
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=1
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Rack-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=21676
                Total time spent by all reduces in occupied slots (ms)=10110
                Total time spent by all map tasks (ms)=21676
                Total time spent by all reduce tasks (ms)=10110
                Total vcore-milliseconds taken by all map tasks=21676
                Total vcore-milliseconds taken by all reduce tasks=10110
                Total megabyte-milliseconds taken by all map tasks=22196224
                Total megabyte-milliseconds taken by all reduce tasks=10352640
        Map-Reduce Framework
                Map input records=3
                Map output records=3
                Map output bytes=183
                Map output materialized bytes=195
                Input split bytes=109
                Combine input records=3
                Combine output records=3
                Reduce input groups=3
                Reduce shuffle bytes=195
                Reduce input records=3
                Reduce output records=3
                Spilled Records=6
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=612
                CPU time spent (ms)=6990
                Physical memory (bytes) snapshot=333955072
                Virtual memory (bytes) snapshot=4147179520
                Total committed heap usage (bytes)=144588800
        HBase Counters
                BYTES_IN_REMOTE_RESULTS=0
                BYTES_IN_RESULTS=255
                MILLIS_BETWEEN_NEXTS=1667
                NOT_SERVING_REGION_EXCEPTION=0
                NUM_SCANNER_RESTARTS=0
                NUM_SCAN_RESULTS_STALE=0
                REGIONS_SCANNED=1
                REMOTE_RPC_CALLS=0
                REMOTE_RPC_RETRIES=0
                ROWS_FILTERED=0
                ROWS_SCANNED=3
                RPC_CALLS=3
                RPC_RETRIES=0
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=0
        File Output Format Counters 
                Bytes Written=0
[root@systemhub511 hbase]# 
```
- æŸ¥çœ‹æ‰§è¡Œç»“æœ
```
hbase(main):002:0> scan 'test002_mr'
ROW                        COLUMN+CELL                                                                 
 1001                      column=info:name, timestamp=1556886127536, value=Test001                    
 1002                      column=info:name, timestamp=1556886127536, value=Test002                    
 1003                      column=info:name, timestamp=1556886127536, value=Test003                    
3 row(s) in 0.0290 seconds
hbase(main):004:0> 
```

#### 6.9.5 è‡ªå®šä¹‰ HBase-MapReduce (2)
- å®ç°å°†HDFSä¸­æ•°æ®å†™å…¥åˆ°HBaseè¡¨ä¸­.
- Create `HDFSMapper.class` / ç”¨äºè¯»å–HDFSä¸­æ–‡ä»¶æ•°æ®.
``` java
package com.geekparkhub.core.hbase.api.hdfs;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HDFSMapper
 * <p>
 */

public class HDFSMapper extends Mapper<LongWritable, Text, NullWritable, Put> {

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        /**
         * è·å–ä¸€è¡Œæ•°æ®
         */
        String line = value.toString();

        /**
         * åˆ‡å‰²æ•°æ®
         */
        String[] split = line.split("\t");

        /**
         * å°è£…å¯¹è±¡
         */
        Put put = new Put(Bytes.toBytes(split[0]));
        put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(split[1]));
        put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("color"), Bytes.toBytes(split[2]));

        /**
         * å†™å‡ºæ•°æ®
         */
        context.write(NullWritable.get(), put);
    }
}
```
- Create `HDFSReducer.class`
``` java
package com.geekparkhub.core.hbase.api.hdfs;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.io.NullWritable;

import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HDFSReducer
 * <p>
 */

public class HDFSReducer extends TableReducer<NullWritable, Put, NullWritable> {

    @Override
    protected void reduce(NullWritable key, Iterable<Put> values, Context context) throws IOException, InterruptedException {
        for (Put value : values) {
            context.write(NullWritable.get(), value);
        }
    }
}
```
- Create `HDFSDriver.class`
``` java
package com.geekparkhub.core.hbase.api.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HDFSDriver
 * <p>
 */

public class HDFSDriver extends Configuration implements Tool {

    Configuration configuration = null;

    public int run(String[] args) throws Exception {
        Job job = Job.getInstance(configuration);
        job.setJarByClass(HDFSDriver.class);
        job.setMapperClass(HDFSMapper.class);
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(Put.class);
        FileInputFormat.setInputPaths(job, args[0]);
        TableMapReduceUtil.initTableReducerJob("test003", HDFSReducer.class, job);
        boolean completion = job.waitForCompletion(true);
        return completion ? 0 : 1;
    }

    public void setConf(Configuration conf) {
        this.configuration = conf;
    }

    public Configuration getConf() {
        return configuration;
    }

    public static void main(String[] args) throws Exception {
        Configuration configuration = HBaseConfiguration.create();
        int run = ToolRunner.run(configuration, new HDFSDriver(), args);
        System.exit(run);
    }
}
```

- è¿è¡Œä»»åŠ¡å‰,å¦‚æœå¾…æ•°æ®å¯¼å…¥è¡¨ä¸å­˜åœ¨,åˆ™éœ€è¦æå‰åˆ›å»º
```
hbase(main):004:0> create 'test003','info'
0 row(s) in 2.9220 seconds

=> Hbase::Table - test003
hbase(main):005:0> 
```
- æ‰“åŒ…è¿è¡Œä»»åŠ¡
```
[root@systemhub511 hbase]# /opt/module/hadoop/bin/yarn jar ./hbase_server-1.0.jar com.geekparkhub.core.hbase.api.hdfs.HDFSDriver /core_flow/test/test002.tsv
File System Counters
                FILE: Number of bytes read=258
                FILE: Number of bytes written=297461
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=173
                HDFS: Number of bytes written=0
                HDFS: Number of read operations=2
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=0
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=4943
                Total time spent by all reduces in occupied slots (ms)=5307
                Total time spent by all map tasks (ms)=4943
                Total time spent by all reduce tasks (ms)=5307
                Total vcore-milliseconds taken by all map tasks=4943
                Total vcore-milliseconds taken by all reduce tasks=5307
                Total megabyte-milliseconds taken by all map tasks=5061632
                Total megabyte-milliseconds taken by all reduce tasks=5434368
        Map-Reduce Framework
                Map input records=3
                Map output records=3
                Map output bytes=246
                Map output materialized bytes=258
                Input split bytes=116
                Combine input records=0
                Combine output records=0
                Reduce input groups=1
                Reduce shuffle bytes=258
                Reduce input records=3
                Reduce output records=3
                Spilled Records=6
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=238
                CPU time spent (ms)=2100
                Physical memory (bytes) snapshot=316829696
                Virtual memory (bytes) snapshot=4133519360
                Total committed heap usage (bytes)=138117120
```
- æŸ¥çœ‹æ‰§è¡Œç»“æœ
```
hbase(main):001:0> scan 'test003'
ROW                        COLUMN+CELL                                                                 
 1001                      column=info:color, timestamp=1556952731547, value=Red                       
 1001                      column=info:name, timestamp=1556952731547, value=Test001                    
 1002                      column=info:color, timestamp=1556952731547, value=Yellow                    
 1002                      column=info:name, timestamp=1556952731547, value=Test002                    
 1003                      column=info:color, timestamp=1556952731547, value=Yellow                    
 1003                      column=info:name, timestamp=1556952731547, value=Test003                    
3 row(s) in 0.5410 seconds
hbase(main):002:0> 
```

### 6.10 HBaseé›†æˆHive
#### 6.10.1 HBaseä¸Hiveå¯¹æ¯”
##### 6.10.1.1 Hive (æ•°æ®ä»“åº“)
- `1. æ•°æ®ä»“åº“`
- Hiveæœ¬è´¨å…¶å®å°±ç›¸å½“äºå°†HDFSä¸­å·²ç»å­˜å‚¨æ–‡ä»¶åœ¨Mysqlä¸­åšäº†ä¸€ä¸ªåŒå°„å…³ç³»,ä»¥æ–¹ä¾¿ä½¿ç”¨HQLç®¡ç†æŸ¥è¯¢.
- `2. ç”¨äºæ•°æ®åˆ†æ/æ¸…æ´—(ETL)`
- Hiveé€‚ç”¨äºç¦»çº¿çš„æ•°æ®åˆ†æå’Œæ¸…æ´—,å»¶è¿Ÿè¾ƒé«˜.
- `3. åŸºäºHDFS / MapReduce`
- Hiveå­˜å‚¨æ•°æ®ä¾æ—§åœ¨DataNodeä¸Š,ç¼–å†™HQLè¯­å¥ç»ˆå°†æ˜¯è½¬æ¢ä¸ºMapReduceä»£ç æ‰§è¡Œ.

##### 6.10.1.2 HBase (æ•°æ®åº“)
- `1. æ•°æ®åº“`
- ä¸€ç§é¢å‘åˆ—å­˜å‚¨éå…³ç³»å‹æ•°æ®åº“.
- `2. ç”¨äºå­˜å‚¨ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®`
- é€‚ç”¨äºå•è¡¨éå…³ç³»å‹æ•°æ®å­˜å‚¨,ä¸é€‚åˆåšå…³è”æŸ¥è¯¢,ç±»ä¼¼JOINç­‰æ“ä½œ.
- `3. åŸºäºHDFS`
- æ•°æ®æŒä¹…åŒ–å­˜å‚¨ä½“ç°å½¢å¼æ˜¯Hfile,å­˜æ”¾äºDataNodeä¸­,è¢«ResionServerä»¥regionå½¢å¼è¿›è¡Œç®¡ç†.
- `4.å»¶è¿Ÿè¾ƒä½ / æ¥å…¥åœ¨çº¿ä¸šåŠ¡ä½¿ç”¨`
- é¢å¯¹å¤§é‡ä¼ä¸šæ•°æ®,HBaseå¯ä»¥ç›´çº¿å•è¡¨å¤§é‡æ•°æ®å­˜å‚¨,åŒæ—¶æä¾›äº†é«˜æ•ˆæ•°æ®è®¿é—®é€Ÿåº¦.

#### 6.10.2 HBaseä¸Hiveé›†æˆ
- HBaseä¸Hiveé›†æˆåœ¨æœ€æ–°ä¸¤ä¸ªç‰ˆæœ¬ä¸­æ— æ³•å…¼å®¹,æ‰€ä»¥é‡æ–°ç¼–è¯‘ : `hive-hbase-handler-1.2.2.jar`
- ç¯å¢ƒå‡†å¤‡
- æ‹·è´Hiveæ‰€ä¾èµ–çš„JaråŒ…(æˆ–è€…ä½¿ç”¨è½¯è¿æ¥å½¢å¼)
```
[root@systemhub711 ~]# export HBASE_HOME=/opt/module/hbase/
[root@systemhub711 ~]# export HIVE_HOME=/opt/module/hive/
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar $HIVE_HOME/lib/hbase-common-1.3.1.jar
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbase-server-1.3.1.jar
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jar
[root@systemhub711 ~]#  ln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jar
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it-1.3.1.jar
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.ja
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar
[root@systemhub711 ~]# ln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar
```
- åŒæ—¶åœ¨`hive-site.xml`ä¸­ä¿®æ”¹zookeeperå±æ€§
``` xml
<property>
  <name>hive.zookeeper.quorum</name>
  <value>systemhub511,systemhub611,systemhub711</value>
  <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
</property>
<property>
  <name>hive.zookeeper.client.port</name>
  <value>2181</value>
  <description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
</property>
```
- `Demo 1` : å»ºç«‹Hiveè¡¨,å…³è”HBaseè¡¨,æ’å…¥æ•°æ®åˆ°Hiveè¡¨åŒæ—¶èƒ½å¤Ÿå½±å“HBaseè¡¨.
- 1.åœ¨Hiveä¸­åˆ›å»ºè¡¨åŒæ—¶å…³è”HBase
- åˆ›å»ºå†…éƒ¨è¡¨è¯­å¥
```
CREATE TABLE hive_hbase_emp_table(
    empno int , 
    ename string , 
    job string , 
    mgr int , 
    hiredate string , 
    sal double , 
    comm double , 
    deptno int 
  )
  STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
  WITH SERDEPROPERTIES("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") 
  TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");
```
- åˆ›å»ºæ•°æ®è¡¨è¿‡ç¨‹
```
hive (default)> CREATE TABLE hive_hbase_emp_table(
              > empno int , 
              > ename string , 
              > job string , 
              > mgr int , 
              > hiredate string , 
              > sal double , 
              > comm double , 
              > deptno int)
              > STORED BY
              > 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
              > WITH SERDEPROPERTIES("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") 
              > TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");
OK
Time taken: 5.183 seconds
```
- åˆ›å»ºå®Œæˆä¹‹å,å¯ä»¥åˆ†åˆ«æŸ¥çœ‹Hiveå’ŒHBaseéƒ½ç”Ÿæˆäº†å¯¹åº”æ•°æ®è¡¨
- æŸ¥çœ‹Hiveæ•°æ®è¡¨
```
hive (default)> show tables;
OK
hive_hbase_emp_table
Time taken: 0.037 seconds, Fetched: 21 row(s)
hive (default)> 
```
- æŸ¥çœ‹HBaseæ•°æ®è¡¨
```
hbase(main):002:0> list
TABLE 
hbase_emp_table
6 row(s) in 0.0370 seconds
=> ["hbase_emp_table", "test", "test001", "test002", "test002_mr", "test003"]
hbase(main):003:0> 
```
- 2.åœ¨Hiveä¸­åˆ›å»ºä¸´æ—¶ä¸­é—´è¡¨,ç”¨äºloadæ–‡ä»¶ä¸­æ•°æ®
```
CREATE TABLE emp(
    empno int , 
    ename string , 
    job string , 
    mgr int , 
    hiredate string , 
    sal double , 
    comm double , 
    deptno int 
  )row format delimited fields terminated by '\t';
```
- 3.é€šè¿‡insertå‘½ä»¤å°†ä¸­é—´è¡¨ä¸­æ•°æ®å¯¼å…¥åˆ°Hiveå…³è”HBaseæ•°æ®è¡¨
```
hive (default)> insert into table hive_hbase_emp_table select * from emp;
Query ID = root_12735_a9ed65e3-5352-4abc-82b1-00d8ae81f021
Total jobs = 1
Launching Job 1 out of 1
```
- 4.æŸ¥çœ‹Hiveä»¥åŠå…³è”HBaseè¡¨ä¸­æ˜¯å¦å·²ç»æˆåŠŸåŒæ­¥æ’å…¥æ•°æ®
- Hive : `select * from hive_hbase_emp_table;`
```
hive (default)> select * from hive_hbase_emp_table;
OK
hive_hbase_emp_table.empno      hive_hbase_emp_table.ename      hive_hbase_emp_table.job        hive_hbase_emp_table.mgr        hive_hbase_emp_table.hiredate  hive_hbase_emp_table.sal hive_hbase_emp_table.comm       hive_hbase_emp_table.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.327 seconds, Fetched: 9 row(s)
hive (default)> 
```
- HBase : `scan 'hbase_emp_table'`
```
hbase(main):003:0> scan 'hbase_emp_table'
ROW                                       COLUMN+CELL                                                                                                           
 7369                                     column=info:comm, timestamp=1557077298959, value=20.0                                                                 
 7369                                     column=info:ename, timestamp=1557077298959, value=SMITH                                                               
 7369                                     column=info:hiredate, timestamp=1557077298959, value=1980-12-17                                                       
 7369                                     column=info:job, timestamp=1557077298959, value=CLERKSKLD                                                             
 7369                                     column=info:mgr, timestamp=1557077298959, value=7902                                                                  
 7369                                     column=info:sal, timestamp=1557077298959, value=800.0                                                                 
 7499                                     column=info:comm, timestamp=1557077298959, value=300.0                                                                
 7499                                     column=info:deptno, timestamp=1557077298959, value=30                                                                 
 7499                                     column=info:ename, timestamp=1557077298959, value=ALLTE                                                               
 7499                                     column=info:hiredate, timestamp=1557077298959, value=1987-02-23                                                       
 7499                                     column=info:job, timestamp=1557077298959, value=SALESMANS                                                             
 7499                                     column=info:mgr, timestamp=1557077298959, value=7689                                                                  
 7499                                     column=info:sal, timestamp=1557077298959, value=1600.0                                                                
 7521                                     column=info:comm, timestamp=1557077298959, value=500.0                                                                
 7521                                     column=info:deptno, timestamp=1557077298959, value=30                                                                 
 7521                                     column=info:ename, timestamp=1557077298959, value=WAROS                                                               
 7521                                     column=info:hiredate, timestamp=1557077298959, value=1984-06-12                                                       
 7521                                     column=info:job, timestamp=1557077298959, value=SJDHHJDJX                                                             
 7521                                     column=info:mgr, timestamp=1557077298959, value=7869                                                                  
 7521                                     column=info:sal, timestamp=1557077298959, value=1250.18                                                               
 7566                                     column=info:comm, timestamp=1557077298959, value=20.0                                                                 
 7566                                     column=info:ename, timestamp=1557077298959, value=JOSSS                                                               
 7566                                     column=info:hiredate, timestamp=1557077298959, value=1874-05-15                                                       
 7566                                     column=info:job, timestamp=1557077298959, value=JDHYHDSDS                                                             
 7566                                     column=info:mgr, timestamp=1557077298959, value=4545                                                                  
 7566                                     column=info:sal, timestamp=1557077298959, value=2894.25                                                               
 7654                                     column=info:comm, timestamp=1557077298959, value=30.0                                                                 
 7654                                     column=info:ename, timestamp=1557077298959, value=SOCTD                                                               
 7654                                     column=info:hiredate, timestamp=1557077298959, value=1996-02-14                                                       
 7654                                     column=info:job, timestamp=1557077298959, value=MANSJUSSD                                                             
 7654                                     column=info:mgr, timestamp=1557077298959, value=4855                                                                  
 7654                                     column=info:sal, timestamp=1557077298959, value=2852.3                                                                
 7698                                     column=info:comm, timestamp=1557077298959, value=30.0                                                                 
 7698                                     column=info:ename, timestamp=1557077298959, value=ADAMS                                                               
 7698                                     column=info:hiredate, timestamp=1557077298959, value=1985-05-16                                                       
 7698                                     column=info:job, timestamp=1557077298959, value=JUSHHWESD                                                             
 7698                                     column=info:mgr, timestamp=1557077298959, value=4552                                                                  
 7698                                     column=info:sal, timestamp=1557077298959, value=25524.02                                                              
 7782                                     column=info:comm, timestamp=1557077298959, value=20.0                                                                 
 7782                                     column=info:ename, timestamp=1557077298959, value=JAMSK                                                               
 7782                                     column=info:hiredate, timestamp=1557077298959, value=1991-06-23                                                       
 7782                                     column=info:job, timestamp=1557077298959, value=KIHNGSEHN                                                             
 7782                                     column=info:mgr, timestamp=1557077298959, value=7769                                                                  
 7782                                     column=info:sal, timestamp=1557077298959, value=1100.0                                                                
 7788                                     column=info:comm, timestamp=1557077298959, value=30.0                                                                 
 7788                                     column=info:ename, timestamp=1557077298959, value=FOESS                                                               
 7788                                     column=info:hiredate, timestamp=1557077298959, value=1994-09-17                                                       
 7788                                     column=info:job, timestamp=1557077298959, value=CLAEDFDFD                                                             
 7788                                     column=info:mgr, timestamp=1557077298959, value=7698                                                                  
 7788                                     column=info:sal, timestamp=1557077298959, value=950.0                                                                 
 7939                                     column=info:comm, timestamp=1557077298959, value=20.0                                                                 
 7939                                     column=info:ename, timestamp=1557077298959, value=KINGS                                                               
 7939                                     column=info:hiredate, timestamp=1557077298959, value=1993-07-12                                                       
 7939                                     column=info:job, timestamp=1557077298959, value=CLADDJHEW                                                             
 7939                                     column=info:mgr, timestamp=1557077298959, value=7566                                                                  
 7939                                     column=info:sal, timestamp=1557077298959, value=3000.0                                                                
9 row(s) in 0.7380 seconds
hbase(main):004:0> 
```

- `Demo 2` : åœ¨HBaseä¸­å·²ç»å­˜å‚¨æŸä¸€å¼ `hbase_emp_table`æ•°æ®è¡¨,ç„¶ååœ¨Hiveä¸­åˆ›å»ºä¸€ä¸ªå¤–éƒ¨è¡¨æ¥å…³è”HBaseä¸­`hbase_emp_table`æ•°æ®è¡¨,ä½¿ä¹‹å¯ä»¥å€ŸåŠ©Hiveæ¥åˆ†æHBaseæ•°æ®è¡¨ä¸­çš„æ•°æ®.
- 1.åœ¨Hiveä¸­åˆ›å»ºå¤–éƒ¨è¡¨
- åˆ›å»ºå¤–éƒ¨è¡¨è¯­å¥
```
CREATE EXTERNAL TABLE relevance_hbase_emp(
    empno int , 
    ename string , 
    job string , 
    mgr int , 
    hiredate string , 
    sal double , 
    comm double , 
    deptno int 
  )
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") 
  TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");
```
- åˆ›å»ºæ•°æ®è¡¨è¿‡ç¨‹
```
hive (default)> CREATE EXTERNAL TABLE relevance_hbase_emp(
              >     empno int , 
              >     ename string , 
              >     job string , 
              >     mgr int , 
              >     hiredate string , 
              >     sal double , 
              >     comm double , 
              >     deptno int 
              >   )
              >   STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
              >   WITH SERDEPROPERTIES("hbase.columns.mapping" = ":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno") 
              >   TBLPROPERTIES ("hbase.table.name" = "hbase_emp_table");
OK
Time taken: 0.347 seconds
hive (default)> 
```
- 2.å…³è”å,å¯ä»¥ä½¿ç”¨Hiveå‡½æ•°è¿›è¡Œæ•°æ®åˆ†ææ“ä½œ
```
hive (default)> select * from relevance_hbase_emp;
OK
relevance_hbase_emp.empno       relevance_hbase_emp.ename       relevance_hbase_emp.job relevance_hbase_emp.mgr relevance_hbase_emp.hiredate    relevance_hbase_emp.sal relevance_hbase_emp.comm        relevance_hbase_emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.208 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.10.3 æ•°æ®å¤‡ä»½ä¸æ¢å¤
##### 6.10.3.1 å¤‡ä»½
- åœæ­¢HBaseæœåŠ¡å,ä½¿ç”¨`distcp`å‘½ä»¤è¿è¡ŒMapReduceä»»åŠ¡è¿›è¡Œå¤‡ä»½,å°†æ•°æ®å¤‡ä»½åˆ°å¦ä¸€ä¸ªåœ°æ–¹,å¯ä»¥æ˜¯åŒä¸€ä¸ªé›†ç¾¤,ä¹Ÿå¯ä»¥æ˜¯ä¸“ç”¨çš„å¤‡ä»½é›†ç¾¤.
- å³æŠŠæ•°æ®è½¬ç§»åˆ°å½“å‰é›†ç¾¤çš„å…¶ä»–ç›®å½•ä¸‹(ä¹Ÿå¯ä»¥ä¸åœ¨åŒä¸€ä¸ªé›†ç¾¤ä¸­),æ‰§è¡Œè¯¥æ“ä½œ.ä¸€å®šè¦å¼€å¯YarnæœåŠ¡.
```
bin/hadoop distcp hdfs://systemhub511:8020/hbase hdfs://systemhub511:8020/HbaseBackup/backup
```
##### 6.10.3.2 æ¢å¤
- éå¸¸ç®€å•,ä¸å¤‡ä»½æ–¹æ³•ä¸€æ ·,å°†æ•°æ®æ•´ä¸ªç§»åŠ¨å›æ¥å³å¯.
```
bin/hadoop distcp hdfs://systemhub511:8020/HbaseBackup/backup hdfs://systemhub511:8020/hbase
```
#### 6.10.4 èŠ‚ç‚¹ç®¡ç†
##### 6.10.4.1 æœå½¹ (commissioning)
> å½“å¯åŠ¨regionserveræ—¶,regionserverä¼šå‘HMasteræ³¨å†Œå¹¶å¼€å§‹æ¥æ”¶æœ¬åœ°æ•°æ®,å¼€å§‹æ—¶,æ–°åŠ å…¥çš„èŠ‚ç‚¹ä¸ä¼šæœ‰ä»»ä½•æ•°æ®,å¹³è¡¡å™¨å¼€å¯çš„æƒ…å†µä¸‹,å°†ä¼šæœ‰æ–°çš„regionç§»åŠ¨åˆ°å¼€å¯RegionServerä¸Š,å¦‚æœå¯åŠ¨å’Œåœæ­¢è¿›ç¨‹æ˜¯ä½¿ç”¨sshå’ŒHBaseè„šæœ¬,é‚£ä¹ˆä¼šå°†æ–°æ·»åŠ èŠ‚ç‚¹çš„ä¸»æœºååŠ å…¥åˆ°`conf/regionservers`æ–‡ä»¶ä¸­.

##### 6.10.4.2 é€€å½¹ (decommissioning)
> é¡¾åæ€ä¹‰,å°±æ˜¯ä»å½“å‰HBaseé›†ç¾¤ä¸­åˆ é™¤æŸä¸ªRegionServer,è¿™ä¸ªè¿‡ç¨‹åˆ†ä¸ºå¦‚ä¸‹å‡ ä¸ªè¿‡ç¨‹:
> 1.åœæ­¢è´Ÿè½½å¹³è¡¡å™¨
```
hbase> balance_switch false
```
> 2.åœ¨é€€å½¹èŠ‚ç‚¹ä¸Šåœæ­¢RegionServer
```
hbase> hbase-daemon.sh stop regionserver
```
> 3.RegionServerä¸€æ—¦åœæ­¢,ä¼šå…³é—­ç»´æŠ¤çš„æ‰€æœ‰region
> 4.Zookeeperä¸Šè¯¥RegionServerèŠ‚ç‚¹æ¶ˆå¤±
> 5.MasterèŠ‚ç‚¹æ£€æµ‹åˆ°è¯¥RegionServerä¸‹çº¿
> 6.RegionServerçš„regionæœåŠ¡å¾—åˆ°é‡æ–°åˆ†é…
> è¯¥å…³é—­æ–¹æ³•æ¯”è¾ƒä¼ ç»Ÿ,éœ€è¦èŠ±è´¹ä¸€å®šæ—¶é—´,è€Œä¸”ä¼šé€ æˆéƒ¨åˆ†regionçŸ­æš‚çš„ä¸å¯ç”¨.
- å¦ä¸€ç§æ–¹æ¡ˆ : 
- 1.RegionServerå…ˆå¸è½½æ‰€ç®¡ç†çš„region
```
bin/graceful_stop.sh <RegionServer-hostname>
```
- 2.è‡ªåŠ¨å¹³è¡¡æ•°æ®
- 3.å’Œç¬¬ä¸€ç§æ–¹æ¡ˆ2~6æ­¥éª¤ä¸€æ ·

#### 6.10.5 ç‰ˆæœ¬ç¡®ç•Œ
##### ç‰ˆæœ¬ä¸‹ç•Œ
> é»˜è®¤ç‰ˆæœ¬ä¸‹ç•Œæ˜¯0,å³ç¦ç”¨,rowç‰ˆæœ¬ä½¿ç”¨æœ€å°æ•°ç›®æ˜¯ä¸ç”Ÿå­˜æ—¶é—´(TTLTime To Live)ç»“åˆ,å¹¶ä¸”æ ¹æ®å®é™…éœ€æ±‚å¯ä»¥æœ‰0æˆ–æ›´å¤šç‰ˆæœ¬,ä½¿ç”¨0,å³åªæœ‰1ä¸ªç‰ˆæœ¬å€¼å†™å…¥cell.
##### ç‰ˆæœ¬ä¸Šç•Œ
> ä¹‹å‰é»˜è®¤ç‰ˆæœ¬ä¸Šç•Œæ˜¯3,ä¹Ÿå°±æ˜¯ä¸€ä¸ªrowä¿ç•™3ä¸ªå‰¯æœ¬(åŸºäºæ—¶é—´æˆ³æ’å…¥),è¯¥å€¼ä¸è¦è®¾è®¡è¿‡å¤§,ä¸€èˆ¬çš„ä¸šåŠ¡ä¸ä¼šè¶…è¿‡100,å¦‚æœcellä¸­å­˜å‚¨æ•°æ®ç‰ˆæœ¬å·è¶…è¿‡äº†3ä¸ª,å†æ¬¡æ’å…¥æ•°æ®æ—¶,æœ€æ–°å€¼ä¼šå°†æœ€æ—§å€¼è¦†ç›–,(ç°ç‰ˆæœ¬å·²é»˜è®¤ä¸º1).

## 7. HBase ä¼˜åŒ–
### 7.1 é«˜å¯ç”¨
> åœ¨HBaseä¸­Hmasterè´Ÿè´£ç›‘æ§RegionServerç”Ÿå‘½å‘¨æœŸ,å‡è¡¡RegionServerè´Ÿè½½,å¦‚æœHmasterå®•æœºäº†,é‚£ä¹ˆæ•´ä¸ªHBaseé›†ç¾¤å°†é™·å…¥ä¸å¥åº·çŠ¶æ€,å¹¶ä¸”æ­¤æ—¶å·¥ä½œçŠ¶æ€å¹¶ä¸ä¼šç»´æŒå¤ªä¹…,æ‰€ä»¥HBaseæ”¯æŒå¯¹Hmasterçš„é«˜å¯ç”¨é…ç½®.
- 1.å…³é—­HBaseé›†ç¾¤ (å¦‚æœæ²¡æœ‰å¼€å¯åˆ™è·³è¿‡æ­¤æ­¥)
```
[root@systemhub511 hbase]# bin/stop-hbase.sh
stopping hbase......................
[root@systemhub511 hbase]# 
```
- 2.åœ¨confç›®å½•ä¸‹åˆ›å»ºbackup-mastersæ–‡ä»¶
```
[root@systemhub511 hbase]# touch conf/backup-masters
```
- 3.åœ¨backup-mastersæ–‡ä»¶ä¸­é…ç½®é«˜å¯ç”¨HMasterèŠ‚ç‚¹
```
[root@systemhub511 hbase]# echo systemhub611 >> conf/backup-masters
```
- 4.å°†`backup-masters`é…ç½®æ–‡ä»¶åˆ†å‘åˆ°å…¶ä»–é›†ç¾¤èŠ‚ç‚¹
```
[root@systemhub511 hbase]# scp -r conf/backup-masters systemhub611:/opt/module/hbase/conf/
backup-masters 100%   13     0.0KB/s   00:00    
[root@systemhub511 hbase]# scp -r conf/backup-masters systemhub711:/opt/module/hbase/conf/
backup-masters 100%   13     0.0KB/s   00:00    
[root@systemhub511 hbase]# 
```
- 5.å¯åŠ¨HBaseæœåŠ¡å¹¶æŸ¥çœ‹æµ‹è¯•
- å¯åŠ¨æœåŠ¡
```
[root@systemhub511 hbase]# bin/start-hbase.sh
starting master, logging to /opt/module/hbase/logs/hbase-root-master-systemhub511.out
systemhub711: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub711.out
systemhub611: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub611.out
systemhub511: starting regionserver, logging to /opt/module/hbase/bin/../logs/hbase-root-regionserver-systemhub511.out
systemhub611: starting master, logging to /opt/module/hbase/bin/../logs/hbase-root-master-systemhub611.out
[root@systemhub511 hbase]# 
```
- æŸ¥çœ‹è¿›ç¨‹,å‘ç°systemhub511/systemhub611åˆ†åˆ«å¯åŠ¨HMasterèŠ‚ç‚¹,ä»è€Œè¾¾åˆ°é«˜å¯ç”¨æ•ˆæœ
```
[root@systemhub511 hbase]# jps.sh
================        root@systemhub511 All Processes         ===========
31267 sun.tools.jps.Jps
5604 org.apache.zookeeper.server.quorum.QuorumPeerMain
7413 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer
28357 org.apache.hadoop.hbase.master.HMaster
5974 org.apache.hadoop.hdfs.server.namenode.NameNode
6182 org.apache.hadoop.hdfs.server.datanode.DataNode
28539 org.apache.hadoop.hbase.regionserver.HRegionServer
7277 org.apache.hadoop.yarn.server.nodemanager.NodeManager
================        root@systemhub611 All Processes         ===========
7603 org.apache.hadoop.yarn.server.nodemanager.NodeManager
6437 org.apache.hadoop.hdfs.server.datanode.DataNode
6101 org.apache.zookeeper.server.quorum.QuorumPeerMain
7450 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
27981 org.apache.hadoop.hbase.regionserver.HRegionServer
28110 org.apache.hadoop.hbase.master.HMaster
30718 sun.tools.jps.Jps
================        root@systemhub711 All Processes         ===========
7665 org.apache.hadoop.yarn.server.nodemanager.NodeManager
7170 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
6210 org.apache.zookeeper.server.quorum.QuorumPeerMain
28345 org.apache.hadoop.hbase.regionserver.HRegionServer
30941 sun.tools.jps.Jps
6543 org.apache.hadoop.hdfs.server.datanode.DataNode
[root@systemhub511 hbase]# 
```
### 7.2 Hadoop é€šç”¨æ€§ä¼˜åŒ–
> 1.NameNodeå…ƒæ•°æ®å¤‡ä»½ä½¿ç”¨SSD.
> 
> 2.å®šæ—¶å¤‡ä»½NameNodeä¸Šå…ƒæ•°æ®æ¯å°æ—¶æˆ–è€…æ¯å¤©å¤‡ä»½,å¦‚æœæ•°æ®æå…¶é‡è¦,å¯ä»¥5~10åˆ†é’Ÿå¤‡ä»½ä¸€æ¬¡,å¤‡ä»½å¯ä»¥é€šè¿‡å®šæ—¶ä»»åŠ¡å¤åˆ¶å…ƒæ•°æ®ç›®å½•å³å¯.
> 
> 3.ä¸ºNameNodeæŒ‡å®šå¤šä¸ªå…ƒæ•°æ®ç›®å½•ä½¿ç”¨`dfs.name.dir`æˆ–è€…`dfs.namenode.name.dir`æŒ‡å®š,è¿™æ ·å¯ä»¥æä¾›å…ƒæ•°æ®çš„å†—ä½™å’Œå¥å£®æ€§,ä»¥å…å‘ç”Ÿæ•…éšœ.
> 
> 4.NameNodeçš„dirè‡ªæ¢å¤è®¾ç½®`dfs.namenode.name.dir.restore`ä¸ºtrue,å…è®¸å°è¯•æ¢å¤ä¹‹å‰å¤±è´¥çš„`dfs.namenode.name.dir`ç›®å½•,åœ¨åˆ›å»ºcheckpointæ—¶åšæ­¤å°è¯•,å¦‚æœè®¾ç½®äº†å¤šä¸ªç£ç›˜,å»ºè®®å…è®¸.
> 
> 5.HDFSä¿è¯RPCè°ƒç”¨ä¼šæœ‰è¾ƒå¤šçº¿ç¨‹æ•°.
- `hdfs-site.xm`
```
å±æ€§ : dfs.namenode.handler.count

è§£é‡Š : è¯¥å±æ€§æ˜¯NameNodeæœåŠ¡é»˜è®¤çº¿ç¨‹æ•°,é»˜è®¤å€¼æ˜¯10,æ ¹æ®æœºå™¨å¯ç”¨å†…å­˜å¯ä»¥è°ƒæ•´ä¸º50~100

å±æ€§ : dfs.datanode.handler.count

è§£é‡Š : è¯¥å±æ€§é»˜è®¤å€¼ä¸º10,æ˜¯DataNodeçš„å¤„ç†çº¿ç¨‹æ•°,å¦‚æœHDFSå®¢æˆ·ç«¯ç¨‹åºè¯»å†™è¯·æ±‚æ¯”è¾ƒå¤š,å¯ä»¥è°ƒé«˜åˆ°15~20,
è®¾ç½®çš„å€¼è¶Šå¤§å†…å­˜æ¶ˆè€—è¶Šå¤š,ä¸è¦è°ƒæ•´çš„è¿‡é«˜,ä¸€èˆ¬ä¸šåŠ¡ä¸­5~10å³å¯
```
> 6.HDFSå‰¯æœ¬æ•°è°ƒæ•´
- `hdfs-site.xml`
```
å±æ€§ : dfs.replication
è§£é‡Š : å¦‚æœæ•°æ®é‡å·¨å¤§,ä¸”ä¸æ˜¯éå¸¸ä¹‹é‡è¦å¯ä»¥è°ƒæ•´ä¸º2~3,å¦‚æœæ•°æ®éå¸¸ä¹‹é‡è¦å¯ä»¥è°ƒæ•´ä¸º3~5
```
> 7.HDFSæ–‡ä»¶å—å¤§å°è°ƒæ•´
- `hdfs-site.xml`
```
å±æ€§ : dfs.blocksize

è§£é‡Š : å—å¤§å°å®šä¹‰,è¯¥å±æ€§åº”è¯¥æ ¹æ®å­˜å‚¨çš„å¤§é‡çš„å•ä¸ªæ–‡ä»¶å¤§å°æ¥è®¾ç½®,å¦‚æœå¤§é‡çš„å•ä¸ªæ–‡ä»¶éƒ½å°äº100M,å»ºè®®è®¾ç½®æˆ64Må—å¤§å°,
å¯¹äºå¤§äº100Mæˆ–è€…è¾¾åˆ°GBçš„æƒ…å†µ,å»ºè®®è®¾ç½®æˆ256M,ä¸€èˆ¬è®¾ç½®èŒƒå›´æ³¢åŠ¨åœ¨64M~256Mä¹‹é—´
```
> 8.MapReduce Jobä»»åŠ¡æœåŠ¡çº¿ç¨‹æ•°è°ƒæ•´
- `mapred-site.xml`
```
å±æ€§ : mapreduce.jobtracker.handler.count
è§£é‡Š : è¯¥å±æ€§æ˜¯Jobä»»åŠ¡çº¿ç¨‹æ•°,é»˜è®¤å€¼æ˜¯10,æ ¹æ®æœºå™¨çš„å¯ç”¨å†…å­˜å¯ä»¥è°ƒæ•´ä¸º50~100
```
> 9.HttpæœåŠ¡å™¨å·¥ä½œçº¿ç¨‹æ•°
- `mapred-site.xml`
```
å±æ€§ : mapreduce.tasktracker.http.threads
è§£é‡Š : å®šä¹‰HTTPæœåŠ¡å™¨å·¥ä½œçº¿ç¨‹æ•°,é»˜è®¤å€¼ä¸º40,å¯¹äºå¤§é›†ç¾¤å¯ä»¥è°ƒæ•´åˆ°80~100
```
> 10.æ–‡ä»¶æ’åºåˆå¹¶ä¼˜åŒ–
- `mapred-site.xml`
```
å±æ€§ : mapreduce.task.io.sort.factor

è§£é‡Š : æ–‡ä»¶æ’åºæ—¶åŒæ—¶åˆå¹¶çš„æ•°æ®æµçš„æ•°é‡,è¿™ä¹Ÿå®šä¹‰äº†åŒæ—¶æ‰“å¼€æ–‡ä»¶çš„ä¸ªæ•°,
é»˜è®¤å€¼ä¸º10,å¦‚æœè°ƒé«˜è¯¥å‚æ•°ï¼Œå¯ä»¥æ˜æ˜¾å‡å°‘ç£ç›˜IOï¼Œå³å‡å°‘æ–‡ä»¶è¯»å–çš„æ¬¡æ•°
```
> 11.è®¾ç½®ä»»åŠ¡å¹¶å‘
- `mapred-site.xml`
```
å±æ€§ : mapreduce.map.speculative

è§£é‡Š : è¯¥å±æ€§å¯ä»¥è®¾ç½®ä»»åŠ¡æ˜¯å¦å¯ä»¥å¹¶å‘æ‰§è¡Œ,å¦‚æœä»»åŠ¡å¤šè€Œå°,
è¯¥å±æ€§è®¾ç½®ä¸ºtrueå¯ä»¥æ˜æ˜¾åŠ å¿«ä»»åŠ¡æ‰§è¡Œæ•ˆç‡,ä½†æ˜¯å¯¹äºå»¶è¿Ÿéå¸¸é«˜çš„ä»»åŠ¡,å»ºè®®æ”¹ä¸ºfalse,è¿™å°±ç±»ä¼¼äºè¿…é›·ä¸‹è½½
```
> 12.MRè¾“å‡ºæ•°æ®å‹ç¼©
- `mapred-site.xml`
```
å±æ€§ : 
mapreduce.map.output.compress
mapreduce.output.fileoutputformat.compress

è§£é‡Š : å¯¹äºå¤§é›†ç¾¤è€Œè¨€,å»ºè®®è®¾ç½®Map-Reduceè¾“å‡ºä¸ºå‹ç¼©æ•°æ®,è€Œå¯¹äºå°é›†ç¾¤,åˆ™ä¸éœ€è¦
```
> 13.ä¼˜åŒ–Mapperå’ŒReducerä¸ªæ•°
- `mapred-site.xml`
```
å±æ€§ :
mapreduce.tasktracker.map.tasks.maximum
mapreduce.tasktracker.reduce.tasks.maximum

è§£é‡Š : ä»¥ä¸Šä¸¤ä¸ªå±æ€§åˆ†åˆ«ä¸ºä¸€ä¸ªå•ç‹¬çš„Jobä»»åŠ¡å¯ä»¥åŒæ—¶è¿è¡Œçš„Mapå’ŒReduceçš„æ•°é‡.
è®¾ç½®ä¸Šé¢ä¸¤ä¸ªå‚æ•°æ—¶,éœ€è¦è€ƒè™‘CPUæ ¸æ•°ã€ç£ç›˜å’Œå†…å­˜å®¹é‡,å‡è®¾ä¸€ä¸ª8æ ¸çš„CPU,ä¸šåŠ¡å†…å®¹éå¸¸æ¶ˆè€—CPU,é‚£ä¹ˆå¯ä»¥è®¾ç½®mapæ•°é‡ä¸º4,
å¦‚æœè¯¥ä¸šåŠ¡ä¸æ˜¯ç‰¹åˆ«æ¶ˆè€—CPUç±»å‹,é‚£ä¹ˆå¯ä»¥è®¾ç½®mapæ•°é‡ä¸º40,reduceæ•°é‡ä¸º20,è¿™äº›å‚æ•°çš„å€¼ä¿®æ”¹å®Œæˆä¹‹å,ä¸€å®šè¦è§‚å¯Ÿæ˜¯å¦æœ‰è¾ƒé•¿ç­‰å¾…çš„ä»»åŠ¡,
å¦‚æœæœ‰çš„è¯,å¯ä»¥å‡å°‘æ•°é‡ä»¥åŠ å¿«ä»»åŠ¡æ‰§è¡Œ,å¦‚æœè®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„å€¼,ä¼šå¼•èµ·å¤§é‡çš„ä¸Šä¸‹æ–‡åˆ‡æ¢,ä»¥åŠå†…å­˜ä¸ç£ç›˜ä¹‹é—´çš„æ•°æ®äº¤æ¢,è¿™é‡Œæ²¡æœ‰æ ‡å‡†çš„é…ç½®æ•°å€¼,
éœ€è¦æ ¹æ®ä¸šåŠ¡å’Œç¡¬ä»¶é…ç½®ä»¥åŠç»éªŒæ¥åšå‡ºé€‰æ‹©,åœ¨åŒä¸€æ—¶åˆ»ä¸è¦åŒæ—¶è¿è¡Œå¤ªå¤šçš„MapReduce,è¿™æ ·ä¼šæ¶ˆè€—è¿‡å¤šçš„å†…å­˜,ä»»åŠ¡ä¼šæ‰§è¡Œçš„éå¸¸ç¼“æ…¢,éœ€è¦æ ¹æ®CPUæ ¸æ•°,
å†…å­˜å®¹é‡è®¾ç½®ä¸€ä¸ªMRä»»åŠ¡å¹¶å‘çš„æœ€å¤§å€¼,ä½¿å›ºå®šæ•°æ®é‡çš„ä»»åŠ¡å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­,é¿å…é¢‘ç¹çš„å†…å­˜å’Œç£ç›˜æ•°æ®äº¤æ¢,ä»è€Œé™ä½ç£ç›˜IO,æé«˜æ€§èƒ½
```
- ä¼°ç®—å…¬å¼ :
```
map=2 +â…”cpu_core

reduce=2 +â…“cpu_core
```
### 7.3 Linuxä¼˜åŒ–
> 1.å¼€å¯æ–‡ä»¶ç³»ç»Ÿé¢„è¯»ç¼“å­˜å¯ä»¥æé«˜è¯»å–é€Ÿåº¦
```
[root@systemhub511 ~]# sudo blockdev --setra 32768 /dev/sda
```
> 2.å…³é—­è¿›ç¨‹ç¡çœ æ± 
> å³ä¸å…è®¸åå°è¿›ç¨‹è¿›å…¥ç¡çœ çŠ¶æ€,å¦‚æœè¿›ç¨‹ç©ºé—²,åˆ™ç›´æ¥killæ‰é‡Šæ”¾èµ„æº
```
[root@systemhub511 ~]# sudo sysctl -w vm.swappiness=0
vm.swappiness = 0
[root@systemhub511 ~]# 
```
> 3.è°ƒæ•´ulimitä¸Šé™,é»˜è®¤å€¼ä¸ºæ¯”è¾ƒå°æ•°å­—
```
ulimit -n æŸ¥çœ‹å…è®¸æœ€å¤§è¿›ç¨‹æ•°

ulimit -u æŸ¥çœ‹å…è®¸æ‰“å¼€æœ€å¤§æ–‡ä»¶æ•°
```
> 4.å¼€å¯é›†ç¾¤æ—¶é—´åŒæ­¥NTP
> é›†ç¾¤ä¸­æŸå°æœºå™¨åŒæ­¥ç½‘ç»œæ—¶é—´æœåŠ¡å™¨æ—¶é—´,é›†ç¾¤ä¸­å…¶ä»–æœºå™¨åˆ™åŒæ­¥è¿™å°æœºå™¨çš„æ—¶é—´
> 
> 5.æ›´æ–°ç³»ç»Ÿè¡¥ä¸
> æ›´æ–°è¡¥ä¸å‰,è¯·å…ˆæµ‹è¯•æ–°ç‰ˆæœ¬è¡¥ä¸å¯¹é›†ç¾¤èŠ‚ç‚¹å…¼å®¹æ€§

### 7.4 Zookeeperä¼˜åŒ–
> 1.ä¼˜åŒ–Zookeeperä¼šè¯è¶…æ—¶æ—¶é—´
- `hbase-site.xml`
```
å‚æ•° : zookeeper.session.timeout

è§£é‡Šï¼šIn hbase-site.xml,set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).
è¯¥å€¼ä¼šç›´æ¥å…³ç³»åˆ°masterå‘ç°æœåŠ¡å™¨å®•æœºçš„æœ€å¤§å‘¨æœŸ,é»˜è®¤å€¼ä¸º30ç§’,å¦‚æœè¯¥å€¼è¿‡å°ä¼šåœ¨HBaseåœ¨å†™å…¥å¤§é‡æ•°æ®å‘ç”Ÿè€ŒGCæ—¶,å¯¼è‡´RegionServerçŸ­æš‚çš„ä¸å¯ç”¨,ä»è€Œæ²¡æœ‰å‘ZKå‘é€å¿ƒè·³åŒ…,æœ€ç»ˆå¯¼è‡´è®¤ä¸ºä»èŠ‚ç‚¹shutdown,ä¸€èˆ¬20å°å·¦å³çš„é›†ç¾¤éœ€è¦é…ç½®5å°zookeeper
```

### 7.5 é¢„åˆ†åŒº
- æ¯ä¸€ä¸ªregionç»´æŠ¤ç€startRowä¸endRowKey,å¦‚æœåŠ å…¥æ•°æ®ç¬¦åˆæŸä¸ªregionç»´æŠ¤rowKeyèŒƒå›´,åˆ™è¯¥æ•°æ®äº¤ç»™è¿™ä¸ªregionç»´æŠ¤,é‚£ä¹ˆä¾ç…§è¿™ä¸ªåŸåˆ™,å¯ä»¥å°†æ•°æ®ç´¢è¦æŠ•æ”¾åˆ†åŒºæå‰å¤§è‡´çš„è§„åˆ’å¥½,ä»¥æé«˜HBaseæ€§èƒ½.
- 1.æ‰‹åŠ¨è®¾å®šé¢„åˆ†åŒº
```
hbase(main):001:0> create 'test004','info','partition1',SPLITS => ['1000','2000','3000','4000']
0 row(s) in 2.8950 seconds

=> Hbase::Table - test004
hbase(main):002:0> 
```
- 2.ç”Ÿæˆ16è¿›åˆ¶åºåˆ—é¢„åˆ†åŒº
```
hbase(main):003:0> create 'test005','info','partition2',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
0 row(s) in 2.2910 seconds

=> Hbase::Table - test005
hbase(main):004:0>
```
- 3.æŒ‰ç…§æ–‡ä»¶ä¸­è®¾ç½®çš„è§„åˆ™é¢„åˆ†åŒº
- vim `splits.txt`
```
AAAA
BBBB
CCCC
DDDD
```
- æŒ‰ç…§æ–‡ä»¶é¢„åˆ†åŒº
```
hbase(main):004:0> create 'test006','partition3',SPLITS_FILE => 'splits.txt'
0 row(s) in 1.2800 seconds

=> Hbase::Table - test006
hbase(main):005:0> 
```
- 4.ä½¿ç”¨JavaAPIåˆ›å»ºé¢„åˆ†åŒº
```
// è‡ªå®šä¹‰ç®—æ³•,äº§ç”Ÿä¸€ç³»åˆ—Hashæ•£åˆ—å€¼å­˜å‚¨åœ¨äºŒç»´æ•°ç»„ä¸­
byte[][] splitKeys = æŸä¸ªæ•£åˆ—å€¼å‡½æ•°
// åˆ›å»ºHBaseAdminå®ä¾‹
HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());
// åˆ›å»ºHTableDescriptorå®ä¾‹
HTableDescriptor tableDesc = new HTableDescriptor(tableName);
// é€šè¿‡HTableDescriptorå®ä¾‹å’Œæ•£åˆ—å€¼äºŒç»´æ•°ç»„åˆ›å»ºå¸¦æœ‰é¢„åˆ†åŒºHBaseè¡¨
hAdmin.createTable(tableDesc, splitKeys);
```


### 7.6 RowKeyè®¾è®¡
- ä¸€æ¡æ•°æ®çš„å”¯ä¸€æ ‡è¯†å°±æ˜¯rowkey,é‚£ä¹ˆè¿™æ¡æ•°æ®å­˜å‚¨äºå“ªä¸ªåˆ†åŒº,å–å†³äºrowkeyå¤„äºå“ªä¸ªä¸€ä¸ªé¢„åˆ†åŒºçš„åŒºé—´å†…,è®¾è®¡rowkeyä¸»è¦ç›®çš„å°±æ˜¯è®©æ•°æ®å‡åŒ€åˆ†å¸ƒäºæ‰€æœ‰regionä¸­,åœ¨ä¸€å®šç¨‹åº¦ä¸Šé˜²æ­¢æ•°æ®å€¾æ–œ.
- 1.`ç”Ÿæˆéšæœºæ•° / hash / æ•£åˆ—å€¼`
```
åŸrowKeyä¸º1001,SHA1åï¼šdd01903921ea24941c26a48f2cec24e0bb0e8cc7
åŸrowKeyä¸º3001,SHA1åï¼š49042c54de64a1e9bf0b33e00245660ef92dc7bd
åŸrowKeyä¸º5001,SHA1åï¼š7b61dec07e02c188790670af43e717f0f46e8913
åœ¨åšæ­¤æ“ä½œä¹‹å‰,ä¸€èˆ¬ä¼šé€‰æ‹©ä»æ•°æ®é›†ä¸­æŠ½å–æ ·æœ¬,æ¥å†³å®šä»€ä¹ˆæ ·çš„rowKeyæ¥Hashåä½œä¸ºæ¯ä¸ªåˆ†åŒºçš„=ä¸´ç•Œå€¼
```
- 2.`å­—ç¬¦ä¸²åè½¬`
```
20100524000001è½¬æˆ10000042507102
20100524000002è½¬æˆ20000042507102
```
- 3.`å­—ç¬¦ä¸²æ‹¼æ¥`
```
20100524000001_a12e
20100524000001_93i7
```


### 7.7 å†…å­˜ä¼˜åŒ–
- HBaseæ“ä½œè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡çš„å†…å­˜å¼€é”€,æ¯•ç«ŸTableæ˜¯å¯ä»¥ç¼“å­˜åœ¨å†…å­˜ä¸­,ä¸€èˆ¬ä¼šåˆ†é…æ•´ä¸ªå¯ç”¨å†…å­˜70%ç»™HBaseçš„Javaå †,ä½†æ˜¯ä¸å»ºè®®åˆ†é…éå¸¸å¤§å †å†…å­˜,å› ä¸ºGCè¿‡ç¨‹æŒç»­å¤ªä¹…ä¼šå¯¼è‡´RegionServerå¤„äºé•¿æœŸä¸å¯ç”¨çŠ¶æ€,ä¸€èˆ¬16~48Gå†…å­˜å³å¯,å¦‚æœå› ä¸ºæ¡†æ¶å ç”¨å†…å­˜è¿‡é«˜å¯¼è‡´ç³»ç»Ÿå†…å­˜ä¸è¶³,æ¡†æ¶ä¸€æ ·ä¼šè¢«ç³»ç»ŸæœåŠ¡æ‹–æ­».


### 7.8 åŸºç¡€ä¼˜åŒ–
> 1.å…è®¸åœ¨HDFSçš„æ–‡ä»¶ä¸­è¿½åŠ å†…å®¹ | `hdfs-site.xml` / `hbase-site.xml`
```
å±æ€§ : dfs.support.append
è§£é‡Š : å¼€å¯HDFSè¿½åŠ åŒæ­¥,å¯ä»¥ä¼˜ç§€é…åˆHBaseæ•°æ®åŒæ­¥å’ŒæŒä¹…åŒ–,é»˜è®¤å€¼ä¸ºtrue
```

> 2.ä¼˜åŒ–DataNodeå…è®¸æœ€å¤§æ–‡ä»¶æ‰“å¼€æ•° | `hdfs-site.xml`
```
å±æ€§ : dfs.datanode.max.transfer.threads
è§£é‡Š : HBaseä¸€èˆ¬éƒ½ä¼šåŒä¸€æ—¶é—´æ“ä½œå¤§é‡çš„æ–‡ä»¶,æ ¹æ®é›†ç¾¤æ•°é‡å’Œè§„æ¨¡ä»¥åŠæ•°æ®åŠ¨ä½œ,è®¾ç½®ä¸º4096æˆ–è€…æ›´é«˜,é»˜è®¤å€¼:4096
```
> 3.ä¼˜åŒ–å»¶è¿Ÿé«˜æ•°æ®æ“ä½œç­‰å¾…æ—¶é—´ | `hdfs-site.xm`
```
å±æ€§ : dfs.image.transfer.timeout
è§£é‡Š : å¦‚æœå¯¹äºæŸä¸€æ¬¡æ•°æ®æ“ä½œæ¥è®²,å»¶è¿Ÿéå¸¸é«˜,socketéœ€è¦ç­‰å¾…æ›´é•¿æ—¶é—´,
å»ºè®®æŠŠè¯¥å€¼è®¾ç½®ä¸ºæ›´å¤§çš„å€¼(é»˜è®¤60000æ¯«ç§’),ä»¥ç¡®ä¿socketä¸ä¼šè¢«timeoutæ‰
```

> 4.ä¼˜åŒ–æ•°æ®å†™å…¥æ•ˆç‡ | `mapred-site.xml`
```
å±æ€§ : 
mapreduce.map.output.compress
mapreduce.map.output.compress.codec

è§£é‡Š : å¼€å¯è¿™ä¸¤ä¸ªæ•°æ®å¯ä»¥å¤§å¤§æé«˜æ–‡ä»¶çš„å†™å…¥æ•ˆç‡,å‡å°‘å†™å…¥æ—¶é—´,
ç¬¬ä¸€ä¸ªå±æ€§å€¼ä¿®æ”¹ä¸ºtrue,ç¬¬äºŒä¸ªå±æ€§å€¼ä¿®æ”¹ä¸º:org.apache.hadoop.io.compress.GzipCodecæˆ–è€…å…¶ä»–å‹ç¼©æ–¹å¼
```
> 5.ä¼˜åŒ–DataNodeå­˜å‚¨
```
å±æ€§ : dfs.datanode.failed.volumes.tolerated
è§£é‡Š : é»˜è®¤ä¸º0,æ„æ€æ˜¯å½“DataNodeä¸­æœ‰ä¸€ä¸ªç£ç›˜å‡ºç°æ•…éšœ,åˆ™ä¼šè®¤ä¸ºè¯¥DataNodeshutdownäº†,
å¦‚æœä¿®æ”¹ä¸º1,åˆ™ä¸€ä¸ªç£ç›˜å‡ºç°æ•…éšœæ—¶,æ•°æ®ä¼šè¢«å¤åˆ¶åˆ°å…¶ä»–æ­£å¸¸çš„DataNodeä¸Š,å½“å‰DataNodeç»§ç»­å·¥ä½œ
```
> 6.ä¼˜åŒ–HStoreæ–‡ä»¶å¤§å° | `hbase-site.xm`
```
å±æ€§ : hbase.hregion.max.filesize
è§£é‡Š : é»˜è®¤å€¼10737418240(10GB),å¦‚æœéœ€è¦è¿è¡ŒHBaseçš„MRä»»åŠ¡,å¯ä»¥å‡å°æ­¤å€¼,
å› ä¸ºä¸€ä¸ªregionå¯¹åº”ä¸€ä¸ªmapä»»åŠ¡,å¦‚æœå•ä¸ªregionè¿‡å¤§,ä¼šå¯¼è‡´mapä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿,
è¯¥å€¼çš„æ„æ€æ˜¯,å¦‚æœHFileå¤§å°è¾¾åˆ°è¿™ä¸ªæ•°å€¼,åˆ™è¿™ä¸ªregionä¼šè¢«åˆ‡åˆ†ä¸ºä¸¤ä¸ªHfile
```
> 7.ä¼˜åŒ–hbaseå®¢æˆ·ç«¯ç¼“å­˜ | `hbase-site.xml`
```
å±æ€§ : hbase.client.write.buffer
è§£é‡Š : ç”¨äºæŒ‡å®šHBaseå®¢æˆ·ç«¯ç¼“å­˜,å¢å¤§è¯¥å€¼å¯ä»¥å‡å°‘RPCè°ƒç”¨æ¬¡æ•°,
ä½†æ˜¯ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜,åä¹‹åˆ™åä¹‹,ä¸€èˆ¬éœ€è¦è®¾å®šä¸€å®šç¼“å­˜å¤§å°,ä»¥è¾¾åˆ°å‡å°‘RPCæ¬¡æ•°çš„ç›®çš„
```

> 8.è®¾ç½®RPCç›‘å¬æ•°é‡ | `hbase-site.xml`
```
å±æ€§ : hbase.regionserver.handler.count
è§£é‡Š : é»˜è®¤å€¼ä¸º30,ç”¨äºæŒ‡å®šRPCç›‘å¬æ•°é‡,
å¯ä»¥æ ¹æ®å®¢æˆ·ç«¯è¯·æ±‚æ•°è¿›è¡Œè°ƒæ•´,è¯»å†™è¯·æ±‚è¾ƒå¤šæ—¶,å¢åŠ æ­¤å€¼
```

> 9.æŒ‡å®šscan.nextæ‰«æHBaseæ‰€è·å–è¡Œæ•° | `hbase-site.xml`
```
å±æ€§ : hbase.client.scanner.caching
è§£é‡Š : ç”¨äºæŒ‡å®šscan.nextæ–¹æ³•è·å–é»˜è®¤è¡Œæ•°,å€¼è¶Šå¤§æ¶ˆè€—å†…å­˜è¶Šå¤§
```

> 10.`flush` / `compact` / `splitæœºåˆ¶`
> 
> å½“MemStoreè¾¾åˆ°é˜ˆå€¼,å°†Memstoreä¸­æ•°æ®Flushè¿›Storefile,
> compactæœºåˆ¶åˆ™æ˜¯æŠŠflushå‡ºæ¥çš„å°æ–‡ä»¶åˆå¹¶æˆStorefileå¤§æ–‡ä»¶,
> splitåˆ™æ˜¯å½“Regionè¾¾åˆ°é˜ˆå€¼,ä¼šæŠŠè¿‡å¤§çš„Regionä¸€åˆ†ä¸ºäºŒ.
> 
> æ¶‰åŠå±æ€§
> 128Må°±æ˜¯Memstoreé»˜è®¤é˜ˆå€¼
```
hbase.hregion.memstore.flush.sizeï¼š134217728
```
> è¿™ä¸ªå‚æ•°ä½œç”¨æ˜¯å½“å•ä¸ªHRegionå†…æ‰€æœ‰Memstoreå¤§å°æ€»å’Œè¶…è¿‡æŒ‡å®šå€¼æ—¶,flushè¯¥HRegionæ‰€æœ‰memstore,RegionServerçš„flushæ˜¯é€šè¿‡å°†è¯·æ±‚æ·»åŠ ä¸€ä¸ªé˜Ÿåˆ—,æ¨¡æ‹Ÿç”Ÿäº§æ¶ˆè´¹æ¨¡å‹æ¥å¼‚æ­¥å¤„ç†,å½“é˜Ÿåˆ—æ¥ä¸åŠæ¶ˆè´¹,äº§ç”Ÿå¤§é‡ç§¯å‹è¯·æ±‚æ—¶,å¯èƒ½ä¼šå¯¼è‡´å†…å­˜é™¡å¢,æœ€åæƒ…å†µæ˜¯è§¦å‘OOM.
```
hbase.regionserver.global.memstore.upperLimitï¼š0.4
hbase.regionserver.global.memstore.lowerLimitï¼š0.38
```
> å½“MemStoreä½¿ç”¨å†…å­˜æ€»é‡è¾¾åˆ°`hbase.regionserver.global.memstore.upperLimit`æŒ‡å®šå€¼æ—¶,å°†ä¼šæœ‰å¤šä¸ªMemStores  flushåˆ°æ–‡ä»¶ä¸­,MemStore  flush é¡ºåºæ˜¯æŒ‰ç…§å¤§å°é™åºæ‰§è¡Œ,ç›´åˆ°åˆ·æ–°åˆ°MemStoreä½¿ç”¨å†…å­˜ç•¥å°äºlowerLimit


## 8. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------