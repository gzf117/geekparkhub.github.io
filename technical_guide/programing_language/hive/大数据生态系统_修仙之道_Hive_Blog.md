	
# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Hive Blog

@(2019-03-18)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Language:Hive|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

##  ğŸ˜ Hive Technology ä¿®ä»™ä¹‹é“ ç‚¼æ°”åŒ–ç¥ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/hive.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------

[TOC]


## 1. Hive åŸºæœ¬æ¦‚è¿°
### 1.1 ğŸ¤” ä»€ä¹ˆæ˜¯Hive ğŸ¤”
> Hive ç”±FaceBookå¼€æºå¹¶ç”¨äºè§£å†³æµ·é‡ç»“æ„åŒ–æ—¥å¿—çš„æ•°æ®ç»Ÿè®¡åˆ†æ.
> 
> Hiveæ˜¯åŸºäºHadoopçš„ä¸€ä¸ªæ•°æ®ä»“åº“å·¥å…·,å¯ä»¥å°†ç»“æ„åŒ–çš„æ•°æ®æ–‡ä»¶æ˜ å°„ä¸ºä¸€å¼ æ•°æ®è¡¨,å¹¶æä¾›ç±»SQLæŸ¥è¯¢åŠŸèƒ½.
> 
> æœ¬è´¨æ˜¯: å°†HQLè½¬åŒ–æˆMapReduceç¨‹åº.
> ![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_001.jpg)
> 
> 1.Hiveå¤„ç†çš„æ•°æ®å­˜å‚¨åœ¨HDFS.
> 
> 2.Hiveåˆ†ææ•°æ®åº•å±‚çš„å®ç°æ˜¯MapReduce.
> 
> 3.æ‰§è¡Œç¨‹åºè¿è¡Œåœ¨Yarnä¸Š.

### 1.1.2 Hive ç‰¹æ€§
> Hiveä½œä¸ºæ•°æ®ä»“åº“è½¯ä»¶,ä½¿ç”¨ç±»SQLçš„HiveQLè¯­è¨€å®ç°æ•°æ®æŸ¥è¯¢,æ‰€æœ‰Hiveæ•°æ®å‡å­˜å‚¨åœ¨Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿä¸­,Hiveå…·æœ‰ä»¥ä¸‹ç‰¹å¾.
> 
> ä½¿ç”¨HiveQLä»¥ç±»ä¼¼SQLæŸ¥è¯¢æ–¹å¼è½»æ¾è®¿é—®æ•°æ®,å°†HQLæŸ¥è¯¢è½¬æ¢ä¸ºMapReduceçš„ä»»åŠ¡åœ¨Hadoopé›†ç¾¤ä¸Šæ‰§è¡Œ,å®ŒæˆETL(æå– / è½¬æ¢ / åŠ è½½ / æŠ¥è¡¨ / æ•°æ®åˆ†æ)ç­‰æ•°æ®ä»“åº“ä»»åŠ¡.
> 
> å¤šç§æ–‡ä»¶æ ¼å¼çš„å…ƒæ•°æ®æœåŠ¡,åŒ…æ‹¬TextFile / SequuenceFile / RCFile / ORCFile,å…¶ä¸­é»˜è®¤æ ¼å¼ä¸ºTextFile.
> 
> ç›´æ¥è®¿é—®HDFSæ–‡ä»¶æˆ–å…¶ä»–æ•°æ®å­˜å‚¨ç³»ç»Ÿ(å¦‚: HBase)ä¸­çš„æ–‡ä»¶.
> 
> æ”¯æŒMapReduce / Teza / Spark ç­‰å¤šç§è®¡ç®—å¼•æ“,å¼€å‘è€…å¯æ ¹æ®ä¸åŒçš„æ•°æ®å¤„ç†åœºæ™¯é€‰æ‹©åˆé€‚çš„è®¡ç®—å¼•æ“.
> 
> æ”¯æŒHPL/SQLç¨‹åºè¯­è¨€,HPL/SQLæ˜¯ä¸€ç§æ··åˆå¼‚æ„çš„è¯­è¨€,å¯ä»¥ç†è§£å‡ ä¹ä»»ä½•ç°æœ‰çš„è¿‡ç¨‹æ€§SQLè¯­è¨€çš„è¯­æ³•å’Œè¯­ä¹‰,æœ‰åŠ©äºå°†ä¼ ç»Ÿæ•°æ®ä»“åº“çš„ä¸šåŠ¡é€»è¾‘è¿ç§»åˆ°Hadoopä¸Š,åœ¨Hadoopä¸Šå®ç°ETLæµç¨‹çš„æœ‰æ•ˆæ–¹å¼.
> 
> å¯ä»¥é€šè¿‡HiveLLAP,Yarnè¿›è¡Œç§’çº§åˆ«çš„æŸ¥è¯¢æ£€ç´¢,LLAPç»“åˆäº†æŒä¹…æŸ¥è¯¢æœåŠ¡å™¨å’Œä¼˜åŒ–çš„å†…å­˜ç¼“å­˜,ä½¿Hiveèƒ½å¤Ÿç«‹å³å¯åŠ¨æŸ¥è¯¢,é¿å…ä¸å¿…è¦çš„ç£ç›˜å¼€é”€,æä¾›è¾ƒä½³çš„æŸ¥è¯¢æ£€ç´¢æ•ˆç‡.


### 1.1.3 Hive åº”ç”¨åœºæ™¯
> Hiveæ„å»ºåœ¨Hadoopæ–‡ä»¶ç³»ç»Ÿä¹‹ä¸Š,Hiveä¸æä¾›å®æ—¶çš„æŸ¥è¯¢å’ŒåŸºäºè¡Œçº§æ•°æ®çš„æ›´æ–°æ“ä½œ,ä¸é€‚åˆéœ€è¦ä½å»¶æ—¶ä½œç”¨çš„åº”ç”¨,å¦‚è”æœºäº‹åŠ¡å¤„ç†ç›¸å…³åº”ç”¨.
| ç±»åˆ«       | å…·ä½“åº”ç”¨åœºæ™¯ |
| :-------- | :--------:|
| æ•°æ®æŒ–æ˜    |   ç”¨æˆ·è¡Œä¸ºåˆ†æ / å…´è¶£åˆ†åŒº / åŒºåŸŸå±•ç¤º |
| éå®æ—¶åˆ†ææŒ–æ˜    |   æ—¥å¿—åˆ†æ / æ–‡æœ¬åˆ†æ |
| æ•°æ®æ±‡æ€»   |   ç”¨æˆ·ç‚¹å‡»é‡ç»Ÿè®¡ / æµé‡ç»Ÿè®¡  |
| æ•°æ®ä»“åº“   |   æ•°æ®æŠ½å– / æ•°æ®åŠ è½½ / æ•°æ®è½¬æ¢ |

### 1.2 HIve ä¼˜ç¼ºç‚¹
#### ä¼˜ç‚¹
> 1.æ“ä½œæ¥å£é‡‡ç”¨ç±»SQLè¯­æ³•,æä¾›å¿«é€Ÿå¼€å‘çš„èƒ½åŠ›(ç®€å•å®¹æ˜“ä¸Šæ‰‹).
> 2.é¿å…äº†å»å†™MapReduce,å‡å°‘å¼€å‘äººå‘˜çš„å­¦ä¹ æˆæœ¬.
> 3.Hiveçš„æ‰§è¡Œå»¶è¿Ÿæ¯”è¾ƒé«˜,å› æ­¤Hiveå¸¸ç”¨äºæ•°æ®åˆ†æ,å¯¹å®æ—¶æ€§è¦æ±‚ä¸é«˜çš„åœºåˆ.
> 4.Hiveä¼˜åŠ¿åœ¨äºå¤„ç†å¤§æ•°æ®,å¯¹äºå¤„ç†å°æ•°æ®æ²¡æœ‰ä¼˜åŠ¿,å› ä¸ºHiveçš„æ‰§è¡Œå»¶è¿Ÿæ¯”è¾ƒé«˜.
> 5.Hiveæ”¯æŒå¼€å‘è€…è‡ªå®šä¹‰å‡½æ•°,å¼€å‘è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚æ¥å®ç°è‡ªå·±çš„å‡½æ•°.
#### ç¼ºç‚¹
> 1.Hiveçš„HQLè¡¨è¾¾èƒ½åŠ›æœ‰é™
> (1) è¿­ä»£å¼ç®—æ³•æ— æ³•è¡¨è¾¾.
> (2) æ•°æ®æŒ–æ˜æ–¹é¢ä¸æ“…é•¿.
> 
> 2.Hiveçš„æ•ˆç‡æ¯”è¾ƒä½
> (1) Hiveè‡ªåŠ¨ç”Ÿæˆçš„MapReduceä½œä¸š,é€šå¸¸æƒ…å†µä¸‹ä¸å¤Ÿæ™ºèƒ½åŒ–.
> (2) Hiveè°ƒä¼˜æ¯”è¾ƒå›°éš¾,ç²’åº¦è¾ƒç²—.

### 1.3 HIve æ¶æ„åŸç† ğŸ¤”ğŸ¤”ğŸ¤”

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_002.jpg)

> Hiveé€šè¿‡ç»™å¼€å‘è€…æä¾›çš„ä¸€ç³»åˆ—äº¤äº’æ¥å£,æ¥æ”¶åˆ°å¼€å‘è€…çš„æŒ‡ä»¤(SQL),ä½¿ç”¨è‡ªå·±çš„Driver,ç»“åˆå…ƒæ•°æ®(MetaStore),å°†è¿™äº›æŒ‡ä»¤ç¿»è¯‘æˆMapReduce,æäº¤åˆ°Hadoopä¸­æ‰§è¡Œ,æœ€å,å°†æ‰§è¡Œè¿”å›çš„ç»“æœè¾“å‡ºåˆ°å¼€å‘è€…äº¤äº’æ¥å£.
> 
#### 1.ç”¨æˆ·æ¥å£: Client
> CLI (Hive Shell) / JDBC/ODBC (Javaè®¿é—®Hive) / WEBUI (æµè§ˆå™¨è®¿é—®Hive).
#### 2.å…ƒæ•°æ®: Metastore
> å…ƒæ•°æ®åŒ…æ‹¬: è¡¨åã€è¡¨æ‰€å±çš„æ•°æ®åº“(é»˜è®¤æ˜¯default)ã€è¡¨çš„æ‹¥æœ‰è€…,åˆ—/åˆ†åŒºå­—æ®µ/è¡¨çš„ç±»å‹(æ˜¯å¦æ˜¯å¤–éƒ¨è¡¨),è¡¨çš„æ•°æ®æ‰€åœ¨ç›®å½•ç­‰.
> 
> é»˜è®¤å­˜å‚¨åœ¨è‡ªå¸¦çš„derbyæ•°æ®åº“ä¸­,æ¨èä½¿ç”¨MySQLå­˜å‚¨Metastore.
#### 3.Hadoop
> ä½¿ç”¨HDFSè¿›è¡Œå­˜å‚¨,ä½¿ç”¨MapReduceè¿›è¡Œè®¡ç®—.
#### 4.é©±åŠ¨å™¨: Driver
> (1) è§£æå™¨(SQL  Parser): å°†SQLå­—ç¬¦ä¸²è½¬æ¢æˆæŠ½è±¡è¯­æ³•æ ‘AST,è¿™ä¸€æ­¥ä¸€èˆ¬éƒ½ç”¨ç¬¬ä¸‰æ–¹å·¥å…·åº“å®Œæˆ,æ¯”å¦‚antlr,å¯¹ASTè¿›è¡Œè¯­æ³•åˆ†æ,æ¯”å¦‚è¡¨æ˜¯å¦å­˜åœ¨ã€å­—æ®µæ˜¯å¦å­˜åœ¨ã€SQLè¯­ä¹‰æ˜¯å¦æœ‰è¯¯.
> 
> (2) ç¼–è¯‘å™¨(Physical Plan): å°†ASTç¼–è¯‘ç”Ÿæˆé€»è¾‘æ‰§è¡Œè®¡åˆ’.
> 
> (3) ä¼˜åŒ–å™¨(Query Optimizer): å¯¹é€»è¾‘æ‰§è¡Œè®¡åˆ’è¿›è¡Œä¼˜åŒ–.
> 
> (4) æ‰§è¡Œå™¨(Execution): æŠŠé€»è¾‘æ‰§è¡Œè®¡åˆ’è½¬æ¢æˆå¯ä»¥è¿è¡Œçš„ç‰©ç†è®¡åˆ’,å¯¹äºHiveæ¥è¯´,å°±æ˜¯MR & Spark.

### 1.4 HIve & æ•°æ®åº“æ¯”è¾ƒ
> ç”±äºHiveé‡‡ç”¨äº†ç±»ä¼¼SQLçš„æŸ¥è¯¢è¯­è¨€HQL(Hive Query Language),å› æ­¤å¾ˆå®¹æ˜“å°†Hiveç†è§£ä¸ºæ•°æ®åº“.
> 
> å…¶å®ä»ç»“æ„ä¸Šæ¥çœ‹,Hiveå’Œæ•°æ®åº“é™¤äº†æ‹¥æœ‰ç±»ä¼¼çš„æŸ¥è¯¢è¯­è¨€,å†æ— ç±»ä¼¼ä¹‹å¤„,æ•°æ®åº“å¯ä»¥ç”¨åœ¨Onlineçš„åº”ç”¨ä¸­,ä½†æ˜¯Hiveæ˜¯ä¸ºæ•°æ®ä»“åº“è€Œè®¾è®¡çš„,æ¸…æ¥šè¿™ä¸€ç‚¹,æœ‰åŠ©äºä»åº”ç”¨è§’åº¦ç†è§£Hiveçš„ç‰¹æ€§.
#### 1.4.1 æŸ¥è¯¢è¯­è¨€
> ç”±äºSQLè¢«å¹¿æ³›çš„åº”ç”¨åœ¨æ•°æ®ä»“åº“ä¸­,å› æ­¤,ä¸“é—¨é’ˆå¯¹Hiveçš„ç‰¹æ€§è®¾è®¡äº†ç±»SQLçš„æŸ¥è¯¢è¯­è¨€HQL,ç†Ÿæ‚‰SQLå¼€å‘çš„å¼€å‘è€…å¯ä»¥å¾ˆæ–¹ä¾¿çš„ä½¿ç”¨Hiveè¿›è¡Œå¼€å‘.
#### 1.4.2 æ•°æ®å­˜å‚¨ä½ç½®
> Hiveæ˜¯å»ºç«‹åœ¨Hadoopä¹‹ä¸Šçš„,æ‰€æœ‰Hiveçš„æ•°æ®éƒ½æ˜¯å­˜å‚¨åœ¨HDFSä¸­çš„,è€Œæ•°æ®åº“åˆ™å¯ä»¥å°†æ•°æ®ä¿å­˜åœ¨å—è®¾å¤‡æˆ–è€…æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­.
#### 1.4.3 æ•°æ®æ›´æ–°
> ç”±äºHiveæ˜¯é’ˆå¯¹æ•°æ®ä»“åº“åº”ç”¨è®¾è®¡çš„,è€Œæ•°æ®ä»“åº“çš„å†…å®¹æ˜¯è¯»å¤šå†™å°‘çš„,å› æ­¤Hiveä¸­ä¸å»ºè®®å¯¹æ•°æ®çš„æ”¹å†™,æ‰€æœ‰çš„æ•°æ®éƒ½æ˜¯åœ¨åŠ è½½çš„æ—¶å€™ç¡®å®šå¥½çš„.
> 
> è€Œæ•°æ®åº“ä¸­çš„æ•°æ®é€šå¸¸æ˜¯éœ€è¦ç»å¸¸è¿›è¡Œä¿®æ”¹çš„,å› æ­¤å¯ä»¥ä½¿ç”¨`INSERTINTO...VALUES`æ·»åŠ æ•°æ®,ä½¿ç”¨`UPDATE...SET`ä¿®æ”¹æ•°æ®.
#### 1.4.4 ç´¢å¼•
> Hiveåœ¨åŠ è½½æ•°æ®çš„è¿‡ç¨‹ä¸­ä¸ä¼šå¯¹æ•°æ®è¿›è¡Œä»»ä½•å¤„ç†,ç”šè‡³ä¸ä¼šå¯¹æ•°æ®è¿›è¡Œæ‰«æ,å› æ­¤ä¹Ÿæ²¡æœ‰å¯¹æ•°æ®ä¸­çš„æŸäº›Keyå»ºç«‹ç´¢å¼•.
> 
> Hiveè¦è®¿é—®æ•°æ®ä¸­æ»¡è¶³æ¡ä»¶çš„ç‰¹å®šå€¼æ—¶,éœ€è¦æš´åŠ›æ‰«ææ•´ä¸ªæ•°æ®,å› æ­¤è®¿é—®å»¶è¿Ÿè¾ƒé«˜.
> 
> ç”±äºMapReduceçš„å¼•å…¥,Hiveå¯ä»¥å¹¶è¡Œè®¿é—®æ•°æ®,å› æ­¤å³ä½¿æ²¡æœ‰ç´¢å¼•,å¯¹äºå¤§æ•°æ®é‡çš„è®¿é—®,Hiveä»ç„¶å¯ä»¥ä½“ç°å‡ºä¼˜åŠ¿.
> 
> æ•°æ®åº“ä¸­,é€šå¸¸ä¼šé’ˆå¯¹ä¸€ä¸ªæˆ–è€…å‡ ä¸ªåˆ—å»ºç«‹ç´¢å¼•,å› æ­¤å¯¹äºå°‘é‡çš„ç‰¹å®šæ¡ä»¶çš„æ•°æ®çš„è®¿é—®,æ•°æ®åº“å¯ä»¥æœ‰å¾ˆé«˜çš„æ•ˆç‡,è¾ƒä½çš„å»¶è¿Ÿ,ç”±äºæ•°æ®çš„è®¿é—®å»¶è¿Ÿè¾ƒé«˜,å†³å®šäº†Hiveä¸é€‚åˆåœ¨çº¿æ•°æ®æŸ¥è¯¢.
#### 1.4.5 æ‰§è¡Œ
> Hiveä¸­å¤§å¤šæ•°æŸ¥è¯¢çš„æ‰§è¡Œæ˜¯é€šè¿‡Hadoopæä¾›çš„MapReduceæ¥å®ç°çš„,è€Œæ•°æ®åº“é€šå¸¸æœ‰è‡ªå·±çš„æ‰§è¡Œå¼•æ“.
#### 1.4.6 æ‰§è¡Œå»¶è¿Ÿ
> Hiveåœ¨æŸ¥è¯¢æ•°æ®çš„æ—¶å€™,ç”±äºæ²¡æœ‰ç´¢å¼•,éœ€è¦æ‰«ææ•´ä¸ªæ•°æ®è¡¨,å› æ­¤å»¶è¿Ÿè¾ƒé«˜,å¦å¤–ä¸€ä¸ªå¯¼è‡´Hiveæ‰§è¡Œå»¶è¿Ÿé«˜çš„å› ç´ æ˜¯MapReduceæ¡†æ¶.
> 
> ç”±äºMapReduceæœ¬èº«å…·æœ‰è¾ƒé«˜çš„å»¶è¿Ÿ,å› æ­¤åœ¨åˆ©ç”¨MapReduceæ‰§è¡ŒHiveæŸ¥è¯¢æ—¶,ä¹Ÿä¼šæœ‰è¾ƒé«˜çš„å»¶è¿Ÿ,ç›¸å¯¹çš„æ•°æ®åº“çš„æ‰§è¡Œå»¶è¿Ÿè¾ƒä½,å½“ç„¶,è¿™ä¸ªä½æ˜¯æœ‰æ¡ä»¶çš„,å³æ•°æ®è§„æ¨¡è¾ƒå°,å½“æ•°æ®è§„æ¨¡å¤§åˆ°è¶…è¿‡æ•°æ®åº“çš„å¤„ç†èƒ½åŠ›çš„æ—¶å€™,Hiveçš„å¹¶è¡Œè®¡ç®—æ˜¾ç„¶èƒ½ä½“ç°å‡ºä¼˜åŠ¿.
#### 1.4.7 å¯æ‰©å±•æ€§
> ç”±äºHiveæ˜¯å»ºç«‹åœ¨Hadoopä¹‹ä¸Šçš„,å› æ­¤Hiveçš„å¯æ‰©å±•æ€§æ˜¯å’ŒHadoopçš„å¯æ‰©å±•æ€§æ˜¯ä¸€è‡´çš„(ä¸–ç•Œä¸Šæœ€å¤§çš„Hadoopé›†ç¾¤åœ¨Yahoo!,2009å¹´çš„è§„æ¨¡åœ¨4000å°èŠ‚ç‚¹å·¦å³),è€Œæ•°æ®åº“ç”±äºACIDè¯­ä¹‰çš„ä¸¥æ ¼é™åˆ¶,æ‰©å±•è¡Œéå¸¸æœ‰é™,ç›®å‰æœ€å…ˆè¿›çš„å¹¶è¡Œæ•°æ®åº“Oracleåœ¨ç†è®ºä¸Šçš„æ‰©å±•èƒ½åŠ›ä¹Ÿåªæœ‰100å°å·¦å³.
#### 1.4.8 æ•°æ®è§„æ¨¡
> ç”±äºHiveå»ºç«‹åœ¨é›†ç¾¤ä¸Šå¹¶å¯ä»¥åˆ©ç”¨MapReduceå¹¶è¡Œè®¡ç®—,å› æ­¤å¯ä»¥æ”¯æŒå¾ˆå¤§è§„æ¨¡çš„æ•°æ®,å¯¹åº”çš„æ•°æ®åº“å¯ä»¥æ”¯æŒçš„æ•°æ®è§„æ¨¡è¾ƒå°.

## 2. Hive å®‰è£…éƒ¨ç½²
### 2.1 Hive Download Link
> 1.Hiveå®˜ç½‘: [hive.apache.org/](http://hive.apache.org/)
> 
> 2.Hiveæ–‡æ¡£: [cwiki.apache.org/confluence/display/Hive/](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)
> 
> 3.Github: [github.com/apache/hive](https://github.com/apache/hive)
> 
> 4.Download Link: [archive.apache.org/dist/hive/](http://archive.apache.org/dist/hive/)
> 
> 5.ä»¥apache-hive-1.2.1-bin.tar.gz ç¨³å®šç‰ˆæœ¬ ä¸ºå®ä¾‹è¿›è¡Œå®‰è£….

#### 1. å°†apache-hive-1.2.1-bin.tar.gzä¸Šä¼ è‡³ linux system/opt/softwareç›®å½•ä¸‹
``` powershell
[root@systemhub711 ~]# cd /opt/software/
[root@systemhub711 software]# ll
total 526728
-rw-r--r--. 1 root root  92834839 Mar 24 23:51 apache-hive-1.2.1-bin.tar.gz
-rwxrwxrwx. 1 root root   9621331 Jan 14 09:36 apache-tomcat-8.5.33.tar.gz
-rwxrwxrwx. 1 root root 212046774 Jan 24 20:37 hadoop-2.7.2.tar.gz
-rwxrwxrwx. 1 root root 189815615 Jan 14 10:22 jdk-8u162-linux-x64.tar.gz
-rwxrwxrwx. 1 root root  35042811 Jan 17 19:18 zookeeper-3.4.10.tar.gz
```
#### 2. è§£å‹apache-hive-1.2.1-bin.tar.gz è‡³ /opt/module/ç›®å½•ä¸‹
``` powershell
[root@systemhub711 software]# tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/
apache-hive-1.2.1-bin/NOTICE
apache-hive-1.2.1-bin/LICENSE
apache-hive-1.2.1-bin/README.txt
apache-hive-1.2.1-bin/RELEASE_NOTES.txt
apache-hive-1.2.1-bin/examples/files/emp.txt
apache-hive-1.2.1-bin/examples/files/type_evolution.avro
apache-hive-1.2.1-bin/examples/files/extrapolate_stats_partial.txt
apache-hive-1.2.1-bin/examples/files/lineitem.txt
```
#### 3. ä¿®æ”¹apache-hive-1.2.1-bin.tar.gzåŒ…åç§°æ›´æ”¹ä¸ºhive
``` powershell
[root@systemhub711 software]# cd ..
[root@systemhub711 opt]# cd module/
[root@systemhub711 module]# ll
total 20
drwxr-xr-x.  8 root root 4096 Mar 24 23:53 apache-hive-1.2.1-bin
drwxr-xr-x.  9 root root 4096 Feb 24 21:55 apache-tomcat
drwxr-xr-x. 12 root root 4096 Feb 27 14:24 hadoop
drwxr-xr-x.  8 uucp  143 4096 Dec 20  2017 jdk1.8.0_162
drwxr-xr-x. 10 1001 1001 4096 Mar 23  2017 zookeeper
[root@systemhub711 module]# mv apache-hive-1.2.1-bin hive
[root@systemhub711 module]# ll
total 20
drwxr-xr-x.  9 root root 4096 Feb 24 21:55 apache-tomcat
drwxr-xr-x. 12 root root 4096 Feb 27 14:24 hadoop
drwxr-xr-x.  8 root root 4096 Mar 24 23:53 hive
drwxr-xr-x.  8 uucp  143 4096 Dec 20  2017 jdk1.8.0_162
drwxr-xr-x. 10 1001 1001 4096 Mar 23  2017 zookeeper
[root@systemhub711 module]# 
```
#### 4. ä¿®æ”¹/opt/module/hive/confç›®å½•ä¸‹hive-env.sh.templateåç§°æ›´æ”¹ä¸ºhive-env.sh
``` powershell
[root@systemhub711 module]# cd /opt/module/hive/conf
[root@systemhub711 conf]# ll
total 188
-rw-rw-r--. 1 root root   1139 Apr 30  2015 beeline-log4j.properties.template
-rw-rw-r--. 1 root root 168431 Jun 19  2015 hive-default.xml.template
-rw-rw-r--. 1 root root   2378 Apr 30  2015 hive-env.sh.template
-rw-rw-r--. 1 root root   2662 Apr 30  2015 hive-exec-log4j.properties.template
-rw-rw-r--. 1 root root   3050 Apr 30  2015 hive-log4j.properties.template
-rw-rw-r--. 1 root root   1593 Apr 30  2015 ivysettings.xml
[root@systemhub711 conf]# mv hive-env.sh.template hive-env.sh
[root@systemhub711 conf]# ll
total 188
-rw-rw-r--. 1 root root   1139 Apr 30  2015 beeline-log4j.properties.template
-rw-rw-r--. 1 root root 168431 Jun 19  2015 hive-default.xml.template
-rw-rw-r--. 1 root root   2378 Apr 30  2015 hive-env.sh
-rw-rw-r--. 1 root root   2662 Apr 30  2015 hive-exec-log4j.properties.template
-rw-rw-r--. 1 root root   3050 Apr 30  2015 hive-log4j.properties.template
-rw-rw-r--. 1 root root   1593 Apr 30  2015 ivysettings.xml
[root@systemhub711 conf]# 
```

#### 5. é…ç½®ç¯å¢ƒå˜é‡
> åœ¨systemä¸­é…ç½®HIVE_HOME
``` powershell
[root@systemhub711 hive]# vim /etc/profile
```
``` powershell

## SET JAVA_HOME
JAVA_HOME=/opt/module/jdk1.8.0_162
PATH=/opt/module/jdk1.8.0_162/bin:$PATH
export JAVA_HOME PATH

## SET TOMCAT_HOME
TOMCAT_HOME=/opt/module/apache-tomcat
export TOMCAT_HOME

## SET HADOOP_HOME
HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

## SET HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```
> é…ç½®å®Œæ¯•,æŒ‰ESC,è¾“å…¥:wq ä¿å­˜å¹¶é€€é€€å‡º.
> 
> æ›´æ–°é…ç½®ä¿¡æ¯
``` powershell
[root@systemhub711 hive]# source /etc/profile
```
> åœ¨hive-env.shé…ç½®HADOOP_HOMEè·¯å¾„ & é…ç½®HIVE_CONF_DIRè·¯å¾„
``` powershell
[root@systemhub711 conf]# echo $HADOOP_HOME
/opt/module/hadoop
[root@systemhub711 conf]# vim hive-env.sh
```
> é…ç½®å®Œæ¯•,æŒ‰ESC,è¾“å…¥:wq ä¿å­˜å¹¶é€€é€€å‡º.
``` powershell
#   fi
# fi

# The heap size of the jvm stared by hive shell script can be controlled via:
#
# export HADOOP_HEAPSIZE=1024
#
# Larger heap size may be required when running queries over large number of files or partitions. 
# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be 
# appropriate for hive server (hwi etc).

# Set HADOOP_HOME to point to a specific hadoop install directory
export HADOOP_HOME=/opt/module/hadoop

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/module/hive/conf
```

### 2.2 å¯åŠ¨Hadoopé›†ç¾¤
#### 2.2.3 å¯åŠ¨ HDFS Service
``` powershell
[root@systemhub511 ~]# cd /opt/module/hadoop/
[root@systemhub511 hadoop]# start-dfs.sh
```
#### 2.2.4 å¯åŠ¨ Yarn Service
``` powershell
[root@systemhub611 ~]# cd /opt/module/hadoop/
[root@systemhub611 hadoop]# start-yarn.sh
```
#### 2.2.5 jps æŸ¥çœ‹é›†ç¾¤è¿›ç¨‹
``` powershell
[root@systemhub511 hadoop]# jps
11121 NameNode
15719 Jps
11323 DataNode
15118 NodeManager
[root@systemhub511 hadoop]# 

[root@systemhub611 hadoop]# jps
9106 NodeManager
5865 DataNode
10650 Jps
7629 ResourceManager
[root@systemhub611 hadoop]# 

[root@systemhub711 hadoop]# jps
3089 Jps
30667 DataNode
1629 NodeManager
30974 SecondaryNameNode
[root@systemhub711 hadoop]# 
```
#### 2.2.6 åœ¨HDFSä¸Šåˆ›å»ºç›®å½•
> åˆ›å»º/tmpå’Œ/user/hive/warehouseä¸¤ä¸ªç›®å½•å¹¶ä¿®æ”¹åŒç»„æƒé™ä¸ºå¯å†™
```
[root@systemhub611 hadoop]# hadoop fs -mkdir /tmp
[root@systemhub511 hadoop]# hadoop fs -mkdir -p  /user/hive/warehouse
```
```
[root@systemhub511 hadoop]# hadoop fs -chmod 777 /tmp
[root@systemhub511 hadoop]# hadoop fs -chmod 777 /user/hive/warehouse
```

#### 2.2.7 HiveåŸºæœ¬æ“ä½œ
##### 2.2.7.1 å¯åŠ¨ Hive Service
``` powershell
[root@systemhub711 hive]# bin/hive
Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> 
```
##### 2.2.7.2 æŸ¥çœ‹æ•°æ®åº“
```
hive> show databases;
OK
default
Time taken: 0.039 seconds, Fetched: 1 row(s) 
```
##### 2.2.7.3 æ‰“å¼€é»˜è®¤æ•°æ®åº“
```
hive> use default;
OK
Time taken: 0.06 seconds
hive> 
```
##### 2.2.7.4 æ˜¾ç¤ºdefaultæ•°æ®åº“ä¸­çš„è¡¨
```
hive> show tables;
OK
Time taken: 0.136 seconds
hive> 
```
##### 2.2.7.5 åˆ›å»ºä¸€å¼ è¡¨
```
hive> create table test(id int,name string);
OK
Time taken: 0.584 seconds
```
##### 2.2.7.6 æ˜¾ç¤ºæ•°æ®åº“ä¸­æœ‰å‡ å¼ è¡¨
```
hive> show tables;
OK
test
Time taken: 0.043 seconds, Fetched: 1 row(s)
hive> 
```
##### 2.2.7.7 æŸ¥çœ‹è¡¨çš„ç»“æ„
```
hive> desc test;
OK
id                      int                                         
name                    string                                      
Time taken: 0.181 seconds, Fetched: 2 row(s)
hive> 
```
##### 2.2.7.8 å‘è¡¨ä¸­æ’å…¥æ•°æ®
```
hive> insert into test values(1,"TestUser001");
Query ID = root_20190325104557_f36e7f6e-0254-4e4e-9216-60f08042dabb
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553477836311_0001, Tracking URL = http://systemhub611:8088/proxy/application_1553477836311_0001/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553477836311_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2019-03-25 10:46:18,198 Stage-1 map = 0%,  reduce = 0%
2019-03-25 10:46:26,679 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.96 sec
MapReduce Total cumulative CPU time: 1 seconds 960 msec
Ended Job = job_1553477836311_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://systemhub511:9000/user/hive/warehouse/test/.hive-staging_hive_2019-03-25_10-45-57_843_1729594574229184317-1/-ext-10000
Loading data to table default.test
Table default.test stats: [numFiles=1, numRows=1, totalSize=14, rawDataSize=13]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.96 sec   HDFS Read: 3566 HDFS Write: 82 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 960 msec
OK
Time taken: 31.477 seconds
hive> 
```
##### 2.2.7.9 æŸ¥è¯¢è¡¨ä¸­æ•°æ®
```
hive> select * from test;
OK
1       TestUser001
Time taken: 0.144 seconds, Fetched: 1 row(s)
hive>
```
##### 2.2.7.10 é€€å‡ºhive
```
hive> quit;
[root@systemhub711 hive]# 
```

#### 2.2.8 Hiveå®æ“æ¡ˆä¾‹
##### 2.2.8.1 å°†æœ¬åœ°æ–‡ä»¶å¯¼å…¥Hiveä¸­
> éœ€æ±‚: å°†æœ¬åœ° /opt/module/datas/test.txt,è¿™ä¸ªç›®å½•ä¸‹çš„æ•°æ®å¯¼å…¥åˆ°hiveçš„test(id int, name string)æ•°æ®è¡¨ä¸­.
###### 1.æ•°æ®å‡†å¤‡
> åœ¨/opt/module/ç›®å½•ä¸‹åˆ›å»ºdatas
> 
> åœ¨/opt/module/datas/ç›®å½•ä¸‹åˆ›å»ºtest.txtæ–‡ä»¶å¹¶æ·»åŠ æ•°æ®,ä»¥Tabé”®é—´éš”
```
[root@systemhub711 ~]# cd /opt/module/
[root@systemhub711 module]# mkdir datas
[root@systemhub711 module]# cd datas/
[root@systemhub711 datas]# touch test.txt
[root@systemhub711 datas]# ll
total 0
-rw-r--r--. 1 root root 0 Mar 25 16:02 test.txt
[root@systemhub711 datas]#
[root@systemhub711 datas]# vim test.txt
```
```
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
```
###### 2.å¯åŠ¨ Hadoopé›†ç¾¤ & Hive
```
[root@systemhub711 ~]# cd /opt/module/hive/
[root@systemhub711 hive]# bin/hive
Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive>
```
###### 3.åˆ›å»ºtest001æ•°æ®è¡¨,å¹¶å£°æ˜æ–‡ä»¶åˆ†éš”ç¬¦"\t"
```
hive> create table test001(id int,name string) row format delimited fields terminated by "\t";
OK
Time taken: 0.222 seconds
hive> 
```
###### 4.å°†æ–‡ä»¶åŠ è½½åˆ°æ•°æ®è¡¨ä¸­
```
hive> load data local inpath "/opt/module/datas/test.txt" into table test001;
Loading data to table default.test
Table default.test stats: [numFiles=1, totalSize=56]
OK
Time taken: 0.479 seconds
hive>
```
###### 5.æŸ¥è¯¢æ•°æ®
```
hive> select * from test;
OK
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 0.076 seconds, Fetched: 4 row(s)
hive> 
```
##### 2.2.8.2 MySqlå®‰è£…
###### 1.å®‰è£…åŒ…å‡†å¤‡
> æŸ¥çœ‹mysqlæ˜¯å¦å®‰è£…,å¦‚æœå®‰è£…äº†,å¸è½½mysql.
```
[root@systemhub711 ~]# rpm -qa|grep mysql
mysql-libs-5.1.71-1.el6.x86_64
[root@systemhub711 ~]# rpm -e --nodeps mysql-libs-5.1.71-1.el6.x86_64
[root@systemhub711 ~]# 
```
> è§£å‹mysql-libs.zipæ–‡ä»¶åˆ°å½“å‰ç›®å½•.
```
[root@systemhub711 ~]# cd /opt/software/
[root@systemhub711 software]# unzip mysql-libs.zip
Archive:  mysql-libs.zip
   creating: mysql-libs/
  inflating: mysql-libs/MySQL-client-5.6.24-1.el6.x86_64.rpm  
  inflating: mysql-libs/mysql-connector-java-5.1.27.tar.gz  
  inflating: mysql-libs/MySQL-server-5.6.24-1.el6.x86_64.rpm  
```

##### 2.2.8.3 å®‰è£…MySqlæœåŠ¡ç«¯
> å®‰è£…mysqlæœåŠ¡ç«¯
```
[root@systemhub711 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm
Preparing...                ########################################### [100%]
   1:MySQL-server           ########################################### [100%]
```
> å®‰è£…mysqlå®¢æˆ·ç«¯
```
[root@systemhub711 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm
Preparing...                ########################################### [100%]
   1:MySQL-client           ########################################### [100%]
[root@systemhub711 mysql-libs]# 
```
> æŸ¥çœ‹äº§ç”Ÿçš„éšæœºå¯†ç 
```
[root@systemhub711 mysql-libs]# cat /root/.mysql_secret
# The random password set for the root user at Mon Mar 25 17:28:10 2019 (local time): 3krFauiObIJZG_xd
[root@systemhub711 mysql-libs]# 
```
> æŸ¥çœ‹mysqlçŠ¶æ€ å¹¶ å¼€å¯æœåŠ¡
```
[root@systemhub711 mysql-libs]# service mysql status
MySQL is not running                                       [å¤±è´¥]
[root@systemhub711 mysql-libs]# service mysql start
Starting MySQL..                                           [ç¡®å®š]
```
> ç™»å½•mysql
```
[root@systemhub711 mysql-libs]# mysql -uroot -p3krFauiObIJZG_xd
Warning: Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.6.24

Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> 
```
> è®¾ç½®è‡ªå®šä¹‰ç™»å½•å¯†ç 
```
mysql> set password=password("000000");
Query OK, 0 rows affected (0.01 sec)
mysql> 
```
##### 2.2.8.4 MySqlæ— ä¸»æœºé…ç½®
> é…ç½®åªè¦æ˜¯rootç”¨æˆ·+password,å³å¯åœ¨ä»»ä½•ä¸»æœºä¸Šéƒ½èƒ½ç™»å½•MySQLæ•°æ®åº“.
> 
> ç™»å½•mysql
```
[root@systemhub711 mysql-libs]# mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.6.24 MySQL Community Server (GPL)

Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> 
```
> æ˜¾ç¤ºå½“å‰æ•°æ®åº“
```
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| test               |
+--------------------+
4 rows in set (0.01 sec)

mysql> 
```
> è¿›å…¥mysqlæ•°æ®åº“
```
mysql> use mysql;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
mysql>
```
> æŸ¥è¯¢mysqlæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨
```
mysql> show tables;
+---------------------------+
| Tables_in_mysql           |
+---------------------------+
| columns_priv              |
| db                        |
| event                     |
| func                      |
| general_log               |
| help_category             |
| help_keyword              |
| help_relation             |
| help_topic                |
| innodb_index_stats        |
| innodb_table_stats        |
| ndb_binlog_index          |
| plugin                    |
| proc                      |
| procs_priv                |
| proxies_priv              |
| servers                   |
| slave_master_info         |
| slave_relay_log_info      |
| slave_worker_info         |
| slow_log                  |
| tables_priv               |
| time_zone                 |
| time_zone_leap_second     |
| time_zone_name            |
| time_zone_transition      |
| time_zone_transition_type |
| user                      |
+---------------------------+
28 rows in set (0.00 sec)
mysql>
```
> æŸ¥è¯¢useræ•°æ®è¡¨
```
mysql> select User, Host, Password from user;
+------+--------------+-------------------------------------------+
| User | Host         | Password                                  |
+------+--------------+-------------------------------------------+
| root | localhost    | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |
| root | systemhub711 | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
| root | 127.0.0.1    | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
| root | ::1          | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
+------+--------------+-------------------------------------------+
4 rows in set (0.01 sec)
mysql> 
```
> ä¿®æ”¹userè¡¨,æŠŠHostè¡¨å†…å®¹ä¿®æ”¹ä¸º`%`
```
mysql> update user set host='%' where host='localhost';
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select User, Host, Password from user;
+------+--------------+-------------------------------------------+
| User | Host         | Password                                  |
+------+--------------+-------------------------------------------+
| root | %            | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |
| root | systemhub711 | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
| root | 127.0.0.1    | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
| root | ::1          | *DB2C3C3C317DE3F3FEFF12C9A60985CA29A7BBCD |
+------+--------------+-------------------------------------------+
4 rows in set (0.00 sec)
mysql> 
```
> åˆ é™¤rootç”¨æˆ·çš„å…¶ä»–host
```
mysql> delete from user where Host='systemhub711';
Query OK, 1 row affected (0.00 sec)

mysql> delete from user where Host='127.0.0.1';
Query OK, 1 row affected (0.00 sec)

mysql> delete from user where Host='::1';
Query OK, 1 row affected (0.00 sec)

mysql> select User, Host, Password from user;
+------+------+-------------------------------------------+
| User | Host | Password                                  |
+------+------+-------------------------------------------+
| root | %    | *032197AE5731D4664921A6CCAC7CFCE6A0698693 |
+------+------+-------------------------------------------+
1 row in set (0.00 sec)
mysql> 
```
> åˆ·æ–°
```
mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)
mysql> 
```
> é€€å‡º
```
mysql> exit;
Bye
[root@systemhub711 mysql-libs]# 
```

##### 2.2.8.5 é…ç½®Metastoreåˆ°MySql
> 1.åœ¨/opt/module/hive/confç›®å½•ä¸‹åˆ›å»ºhive-site.xmlé…ç½®æ–‡ä»¶.
```
[root@systemhub711 ~]# cd /opt/module/hive/conf/
[root@systemhub711 conf]# touch hive-site.xml
[root@systemhub711 conf]# vim hive-site.xml
```
> 2.æ ¹æ®å®˜æ–¹æ–‡æ¡£é…ç½®å‚æ•°,æ‹·è´æ•°æ®åˆ°hive-site.xmlé…ç½®æ–‡ä»¶ä¸­ | [Hiveå®˜æ–¹é…ç½®æ–‡æ¡£](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration).
``` xml
<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://systemhub711:3306/metastore?createDatabaseIfNotExist=true</value>
      <description>JDBC connect string for a JDBC metastore</description>
    </property>

    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
    </property>

    <property>
       <name>javax.jdo.option.ConnectionUserName</name>
       <value>root</value>
       <description>username to use against metastore database</description>
    </property>

    <property>
       <name>javax.jdo.option.ConnectionPassword</name>
       <value>000000</value>
       <description>passwordto use against metastore database</description>
    </property>

</configuration>
```
> é©±åŠ¨æ‹·è´
> å°†/opt/software/mysql-libsç›®å½•ä¸‹è§£å‹mysql-connector-java-5.1.27.tar.gzé©±åŠ¨åŒ….
```
[root@systemhub711 opt]# cd software/mysql-libs
[root@systemhub711 mysql-libs]# tar -zvxf mysql-connector-java-5.1.27.tar.gz
mysql-connector-java-5.1.27/
mysql-connector-java-5.1.27/docs/
mysql-connector-java-5.1.27/src/
mysql-connector-java-5.1.27/src/com/
mysql-connector-java-5.1.27/src/com/mysql/
mysql-connector-java-5.1.27/src/com/mysql/jdbc/
mysql-connector-java-5.1.27/src/com/mysql/jdbc/authentication/
```
> å°†mysql-connector-java-5.1.27-bin.jaråˆ°/opt/module/hive/lib/
```
[root@systemhub711 mysql-libs]# cd mysql-connector-java-5.1.27
[root@systemhub711 mysql-connector-java-5.1.27]# ll
total 1272
-rw-r--r--. 1 root root  47173 Oct 24  2013 build.xml
-rw-r--r--. 1 root root 222520 Oct 24  2013 CHANGES
-rw-r--r--. 1 root root  18122 Oct 24  2013 COPYING
drwxr-xr-x. 2 root root   4096 Mar 25 20:56 docs
-rw-r--r--. 1 root root 872303 Oct 24  2013 mysql-connector-java-5.1.27-bin.jar
-rw-r--r--. 1 root root  61423 Oct 24  2013 README
-rw-r--r--. 1 root root  63674 Oct 24  2013 README.txt
drwxr-xr-x. 7 root root   4096 Oct 24  2013 src
[root@systemhub711 mysql-connector-java-5.1.27]# cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/
[root@systemhub711 mysql-connector-java-5.1.27]# cd /opt/module/hive/lib/
[root@systemhub711 lib]# ll
-rw-r--r--.  1 root root   872303 Mar 25 20:58 mysql-connector-java-5.1.27-bin.jar
```
> é…ç½®å®Œæ¯•å,å…ˆå¯åŠ¨MySQL,æŸ¥çœ‹æœ‰å‡ ä¸ªæ•°æ®åº“,ä¾ç„¶è¿˜æ˜¯å››ä¸ª.
```
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| test               |
+--------------------+
4 rows in set (0.01 sec)
mysql> 
```
> å†æ¬¡æ‰“å¼€å¤šä¸ªçª—å£,åˆ†åˆ«å¯åŠ¨hive.
```
[root@systemhub711 hive]# bin/hive

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive>
```
> å¯åŠ¨hiveå,å›åˆ°MySQLçª—å£æŸ¥çœ‹æ•°æ®åº“,æ˜¾ç¤ºå¢åŠ äº†metastoreæ•°æ®åº“.
```
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| metastore          |
| mysql              |
| performance_schema |
| test               |
+--------------------+
5 rows in set (0.00 sec)
mysql> 
```
> å»metastoreæ•°æ®åº“ å–½ä¸€çœ¼.
```
mysql> use metastore;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
mysql> show tables;
+---------------------------+
| Tables_in_metastore       |
+---------------------------+
| BUCKETING_COLS            |
| CDS                       |
| COLUMNS_V2                |
| DATABASE_PARAMS           |
| DBS                       |
| FUNCS                     |
| FUNC_RU                   |
| GLOBAL_PRIVS              |
| PARTITIONS                |
| PARTITION_KEYS            |
| PARTITION_KEY_VALS        |
| PARTITION_PARAMS          |
| PART_COL_STATS            |
| ROLES                     |
| SDS                       |
| SD_PARAMS                 |
| SEQUENCE_TABLE            |
| SERDES                    |
| SERDE_PARAMS              |
| SKEWED_COL_NAMES          |
| SKEWED_COL_VALUE_LOC_MAP  |
| SKEWED_STRING_LIST        |
| SKEWED_STRING_LIST_VALUES |
| SKEWED_VALUES             |
| SORT_COLS                 |
| TABLE_PARAMS              |
| TAB_COL_STATS             |
| TBLS                      |
| VERSION                   |
+---------------------------+
29 rows in set (0.00 sec)
mysql>
```

##### 2.2.8.6 Hiveå¸¸ç”¨äº¤äº’å‘½ä»¤
###### 1. Hive å¸®åŠ©æŒ‡ä»¤
```
[root@systemhub711 hive]# bin/hive -help
usage: hive
 -d,--define <key=value>          Variable subsitution to apply to hive
                                  commands. e.g. -d A=B or --define A=B
    --database <databasename>     Specify the database to use
 -e <quoted-query-string>         SQL from command line
 -f <filename>                    SQL from files
 -H,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
    --hivevar <key=value>         Variable subsitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i <filename>                    Initialization SQL file
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the
                                  console)
[root@systemhub711 hive]# 
```

###### 2. -e ä¸è¿›å…¥hiveäº¤äº’çª—å£,å³å¯æ‰§è¡Œsqlè¯­å¥
```
[root@systemhub711 hive]# bin/hive -e "select * from test";

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
OK
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 2.328 seconds, Fetched: 4 row(s)
[root@systemhub711 hive]# 
```

###### 3. -f æ‰§è¡Œè„šæœ¬ä¸­,æ‰§è¡Œsqlè¯­å¥
> åœ¨/opt/module/datasç›®å½•ä¸‹åˆ›å»º test_hivef.hqlæ–‡ä»¶
```
[root@systemhub711 hive]# cd ..
[root@systemhub711 module]# cd datas/
[root@systemhub711 datas]# touch test_hivef.hql
[root@systemhub711 datas]# ll
total 4
-rw-r--r--. 1 root root  0 Mar 25 22:24 test_hivef.hql
-rw-r--r--. 1 root root 56 Mar 25 16:46 test.txt
[root@systemhub711 datas]# vim test_hivef.hql
```
> æ–‡ä»¶ä¸­å†™å…¥æ­£ç¡®çš„sqlè¯­å¥
```
select * from test;
```
> æ‰§è¡Œæ–‡ä»¶ä¸­çš„sqlè¯­å¥
```
[root@systemhub711 datas]# cd /opt/module/hive/
[root@systemhub711 hive]# bin/hive -f /opt/module/datas/test_hivef.hql

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
OK
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 2.012 seconds, Fetched: 4 row(s)
[root@systemhub711 hive]#
```
> æ‰§è¡Œæ–‡ä»¶ä¸­çš„sqlè¯­å¥å¹¶å°†ç»“æœå†™å…¥æ–‡ä»¶ä¸­
```
[root@systemhub711 hive]# bin/hive -f /opt/module/datas/test_hivef.hql > /opt/module/datas/test_hivef_result.txt

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
OK
Time taken: 1.794 seconds, Fetched: 4 row(s)
[root@systemhub711 hive]# cat /opt/module/datas/test_hivef_result.txt
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
[root@systemhub711 hive]# 
```

##### 2.2.8.7 Hive å…¶ä»–å‘½ä»¤æ“ä½œ
###### 2.2.8.7.1 é€€å‡ºhiveçª—å£
> åœ¨æ–°ç‰ˆçš„oracleæ•°æ®åº“ä¸­ä¸¤ä¸ªé€€å‡ºæŒ‡ä»¤å·²ç»æ²¡æœ‰åŒºåˆ«äº†,åœ¨ä»¥å‰çš„ç‰ˆæœ¬æ˜¯æœ‰åŒºåˆ«.
> exit:å…ˆéšæ€§æäº¤æ•°æ®,å†é€€å‡º; quit:ä¸æäº¤æ•°æ®,é€€å‡º;
```
[root@systemhub711 hive]# bin/hive
Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> quit;
[root@systemhub711 hive]# 
[root@systemhub711 hive]# bin/hive
Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> exit;
[root@systemhub711 hive]# 
```
###### 2.2.8.7.2 åœ¨Hive CLIå‘½ä»¤çª—å£ä¸­æŸ¥çœ‹hdfsæ–‡ä»¶ç³»ç»Ÿ
```
hive> dfs -ls / ;
Found 3 items
drwxr-xr-x   - root supergroup          0 2019-01-27 14:35 /group
drwxrwxrwx   - root supergroup          0 2019-01-25 10:09 /tmp
drwxr-xr-x   - root supergroup          0 2019-01-25 09:58 /user
hive> 
```
###### 2.2.8.7.3 åœ¨Hive CLIå‘½ä»¤çª—å£ä¸­æŸ¥çœ‹hdfsæœ¬åœ°ç³»ç»Ÿ
```
hive> ! ls /opt/module;
apache-tomcat
datas
hadoop
hive
jdk1.8.0_162
zookeeper
hive> 
```
###### 2.2.8.7.4 æŸ¥çœ‹åœ¨Hiveä¸­è¾“å…¥æ‰€æœ‰å†å²å‘½ä»¤
> è¿›å…¥åˆ°å½“å‰ç”¨æˆ·çš„æ ¹ç›®å½•æ‰§è¡Œå†å²å‘½ä»¤.
```
[root@systemhub711 module]# cd
[root@systemhub711 ~]# cat .hivehistory
show databases;
show databases;
use default;
show tables;
create table test(id int,name string);
show tables;
insert into test(1,"TestUser001");
insert into test value(1,"TestUser001");
insert into test values(1,"TestUser001");
select * form test;
select * from test;
desc test;
quit;
load data local inpath "/opt/module/datas/test.txt" into table test;
select * form test;
select * from test;
drop table test;
create table test(id int,name string) row format delimited fields terminated by "\t";
load data local inpath "/opt/module/datas/test.txt" into table test;
select * from test;
exit;
[root@systemhub711 ~]# 
```

##### 2.2.8.8 Hiveå¸¸è§å±æ€§é…ç½®
###### 2.2.8.8.1 Hiveæ•°æ®ä»“åº“ä½ç½®é…ç½®
> 1.Defaultæ•°æ®ä»“åº“æœ€åŸå§‹ä½ç½®æ˜¯åœ¨hdfsä¸Šçš„: /user/hive/warehouseè·¯å¾„ä¸‹.
> 
> 2.åœ¨ä»“åº“ç›®å½•ä¸‹,æ²¡æœ‰å¯¹é»˜è®¤çš„æ•°æ®åº“defaultåˆ›å»ºæ–‡ä»¶å¤¹,å¦‚æœæŸå¼ æ•°æ®è¡¨å±äºdefaultæ•°æ®åº“,ç›´æ¥ä¼šåœ¨æ•°æ®ä»“åº“ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹.
> 
> 3.ä¿®æ”¹defaultæ•°æ®ä»“åº“åŸå§‹ä½ç½®.
> 1.åœ¨hive-site.xmlé…ç½®æ–‡ä»¶ä¸­è¿½åŠ ä¸€ä¸‹é…ç½®ä¿¡æ¯.
``` xml
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/user/hive/warehouse</value>
  <description>location of default database for the warehouse</description>
</property>
```
###### 2.2.8.8.2 é…ç½®æŸ¥è¯¢åä¿¡æ¯æ˜¾ç¤º
> 1.åœ¨hive-site.xmlæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹é…ç½®ä¿¡æ¯,å°±å¯ä»¥å®ç°æ˜¾ç¤ºå½“å‰æ•°æ®åº“,ä»¥åŠæŸ¥è¯¢è¡¨çš„å¤´ä¿¡æ¯é…ç½®.
``` xml
<property> 
  <name>hive.cli.print.header</name>  
  <value>true</value> 
</property>

<property> 
  <name>hive.cli.print.current.db</name>  
  <value>true</value> 
</property>
```
> 2.é‡æ–°å¯åŠ¨hive,å¯¹æ¯”é…ç½®å‰åå·®å¼‚.
> 
> é…ç½®å‰.
```
hive> select * from test;
OK
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 0.808 seconds, Fetched: 4 row(s)
hive> 
```

> é…ç½®å.
```
hive (default)> select * from test;
OK
test.id test.name
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 1.708 seconds, Fetched: 4 row(s)
hive (default)> 
```

###### 2.2.8.8.3 Hiveè¿è¡Œæ—¥å¿—ä¿¡æ¯é…ç½®
> 1.Hiveçš„logé»˜è®¤å­˜æ”¾åœ¨/tmp/root/hive.logç›®å½•ä¸‹(å½“å‰ç”¨æˆ·åä¸‹).
> 2.ä¿®æ”¹hiveçš„logå­˜æ”¾æ—¥å¿—åˆ°/opt/module/hive/logs
> 2.1å°†hive-log4j.properties.templateæ–‡ä»¶åç§°ä¿®æ”¹ä¸ºhive-log4j.properties
```
[root@systemhub711 conf]# pwd
/opt/module/hive/conf
[root@systemhub711 conf]# ll
total 192
-rw-rw-r--. 1 root root   1139 Apr 30  2015 beeline-log4j.properties.template
-rw-rw-r--. 1 root root 168431 Jun 19  2015 hive-default.xml.template
-rw-rw-r--. 1 root root   2401 Mar 25 00:10 hive-env.sh
-rw-rw-r--. 1 root root   2662 Apr 30  2015 hive-exec-log4j.properties.template
-rw-rw-r--. 1 root root   3050 Apr 30  2015 hive-log4j.properties.template
-rw-r--r--. 1 root root   1416 Mar 25 23:11 hive-site.xml
-rw-rw-r--. 1 root root   1593 Apr 30  2015 ivysettings.xml
[root@systemhub711 conf]# mv hive-log4j.properties.template hive-log4j.properties
[root@systemhub711 conf]# ll
total 192
-rw-rw-r--. 1 root root   1139 Apr 30  2015 beeline-log4j.properties.template
-rw-rw-r--. 1 root root 168431 Jun 19  2015 hive-default.xml.template
-rw-rw-r--. 1 root root   2401 Mar 25 00:10 hive-env.sh
-rw-rw-r--. 1 root root   2662 Apr 30  2015 hive-exec-log4j.properties.template
-rw-rw-r--. 1 root root   3050 Apr 30  2015 hive-log4j.properties
-rw-r--r--. 1 root root   1416 Mar 25 23:11 hive-site.xml
-rw-rw-r--. 1 root root   1593 Apr 30  2015 ivysettings.xml
[root@systemhub711 conf]# 
```
> åœ¨hive-log4j.propertiesé…ç½®æ–‡ä»¶ä¸­ä¿®æ”¹logå­˜æ”¾ä½ç½®.
```
[root@systemhub711 conf]# vim hive-log4j.properties
```

```
-rw-r--r--. 1 root root   1416 Mar 25 23:11 hive-site.xml
-rw-rw-r--. 1 root root   1593 Apr 30  2015 ivysettings.xml
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Define some default values that can be overridden by system properties
hive.log.threshold=ALL
hive.root.logger=INFO,DRFA
hive.log.dir=/opt/module/hive/logs
```

###### 2.2.8.8.4 å‚æ•°é…ç½®æ–¹å¼
> 1.æŸ¥çœ‹å½“å‰æ‰€æœ‰çš„é…ç½®ä¿¡æ¯.
```
hive (default)> set;
_hive.hdfs.session.path=/tmp/hive/root/3227b090-9545-485f-bae7-b9b31cadc61d
_hive.local.session.path=/tmp/root/3227b090-9545-485f-bae7-b9b31cadc61d
_hive.tmp_table_space=/tmp/hive/root/3227b090-9545-485f-bae7-b9b31cadc61d/_tmp_space.db
datanucleus.autoCreateSchema=true
datanucleus.autoStartMechanismMode=checked
datanucleus.cache.level2=false
datanucleus.cache.level2.type=none
datanucleus.connectionPoolingType=BONECP
datanucleus.fixedDatastore=false
```

> 2.å‚æ•°çš„é…ç½®ä¸‰ç§æ–¹å¼.
> 
> 2.1 é…ç½®æ–‡ä»¶æ–¹å¼
> 
> é»˜è®¤é…ç½®æ–‡ä»¶: hive-default.xml
> 
> å¼€å‘è€…è‡ªå®šä¹‰é…ç½®æ–‡ä»¶: hive-site.xml
> 
> æ³¨æ„: å¼€å‘è€…è‡ªå®šä¹‰é…ç½®ä¼šè¦†ç›–é»˜è®¤é…ç½®,å¦å¤–,Hiveä¹Ÿä¼šè¯»å…¥Hadoopçš„é…ç½®,å› ä¸ºHiveæ˜¯ä½œä¸ºHadoopçš„å®¢æˆ·ç«¯å¯åŠ¨çš„,Hiveçš„é…ç½®ä¼šè¦†ç›–Hadoopçš„é…ç½®,é…ç½®æ–‡ä»¶çš„è®¾å®šå¯¹æœ¬æœºå¯åŠ¨çš„æ‰€æœ‰Hiveè¿›ç¨‹éƒ½æœ‰æ•ˆ.
> 
> 2.2 å‘½ä»¤è¡Œå‚æ•°æ–¹å¼
> 
> å¯åŠ¨Hiveæ—¶,å¯ä»¥åœ¨å‘½ä»¤è¡Œæ·»åŠ `-hiveconf param=value`æ¥è®¾å®šå‚æ•°.
> `ä»…å¯¹æœ¬æ¬¡hiveå¯åŠ¨æœ‰æ•ˆ`
```
[root@systemhub711 hive]# bin/hive -hiveconf mapred.reduce.tasks=10
Logging initialized using configuration in file:/opt/module/hive/conf/hive-log4j.properties
hive (default)> 
```
> æŸ¥çœ‹å‚æ•°è®¾ç½®
```
hive (default)> set mapred.reduce.tasks;
mapred.reduce.tasks=10
hive (default)> 
```
> 2.3 å‚æ•°å£°æ˜æ–¹å¼
> å¯ä»¥åœ¨HQLä¸­ä½¿ç”¨SETå…³é”®å­—è®¾å®šå‚æ•°.
> `ä»…å¯¹æœ¬æ¬¡hiveå¯åŠ¨æœ‰æ•ˆ`
```
[root@systemhub711 hive]# bin/hive -hiveconf mapred.reduce.tasks=100
Logging initialized using configuration in file:/opt/module/hive/conf/hive-log4j.properties
hive (default)> 
```
> æŸ¥çœ‹å‚æ•°è®¾ç½®
```
hive (default)> set mapred.reduce.tasks;
mapred.reduce.tasks=100
hive (default)> 
```
> ä¸Šè¿°ä¸‰ç§è®¾å®šæ–¹å¼çš„ä¼˜å…ˆçº§ä¾æ¬¡é€’å¢.
> 
> å³é…ç½®æ–‡ä»¶ `<` å‘½ä»¤è¡Œå‚æ•° `<` å‚æ•°å£°æ˜.
> 
> æ³¨æ„æŸäº›ç³»ç»Ÿçº§çš„å‚æ•°,ä¾‹å¦‚log4jç›¸å…³çš„è®¾å®š,å¿…é¡»ç”¨å‰ä¸¤ç§æ–¹å¼è®¾å®š,å› ä¸ºé‚£äº›å‚æ•°çš„è¯»å–åœ¨ä¼šè¯å»ºç«‹ä»¥å‰å·²ç»å®Œæˆäº†.


## 3. Hive æ•°æ®ç±»å‹
### 3.1 åŸºæœ¬æ•°æ®ç±»å‹
> å¯¹äºHiveçš„Stringç±»å‹ç›¸å½“äºæ•°æ®åº“çš„varcharç±»å‹,è¯¥ç±»å‹æ˜¯ä¸€ä¸ªå¯å˜çš„å­—ç¬¦ä¸²,ä¸è¿‡å®ƒä¸èƒ½å£°æ˜å…¶ä¸­æœ€å¤šèƒ½å­˜å‚¨å¤šå°‘ä¸ªå­—ç¬¦,ç†è®ºä¸Šå®ƒå¯ä»¥å­˜å‚¨2GBçš„å­—ç¬¦æ•°.

| Hive æ•°æ®ç±»å‹      |  Java æ•°æ®ç±»å‹ |   é•¿åº¦   |   æ•°å€¼   |
| :--------: | :--------:| :------: | :------: |
| TINYINT    |   byte |  1byteæœ‰ç¬¦å·æ•´æ•°  |  20  |
| SMALINT    |   short |  2byteæœ‰ç¬¦å·æ•´æ•°  |  20  |
| INT    |   int |  4byteæœ‰ç¬¦å·æ•´æ•°  |  20  |
| BIGINT    |   long |  8byteæœ‰ç¬¦å·æ•´æ•°  |  20  |
| BOOLEAN    |   boolean |  å¸ƒå°”ç±»å‹,trueæˆ–è€…false  |  TRUE / FALSE  |
| FLOAT    |   float |  å•ç²¾åº¦æµ®ç‚¹æ•°  |  3.14159  |
| DOUBLE    |   double |  åŒç²¾åº¦æµ®ç‚¹æ•°  |  3.14159  |
| STRING    |   string |  å­—ç¬¦ä¸²ç±»å‹,å¯ä»¥æŒ‡å®šå­—ç¬¦é›†,å¯ä»¥ä½¿ç”¨å•å¼•å·æˆ–è€…åŒå¼•å·  |  'now is the time' /  "for all good men"  |
| TIMESTAMP    |   |  æ—¶é—´ç±»å‹  |   |
| BINARY    |   |  å­—èŠ‚æ•°ç»„  |    |

### 3.2 é›†åˆæ•°æ®ç±»å‹
> Hiveæœ‰ä¸‰ç§å¤æ‚æ•°æ®ç±»å‹ARRAYã€MAP å’ŒSTRUCT,ARRAYå’ŒMAPä¸Javaä¸­çš„Arrayå’ŒMapç±»ä¼¼,è€ŒSTRUCTä¸Cè¯­è¨€ä¸­çš„Structç±»ä¼¼,å®ƒå°è£…äº†ä¸€ä¸ªå‘½åå­—æ®µé›†åˆ,å¤æ‚æ•°æ®ç±»å‹å…è®¸ä»»æ„å±‚æ¬¡çš„åµŒå¥—.
> 
| æ•°æ®ç±»å‹      |     æè¿° |   è¯­æ³•ç¤ºä¾‹   |
| :--------: | :--------:| :------: |
| STRUCT    |   å’Œcè¯­è¨€ä¸­çš„structç±»ä¼¼,éƒ½å¯ä»¥é€šè¿‡â€œç‚¹â€ç¬¦å·è®¿é—®å…ƒç´ å†…å®¹,ä¾‹å¦‚å¦‚æœæŸä¸ªåˆ—çš„æ•°æ®ç±»å‹æ˜¯STRUCT{first  STRING,last STRING},é‚£ä¹ˆç¬¬1ä¸ªå…ƒç´ å¯ä»¥é€šè¿‡å­—æ®µ.firstæ¥å¼•ç”¨. |  struct()  |
| MAP | MAPæ˜¯ä¸€ç»„é”®-å€¼å¯¹å…ƒç»„é›†åˆ,ä½¿ç”¨æ•°ç»„è¡¨ç¤ºæ³•å¯ä»¥è®¿é—®æ•°æ®,ä¾‹å¦‚å¦‚æœæŸä¸ªåˆ—çš„æ•°æ®ç±»å‹æ˜¯MAP,å…¶ä¸­é”®->å€¼å¯¹æ˜¯â€™firstâ€™->â€™Johnâ€™å’Œâ€™lastâ€™->â€™Doeâ€™,é‚£ä¹ˆå¯ä»¥é€šè¿‡å­—æ®µå[â€˜lastâ€™]è·å–æœ€åä¸€ä¸ªå…ƒç´ . | map() |
| ARRAY | æ•°ç»„æ˜¯ä¸€ç»„å…·æœ‰ç›¸åŒç±»å‹å’Œåç§°çš„å˜é‡çš„é›†åˆ,è¿™äº›å˜é‡ç§°ä¸ºæ•°ç»„çš„å…ƒç´ ,æ¯ä¸ªæ•°ç»„å…ƒç´ éƒ½æœ‰ä¸€ä¸ªç¼–å·,ç¼–å·ä»é›¶å¼€å§‹,ä¾‹å¦‚æ•°ç»„å€¼ä¸º[â€˜Johnâ€™, â€˜Doeâ€™],é‚£ä¹ˆç¬¬2ä¸ªå…ƒç´ å¯ä»¥é€šè¿‡æ•°ç»„å[1]è¿›è¡Œå¼•ç”¨. | Array() |

#### 3.2.1 æ¡ˆä¾‹å®æ“
##### 3.2.1.1 ç”¨JSONæ ¼å¼æ¥è¡¨ç¤ºå…¶æ•°æ®ç»“æ„.
``` json
{
    "name": "TestUser",
    // Array åˆ—è¡¨
    "friends": ["TestUser001" , "TestUser002"] ,
    // Map é”®å€¼
    "children": {
        "TestUser003": "003",
        "TestUser004": "004"
    },
    // Struct ç»“æ„
    "address": {
        "street": "china",
        "city": "beijing"
    }
}
```

##### 3.2.1.2 åŸºäºä¸Šè¿°æ•°æ®ç»“æ„,åœ¨Hiveé‡Œåˆ›å»ºå¯¹åº”çš„è¡¨,å¹¶å¯¼å…¥æ•°æ®.
> åˆ›å»ºæœ¬åœ°æµ‹è¯•æ–‡ä»¶test001.txt
> 
> æ³¨: MAP,STRUCTå’ŒARRAYé‡Œçš„å…ƒç´ é—´å…³ç³»éƒ½å¯ä»¥ç”¨åŒä¸€ä¸ªå­—ç¬¦`_`ä¸‹åˆ’çº¿è¡¨ç¤º.
```
TestUser,TestUser001_TestUser002,TestUser003:003_TestUser004:004,china_beijing
DemoUser,DemoUser001_DemoUser002,DemoUser003:003_DemoUser004:004,china_beijing
```
> åœ¨Hiveåˆ›å»ºtestæµ‹è¯•è¡¨
> 
> SQL File
``` sql
create table test001(
name string,
firends array<string>,
children map<string,int>,
address struct<street:string,city:string>
)
row format delimited fields terminated by ','
collection items terminated by '_'
map keys terminated by ':'
lines terminated by '\n';
```
> å­—æ®µè§£é‡Š
```
è¡¨ç¤ºåˆ—åˆ†éš”ç¬¦
row format delimited fields terminated by ','

è¡¨ç¤ºMAP STRUCT å’Œ ARRAY åˆ†éš”ç¬¦(æ•°æ®åˆ†å‰²ç¬¦å·)
collection items terminated by '_'

è¡¨ç¤ºMAPä¸­çš„keyä¸valueåˆ†éš”ç¬¦
map keys terminated by ':'

è¡¨ç¤ºè¡Œåˆ†éš”ç¬¦
lines terminated by '\n';
```
> create table
```
hive (default)> set hive.cli.print.current.db=true;
hive (default)> create table test001(
              > name string,
              > firends array<string>,
              > children map<string,int>,
              > address struct<street:string,city:string>
              > )
              > row format delimited fields terminated by ','
              > collection items terminated by '_'
              > map keys terminated by ':'
              > lines terminated by '\n';
OK
Time taken: 0.269 seconds
hive (default)> show tables;
OK
tab_name
test
test001
Time taken: 0.053 seconds, Fetched: 2 row(s)
hive (default)> 
```
> å¯¼å…¥æ–‡æœ¬æ•°æ®åˆ°æµ‹è¯•è¡¨
```
hive (default)> load data local inpath '/opt/module/datas/test001.txt' into table test001;
Loading data to table default.test001
Table default.test001 stats: [numFiles=1, totalSize=158]
OK
Time taken: 1.268 seconds
hive (default)> 
```
> æŸ¥è¯¢å…¨éƒ¨æ•°æ®
```
hive (default)> select * from test001;
OK
test001.name    test001.firends test001.children        test001.address
TestUser        ["TestUser001","TestUser002"]   {"TestUser003":3,"TestUser004":4}       {"street":"china","city":"beijing"}
DemoUser        ["DemoUser001","DemoUser002"]   {"DemoUser003":3,"DemoUser004":4}       {"street":"china","city":"beijing"}
Time taken: 0.121 seconds, Fetched: 2 row(s)
hive (default)>	
```
> æŸ¥è¯¢ä¸‰ç§é›†åˆåˆ—è¡¨æ•°æ®
> ä»¥ä¸‹åˆ†åˆ«æ˜¯ARRAY / MAP / STRUCTçš„è®¿é—®æ–¹å¼.
```
hive (default)> select firends[1],children['TestUser003'],address.city from test001 where name="TestUser";
OK
_c0     _c1     city
TestUser002     3       beijing
Time taken: 0.32 seconds, Fetched: 1 row(s)
hive (default)>
```

### 3.3 ç±»å‹è½¬åŒ–
> Hiveçš„åŸå­æ•°æ®ç±»å‹æ˜¯å¯ä»¥è¿›è¡Œéšå¼è½¬æ¢çš„,ç±»ä¼¼äºJavaçš„ç±»å‹è½¬æ¢.
> 
> ä¾‹å¦‚æŸè¡¨è¾¾å¼ä½¿ç”¨INTç±»å‹,TINYINTä¼šè‡ªåŠ¨è½¬æ¢ä¸ºINTç±»å‹,ä½†æ˜¯Hiveä¸ä¼šè¿›è¡Œåå‘è½¬åŒ–.
> 
> ä¾‹å¦‚,æŸè¡¨è¾¾å¼ä½¿ç”¨TINYINTç±»å‹,INTä¸ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºTINYINTç±»å‹,å®ƒä¼šè¿”å›é”™è¯¯,é™¤éä½¿ç”¨`CAST`æ“ä½œ.
#### 3.3.1 éšå¼ç±»å‹è½¬æ¢è§„åˆ™
> 1.ä»»ä½•æ•´æ•°ç±»å‹éƒ½å¯ä»¥éšå¼åœ°è½¬æ¢ä¸ºä¸€ä¸ªèŒƒå›´æ›´å¹¿çš„ç±»å‹,å¦‚TINYINTå¯ä»¥è½¬æ¢æˆINT,INTå¯ä»¥è½¬æ¢æˆBIGINT.
> 
> 2.æ‰€æœ‰æ•´æ•°ç±»å‹ã€FLOATå’ŒSTRINGç±»å‹éƒ½å¯ä»¥éšå¼åœ°è½¬æ¢æˆDOUBLE.
> 
> 3.TINYINT,SMALLINT,INTéƒ½å¯ä»¥è½¬æ¢ä¸ºFLOAT.
> 
> 4.BOOLEANç±»å‹ä¸å¯ä»¥è½¬æ¢ä¸ºä»»ä½•å…¶å®ƒçš„ç±»å‹.

#### 3.3.2 ä½¿ç”¨CASTæ˜¾ç¤ºè¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢
> ä¾‹å¦‚`CAST('1' AS INT)`å°†æŠŠå­—ç¬¦ä¸²`'1'` è½¬æ¢æˆæ•´æ•°1,å¦‚æœå¼ºåˆ¶ç±»å‹è½¬æ¢å¤±è´¥,å¦‚æ‰§è¡Œ`CAST('X' AS INT)`,è¡¨è¾¾å¼è¿”å›ç©ºå€¼NULL.

## 4. DDL æ•°æ®å®šä¹‰
> DDL(Data Definition Language)æ•°æ®åº“æ¨¡å¼å®šä¹‰è¯­è¨€,æ˜¯ç”¨äºæè¿°æ•°æ®åº“ä¸­è¦å­˜å‚¨çš„ç°å®ä¸–ç•Œå®ä½“çš„è¯­è¨€.

### 4.1 åˆ›å»ºæ•°æ®åº“
> åˆ›å»ºä¸€ä¸ªæ•°æ®åº“,æ•°æ®åº“åœ¨HDFSä¸Šçš„é»˜è®¤å­˜å‚¨è·¯å¾„æ˜¯/user/hive/warehouse/*.db
> 
> é¿å…è¦åˆ›å»ºçš„æ•°æ®åº“å·²ç»å­˜åœ¨é”™è¯¯,å¢åŠ if not existsåˆ¤æ–­.
```
hive (default)> create database if not exists hive_db;
OK
Time taken: 0.131 seconds
hive (default)> 
```
> å½“å‰åˆ›å»ºçš„æ•°æ®åº“ä¼šå­˜æ”¾æŒ‡å®šåœ¨HDFSè·¯å¾„.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_003.jpg)

### 4.2 æŸ¥è¯¢æ•°æ®åº“
#### 4.2.1 æ˜¾ç¤ºæ•°æ®åº“
> 1.æ˜¾ç¤ºæ•°æ®åº“
```
hive (default)> show databases;
OK
database_name
default
hive_db
Time taken: 0.131 seconds, Fetched: 2 row(s)
hive (default)> 
```
> 2.è¿‡æ»¤æ˜¾ç¤ºæŸ¥è¯¢çš„æ•°æ®åº“
```
hive (default)> show databases like 'hive_db*';
OK
database_name
hive_db
Time taken: 0.039 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 4.2.2 æŸ¥çœ‹æ•°æ®åº“è¯¦æƒ…
> 1.æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
```
hive (default)> desc database hive_db;
OK
db_name comment location        owner_name      owner_type      parameters
hive_db         hdfs://systemhub511:9000/user/hive/warehouse/hive_db.db root    USER    
Time taken: 0.042 seconds, Fetched: 1 row(s)
hive (default)> 
```
> 2.æ˜¾ç¤ºæ•°æ®åº“è¯¦ç»†ä¿¡æ¯
```
hive (default)> desc database extended hive_db;
OK
db_name comment location        owner_name      owner_type      parameters
hive_db         hdfs://systemhub511:9000/user/hive/warehouse/hive_db.db root    USER    
Time taken: 0.027 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 4.2.3 åˆ‡æ¢å½“å‰æ•°æ®åº“
```
hive (default)> use hive_db;
OK
Time taken: 0.028 seconds
hive (hive_db)> 
```

### 4.3 ä¿®æ”¹æ•°æ®åº“
> ç”¨æˆ·å¯ä»¥ä½¿ç”¨`ALTER  DATABASE`å‘½ä»¤ä¸ºæŸä¸ªæ•°æ®åº“çš„`DBPROPERTIES`è®¾ç½®é”®-å€¼å¯¹å±æ€§å€¼,æ¥æè¿°è¿™ä¸ªæ•°æ®åº“çš„å±æ€§ä¿¡æ¯.
> 
> æ•°æ®åº“çš„å…¶ä»–å…ƒæ•°æ®ä¿¡æ¯éƒ½æ˜¯ä¸å¯æ›´æ”¹çš„,åŒ…æ‹¬æ•°æ®åº“åå’Œæ•°æ®åº“æ‰€åœ¨çš„ç›®å½•ä½ç½®.
```
hive (hive_db)> alter database hive_db set dbproperties('createtime'='2090-00-00');
OK
Time taken: 0.073 seconds
hive (hive_db)> desc database extended hive_db;
OK
db_name comment location        owner_name      owner_type      parameters
hive_db         hdfs://systemhub511:9000/user/hive/warehouse/hive_db.db root    USER    {createtime=2090-00-00}
Time taken: 0.034 seconds, Fetched: 1 row(s)
hive (hive_db)> 
```

### 4.4 åˆ é™¤æ•°æ®åº“
> å¦‚æœæ•°æ®åº“ä¸ä¸ºç©º,å¯ä»¥é‡‡ç”¨`cascade`å‘½ä»¤,å¼ºåˆ¶åˆ é™¤.
```
hive (default)> drop database hive_db cascade;
Time taken: 0.034 seconds
```

### 4.5 åˆ›å»ºè¡¨
#### 4.5.1 å»ºè¡¨è¯­æ³•
``` sql
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
[(col_name data_type [COMMENT col_comment], ...)] 
[COMMENT table_comment] 
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] 
INTO num_buckets BUCKETS] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
```
> å­—æ®µè§£é‡Šè¯´æ˜
> 
> 1.`CREATE  TABLE` åˆ›å»ºä¸€ä¸ªæŒ‡å®šåå­—çš„è¡¨,å¦‚æœç›¸åŒåå­—çš„è¡¨å·²ç»å­˜åœ¨,åˆ™æŠ›å‡ºå¼‚å¸¸,ç”¨æˆ·å¯ä»¥ç”¨`IF NOT EXISTS` é€‰é¡¹æ¥å¿½ç•¥è¿™ä¸ªå¼‚å¸¸.
> 
> 2.`EXTERNAL`å…³é”®å­—å¯ä»¥è®©ç”¨æˆ·åˆ›å»ºä¸€ä¸ªå¤–éƒ¨è¡¨,åœ¨å»ºè¡¨çš„åŒæ—¶æŒ‡å®šä¸€ä¸ªæŒ‡å‘å®é™…æ•°æ®çš„è·¯å¾„(LOCATION)Hiveåˆ›å»ºå†…éƒ¨è¡¨æ—¶,ä¼šå°†æ•°æ®ç§»åŠ¨åˆ°æ•°æ®ä»“åº“æŒ‡å‘çš„è·¯å¾„,è‹¥åˆ›å»ºå¤–éƒ¨è¡¨,ä»…è®°å½•æ•°æ®æ‰€åœ¨çš„è·¯å¾„,ä¸å¯¹æ•°æ®çš„ä½ç½®åšä»»ä½•æ”¹å˜,åœ¨åˆ é™¤è¡¨çš„æ—¶å€™,å†…éƒ¨è¡¨çš„å…ƒæ•°æ®å’Œæ•°æ®ä¼šè¢«ä¸€èµ·åˆ é™¤,è€Œå¤–éƒ¨è¡¨åªåˆ é™¤å…ƒæ•°æ®,ä¸åˆ é™¤æ•°æ®.
> 
> 3.`COMMENT`: ä¸ºè¡¨å’Œåˆ—æ·»åŠ æ³¨é‡Š.
> 
> 4.`PARTITIONED BY`åˆ›å»ºåˆ†åŒºè¡¨.
> 
> 5.`CLUSTERED BY`åˆ›å»ºåˆ†æ¡¶è¡¨.
> 
> 6.`SORTED BY`ä¸å¸¸ç”¨.
> 
> 7.`ROW FORMAT DELIMITED  [FIELDS TERMINATED BY char] [COLLECTIONITEMS TERMINATED BY char]`
> `[MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]  SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]`
> 
> å¼€å‘è€…åœ¨å»ºè¡¨çš„æ—¶å€™å¯ä»¥è‡ªå®šä¹‰SerDeæˆ–è€…ä½¿ç”¨è‡ªå¸¦çš„SerDe,å¦‚æœæ²¡æœ‰æŒ‡å®šROW FORMAT æˆ–è€…ROW FORMAT DELIMITED,å°†ä¼šä½¿ç”¨è‡ªå¸¦çš„SerDe,åœ¨å»ºè¡¨çš„æ—¶å€™,ç”¨æˆ·è¿˜éœ€è¦ä¸ºè¡¨æŒ‡å®šåˆ—,å¼€å‘è€…åœ¨æŒ‡å®šè¡¨çš„åˆ—çš„åŒæ—¶ä¹Ÿä¼šæŒ‡å®šè‡ªå®šä¹‰çš„SerDe,Hiveé€šè¿‡SerDeç¡®å®šè¡¨çš„å…·ä½“çš„åˆ—çš„æ•°æ®.
> 
> 8.`STORED AS` æŒ‡å®šå­˜å‚¨æ–‡ä»¶ç±»å‹.
> å¸¸ç”¨çš„å­˜å‚¨æ–‡ä»¶ç±»å‹: `SEQUENCEFILE(äºŒè¿›åˆ¶åºåˆ—æ–‡ä»¶)`,`TEXTFILE(æ–‡æœ¬)`,`RCFILE(åˆ—å¼å­˜å‚¨æ ¼å¼æ–‡ä»¶)`,å¦‚æœæ–‡ä»¶æ•°æ®æ˜¯çº¯æ–‡æœ¬,å¯ä»¥ä½¿ç”¨`STORED  AS  TEXTFILE`,å¦‚æœæ•°æ®éœ€è¦å‹ç¼©,ä½¿ç”¨`STORED AS SEQUENCEFILE`.
> 
> 9.`LOCATION`: æŒ‡å®šè¡¨åœ¨HDFSä¸Šçš„å­˜å‚¨ä½ç½®.
> 
> 10.`LIKE`å…è®¸å¼€å‘è€…å¤åˆ¶ç°æœ‰çš„è¡¨ç»“æ„,ä½†æ˜¯ä¸å¤åˆ¶æ•°æ®.

#### 4.5.2 ç®¡ç†è¡¨
##### 4.5.2.1 ç†è®º
> é»˜è®¤åˆ›å»ºçš„è¡¨éƒ½æ˜¯æ‰€è°“çš„ç®¡ç†è¡¨,æœ‰æ—¶ä¹Ÿè¢«ç§°ä¸ºå†…éƒ¨è¡¨.
> 
> å› ä¸ºè¿™ç§è¡¨,Hiveä¼šæˆ–å¤šæˆ–å°‘åœ°åˆ¶ç€æ•°æ®çš„ç”Ÿå‘½å‘¨æœŸ.
> 
> Hiveé»˜è®¤æƒ…å†µä¸‹ä¼šå°†è¿™äº›è¡¨çš„æ•°æ®å­˜å‚¨åœ¨ç”±é…ç½®é¡¹`hive.metastore.warehouse.dir`(ä¾‹å¦‚/user/hive/warehouse)æ‰€å®šä¹‰çš„ç›®å½•çš„å­ç›®å½•ä¸‹,å½“åˆ é™¤ä¸€ä¸ªç®¡ç†è¡¨æ—¶,Hiveä¹Ÿä¼šåˆ é™¤è¿™ä¸ªè¡¨ä¸­æ•°æ®,ç®¡ç†è¡¨ä¸é€‚åˆå’Œå…¶ä»–å·¥å…·å…±äº«æ•°æ®.

##### 4.5.2.2 æ¡ˆä¾‹å®æ“
> 1.æ™®é€šåˆ›å»ºè¡¨.
```
hive (default)> create table if not exists test004(id int, name string)row format delimited fields terminated by '\t' stored as textfile location '/user/hive/warehouse/test004';
OK
Time taken: 0.098 seconds
hive (default)> 
```
> 2.æ ¹æ®æŸ¥è¯¢ç»“æœåˆ›å»ºè¡¨(æŸ¥è¯¢çš„ç»“æœä¼šæ·»åŠ åˆ°æ–°åˆ›å»ºçš„è¡¨ä¸­).
```
hive (default)> show tables;
OK
tab_name
test
test001
Time taken: 0.056 seconds, Fetched: 2 row(s)
hive (default)> create table test002 as select * from test001;
Query ID = root_20190328000840_11ef1852-3fee-44f5-a4d4-a15042411c6a
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553696946343_0001, Tracking URL = http://systemhub611:8088/proxy/application_1553696946343_0001/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553696946343_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.34 sec
MapReduce Total cumulative CPU time: 1 seconds 340 msec
Ended Job = job_1553696946343_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to: hdfs://systemhub511:9000/user/hive/warehouse/.hive-staging_hive_2019-03-28_00-08-40_557_7477541425251721483-1/-ext-10001
Moving data to: hdfs://systemhub511:9000/user/hive/warehouse/test002
Table default.test002 stats: [numFiles=1, numRows=2, totalSize=150, rawDataSize=148]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.34 sec   HDFS Read: 3913 HDFS Write: 222 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 340 msec
OK
test001.name    test001.firends test001.children        test001.address
Time taken: 33.974 seconds
hive (default)> show tables;
OK
tab_name
test
test001
test002
Time taken: 0.041 seconds, Fetched: 3 row(s)
hive (default)> select * from test002;
OK
test002.name    test002.firends test002.children        test002.address
TestUser        ["TestUser001","TestUser002"]   {"TestUser003":3,"TestUser004":4}       {"street":"china","city":"beijing"}
DemoUser        ["DemoUser001","DemoUser002"]   {"DemoUser003":3,"DemoUser004":4}       {"street":"china","city":"beijing"}
Time taken: 0.157 seconds, Fetched: 2 row(s)
hive (default)> 
```
> 3.æ ¹æ®å·²ç»å­˜åœ¨çš„è¡¨ç»“æ„åˆ›å»ºè¡¨.
```
hive (default)> create table test003 like test;
OK
Time taken: 0.116 seconds
hive (default)> select * from test003;
OK
test003.id      test003.name
Time taken: 0.066 seconds
hive (default)> 
```
> 4.æŸ¥è¯¢è¡¨çš„ç±»å‹.
```
hive (default)> desc formatted test002;
OK
col_name        data_type       comment
# col_name              data_type               comment             
                 
name                    string                                      
firends                 array<string>                               
children                map<string,int>                             
address                 struct<street:string,city:string>                           
                 
# Detailed Table Information             
Database:               default                  
Owner:                  root                         
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://systemhub511:9000/user/hive/warehouse/test002     
Table Type:             MANAGED_TABLE            
Table Parameters:                
        COLUMN_STATS_ACCURATE   true                
        numFiles                1                   
        numRows                 2                   
        rawDataSize             148                 
        totalSize               150                 
        transient_lastDdlTime   1553702954          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        serialization.format    1                   
Time taken: 0.179 seconds, Fetched: 34 row(s)
hive (default)> 
```
#### 4.5.3 å¤–éƒ¨è¡¨
##### 4.5.3.1 ç†è®º
> å› ä¸ºè¡¨æ˜¯å¤–éƒ¨è¡¨,æ‰€æœ‰Hiveå¹¶éè®¤ä¸ºå…¶å®Œå…¨æ‹¥æœ‰è¿™ä»½æ•°æ®,åˆ é™¤è¯¥è¡¨å¹¶ä¸ä¼šåˆ é™¤æ‰è¿™ä»½æ•°æ®,ä¸è¿‡æè¿°è¡¨çš„å…ƒæ•°æ®ä¿¡æ¯ä¼šè¢«åˆ é™¤æ‰.
##### 4.5.3.2 ç®¡ç†è¡¨å’Œå¤–éƒ¨è¡¨çš„ä½¿ç”¨åœºæ™¯
> æ¯å¤©å°†æ”¶é›†åˆ°çš„ç½‘ç«™æ—¥å¿—å®šæœŸæµå…¥HDFSæ–‡æœ¬æ–‡ä»¶,åœ¨å¤–éƒ¨è¡¨(åŸå§‹æ—¥å¿—è¡¨)åŸºç¡€ä¸Šåšå¤§é‡çš„ç»Ÿè®¡åˆ†æ,ç”¨åˆ°çš„ä¸­é—´è¡¨ã€ç»“æœè¡¨ä½¿ç”¨å†…éƒ¨è¡¨å­˜å‚¨,æ•°æ®é€šè¿‡SELECT+INSERTè¿›å…¥å†…éƒ¨è¡¨.


##### 4.5.3.3 æ¡ˆä¾‹å®æ“
> åˆ†åˆ«åˆ›å»ºéƒ¨é—¨å’Œå‘˜å·¥å¤–éƒ¨è¡¨,å¹¶å‘è¡¨ä¸­å¯¼å…¥æ•°æ®.
###### 1.åŸå§‹æ•°æ®
> dept.txt & emp.txt
```
[root@systemhub711 ~]# cd /opt/module/datas/
[root@systemhub711 datas]# vim dept.txt

50      ACCOUNTING      1900
60      RESEARCH        1800
70      SALES           1700
80      OPERATIONS      1700
```
###### 2.å»ºè¡¨è¯­å¥
```
[root@systemhub711 datas]# vim emp.txt

7369    SMITH   CLERKSKLD       7902    1980-12-17      800.00  20
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.00 300.00  30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.00  30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.30 30
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.00 20
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.00  30
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.00 20
```
> åˆ›å»ºéƒ¨é—¨è¡¨
```
hive (default)> create external table dept(deptid int,dname string,loc int)
> row format delimited fields terminated by '\t';
OK
Time taken: 0.137 seconds
hive (default)> 
```
> åˆ›å»ºå‘˜å·¥è¡¨
```
hive (default)> create external table if not exists emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)
> row format delimited fields terminated by '\t';
OK
Time taken: 0.121 seconds
hive (default)> 
```
###### 3.å‘å¤–éƒ¨è¡¨ä¸­å¯¼å…¥æ•°æ®
```
hive (default)> load data local inpath '/opt/module/datas/dept.txt' into table dept;
Loading data to table default.dept
Table default.dept stats: [numFiles=1, totalSize=70]
OK
Time taken: 0.535 seconds
hive (default)>
```
```
hive (default)> load data local inpath '/opt/module/datas/emp.txt' into table emp;
Loading data to table default.emp
Table default.emp stats: [numFiles=1, totalSize=445]
OK
Time taken: 0.302 seconds
hive (default)> 
```
###### 4.æŸ¥çœ‹åˆ›å»ºçš„è¡¨
```
hive (default)> select * from dept;
OK
dept.deptid     dept.dname      dept.loc
50      ACCOUNTING      1900
60      RESEARCH        1800
70      SALES   NULL
80      OPERATIONS      1700
Time taken: 0.098 seconds, Fetched: 4 row(s)
hive (default)> select * from emp;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.063 seconds, Fetched: 9 row(s)
hive (default)> 
```
###### 5.æŸ¥çœ‹è¡¨æ ¼å¼åŒ–æ•°æ®
```
hive (default)> desc formatted dept;
OK
col_name        data_type       comment
# col_name              data_type               comment             
                 
deptid                  int                                         
dname                   string                                      
loc                     int                                         
                 
# Detailed Table Information             
Database:               default                  
Owner:                  root                        
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://systemhub511:9000/user/hive/warehouse/dept        
Table Type:             EXTERNAL_TABLE           
Table Parameters:                
        COLUMN_STATS_ACCURATE   true                
        EXTERNAL                TRUE                
        numFiles                1                   
        totalSize               70                  
        transient_lastDdlTime   1553705763          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \t                  
        serialization.format    \t                  
Time taken: 0.159 seconds, Fetched: 33 row(s)
hive (default)> 
```
###### 6.åˆ é™¤å¤–éƒ¨deptæ•°æ®è¡¨,å¹¶æŸ¥çœ‹ç»“æœ
> é€šè¿‡æŸ¥çœ‹ç»“æœæ¥çœ‹,åªæ˜¯åˆ é™¤äº†æè¿°è¡¨çš„å…ƒæ•°æ®ä¿¡æ¯,è€Œä¸ä¼šåˆ é™¤è¯¥æ•°æ®è¡¨çš„å…ƒæ•°æ®.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_004.jpg)
> å¯é€šè¿‡å†æ¬¡åˆ›å»ºå¤–éƒ¨deptæ•°æ®è¡¨,ä¾æ—§å¯ä»¥æŸ¥çœ‹åˆ°deptå…ƒæ•°æ®æ‰€åœ¨.
```
hive (default)> create external table if not exists dept(deptid int,dname string,loc int)row format delimited fields terminated by '\t';
OK
Time taken: 0.039 seconds
hive (default)> select * from dept;
OK
dept.deptid     dept.dname      dept.loc
50      ACCOUNTING      1900
60      RESEARCH        1800
70      SALES   NULL
80      OPERATIONS      1700
Time taken: 0.06 seconds, Fetched: 4 row(s)
hive (default)> 
```
#### 4.5.4 ç®¡ç†éƒ¨ä¸å¤–éƒ¨è¡¨ç›¸äº’è½¬æ¢ 
> æ³¨: ('EXTERNAL'='TRUE') & ('EXTERNAL'='FALSE') ä¸ºå›ºå®šå†™æ³•,åŒºåˆ†å¤§å°å†™.
##### æŸ¥è¯¢å†…éƒ¨è¡¨æ•°æ®è¡¨ç±»å‹
```
hive (default)> desc formatted test001;
Table Type:             MANAGED_TABLE  
```
##### ä¿®æ”¹å†…éƒ¨è¡¨ä¸ºå¤–éƒ¨è¡¨
```
alter table test001 set tblproperties('EXTERNAL'='TRUE');
```
##### æŸ¥è¯¢å¤–éƒ¨è¡¨æ•°æ®è¡¨ç±»å‹
```
hive (default)> desc formatted test001;
Table Type:             EXTERNAL_TABLE  
```
##### ä¿®æ”¹å¤–éƒ¨è¡¨ä¸ºå†…éƒ¨è¡¨
```
alter table test001 set tblproperties('EXTERNAL'='FALSE');
```
##### æŸ¥è¯¢å†…éƒ¨è¡¨æ•°æ®è¡¨ç±»å‹
```
hive (default)> desc formatted test001;
Table Type:             MANAGED_TABLE  
```


### 4.6 åˆ†åŒºè¡¨
> åˆ†åŒºè¡¨å®é™…ä¸Šå°±æ˜¯å¯¹åº”ä¸€ä¸ªHDFSæ–‡ä»¶ç³»ç»Ÿä¸Šçš„ç‹¬ç«‹çš„æ–‡ä»¶å¤¹,è¯¥æ–‡ä»¶å¤¹ä¸‹æ˜¯è¯¥åˆ†åŒºæ‰€æœ‰çš„æ•°æ®æ–‡ä»¶,Hiveä¸­çš„åˆ†åŒºå°±æ˜¯åˆ†ç›®å½•,æŠŠä¸€ä¸ªå¤§çš„æ•°æ®é›†æ ¹æ®ä¸šåŠ¡éœ€è¦åˆ†å‰²æˆå°çš„æ•°æ®é›†,åœ¨æŸ¥è¯¢æ—¶é€šè¿‡WHEREå­å¥ä¸­çš„è¡¨è¾¾å¼é€‰æ‹©æŸ¥è¯¢æ‰€éœ€è¦çš„æŒ‡å®šçš„åˆ†åŒº,è¿™æ ·çš„æŸ¥è¯¢æ•ˆç‡ä¼šæé«˜å¾ˆå¤š.
#### 4.6.1 åˆ›å»ºåˆ†åŒºè¡¨è¯­æ³•
```
hive (default)> create table dept_partition(deptno int, dname string, loc string)
              > partitioned by (month string)
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.614 seconds
hive (default)> 
```
#### 4.6.2 åŠ è½½æ•°æ®åˆ°åˆ†åŒºè¡¨ä¸­
```
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-00');
Loading data to table default.dept_partition partition (month=2020-00-00)
Partition default.dept_partition{month=2020-00-00} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 2.37 seconds
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-01');
Loading data to table default.dept_partition partition (month=2020-00-01)
Partition default.dept_partition{month=2020-00-01} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 0.701 seconds
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-02');
Loading data to table default.dept_partition partition (month=2020-00-02)
Partition default.dept_partition{month=2020-00-02} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 0.558 seconds
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-03');
Loading data to table default.dept_partition partition (month=2020-00-03)
Partition default.dept_partition{month=2020-00-03} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 0.46 seconds
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-04');
Loading data to table default.dept_partition partition (month=2020-00-04)
Partition default.dept_partition{month=2020-00-04} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 0.461 seconds
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-05');
Loading data to table default.dept_partition partition (month=2020-00-05)
Partition default.dept_partition{month=2020-00-05} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 0.422 seconds
hive (default)>
```
#### 4.6.3 æŸ¥è¯¢åˆ†åŒºè¡¨ä¸­æ•°æ®
##### 4.6.3.1 å•åˆ†åŒºæŸ¥è¯¢
```
hive (default)> select * from dept_partition where month='2020-00-05';
OK
dept_partition.deptno   dept_partition.dname    dept_partition.loc      dept_partition.month
50      ACCOUNTING      1900    2020-00-05
60      RESEARCH        1800    2020-00-05
70      SALES           2020-00-05
80      OPERATIONS      1700    2020-00-05
Time taken: 1.186 seconds, Fetched: 4 row(s)
hive (default)>
```
##### 4.6.3.2 å¤šåˆ†åŒºè”åˆæŸ¥è¯¢
```
hive (default)> select * from dept_partition where month='2020-00-05'
              > union
              > select * from dept_partition where month='2020-00-04'
              > union
              > select * from dept_partition where month='2020-00-03'
              > union
              > select * from dept_partition where month='2020-00-02'
              > union
              > select * from dept_partition where month='2020-00-01';
Query ID = root_201346_e7c134f9-1082-4bd6-9588-951592861d78
Total jobs = 4
Launching Job 1 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553737906374_0001, Tracking URL = http://systemhub611:8088/proxy/application_1553737906374_0001/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553737906374_0001
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.5 sec
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.42 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.2 sec
MapReduce Total cumulative CPU time: 5 seconds 200 msec
Ended Job = job_1553737906374_0001
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553737906374_0002, Tracking URL = http://systemhub611:8088/proxy/application_1553737906374_0002/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553737906374_0002
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
Stage-2 map = 0%,  reduce = 0%
Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.87 sec
Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.42 sec
MapReduce Total cumulative CPU time: 4 seconds 420 msec
Ended Job = job_1553737906374_0002
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553737906374_0003, Tracking URL = http://systemhub611:8088/proxy/application_1553737906374_0003/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553737906374_0003
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 1
Stage-3 map = 0%,  reduce = 0%
Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.67 sec
Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 4.41 sec
MapReduce Total cumulative CPU time: 4 seconds 410 msec
Ended Job = job_1553737906374_0003
Launching Job 4 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553737906374_0004, Tracking URL = http://systemhub611:8088/proxy/application_1553737906374_0004/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553737906374_0004
Hadoop job information for Stage-4: number of mappers: 2; number of reducers: 1
Stage-4 map = 0%,  reduce = 0%
Stage-4 map = 50%,  reduce = 0%, Cumulative CPU 1.21 sec
Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.61 sec
Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.5 sec
MapReduce Total cumulative CPU time: 4 seconds 500 msec
Ended Job = job_1553737906374_0004
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 5.2 sec   HDFS Read: 15645 HDFS Write: 434 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 4.42 sec   HDFS Read: 14689 HDFS Write: 603 SUCCESS
Stage-Stage-3: Map: 2  Reduce: 1   Cumulative CPU: 4.41 sec   HDFS Read: 14810 HDFS Write: 772 SUCCESS
Stage-Stage-4: Map: 2  Reduce: 1   Cumulative CPU: 4.5 sec   HDFS Read: 15517 HDFS Write: 545 SUCCESS
Total MapReduce CPU Time Spent: 18 seconds 530 msec
OK
_u5.deptno      _u5.dname       _u5.loc _u5.month
50      ACCOUNTING      1900    2020-00-01
50      ACCOUNTING      1900    2020-00-02
50      ACCOUNTING      1900    2020-00-03
50      ACCOUNTING      1900    2020-00-04
50      ACCOUNTING      1900    2020-00-05
60      RESEARCH        1800    2020-00-01
60      RESEARCH        1800    2020-00-02
60      RESEARCH        1800    2020-00-03
60      RESEARCH        1800    2020-00-04
60      RESEARCH        1800    2020-00-05
70      SALES           2020-00-01
70      SALES           2020-00-02
70      SALES           2020-00-03
70      SALES           2020-00-04
70      SALES           2020-00-05
80      OPERATIONS      1700    2020-00-01
80      OPERATIONS      1700    2020-00-02
80      OPERATIONS      1700    2020-00-03
80      OPERATIONS      1700    2020-00-04
80      OPERATIONS      1700    2020-00-05
Time taken: 164.191 seconds, Fetched: 20 row(s)
hive (default)>
```

#### 4.6.4 å¢åŠ åˆ†åŒº
##### 4.6.4.1 åˆ›å»ºå•ä¸ªåˆ†åŒº
```
hive (default)> alter table dept_partition add partition(month='2020-00-06');
OK
Time taken: 0.501 seconds
hive (default)> 	
```
##### 4.6.4.2 åŒæ—¶åˆ›å»ºå¤šä¸ªåˆ†åŒº
```
hive (default)> alter table dept_partition add partition(month='2020-00-07') partition(month='2020-00-08');
OK
Time taken: 0.184 seconds
hive (default)> 
```
#### 4.6.5 åˆ é™¤åˆ†åŒº
##### 4.6.1 åˆ é™¤å•ä¸ªåˆ†åŒº
```
hive (default)> alter table dept_partition drop partition(month='2020-00-06');
Dropped the partition month=2020-00-06
OK
Time taken: 0.273 seconds
hive (default)> 
```
##### 4.6.2 åŒæ—¶åˆ é™¤å¤šä¸ªåˆ†åŒº
```
hive (default)> alter table dept_partition drop partition(month='2020-00-07'),partition(month='2020-00-08');
Dropped the partition month=2020-00-07
Dropped the partition month=2020-00-08
OK
Time taken: 0.837 seconds
hive (default)> 
```
#### 4.6.6 æŸ¥çœ‹åˆ†åŒºè¡¨æœ‰å¤šå°‘åˆ†åŒº
```
hive (default)> show partitions dept_partition;
OK
partition
month=2020-00-00
month=2020-00-01
month=2020-00-02
month=2020-00-03
month=2020-00-04
month=2020-00-05
Time taken: 0.155 seconds, Fetched: 6 row(s)
hive (default)> 
```

#### 4.6.7 æŸ¥çœ‹åˆ†åŒºè¡¨ç»“æ„
```
hive (default)> desc formatted dept_partition;
OK
col_name        data_type       comment
# col_name              data_type               comment             
                 
deptno                  int                                         
dname                   string                                      
loc                     string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
month                   string                                      
                 
# Detailed Table Information             
Database:               default                  
Owner:                  root                        
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://systemhub511:9000/user/hive/warehouse/dept_partition      
Table Type:             MANAGED_TABLE            
Table Parameters:                
        transient_lastDdlTime   1553753699          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \t                  
        serialization.format    \t                  
Time taken: 0.183 seconds, Fetched: 34 row(s)
hive (default)>
```

#### 4.6.8 åˆ†åŒºè¡¨æ³¨æ„äº‹é¡¹
##### 4.6.8.1 åˆ›å»ºäºŒçº§åˆ†åŒºè¡¨
```
hive (default)> create table dept_partition002(deptno int, dname string, loc string)
              > partitioned by (month string,day string)
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.215 seconds
hive (default)> 
```
##### 4.6.8.2 åŠ è½½æ•°æ®
###### 4.6.8.2.1 åŠ è½½æ•°æ®åˆ°äºŒçº§åˆ†åŒºè¡¨
```
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition002 partition(month='2020-00-06', day='0101');
Loading data to table default.dept_partition002 partition (month=2020-00-06, day=0101)
Partition default.dept_partition002{month=2020-00-06, day=0101} stats: [numFiles=1, numRows=0, totalSize=70, rawDataSize=0]
OK
Time taken: 1.914 seconds
hive (default)> 
```
###### 4.6.8.2.2 æŸ¥è¯¢åˆ†åŒºæ•°æ®
```
hive (default)> select * from dept_partition002 where month='2020-00-06' and day='0101';
OK
dept_partition002.deptno        dept_partition002.dname dept_partition002.loc   dept_partition002.month dept_partition002.day
50      ACCOUNTING      1900    2020-00-06      0101
60      RESEARCH        1800    2020-00-06      0101
70      SALES           2020-00-06      0101
80      OPERATIONS      1700    2020-00-06      0101
Time taken: 0.284 seconds, Fetched: 4 row(s)
hive (default)>  
```
##### 4.6.8.3 å°†æ•°æ®ç›´æ¥ä¸Šä¼ åˆ°åˆ†åŒºç›®å½•,è®©åˆ†åŒºè¡¨ä¸æ•°æ®äº§ç”Ÿå…³è”çš„æ–¹å¼
###### 4.6.8.3.1 æ–¹å¼ä¸€: ä¸Šä¼ æ•°æ®åä¿®å¤
> ä¸Šä¼ æ•°æ®
```
hive (default)> dfs -mkdir -p /user/hive/warehouse/dept_partition/month=2020-00-10;
hive (default)> 
```
> æŸ¥è¯¢æ•°æ®(æŸ¥è¯¢ä¸åˆ°åˆšä¸Šä¼ çš„æ•°æ®)
```
hive (default)> select * from dept_partition where month='2020-00-10';
OK
dept_partition.deptno   dept_partition.dname    dept_partition.loc      dept_partition.month
Time taken: 0.147 seconds
hive (default)> 
```
> æ‰§è¡Œä¿®å¤å‘½ä»¤
```
hive (default)> msck repair table dept_partition;
OK
Partitions not in metastore:    dept_partition:month=2020-00-10
Repair: Added partition to metastore dept_partition:month=2020-00-10
Time taken: 0.297 seconds, Fetched: 2 row(s)
hive (default)> 
```

###### 4.6.8.3.2 æ–¹å¼äºŒ: ä¸Šä¼ æ•°æ®åæ·»åŠ åˆ†åŒº
> ä¸Šä¼ æ•°æ®
```
hive (default)> dfs -mkdir -p /user/hive/warehouse/dept_partition/month=2020-00-12;
hive (default)> 
```
> æ‰§è¡Œæ·»åŠ åˆ†åŒº
```
alter table dept_partition drop partition(month='2020-00-06',day='0101');
OK
Time taken: 0.273 seconds
hive (default)> 
```
> æŸ¥è¯¢æ•°æ®
```
hive (default)> select * from dept_partition002 where month='2020-00-06' and day='0101';
OK
dept_partition002.deptno        dept_partition002.dname dept_partition002.loc   dept_partition002.month dept_partition002.day
50      ACCOUNTING      1900    2020-00-06      0101
60      RESEARCH        1800    2020-00-06      0101
70      SALES           2020-00-06      0101
80      OPERATIONS      1700    2020-00-06      0101
Time taken: 0.284 seconds, Fetched: 4 row(s)
hive (default)>  
```
###### 4.6.8.3.3 æ–¹å¼ä¸‰: ä¸Šä¼ æ•°æ®åloadæ•°æ®åˆ°åˆ†åŒº
> åˆ›å»ºç›®å½•
```
hive (default)> dfs -mkdir -p /user/hive/warehouse/dept_partition/month=2020-00-06/day=0101;
hive (default)> 
```
> ä¸Šä¼ æ•°æ®
```
hive (default)> load data local inpath '/opt/module/datas/dept.txt'into table default.dept_partition partition(month='2020-00-06', day='0101');
```
> æŸ¥è¯¢æ•°æ®
```
hive (default)> select * from dept_partition where month='2020-00-06' and day='0101';
OK
dept_partition002.deptno        dept_partition002.dname dept_partition002.loc   dept_partition002.month dept_partition002.day
50      ACCOUNTING      1900    2020-00-06      0101
60      RESEARCH        1800    2020-00-06      0101
70      SALES           2020-00-06      0101
80      OPERATIONS      1700    2020-00-06      0101
Time taken: 0.284 seconds, Fetched: 4 row(s)
hive (default)>  
```

### 4.7 ä¿®æ”¹è¡¨
#### 4.7.1 é‡å‘½åè¡¨
##### 4.7.1.1 è¯­æ³•
``` sql
ALTER TABLE table_name RENAME TO new_table_name;
```
##### 4.7.1.2 å®æ“æ¡ˆä¾‹
```
hive (default)> alter table dept_partition002 rename to dept_partition003;
OK
Time taken: 0.324 seconds
hive (default)> 
```
#### 4.7.2 å¢åŠ /ä¿®æ”¹/æ›¿æ¢åˆ—ä¿¡æ¯
##### 4.7.2.1 è¯­æ³•
###### 4.7.2.1.1 æ›´æ–°åˆ—
```
ALTER   TABLE   table_name   CHANGE   [COLUMN]   col_old_name   col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]
```
###### 4.7.2.1.2 å¢åŠ å’Œæ›¿æ¢åˆ—
```
ALTER    TABLE    table_name    ADD|REPLACE    COLUMNS    (col_name    data_type [COMMENT col_comment], ...)
```
> æ³¨ï¼šADDæ˜¯ä»£è¡¨æ–°å¢ä¸€å­—æ®µ,å­—æ®µä½ç½®åœ¨æ‰€æœ‰åˆ—åé¢(partitionåˆ—å‰),REPLACEåˆ™æ˜¯è¡¨ç¤ºæ›¿æ¢è¡¨ä¸­æ‰€æœ‰å­—æ®µ.

##### 4.7.2.2 å®æ“æ¡ˆä¾‹
###### 4.7.2.2.1 æŸ¥è¯¢è¡¨ç»“æ„
```
hive (default)> desc dept_partition;
OK
col_name        data_type       comment
deptno                  int                                         
dname                   string                                      
loc                     string                                      
month                   string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
month                   string                                      
Time taken: 0.122 seconds, Fetched: 9 row(s)
hive (default)> 
```
###### 4.7.2.2.2 æ·»åŠ åˆ—
```
hive (default)> alter table dept_partition add columns(deptdesc string);
OK
Time taken: 0.133 seconds
hive (default)>
```
> æŸ¥è¯¢è¡¨ç»“æ„
```
hive (default)> desc dept_partition;
OK
col_name        data_type       comment
deptno                  int                                         
dname                   string                                      
loc                     string                                      
deptdesc                string                                      
month                   string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
month                   string                                      
Time taken: 0.09 seconds, Fetched: 10 row(s)
hive (default)> 
```
###### 4.7.2.2.3 æ›´æ–°åˆ—
```
hive (default)> alter table dept_partition change column deptdesc desc int;
OK
Time taken: 0.113 seconds
hive (default)> 
```
> æŸ¥è¯¢è¡¨ç»“æ„
```
hive (default)> desc dept_partition;
OK
col_name        data_type       comment
deptno                  int                                         
dname                   string                                      
loc                     string                                      
desc                    int                                         
month                   string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
month                   string                                      
Time taken: 0.093 seconds, Fetched: 10 row(s)
hive (default)> 
```
###### 4.7.2.2.4 æ›¿æ¢åˆ—
```
hive (default)> alter table dept_partition replace columns(deptno string,dname string,loc string);
OK
Time taken: 0.084 seconds
hive (default)> 
```
> æŸ¥è¯¢è¡¨ç»“æ„
```
hive (default)> desc dept_partition;
OK
col_name        data_type       comment
deptno                  string                                      
dname                   string                                      
loc                     string                                      
month                   string                                      
                 
# Partition Information          
# col_name              data_type               comment             
                 
month                   string                                      
Time taken: 0.094 seconds, Fetched: 9 row(s)
hive (default)> 
```
###### 4.7.2.2.5 åˆ é™¤è¡¨
```
hive (default)> drop table dept_partition003;
OK
Time taken: 0.343 seconds
hive (default)> 
```

## 5. DML æ•°æ®æ“ä½œ
> DML(Data Manipulation Language)æ•°æ®æ“çºµè¯­è¨€,è´Ÿè´£å¯¹æ•°æ®åº“å¯¹è±¡è¿è¡Œæ•°æ®è®¿é—®å·¥ä½œçš„æŒ‡ä»¤é›†.

### 5.1 æ•°æ®å¯¼å…¥
#### 5.1.1 (Load æ¨¡å¼)å‘è¡¨ä¸­è£…è½½æ•°æ®
##### 5.1.1.1 è¯­æ³•
```
load  data  [local]  inpath  'FilePath'  [overwrite]  into  table  TableName [partition (partcol1=val1,...)];
```
> 1.`load data`: è¡¨ç¤ºåŠ è½½æ•°æ®.
> 
> 2.`local`: è¡¨ç¤ºä»æœ¬åœ°åŠ è½½æ•°æ®åˆ°hiveè¡¨,å¦åˆ™ä»HDFSåŠ è½½æ•°æ®åˆ°hiveè¡¨.
> 
> 3.`inpath`: è¡¨ç¤ºåŠ è½½æ•°æ®çš„è·¯å¾„.
> 
> 4.`into table`: è¡¨ç¤ºåŠ è½½åˆ°å“ªå¼ æ•°æ®è¡¨.
> 
> 5.`TableName`: è¡¨ç¤ºå…·ä½“æ•°æ®è¡¨.
> 
> 6.`overwrite`: è¡¨ç¤ºè¦†ç›–è¡¨ä¸­å·²æœ‰æ•°æ®,å¦åˆ™è¡¨ç¤ºè¿½åŠ .
> 
> 7.`partition`: è¡¨ç¤ºä¸Šä¼ åˆ°æŒ‡å®šåˆ†åŒº.

##### 5.1.2.2 å®æ“æ¡ˆä¾‹
###### 5.1.2.2.1 åˆ›å»ºä¸€å¼ æ•°æ®è¡¨
```
hive (default)> create table test005(id string,name string)row format delimited fields terminated by '\t';
OK
Time taken: 0.081 seconds
hive (default)> 
```
###### 5.1.2.2.2 åŠ è½½æœ¬åœ°æ–‡ä»¶åˆ°hive
```
hive (default)> load data local inpath '/opt/module/datas/test.txt' into table test005;
Loading data to table default.test005
Table default.test005 stats: [numFiles=1, totalSize=56]
OK
Time taken: 0.322 seconds
hive (default)> 
```
###### 5.1.2.2.3 åŠ è½½HDFSæ–‡ä»¶åˆ°hiveä¸­
> ä¸Šä¼ æ–‡ä»¶åˆ°HDFS
```
hive (default)> dfs -mkdir -p /user/geekparkhub/hive;
hive (default)> dfs -put /opt/module/datas/test.txt /user/geekparkhub/hive/test.txt;
```
###### 5.1.2.2.4 åŠ è½½æ•°æ®è¦†ç›–è¡¨ä¸­å·²æœ‰çš„æ•°æ®
> åŠ è½½æ•°æ®è¦†ç›–è¡¨ä¸­å·²æœ‰çš„æ•°æ®
```
hive (default)> load data inpath '/user/geekparkhub/hive/test.txt' overwrite into table test005;
Loading data to table default.test005
Table default.test005 stats: [numFiles=1, numRows=0, totalSize=56, rawDataSize=0]
OK
Time taken: 0.21 seconds
hive (default)> 
```

#### 5.1.2 (Insert æ¨¡å¼) é€šè¿‡æŸ¥è¯¢è¯­å¥å‘è¡¨ä¸­æ’å…¥æ•°æ®
##### 5.1.2.1 åŸºæœ¬æ¨¡å¼æ’å…¥ (æ ¹æ®å•å¼ è¡¨æŸ¥è¯¢ç»“æœ)
```
hive (default)> insert overwrite table dept_partition002 partition(month='2020-00-11') select id, name from dept_partition where month='2020-00-12';
```
##### 5.1.2.2 å¤šæ’å…¥æ¨¡å¼ (æ ¹æ®å¤šå¼ è¡¨æŸ¥è¯¢ç»“æœ)
```
hive (default)> from dept_partition
insert overwrite table dept_partition002 partition(month='2020-00-13')
select id, name where month='2020-00-10'
insert overwrite table dept_partition002 partition(month='2020-00-14')
select id, name where month='2020-00-10';
```

#### 5.1.3 (As Select æ¨¡å¼) æŸ¥è¯¢è¯­å¥ä¸­åˆ›å»ºè¡¨å¹¶åŠ è½½æ•°æ®
> æ ¹æ®æŸ¥è¯¢ç»“æœåˆ›å»ºè¡¨ (æŸ¥è¯¢çš„ç»“æœä¼šæ·»åŠ åˆ°æ–°åˆ›å»ºçš„è¡¨ä¸­)
```
create table if not exists test006 as select * from test;
```
#### 5.1.4 åˆ›å»ºè¡¨æ—¶é€šè¿‡LocationæŒ‡å®šåŠ è½½æ•°æ®è·¯å¾„
##### 5.1.4.1 åˆ›å»ºè¡¨,å¹¶æŒ‡å®šåœ¨HDFSä¸Šçš„ä½ç½®
```
hive (default)> create table test007 like test;
OK
Time taken: 0.108 seconds
hive (default)>
```
##### 5.1.4.2 ä¸Šä¼ æ•°æ®åˆ°hdfsä¸Š
```
hive (default)> dfs -put /opt/module/datas/test.txt /user/hive/warehouse/test007;
```
##### 5.1.4.3 æŸ¥è¯¢æ•°æ®
```
hive (default)> select * from test007;
OK
test007.id      test007.name
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 0.087 seconds, Fetched: 4 row(s)
hive (default)> 
```

#### 5.1.5 Importæ•°æ®åˆ°æŒ‡å®šHiveè¡¨ä¸­
> å…ˆç”¨exportå¯¼å‡ºå,å†å°†æ•°æ®å¯¼å…¥.
```
hive (default)> import table test006 from
              > '/user/geekparkhub/export/test002';
Copying data from hdfs://systemhub511:9000/user/geekparkhub/export/test002/data
Copying file: hdfs://systemhub511:9000/user/geekparkhub/export/test002/data/test.txt
Loading data to table default.test006
OK
Time taken: 0.571 seconds
hive (default)> 
```

### 5.2 æ•°æ®å¯¼å‡º
#### 5.2.1 Insert å¯¼å‡º
##### 5.2.1 å°†æŸ¥è¯¢çš„ç»“æœå¯¼å‡ºåˆ°æœ¬åœ°
```
hive (default)> insert overwrite local directory '/opt/module/datas/export/test'
              > select * from test;
Query ID = root_20190328214019_01955e31-2364-4c10-8661-cafb4bd9bb38
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553780256887_0003, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0003/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
MapReduce Total cumulative CPU time: 1 seconds 350 msec
Ended Job = job_1553780256887_0003
Copying data to local directory /opt/module/datas/export/test
Copying data to local directory /opt/module/datas/export/test
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.35 sec   HDFS Read: 2982 HDFS Write: 56 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 350 msec
OK
test.id test.name
Time taken: 24.917 seconds
hive (default)> 
```
```
[root@systemhub711 ~]# cat /opt/module/datas/export/test/000000_0
1TestUser001
2TestUser002
3TestUser003
4TestUser004
[root@systemhub711 ~]# 
```
##### 5.2.2 å°†æŸ¥è¯¢çš„ç»“æœæ ¼å¼åŒ–å¯¼å‡ºåˆ°æœ¬åœ°
```
hive (default)> insert overwrite local directory '/opt/module/datas/export/test001'
              > row format delimited fields terminated by '\t'
              > select * from test;
Query ID = root_20190328214116_2a678b33-dc52-4aef-97d4-7c4932d14dc2
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553780256887_0004, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0004/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.32 sec
MapReduce Total cumulative CPU time: 1 seconds 320 msec
Ended Job = job_1553780256887_0004
Copying data to local directory /opt/module/datas/export/test001
Copying data to local directory /opt/module/datas/export/test001
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.32 sec   HDFS Read: 2995 HDFS Write: 56 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 320 msec
OK
test.id test.name
Time taken: 24.856 seconds
hive (default)> 
```
```
[root@systemhub711 ~]# cat /opt/module/datas/export/test001/000000_0
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
[root@systemhub711 ~]# 
```
##### 5.2.3 å°†æŸ¥è¯¢çš„ç»“æœå¯¼å‡ºåˆ°HDFS
```
hive (default)> insert overwrite directory '/user/geekparkhub/export/test'
              > row format delimited fields terminated by '\t'
              > select * from test;
Query ID = root_20190328214749_261af019-2978-43a9-99be-4cce6bf63837
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553780256887_0005, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0005/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.3 sec
MapReduce Total cumulative CPU time: 1 seconds 300 msec
Ended Job = job_1553780256887_0005
Stage-3 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
Moving data to: hdfs://systemhub511:9000/user/geekparkhub/export/test/.hive-staging_hive_2019-03-28_21-47-49_139_9022836247083242678-1/-ext-10000
Moving data to: /user/geekparkhub/export/test
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.3 sec   HDFS Read: 2983 HDFS Write: 56 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 300 msec
OK
test.id test.name
Time taken: 24.801 seconds
hive (default)> 
```
```
hive (default)> dfs -cat /user/geekparkhub/export/test/*;
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
hive (default)> 
```

#### 5.2.2 HadoopæŒ‡ä»¤å¯¼å‡ºåˆ°æœ¬åœ°
```
hive (default)> dfs -get /user/hive/warehouse/dept_partition/month=2020-00-00/dept.txt /opt/module/datas/test002.txt;
hive (default)> 
```
#### 5.2.3 Hive ShellæŒ‡ä»¤å¯¼å‡º
```
[root@systemhub711 hive]# bin/hive -e 'select * from test;' > /opt/module/datas/test002.txt; 

Logging initialized using configuration in file:/opt/module/hive/conf/hive-log4j.properties
OK
Time taken: 2.32 seconds, Fetched: 4 row(s)
[root@systemhub711 hive]# 
```
#### 5.2.4 Exportå¯¼å‡ºåˆ°HDFSä¸Š
```
hive (default)> export table test to '/user/geekparkhub/export/test002';
Copying data from file:/tmp/root/16f13154-2779-4719-89d3-3453b1468948/hive_411_6501223345710054079-1/-local-10000/_metadata
Copying file: file:/tmp/root/16f13154-2779-4719-89d3-3453b1468948/hive_411_6501223345710054079-1/-local-10000/_metadata
Copying data from hdfs://systemhub511:9000/user/hive/warehouse/test
Copying file: hdfs://systemhub511:9000/user/hive/warehouse/test/test.txt
OK
Time taken: 0.458 seconds
hive (default)>
hive (default)> dfs -cat /user/geekparkhub/export/test002/data/test.txt;
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
hive (default)> 
```

### 5.3 æ¸…é™¤è¡¨ä¸­æ•°æ® (Truncate)
> æ³¨æ„: Truncateåªèƒ½åˆ é™¤ç®¡ç†è¡¨,ä¸èƒ½åˆ é™¤å¤–éƒ¨è¡¨ä¸­æ•°æ®
```
hive (default)> select * from test007;
OK
test007.id      test007.name
1       TestUser001
2       TestUser002
3       TestUser003
4       TestUser004
Time taken: 0.079 seconds, Fetched: 4 row(s)
hive (default)> truncate table test007;
OK
Time taken: 0.083 seconds
hive (default)> select * from test007;
OK
test007.id      test007.name
Time taken: 0.078 seconds
hive (default)> 
```

## 6. æŸ¥è¯¢
> æŸ¥è¯¢åŸºæœ¬è¯­æ³•
```
[WITH CommonTableExpression (, CommonTableExpression)*]
(Note: Only available starting with Hive0.13.0)
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list| [DISTRIBUTE BY col_list] [SORT BY col_list]]
[LIMIT number]
```

### 6.1 åŸºæœ¬æŸ¥è¯¢ (Select...From)
#### 6.1.1 å…¨è¡¨å’Œç‰¹å®šåˆ—æŸ¥è¯¢
> æ³¨æ„:
> 
> 1.SQL è¯­è¨€å¤§å°å†™ä¸æ•æ„Ÿ.
> 
> 2.SQL å¯ä»¥å†™åœ¨ä¸€è¡Œæˆ–è€…å¤šè¡Œ.
> 
> 3.å…³é”®å­—ä¸èƒ½è¢«ç¼©å†™ä¹Ÿä¸èƒ½åˆ†è¡Œ.
> 
> 4.å„å­å¥ä¸€èˆ¬è¦åˆ†è¡Œå†™.
> 
> 5.ä½¿ç”¨ç¼©è¿›æé«˜è¯­å¥çš„å¯è¯»æ€§.
##### 6.1.1.1 å…¨è¡¨æŸ¥è¯¢
```
hive (default)> select * from emp;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0    
Time taken: 0.129 seconds, Fetched: 9 row(s)
hive (default)> 
```
##### 6.1.1.2 é€‰æ‹©ç‰¹å®šåˆ—æŸ¥è¯¢
```
hive (default)> select empno,ename from emp;
OK
empno   ename
7369    SMITH
7499    ALLTE
7521    WAROS
7566    JOSSS
7654    SOCTD
7698    ADAMS
7782    JAMSK
7788    FOESS
7939    KINGS
Time taken: 0.123 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.1.2 åˆ—åˆ«å
> åœ¨åˆ—åå’Œåˆ«åä¹‹é—´åŠ å…¥å…³é”®å­—`AS`
```
hive (default)> select empno as no,ename as name from emp;
OK
no      name
7369    SMITH
7499    ALLTE
7521    WAROS
7566    JOSSS
7654    SOCTD
7698    ADAMS
7782    JAMSK
7788    FOESS
7939    KINGS
Time taken: 0.078 seconds, Fetched: 9 row(s)
hive (default)> 
```
#### 6.1.3 ç®—æœ¯è¿ç®—ç¬¦

| è¿ç®—ç¬¦      |     æè¿° |
| :--------: | :--------:|
| A + B    |   Aå’ŒB ç›¸åŠ  |
| A - B   |   Aå‡B |
| A * B    |   Aå’ŒB ç›¸ä¹˜ |
| A / B    |   Aé™¤ä»¥B |
| A % B    |   Aå¯¹Bå–ä½™ |
| A & B    |   Aå’ŒBæŒ‰ä½å–ä¸ |
| AB    |   Aå’ŒBæŒ‰ä½å–æˆ– |
| A+B    |   Aå’ŒBæŒ‰ä½å–å¼‚æˆ– |
| ~A    |   AæŒ‰ä½å–å |

> æŸ¥è¯¢æ‰€å‘˜å·¥è–ªæ°´+1
```
hive (default)> select sal +1 as wage from emp;
OK
wage
801.0
1601.0
1251.18
2895.25
2853.3
25525.02
1101.0
951.0
3001.0
Time taken: 0.063 seconds, Fetched: 9 row(s)
hive (default)> 
```
#### 6.1.4 å¸¸ç”¨å‡½æ•°
#### 6.1.4.1 æ±‚æ€»è¡Œæ•° (count)
```
hive (default)> select count(*) cnt from emp;
Query ID = root_20190328225729_5783bab3-92a5-4a6c-b830-c0f8e7127225
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553780256887_0006, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0006/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.55 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.43 sec
MapReduce Total cumulative CPU time: 3 seconds 430 msec
Ended Job = job_1553780256887_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.43 sec   HDFS Read: 7647 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 430 msec
OK
cnt
9
Time taken: 42.734 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 6.1.4.2 æ±‚å·¥èµ„çš„æœ€å¤§å€¼ (max)
```
hive (default)> select max(sal) max_wage from emp;
Query ID = root_20190328225946_778636eb-c34a-4a75-b8e5-68684633a0a3
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553780256887_0007, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0007/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.33 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.07 sec
MapReduce Total cumulative CPU time: 3 seconds 70 msec
Ended Job = job_1553780256887_0007
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.07 sec   HDFS Read: 7836 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 70 msec
OK
max_wage
25524.02
Time taken: 31.112 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 6.1.4.3 æ±‚å·¥èµ„çš„æœ€å°å€¼ (min)
```
hive (default)> select min(sal) min_wage from emp;
Query ID = root_20190328230109_5dbce45f-9887-44cc-b6fb-5aa64ac11a50
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553780256887_0008, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0008/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.52 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.42 sec
MapReduce Total cumulative CPU time: 3 seconds 420 msec
Ended Job = job_1553780256887_0008
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.42 sec   HDFS Read: 7836 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 420 msec
OK
min_wage
800.0
Time taken: 30.23 seconds, Fetched: 1 row(s)
hive (default)>
```
#### 6.1.4.4 æ±‚å·¥èµ„çš„æ€»å’Œ (sum)
```
hive (default)> select sum(sal) sum_wage from emp;
Query ID = root_20190328230304_600ecafd-b2a6-4b53-8ba8-fb9938c9f0c9
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553780256887_0010, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0010/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.36 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.13 sec
MapReduce Total cumulative CPU time: 3 seconds 130 msec
Ended Job = job_1553780256887_0010
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.13 sec   HDFS Read: 7830 HDFS Write: 9 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 130 msec
OK
sum_wage
39970.75
Time taken: 33.111 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 6.1.4.5 æ±‚å·¥èµ„çš„å¹³å‡å€¼ (avg)
```
hive (default)> select avg(sal) avg_wage from emp;
Query ID = root_20190328230301_15ba0d3a-130a-42a5-9b9a-a313f187e814
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553780256887_0009, Tracking URL = http://systemhub611:8088/proxy/application_1553780256887_0009/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553780256887_0009
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.21 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.96 sec
MapReduce Total cumulative CPU time: 2 seconds 960 msec
Ended Job = job_1553780256887_0009
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.96 sec   HDFS Read: 8011 HDFS Write: 18 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 960 msec
OK
avg_wage
4441.194444444444
Time taken: 67.764 seconds, Fetched: 1 row(s)
hive (default)> 
```

#### 6.1.5 Limitè¯­å¥
> å…¸å‹çš„æŸ¥è¯¢ä¼šè¿”å›å¤šè¡Œæ•°æ®,LIMITå­å¥ç”¨äºé™åˆ¶è¿”å›çš„è¡Œæ•°.
```
hive (default)> select * from emp limit 5;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
Time taken: 0.135 seconds, Fetched: 5 row(s)
hive (default)> 
```

### 6.2 Whereè¯­å¥
> 1.ä½¿ç”¨`WHERE`å­å¥,å°†ä¸æ»¡è¶³æ¡ä»¶çš„è¡Œè¿‡æ»¤æ‰.
> 2.`WHERE`å­å¥ç´§éš`FROM`å­å¥.
> 3.æ¡ˆä¾‹å®æ“
> æŸ¥è¯¢å‡ºè–ªæ°´å¤§äº1000çš„æ‰€æœ‰å‘˜å·¥.
```
hive (default)> select * from emp where sal > 1000;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.26 seconds, Fetched: 7 row(s)
hive (default)> 
```
#### 6.2.1 æ¯”è¾ƒè¿ç®—ç¬¦ (Between / In / Is Null)
> ä¸‹é¢è¡¨ä¸­æè¿°äº†è°“è¯æ“ä½œç¬¦,è¿™äº›æ“ä½œç¬¦åŒæ ·å¯ä»¥ç”¨äºJOIN...ONå’ŒHAVINGè¯­å¥ä¸­.
##### 6.2.1.1 æ¯”è¾ƒè¿ç®—ç¬¦å¯¹ç…§è¡¨
| æ“ä½œç¬¦      |     æ”¯æŒæ•°æ®ç±»å‹ |   æè¿°   |
| :--------: | :--------:| :------: |
| A=B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  å¦‚æœAç­‰äºBåˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A<=>B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  å¦‚æœAå’ŒBéƒ½ä¸ºNULL,åˆ™è¿”å›TRUE,å…¶ä»–çš„å’Œç­‰å·(=)æ“ä½œç¬¦çš„ç»“æœä¸€è‡´,å¦‚æœä»»ä¸€ä¸ºNULLåˆ™ç»“æœä¸ºNULL.  |
| A<>B, A!=B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  Aæˆ–è€…Bä¸ºNULLåˆ™è¿”å›NULL,å¦‚æœAä¸ç­‰äºB,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A<B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  Aæˆ–è€…Bä¸ºNULL,åˆ™è¿”å›NULL,å¦‚æœAå°äºB,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A<=B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  Aæˆ–è€…Bä¸ºNULL,åˆ™è¿”å›NULL,å¦‚æœAå°äºç­‰äºB,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A>B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  Aæˆ–è€…Bä¸ºNULL,åˆ™è¿”å›NULL,å¦‚æœAå¤§äºB,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A>=B    |   åŸºæœ¬æ•°æ®ç±»å‹ |  Aæˆ–è€…Bä¸ºNULL,åˆ™è¿”å›NULL,å¦‚æœAå¤§äºç­‰äºB,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A [NOT] BETWEEN B AND C    |   åŸºæœ¬æ•°æ®ç±»å‹ |  å¦‚æœA,Bæˆ–è€…Cä»»ä¸€ä¸ºNULL,åˆ™ç»“æœä¸ºNULL,å¦‚æœAçš„å€¼å¤§äºç­‰äºBè€Œä¸”å°äºæˆ–ç­‰äºC,åˆ™ç»“æœä¸ºTRUE,åä¹‹ä¸ºFALSE,å¦‚æœä½¿ç”¨NOTå…³é”®å­—åˆ™å¯è¾¾åˆ°ç›¸åçš„æ•ˆæœ.  |
| A IS NULL    |   æ‰€æœ‰æ•°æ®ç±»å‹ |  å¦‚æœAç­‰äºNULL,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE.  |
| A IS NOT NULL    |   æ‰€æœ‰æ•°æ®ç±»å‹ |  å¦‚æœAä¸ç­‰äºNULL,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE. |
| IN(Num1,Num2)    |   æ‰€æœ‰æ•°æ®ç±»å‹ |  ä½¿ç”¨INè¿ç®—æ˜¾ç¤ºåˆ—è¡¨ä¸­çš„å€¼.  |
| A [NOT] LIKE B    |   STRING ç±»å‹ |  Bæ˜¯ä¸€ä¸ªSQLä¸‹çš„ç®€å•æ­£åˆ™è¡¨è¾¾å¼,å¦‚æœAä¸å…¶åŒ¹é…çš„è¯,åˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE,Bçš„è¡¨è¾¾å¼è¯´æ˜å¦‚ä¸‹:â€˜x%â€™è¡¨ç¤ºAå¿…é¡»ä»¥å­—æ¯â€˜xâ€™å¼€å¤´,â€˜%xâ€™è¡¨ç¤ºAå¿…é¡»ä»¥å­—æ¯â€™xâ€™ç»“å°¾,è€Œâ€˜%x%â€™è¡¨ç¤ºAåŒ…å«æœ‰å­—æ¯â€™xâ€™,å¯ä»¥ä½äºå¼€å¤´,ç»“å°¾æˆ–è€…å­—ç¬¦ä¸²ä¸­é—´,å¦‚æœä½¿ç”¨NOT,å…³é”®å­—åˆ™å¯è¾¾åˆ°ç›¸åçš„æ•ˆæœ.  |
| A RLIKE B,A REGEXP B    |   STRING ç±»å‹ |  Bæ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼,å¦‚æœAä¸å…¶åŒ¹é…ï¼Œåˆ™è¿”å›TRUE,åä¹‹è¿”å›FALSE,åŒ¹é…ä½¿ç”¨çš„æ˜¯JDKä¸­çš„æ­£åˆ™è¡¨è¾¾å¼æ¥å£å®ç°çš„,å› ä¸ºæ­£åˆ™ä¹Ÿä¾æ®å…¶ä¸­çš„è§„åˆ™,ä¾‹å¦‚,æ­£åˆ™è¡¨è¾¾å¼å¿…é¡»å’Œæ•´ä¸ªå­—ç¬¦ä¸²Aç›¸åŒ¹é…,è€Œä¸æ˜¯åªéœ€ä¸å…¶å­—ç¬¦ä¸²åŒ¹é….  |

##### 6.2.1.2 æ¡ˆä¾‹å®æ“
###### 6.2.1.2.1 æŸ¥è¯¢è–ªæ°´ç­‰äº3000çš„æ‰€æœ‰å‘˜å·¥
```
hive (default)> select * from emp where sal = 3000;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.063 seconds, Fetched: 1 row(s)
hive (default)> 
```
###### 6.2.1.2.2 æŸ¥è¯¢å·¥èµ„åœ¨500åˆ°1000çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where sal between 500 and 1000;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
Time taken: 0.086 seconds, Fetched: 2 row(s)
hive (default)> 
```
###### 6.2.1.2.3 æŸ¥è¯¢commä¸ºç©ºçš„æ‰€æœ‰å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where comm is null;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
Time taken: 0.077 seconds
hive (default)> 
```
###### 6.2.1.2.4 æŸ¥è¯¢å·¥èµ„æ˜¯1500å’Œ3000çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where sal IN(1500,3000);
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.117 seconds, Fetched: 1 row(s)
hive (default)> 
```


#### 6.2.2 Likeå’ŒRLike
> 1.ä½¿ç”¨`LIKE`è¿ç®—é€‰æ‹©ç±»ä¼¼çš„å€¼.
> 
> 2.é€‰æ‹©æ¡ä»¶å¯ä»¥åŒ…å«å­—ç¬¦æˆ–æ•°å­—:
> `%` ä»£è¡¨é›¶ä¸ªæˆ–å¤šä¸ªå­—ç¬¦(ä»»æ„ä¸ªå­—ç¬¦)
> `_` ä»£è¡¨ä¸€ä¸ªå­—ç¬¦
> 
> 3.`RLIKE`å­å¥æ˜¯Hiveä¸­è¿™ä¸ªåŠŸèƒ½çš„ä¸€ä¸ªæ‰©å±•,å…¶å¯ä»¥é€šè¿‡Javaçš„æ­£åˆ™è¡¨è¾¾å¼è¿™ä¸ªæ›´å¼ºå¤§çš„è¯­è¨€æ¥æŒ‡å®šåŒ¹é…æ¡ä»¶.
##### 6.2.2.1 æ¡ˆä¾‹å®æ“
###### 6.2.2.1.1 æŸ¥æ‰¾ä»¥2å¼€å¤´è–ªæ°´çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where sal LIKE '2%';
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
Time taken: 0.134 seconds, Fetched: 3 row(s)
hive (default)> 
```
###### 6.2.2.1.2 æŸ¥æ‰¾ç¬¬äºŒä¸ªæ•°å€¼ä¸º2è–ªæ°´çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where sal LIKE '_2%';
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
Time taken: 0.075 seconds, Fetched: 1 row(s)
hive (default)> 
```
###### 6.2.2.1.3 æŸ¥æ‰¾è–ªæ°´ä¸­å«æœ‰2çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where sal RLIKE '[2]';
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
Time taken: 0.088 seconds, Fetched: 4 row(s)
hive (default)> 
```

#### 6.2.3 é€»è¾‘è¿ç®—ç¬¦ (And / Or / Not)

| æ“ä½œç¬¦      |     å«ä¹‰ |
| :--------: | :--------:|
| AND    |   é€»è¾‘å¹¶ |
| OR    |   é€»è¾‘æˆ– |
| NOT    |   é€»è¾‘å¦ |
##### 6.2.3.1 æ¡ˆä¾‹å®æ“
###### 6.2.3.1.1 æŸ¥è¯¢è–ªæ°´å¤§äº1000,éƒ¨é—¨æ˜¯30
```
hive (default)> select * from emp where sal > 1000 AND deptno = 30;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
Time taken: 0.107 seconds, Fetched: 2 row(s)
hive (default)> 
```
###### 6.2.3.1.2 æŸ¥è¯¢è–ªæ°´å¤§äº1000,æˆ–è€…éƒ¨é—¨æ˜¯30
```
hive (default)> select * from emp where sal > 1000 or deptno = 30;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.088 seconds, Fetched: 7 row(s)
hive (default)> 
```
###### 6.2.3.1.3 æŸ¥è¯¢é™¤äº†20éƒ¨é—¨å’Œ30éƒ¨é—¨ä»¥å¤–çš„å‘˜å·¥ä¿¡æ¯
```
hive (default)> select * from emp where deptno not IN(30,20);
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
Time taken: 0.067 seconds
hive (default)> 
```

### 6.3 åˆ†ç»„
#### 6.3.1 Group By è¯­å¥
> `GROUP  BY`è¯­å¥é€šå¸¸ä¼šå’Œèšåˆå‡½æ•°ä¸€èµ·ä½¿ç”¨,æŒ‰ç…§ä¸€ä¸ªæˆ–è€…å¤šä¸ªåˆ—é˜Ÿç»“æœè¿›è¡Œåˆ†ç»„,ç„¶åå¯¹æ¯ä¸ªç»„æ‰§è¡Œèšåˆæ“ä½œ.
> 
> 1.è®¡ç®—empè¡¨æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡å·¥èµ„.
```
hive (default)> select avg(sal) avg_sal from emp
              > group by deptno;
Query ID = root_20190329100652_589ff409-e520-40bc-9cb4-4637d9775441
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553824894278_0001, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0001/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.72 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.55 sec
MapReduce Total cumulative CPU time: 3 seconds 550 msec
Ended Job = job_1553824894278_0001
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.55 sec   HDFS Read: 8446 HDFS Write: 37 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 550 msec
OK
avg_sal
5302.938571428572
1425.0900000000001
Time taken: 39.062 seconds, Fetched: 2 row(s)
hive (default)> 
```
> 2.è®¡ç®—empæ¯ä¸ªéƒ¨é—¨ä¸­æ¯ä¸ªå²—ä½çš„æœ€é«˜è–ªæ°´.
```
hive (default)> select deptno,job,avg(sal) avg_sal from emp
              > group by deptno,job;
Query ID = root_20190329101147_338b3ac5-d29e-46eb-abae-d4d2a3bcc3ae
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553824894278_0002, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0002/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.21 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.97 sec
MapReduce Total cumulative CPU time: 2 seconds 970 msec
Ended Job = job_1553824894278_0002
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.97 sec   HDFS Read: 8904 HDFS Write: 182 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 970 msec
OK
deptno  job     avg_sal
NULL    CLADDJHEW       3000.0
NULL    CLAEDFDFD       950.0
NULL    CLERKSKLD       800.0
NULL    JDHYHDSDS       2894.25
NULL    JUSHHWESD       25524.02
NULL    KIHNGSEHN       1100.0
NULL    MANSJUSSD       2852.3
30      SALESMANS       1600.0
30      SJDHHJDJX       1250.18
Time taken: 28.517 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.3.2 Having è¯­å¥
> havingä¸whereä¸åŒç‚¹
> 
> whereé’ˆå¯¹è¡¨ä¸­çš„åˆ—å‘æŒ¥ä½œç”¨,æŸ¥è¯¢æ•°æ®.
> havingé’ˆå¯¹æŸ¥è¯¢ç»“æœä¸­çš„åˆ—å‘æŒ¥ä½œç”¨,ç­›é€‰æ•°æ®.
> whereåé¢ä¸èƒ½å†™åˆ†ç»„å‡½æ•°,è€Œhavingåé¢å¯ä»¥ä½¿ç”¨åˆ†ç»„å‡½æ•°.
> havingåªç”¨äºgroup byåˆ†ç»„ç»Ÿè®¡è¯­å¥.

> æ±‚æ¯ä¸ªéƒ¨é—¨çš„å¹³å‡è–ªæ°´å¤§äº2000çš„éƒ¨é—¨
```
hive (default)> select deptno,avg(sal) avg_sal from emp
              > group by deptno
              > having avg_sal > 2000;
Query ID = root_20190329102456_d4f50a80-3441-421c-9e89-acf8665d98aa
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1553824894278_0003, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0003/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
Stage-1 map = 0%,  reduce = 0%
Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.2 sec
Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.65 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1553824894278_0003
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 8985 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 650 msec
OK
deptno  avg_sal
NULL    5302.938571428572
Time taken: 29.14 seconds, Fetched: 1 row(s)
hive (default)> 
```

### 6.4 Joinè¯­å¥
#### 6.4.1 ç­‰å€¼Join
> Hiveæ”¯æŒé€šå¸¸çš„`SQL JOIN`è¯­å¥,ä½†æ˜¯åªæ”¯æŒç­‰å€¼è¿æ¥,ä¸æ”¯æŒéç­‰å€¼è¿æ¥.
```
hive (default)> select e.empno,e.ename,d.dname
              > from emp e join dept d
              > on e.deptno=d.deptid;
Query ID = root_d5dfa88a-b9a2-4377-aebe-1328cf0b6653
Total jobs = 1
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Execution log at: /tmp/root/aebe-1328cf0b6653.log
Starting to launch local task to process map join;      maximum memory = 518979584
Dump the side-table for tag: 1 with group count: 4 into file: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/-local-10003/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
Uploaded 1 File to: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/hive_2019-03-29_11-02-28_999_8089713357386975442-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile01--.hashtable (373 bytes)
End of local task; Time Taken: 1.969 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553824894278_0004, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0004/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0004
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
Stage-3 map = 0%,  reduce = 0%
Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec
MapReduce Total cumulative CPU time: 1 seconds 810 msec
Ended Job = job_1553824894278_0004
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 1.81 sec   HDFS Read: 7157 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 810 msec
OK
e.empno e.ename d.dname
Time taken: 29.819 seconds
hive (default)> 
```
#### 6.4.2 è¡¨åˆ«å
> 1.å¥½å¤„:
> ä½¿ç”¨åˆ«åå¯ä»¥ç®€åŒ–æŸ¥è¯¢.
> ä½¿ç”¨è¡¨åå‰ç¼€å¯ä»¥æé«˜æ‰§è¡Œæ•ˆç‡.
> 
> æ¡ˆä¾‹å®æ“
```
hive (default)> select e.empno,e.ename,d.dname
              > from emp e join dept d
              > on e.deptno=d.deptid;
```
#### 6.4.3 å†…è¿æ¥
> å†…è¿æ¥: åªæœ‰è¿›è¡Œè¿æ¥çš„ä¸¤ä¸ªè¡¨ä¸­éƒ½å­˜åœ¨ä¸è¿æ¥æ¡ä»¶ç›¸åŒ¹é…çš„æ•°æ®æ‰ä¼šè¢«ä¿ç•™ä¸‹.
#### 6.4.4 å·¦å¤–è¿æ¥
> å·¦å¤–è¿æ¥: JOINæ“ä½œç¬¦å·¦è¾¹è¡¨ä¸­ç¬¦åˆWHEREå­å¥çš„æ‰€æœ‰è®°å½•å°†ä¼šè¢«è¿”å›.
```
hive (default)> select e.empno,e.ename,d.dname
              > from emp e left join dept d
              > on e.deptno=d.deptid;
Query ID = root_20190329112652_1446145a-1749-4a6f-85ad-b4576ee31e0f
Total jobs = 1
WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Execution log at: /tmp/root/root_20190329112652_1446145a-1749-4a6f-85ad-b4576ee31e0f.log
Starting to launch local task to process map join;      maximum memory = 518979584
2019-03-29 11:26:59     Dump the side-table for tag: 1 with group count: 4 into file: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/hive_2019-03-29_11-26-52_365_8094669471916599336-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2019-03-29 11:26:59     Uploaded 1 File to: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/hive_2019-03-29_11-26-52_365_8094669471916599336-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile11--.hashtable (373 bytes)
End of local task; Time Taken: 1.522 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553824894278_0005, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0005/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0005
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
Stage-3 map = 0%,  reduce = 0%
Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.44 sec
MapReduce Total cumulative CPU time: 1 seconds 440 msec
Ended Job = job_1553824894278_0005
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 1.44 sec   HDFS Read: 7035 HDFS Write: 126 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 440 msec
OK
e.empno e.ename d.dname
7369    SMITH   NULL
7499    ALLTE   NULL
7521    WAROS   NULL
7566    JOSSS   NULL
7654    SOCTD   NULL
7698    ADAMS   NULL
7782    JAMSK   NULL
7788    FOESS   NULL
7939    KINGS   NULL
Time taken: 32.565 seconds, Fetched: 9 row(s)
hive (default)> 
```
#### 6.4.5 å³å¤–è¿æ¥
> å³å¤–è¿æ¥: JOINæ“ä½œç¬¦å³è¾¹è¡¨ä¸­ç¬¦åˆWHEREå­å¥çš„æ‰€æœ‰è®°å½•å°†ä¼šè¢«è¿”å›.
```
hive (default)> select e.empno,e.ename,d.dname
              > from emp e right join dept d
              > on e.deptno=d.deptid;
Query ID = root_20190329115001_8d324711-c545-4ea3-bef5-28cc14a11989
Total jobs = 1
19/03/29 11:50:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Execution log at: /tmp/root/root_20190329115001_8d324711-c545-4ea3-bef5-28cc14a11989.log
Starting to launch local task to process map join;      maximum memory = 518979584
2019-03-29 11:50:08     Dump the side-table for tag: 0 with group count: 2 into file: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/hive_2019-03-29_11-50-01_720_5879451311571173295-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile20--.hashtable
2019-03-29 11:50:09     Uploaded 1 File to: file:/tmp/root/c9582a8b-1e8f-446e-b899-f5114533b280/hive_2019-03-29_11-50-01_720_5879451311571173295-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile20--.hashtable (413 bytes)
End of local task; Time Taken: 1.56 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1553824894278_0006, Tracking URL = http://systemhub611:8088/proxy/application_1553824894278_0006/
Kill Command = /opt/module/hadoop/bin/hadoop job  -kill job_1553824894278_0006
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
Stage-3 map = 0%,  reduce = 0%
Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.42 sec
MapReduce Total cumulative CPU time: 1 seconds 420 msec
Ended Job = job_1553824894278_0006
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 1.42 sec   HDFS Read: 6640 HDFS Write: 61 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 420 msec
OK
e.empno e.ename d.dname
NULL    NULL    ACCOUNTING
NULL    NULL    RESEARCH
NULL    NULL    SALES
NULL    NULL    OPERATIONS
Time taken: 29.374 seconds, Fetched: 4 row(s)
hive (default)> 
```

#### 6.4.6 æ»¡å¤–è¿æ¥
> æ»¡å¤–è¿æ¥: å°†ä¼šè¿”å›æ‰€æœ‰è¡¨ä¸­ç¬¦åˆWHEREè¯­å¥æ¡ä»¶çš„æ‰€æœ‰è®°å½•,å¦‚æœä»»ä¸€è¡¨çš„æŒ‡å®šå­—æ®µæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å€¼çš„è¯,é‚£ä¹ˆå°±ä½¿ç”¨NULLå€¼æ›¿ä»£.
```
hive (default)> select e.empno,e.ename,d.dname
              > from emp e full join dept d
              > on e.deptno=d.deptid;
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 5.24 sec   HDFS Read: 13887 HDFS Write: 187 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 240 msec
OK
e.empno e.ename d.dname
7939    KINGS   NULL
7788    FOESS   NULL
7782    JAMSK   NULL
7698    ADAMS   NULL
7654    SOCTD   NULL
7566    JOSSS   NULL
7369    SMITH   NULL
7521    WAROS   NULL
7499    ALLTE   NULL
NULL    NULL    ACCOUNTING
NULL    NULL    RESEARCH
NULL    NULL    SALES
NULL    NULL    OPERATIONS
Time taken: 35.59 seconds, Fetched: 13 row(s)
hive (default)> 
```

#### 6.4.7 å¤šè¡¨è¿æ¥
> æ³¨æ„: è¿æ¥nä¸ªè¡¨,è‡³å°‘éœ€è¦n-1ä¸ªè¿æ¥æ¡ä»¶.
> ä¾‹å¦‚: è¿æ¥ä¸‰ä¸ªè¡¨,è‡³å°‘éœ€è¦ä¸¤ä¸ªè¿æ¥æ¡ä»¶.
> 0.æ•°æ®å‡†å¤‡
```
[root@systemhub711 ~]# cd /opt/module/datas/
[root@systemhub711 datas]# vim location.txt
```
```
1700    Beijing
1800    London
1900    Tokyo
```
> 1.åˆ›å»ºä½ç½®è¡¨
```
hive (default)> create table if not exists default.location(loc int,loc_name string)row format delimited fields terminated by '\t';
OK
Time taken: 0.328 seconds
hive (default)> 
```
> 2.å¯¼å…¥æ•°æ®
```
hive (default)> load data local inpath '/opt/module/datas/location.txt' into table location;
Loading data to table default.location
Table default.location stats: [numFiles=1, totalSize=36]
OK
Time taken: 0.386 seconds
hive (default)> select * from location;
OK
location.loc    location.loc_name
1700    Beijing
1800    London
1900    Tokyo
Time taken: 0.077 seconds, Fetched: 3 row(s)
hive (default)> 
```
> 3.å¤šè¡¨è¿æ¥æŸ¥è¯¢
```
hive (default)> select e.ename,d.dname,l.loc_name
              > from emp e join dept d
              > on e.deptno=d.deptid
              > join location l
              > on d.loc=l.loc;
```
> å¤§å¤šæ•°æƒ…å†µä¸‹,Hiveä¼šå¯¹æ¯å¯¹JOINè¿æ¥å¯¹è±¡å¯åŠ¨ä¸€ä¸ªMapReduceä»»åŠ¡.
> 
> æœ¬ä¾‹ä¸­ä¼šé¦–å…ˆå¯åŠ¨ä¸€ä¸ªMapReduce jobå¯¹è¡¨eå’Œè¡¨dè¿›è¡Œè¿æ¥æ“ä½œ,ç„¶åä¼šå†å¯åŠ¨ä¸€ä¸ªMapReduce jobå°†ç¬¬ä¸€ä¸ªMapReduce jobçš„è¾“å‡ºå’Œè¡¨l;è¿›è¡Œè¿æ¥æ“ä½œ.
> 
> æ³¨æ„: ä¸ºä»€ä¹ˆä¸æ˜¯è¡¨då’Œè¡¨lå…ˆè¿›è¡Œè¿æ¥æ“ä½œå‘¢? è¿™æ˜¯å› ä¸ºHiveæ€»æ˜¯æŒ‰ç…§ä»å·¦åˆ°å³çš„é¡ºåºæ‰§è¡Œçš„.

#### 6.4.8 ç¬›å¡å°”ç§¯
> 1.ç¬›å¡å°”é›†ä¼šåœ¨ä¸‹é¢æ¡ä»¶ä¸‹äº§ç”Ÿ:
> 
> çœç•¥è¿æ¥æ¡ä»¶.
> 
> è¿æ¥æ¡ä»¶æ— æ•ˆ.
> 
> æ‰€æœ‰è¡¨ä¸­çš„æ‰€æœ‰è¡Œäº’ç›¸è¿æ¥.
> 
> æ¡ˆä¾‹å®æ“
```
hive (default)> select empno, deptno from emp, dept;
Warning: Map Join MAPJOIN[7][bigTable=emp] in task 'Stage-3:MAPRED' is a cross product
Query ID = root_20190329124854_dc330707-1941-4608-8244-1d5bd9214e38
Total jobs = 1
Total MapReduce CPU Time Spent: 1 seconds 470 msec
OK
empno   deptno
7369    NULL
7369    NULL
7369    NULL
7369    NULL
7499    30
7499    30
7499    30
7499    30
7521    30
```
#### 6.4.9 è¿æ¥è°“è¯ä¸­ä¸æ”¯æŒor
> é”™è¯¯ç¤ºèŒƒ
```
hive (default)> select e.empno,e.ename,d.deptno from emp e join dept d on e.deptno=d.deptno or e.ename=d.ename;

FAILED: SemanticException [Error 10019]: Line 1:58 OR not supported in JOIN currently 'ename'
hive (default)> 
```

### 6.5 æ’åº
#### 6.5.1 å…¨å±€æ’åº (Order By)
> Order By: å…¨å±€æ’åº,ä¸€ä¸ªMapReduce
##### 6.5.1.1 ä½¿ç”¨`ORDER BY` å­å¥æ’åº
> ASC (ascend) å‡åº (é»˜è®¤)
> DESC (descend) é™åº
##### 6.5.1.2 `ORDER BY` å­å¥åœ¨`SELECT`è¯­å¥ç»“å°¾
##### 6.5.1.3 æ¡ˆä¾‹å®æ“
###### 6.5.1.3.1 æŸ¥è¯¢å‘˜å·¥ä¿¡æ¯æŒ‰å·¥èµ„å‡åºæ’åˆ—
```
hive (default)> select * from emp order by sal;
Query ID = root_20190329130738_e22bdd23-0e80-43cb-8667-dde41851efa4
Total jobs = 1
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.13 sec   HDFS Read: 8460 HDFS Write: 472 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 130 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
Time taken: 29.233 seconds, Fetched: 9 row(s)
hive (default)>
```
###### 6.5.1.3.2 æŸ¥è¯¢å‘˜å·¥ä¿¡æ¯æŒ‰å·¥èµ„é™åºæ’åˆ—
```
hive (default)> select * from emp order by sal desc;
Query ID = root_20190329131750_d47ccdca-56fa-4b3f-aac1-e9d35e5e44e3
Total MapReduce CPU Time Spent: 3 seconds 450 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
```

#### 6.5.2 æŒ‰ç…§åˆ«åæ’åº
> æŒ‰ç…§å‘˜å·¥è–ªæ°´çš„2å€æ’åº
```
hive (default)> select ename,sal*2 twosal from emp order by twosal;
Query ID = root_20190329132035_8bf552a3-03b8-4e33-835b-56f26c8a5a12
Total MapReduce CPU Time Spent: 3 seconds 490 msec
OK
ename   twosal
SMITH   1600.0
FOESS   1900.0
JAMSK   2200.0
WAROS   2500.36
ALLTE   3200.0
SOCTD   5704.6
JOSSS   5788.5
KINGS   6000.0
ADAMS   51048.04
Time taken: 31.107 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.5.3 å¤šåˆ—æ’åº
> æŒ‰ç…§éƒ¨é—¨å’Œå·¥èµ„å‡åºæ’åº
```
hive (default)> select ename,deptno,sal from emp order by deptno,sal;
Query ID = root_20190329132255_7a5c8581-0c60-4a17-a726-5f8c1f8d8b36
Total MapReduce CPU Time Spent: 2 seconds 770 msec
OK
ename   deptno  sal
SMITH   NULL    800.0
FOESS   NULL    950.0
JAMSK   NULL    1100.0
SOCTD   NULL    2852.3
JOSSS   NULL    2894.25
KINGS   NULL    3000.0
ADAMS   NULL    25524.02
WAROS   30      1250.18
ALLTE   30      1600.0
Time taken: 28.476 seconds, Fetched: 9 row(s)
hive (default)> 
```
#### 6.5.4 MapReduceå†…éƒ¨æ’åº (Sort By)
> Sort By: æ¯ä¸ªMapReduceå†…éƒ¨è¿›è¡Œæ’åº,å¯¹å…¨å±€ç»“æœé›†æ¥è¯´ä¸æ˜¯æ’åº.
> 
> 1.è®¾ç½®reduceä¸ªæ•°.
```
hive (default)> set mapreduce.job.reduces=3;
```
> 
> 2.æŸ¥çœ‹è®¾ç½®reduceä¸ªæ•°.
```
hive (default)> set mapreduce.job.reduces;
mapreduce.job.reduces=3
hive (default)> 
```
> 
> 3.æ ¹æ®éƒ¨é—¨ç¼–å·é™åºæŸ¥çœ‹å‘˜å·¥ä¿¡æ¯.
```
hive (default)> select * from emp sort by empno desc;
Query ID = root_20190329133608_60e6d25c-1197-4a09-8c96-df4fdbb35628
Stage-Stage-1: Map: 1  Reduce: 3   Cumulative CPU: 7.96 sec   HDFS Read: 16132 HDFS Write: 472 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 960 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
Time taken: 32.412 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.5.5 åˆ†åŒºæ’åº (Distribute By)
> Distribute By: ç±»ä¼¼MRä¸­partition,è¿›è¡Œåˆ†åŒº,ç»“åˆ`sort by`ä½¿ç”¨.
> æ³¨æ„,Hiveè¦æ±‚`DISTRIBUTE BY`è¯­å¥è¦å†™åœ¨`SORT BY`è¯­å¥ä¹‹å‰.
> å¯¹äº`distribute by`è¿›è¡Œæµ‹è¯•,ä¸€å®šè¦åˆ†é…å¤šreduceè¿›è¡Œå¤„ç†,å¦åˆ™æ— æ³•çœ‹åˆ°`distribute by`çš„æ•ˆæœ.
> 

#### 6.5.6 Cluster By
> å½“`distribute by`å’Œ`sorts by`å­—æ®µç›¸åŒæ—¶,å¯ä»¥ä½¿ç”¨`cluster by`æ–¹å¼.
> 
> `cluster by`é™¤äº†å…·æœ‰`distribute by`çš„åŠŸèƒ½å¤–è¿˜å…¼å…·`sort by`çš„åŠŸèƒ½,ä½†æ˜¯æ’åºåªèƒ½æ˜¯å€’åºæ’åº,ä¸èƒ½æŒ‡å®šæ’åºè§„åˆ™ä¸º`ASC`æˆ–è€…`DESC`.
> 1.ä»¥ä¸‹ä¸¤ç§å†™æ³•ç­‰ä»·
```
hive (default)> select * from emp cluster by deptno;

hive (default)> select * from emp distribute by deptno sort by deptno;
```
> æ³¨æ„: æŒ‰ç…§éƒ¨é—¨ç¼–å·åˆ†åŒº,ä¸ä¸€å®šå°±æ˜¯å›ºå®šæ­»çš„æ•°å€¼,å¯ä»¥æ˜¯20å·å’Œ30å·éƒ¨é—¨åˆ†åˆ°ä¸€ä¸ªåˆ†åŒºé‡Œé¢å».

### 6.6 åˆ†æ¡¶åŠæŠ½æ ·æŸ¥è¯¢
#### 6.6.1 åˆ†æ¡¶è¡¨æ•°æ®å­˜å‚¨
> åˆ†åŒºé’ˆå¯¹çš„æ˜¯æ•°æ®çš„å­˜å‚¨è·¯å¾„,åˆ†æ¡¶é’ˆå¯¹çš„æ˜¯æ•°æ®æ–‡ä»¶.
> 
> åˆ†åŒºæä¾›ä¸€ä¸ªéš”ç¦»æ•°æ®å’Œä¼˜åŒ–æŸ¥è¯¢çš„ä¾¿åˆ©æ–¹å¼,ä¸è¿‡å¹¶éæ‰€æœ‰çš„æ•°æ®é›†éƒ½å¯å½¢æˆåˆç†çš„åˆ†åŒº,ç‰¹åˆ«æ˜¯ä¹‹å‰æ‰€æåˆ°è¿‡çš„è¦ç¡®å®šåˆé€‚çš„åˆ’åˆ†å¤§å°è¿™ä¸ªç–‘è™‘.
> 
> åˆ†æ¡¶æ˜¯å°†æ•°æ®é›†åˆ†è§£æˆæ›´å®¹æ˜“ç®¡ç†çš„è‹¥å¹²éƒ¨åˆ†çš„å¦ä¸€ä¸ªæŠ€æœ¯.
> 
##### 6.6.1.1 å…ˆåˆ›å»ºåˆ†æ¡¶è¡¨,é€šè¿‡ç›´æ¥å¯¼å…¥æ•°æ®æ–‡ä»¶çš„æ–¹å¼
###### 6.6.1.1.1 æ•°æ®å‡†å¤‡
```
[root@systemhub711 datas]# vim test_bucket.txt
```
```
1001    101
1002    102
1003    103
1004    104
1005    105
1006    106
1007    107
1008    108
1009    109
1010    110
1011    111
1012    112
1013    113
1014    114
1015    115
1016    116
```
###### 6.6.1.1.2 åˆ›å»ºåˆ†æ¡¶è¡¨
```
hive (default)> create table test_bucket(id int,name string)
              > clustered by (id)
              > into 4 buckets
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.138 seconds
hive (default)> 
```
###### 6.6.1.1.3 æŸ¥çœ‹è¡¨ç»“æ„
```
hive (default)> desc formatted test_bucket;
OK
col_name        data_type       comment
# col_name              data_type               comment             
                 
id                      int                                         
name                    string                                      
                 
# Detailed Table Information             
Database:               default                  
Owner:                  root                       
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://systemhub511:9000/user/hive/warehouse/test_bucket         
Table Type:             MANAGED_TABLE            
Table Parameters:                
        transient_lastDdlTime   1553865393          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            4                        
Bucket Columns:         [id]                     
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \t                  
        serialization.format    \t                  
Time taken: 0.123 seconds, Fetched: 28 row(s)
hive (default)> 
```
###### 6.6.1.1.4 å¯¼å…¥æ•°æ®åˆ°åˆ†æ¡¶è¡¨ä¸­
```
hive (default)> load data local inpath '/opt/module/datas/test_bucket.txt' into table test_bucket;
Loading data to table default.test_bucket
Table default.test_bucket stats: [numFiles=1, totalSize=144]
OK
Time taken: 0.345 seconds
hive (default)> 
```
###### 6.6.1.1.5 æŸ¥çœ‹åˆ›å»ºçš„åˆ†æ¡¶è¡¨ä¸­æ˜¯å¦åˆ†æˆ4ä¸ªæ¡¶
> å‘ç°å¹¶æ²¡æœ‰åˆ†æˆ4ä¸ªæ¡¶,æ˜¯ä»€ä¹ˆåŸå› å‘¢
```
hive (default)> dfs -cat /user/hive/warehouse/test_bucket/*;
1001    101
1002    102
1003    103
1004    104
1005    105
1006    106
1007    107
1008    108
1009    109
1010    110
1011    111
1012    112
1013    113
1014    114
1015    115
1016    116
hive (default)> 
```

##### 6.6.1.2 åˆ›å»ºåˆ†æ¡¶è¡¨æ—¶,æ•°æ®é€šè¿‡å­æŸ¥è¯¢çš„æ–¹å¼å¯¼å…¥
###### 6.6.1.2.1 æ–°å»ºä¸€ä¸ªæ™®é€štest_buckè¡¨
```
hive (default)> create table test_buck(id int,name string)
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.084 seconds
hive (default)> 
```
###### 6.6.1.2.2 å‘æ™®é€štest_buckè¡¨ä¸­å¯¼å…¥æ•°æ®
```
hive (default)> load data local inpath '/opt/module/datas/test_bucket.txt' into table test_buck;
Loading data to table default.test_buck
Table default.test_buck stats: [numFiles=1, totalSize=144]
OK
Time taken: 0.239 seconds
hive (default)> 
```
###### 6.6.1.2.3 æ¸…ç©ºtest_bucketè¡¨ä¸­æ•°æ®
```
hive (default)> truncate table test_bucket;
OK
Time taken: 0.151 seconds
hive (default)> 
```
###### 6.6.1.2.4 é€šè¿‡å­æŸ¥è¯¢çš„æ–¹å¼,å¯¼å…¥æ•°æ®åˆ°åˆ†æ¡¶è¡¨
```
hive (default)> insert into table test_bucket
              > select * from test_buck;
Query ID = root_20190329213846_8d66d5f4-e16d-4725-9231-e9b9dd84c1de
Loading data to table default.test_bucket
Table default.test_bucket stats: [numFiles=1, numRows=16, totalSize=144, rawDataSize=128]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.29 sec   HDFS Read: 3550 HDFS Write: 220 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 290 msec
OK
test_buck.id    test_buck.name
Time taken: 20.851 seconds
hive (default)> 
```
###### 6.6.1.2.5 å‘ç°è¿˜æ˜¯åªæœ‰ä¸€ä¸ªåˆ†æ¡¶
```
hive (default)> dfs -cat /user/hive/warehouse/test_bucket/*;
1001    101
1002    102
1003    103
1004    104
1005    105
1006    106
1007    107
1008    108
1009    109
1010    110
1011    111
1012    112
1013    113
1014    114
1015    115
1016    116
hive (default)> 
```
###### 6.6.1.2.6 è®¾ç½®åˆ†æ¡¶å±æ€§
```
hive (default)> set hive.enforce.bucketing=true;
hive (default)> set mapreduce.job.reduces=-1;
hive (default)> insert into table test_bucket
              > select id,name from test_buck cluster by(id);
Loading data to table default.test_bucket
Table default.test_bucket stats: [numFiles=5, numRows=32, totalSize=288, rawDataSize=256]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 4   Cumulative CPU: 9.76 sec   HDFS Read: 15857 HDFS Write: 444 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 760 msec
OK
id      name
Time taken: 37.969 seconds
hive (default)> 
```
###### 6.6.1.2.7 æŸ¥è¯¢åˆ†æ¡¶æ•°æ®
> æŸ¥çœ‹åˆ†æ¡¶æ–‡ä»¶
```
hive (default)> dfs -ls /user/hive/warehouse/test_bucket/*;
-rwxrwxrwx 3 root supergroup 14 /user/hive/warehouse/test_bucket/000000_0
-rwxrwxrwx 3 root supergroup 36 /user/hive/warehouse/test_bucket/000001_0
-rwxrwxrwx 3 root supergroup 36 /user/hive/warehouse/test_bucket/000002_0
-rwxrwxrwx 3 root supergroup 36 /user/hive/warehouse/test_bucket/000003_0
hive (default)> 
```
> æŸ¥çœ‹åˆ†æ¡¶æ•°æ®
```
hive (default)> select * from test_bucket;
OK
test_bucket.id  test_bucket.name
1001    101
1002    102
1003    103
1004    104
1005    105
1006    106
1007    107
1008    108
1009    109
1010    110
1011    111
1012    112
1013    113
1014    114
1015    115
1016    116
1004    104
1008    108
1012    112
1016    116
1001    101
1005    105
1009    109
1013    113
1002    102
1006    106
1010    110
1014    114
1003    103
1007    107
1011    111
1015    115
Time taken: 0.081 seconds, Fetched: 32 row(s)
hive (default)> 
```


#### 6.6.2 åˆ†æ¡¶æŠ½æ ·æŸ¥è¯¢
> å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†,æœ‰æ—¶å¼€å‘è€…éœ€è¦ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„æŸ¥è¯¢ç»“æœè€Œä¸æ˜¯å…¨éƒ¨ç»“æœ,Hiveå¯ä»¥é€šè¿‡å¯¹è¡¨è¿›è¡ŒæŠ½æ ·æ¥æ»¡è¶³è¿™ä¸ªéœ€æ±‚.
> 
#### 6.6.2.1 æŸ¥è¯¢test_bucketè¡¨ä¸­çš„æ•°æ®
```
hive (default)> select * from test_bucket tablesample(bucket 1 out of 4 on id);
OK
test_bucket.id  test_bucket.name
1004    104
1008    108
1012    112
1016    116
1004    104
1008    108
1012    112
1016    116
Time taken: 0.127 seconds, Fetched: 8 row(s)
hive (default)> 
```
> æ³¨: `tablesample`æ˜¯æŠ½æ ·è¯­å¥,è¯­æ³•: `TABLESAMPLE(BUCKET x OUT OF y)`
> 
> `x`è¡¨ç¤ºä»å“ªä¸ªbucketå¼€å§‹æŠ½å–,ä¾‹å¦‚,tableæ€»bucketæ•°ä¸º4,tablesample(bucket 4 out of 4),è¡¨ç¤ºæ€»å…±æŠ½å–(4/4=)ä¸ªbucketçš„æ•°æ®,æŠ½å–ç¬¬4ä¸ªbucketæ•°æ®.
> 
> `y`å¿…é¡»æ˜¯tableæ€»bucketæ•°çš„å€æ•°æˆ–è€…å› å­,hiveæ ¹æ®yçš„å¤§å°,å†³å®šæŠ½æ ·çš„æ¯”ä¾‹,ä¾‹å¦‚tableæ€»å…±åˆ†äº†4ä»½,å½“y=2æ—¶,æŠ½å–(4/2=)2ä¸ªbucketæ•°æ®,å½“y=8æ—¶,æŠ½å–(4/8=)1/2ä¸ªbucketæ•°æ®.
> 
> æ³¨æ„: xçš„å€¼å¿…é¡»å°äºç­‰äºyçš„å€¼,å¦åˆ™
```
FAILED:   SemanticException   [Error   10061]:   Numerator   should   not   be   bigger   than denominator in sample clause for table test_bucket
```

#### 6.6.3 æ•°æ®å—æŠ½æ ·
> Hiveæä¾›äº†å¦å¤–ä¸€ç§æŒ‰ç…§ç™¾åˆ†æ¯”è¿›è¡ŒæŠ½æ ·çš„æ–¹å¼,è¿™ç§æ˜¯åŸºäºè¡Œæ•°çš„,æŒ‰ç…§è¾“å…¥è·¯å¾„ä¸‹çš„æ•°æ®å—ç™¾åˆ†æ¯”è¿›è¡Œçš„æŠ½æ ·.
```
hive (default)> select * from test_buck tablesample(0.1 percent);
OK
test_buck.id    test_buck.name
1001    101
1002    102
1003    103
1004    104
1005    105
1006    106
1007    107
1008    108
1009    109
1010    110
1011    111
1012    112
1013    113
1014    114
1015    115
1016    116
Time taken: 0.115 seconds, Fetched: 16 row(s)
hive (default)> 
```
> æç¤º: è¿™ç§æŠ½æ ·æ–¹å¼ä¸ä¸€å®šé€‚ç”¨äºæ‰€æœ‰çš„æ–‡ä»¶æ ¼å¼,å¦å¤–è¿™ç§æŠ½æ ·çš„æœ€å°æŠ½æ ·å•å…ƒæ˜¯ä¸€ä¸ªHDFSæ•°æ®å—,å› æ­¤,å¦‚æœè¡¨çš„æ•°æ®å¤§å°å°äºæ™®é€šçš„å—å¤§å°128Mçš„è¯,é‚£ä¹ˆå°†ä¼šè¿”å›æ‰€æœ‰è¡Œ.

### 6.7 å…¶ä»–å¸¸ç”¨æŸ¥è¯¢å‡½æ•°
#### 6.7.1 ç©ºå­—æ®µèµ‹å€¼
##### 6.7.1.1 å‡½æ•°è¯´æ˜
> NVL:ç»™å€¼ä¸ºNULLçš„æ•°æ®èµ‹å€¼,æ ¼å¼æ˜¯`NVL(string1,replace_with)`.
> 
> å®ƒçš„åŠŸèƒ½æ˜¯å¦‚æœstring1ä¸ºNULL,åˆ™NVLå‡½æ•°è¿”å›replace_withçš„å€¼,å¦åˆ™è¿”å›string1çš„å€¼,å¦‚æœä¸¤ä¸ªå‚æ•°ä¸ºNULL,åˆ™è¿”å›NULL.
##### 6.7.1.2 æ•°æ®å‡†å¤‡ é‡‡ç”¨empæ•°æ®è¡¨
##### 6.7.1.3 æŸ¥è¯¢deptnoå­—æ®µä¸ºNULL,åˆ™ç”¨-1ä»£æ›¿
```
hive (default)> select empno,nvl(deptno,-1) from emp;
OK
empno   _c1
7369    -1
7499    30
7521    30
7566    -1
7654    -1
7698    -1
7782    -1
7788    -1
7939    -1
Time taken: 0.066 seconds, Fetched: 9 row(s)
hive (default)> 
```

#### 6.7.2 CASE WHEN
##### 6.7.2.1 æ•°æ®å‡†å¤‡
> userè¡¨ç¤ºç”¨æˆ·å, a&bè¡¨ç¤ºéƒ¨é—¨,ç”·å¥³è¡¨ç¤ºæ€§åˆ«
```
user001 A Male
user002 A Male
user003 B Female
user004 A Female
user005 B Female
user006 B Female
```
##### 6.7.2.2 éœ€æ±‚
> è®¡ç®—å‡ºä¸åŒéƒ¨åˆ†å‘˜å·¥æ€§åˆ«æœ‰å¤šå°‘

##### 6.7.2.3 åˆ›å»ºæœ¬åœ°æ–‡ä»¶å¹¶å¯¼å…¥æ•°æ®
```
[root@systemhub711 datas]# vim emp_sex.txt
```

##### 6.7.2.4 åˆ›å»ºè¡¨å¹¶å¯¼å…¥æ•°æ®
```
hive (default)> create table emp_sex(name string,dept_id string,sex string) row format delimited fields terminated by ' ';
OK
Time taken: 0.11 seconds
hive (default)> load data local inpath '/opt/module/datas/emp_sex.txt' into table emp_sex;
Loading data to table default.emp_sex
Table default.emp_sex stats: [numFiles=1, totalSize=84]
OK
Time taken: 0.255 seconds
hive (default)> 
```
##### 6.7.2.5 æŒ‰ç…§éœ€æ±‚æŸ¥è¯¢æ•°æ®
```
hive (default)>  select dept_id,
              > sum(case sex when 'Male' then 1 else 0 end) male_count,
              > sum(case sex when 'Female' then 1 else 0 end) female_count
              > from emp_sex
              > group by dept_id;
Query ID = root_20190329235632_478f9985-3a7f-4c56-8648-fc0c2a760985
Total MapReduce CPU Time Spent: 3 seconds 320 msec
OK
dept_id male_count female_count
A       2       	1
B       1       	2
```


#### 6.7.3 è¡Œè½¬åˆ—
##### 6.7.3.0 ç›¸å…³å‡½æ•°è¯´æ˜
> `CONCAT(string A/col,string B/col...)` : è¿”å›è¾“å…¥å­—ç¬¦ä¸²è¿æ¥åçš„ç»“æœ,æ”¯æŒä»»æ„ä¸ªè¾“å…¥å­—ç¬¦ä¸²;
> 
> `CONCAT_WS(separator,str1,str2,...)` : å®ƒæ˜¯ä¸€ä¸ªç‰¹æ®Šå½¢å¼çš„`CONCAT()`,ç¬¬ä¸€ä¸ªå‚æ•°å‰©ä½™å‚æ•°é—´çš„åˆ†éš”ç¬¦,åˆ†éš”ç¬¦å¯ä»¥æ˜¯ä¸å‰©ä½™å‚æ•°ä¸€æ ·çš„å­—ç¬¦ä¸²,å¦‚æœåˆ†éš”ç¬¦æ˜¯NULL,è¿”å›å€¼ä¹Ÿå°†ä¸ºNULL,è¿™ä¸ªå‡½æ•°ä¼šè·³è¿‡åˆ†éš”ç¬¦å‚æ•°åçš„ä»»ä½•NULLå’Œç©ºå­—ç¬¦ä¸²,åˆ†éš”ç¬¦å°†è¢«åŠ åˆ°è¢«è¿æ¥çš„å­—ç¬¦ä¸²ä¹‹é—´.
> 
> `COLLECT_SET(col)` : å‡½æ•°åªæ¥å—åŸºæœ¬æ•°æ®ç±»å‹,å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯å°†æŸå­—æ®µçš„å€¼è¿›è¡Œå»é‡æ±‡æ€»,äº§ç”Ÿarrayç±»å‹å­—æ®µ.

##### 6.7.3.1 æ•°æ®å‡†å¤‡
```
TestUser001     ç™½ç¾Šåº§  A
TestUser002     å°„æ‰‹åº§  A
TestUser003     ç™½ç¾Šåº§  B
TestUser004     ç™½ç¾Šåº§  A
TestUser005     å°„æ‰‹åº§  A
```
##### 6.7.3.2 éœ€æ±‚
> æŠŠæ˜Ÿåº§å’Œè¡€å‹ä¸€æ ·çš„äººå½’ç±»åˆ°ä¸€èµ·.
> 
##### 6.7.3.3 åˆ›å»ºæœ¬åœ°æ–‡ä»¶å¯¼å…¥æ•°æ®
> vim constellation.txt
```
TestUser001     ç™½ç¾Šåº§  A
TestUser002     å°„æ‰‹åº§  A
TestUser003     ç™½ç¾Šåº§  B
TestUser004     ç™½ç¾Šåº§  A
TestUser005     å°„æ‰‹åº§  A
```
##### 6.7.3.4 åˆ›å»ºhiveè¡¨å¹¶å¯¼å…¥æ•°æ®
```
hive (default)> create table person_info(name string,constellaction string,blood_type string)
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.17 seconds
hive (default)> load data local inpath '/opt/module/datas/constellation.txt' into table person_info;
Loading data to table default.person_info
Table default.person_info stats: [numFiles=1, totalSize=120]
OK
Time taken: 1.473 seconds
hive (default)> 
```
##### 6.7.3.5 æŒ‰éœ€æ±‚æŸ¥è¯¢æ•°æ®
```
hive (default)> select pi.base,concat_ws(" | ",collect_set(pi.name)) name from (select name,concat(constellaction," , ",blood_type) base from person_info) pi group by pi.base;
Query ID = root_20190331163711_a233152e-be3a-40d4-8f7b-db294c916cad
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.39 sec   HDFS Read: 8077 HDFS Write: 123 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 390 msec
OK
pi.base name
å°„æ‰‹åº§ , A      TestUser002 | TestUser005
ç™½ç¾Šåº§ , A      TestUser001 | TestUser004
ç™½ç¾Šåº§ , B      TestUser003
Time taken: 50.112 seconds, Fetched: 3 row(s)
hive (default)> 
```

#### 6.7.4 åˆ—è½¬è¡Œ
##### 6.7.4.0 ç›¸å…³å‡½æ•°è¯´æ˜
> `EXPLODE(col)` : å°†hiveä¸€åˆ—ä¸­å¤æ‚çš„arrayæˆ–è€…mapç»“æ„æ‹†åˆ†æˆå¤šè¡Œ.
> 
> `LATERAL VIEW` : 
> 
> ç”¨æ³• : `LATERAL VIEW udtf(expression) tableAlias AS columnAlias`
> 
> è§£é‡Š : ç”¨äºsplit,explodeç­‰UDTFä¸€èµ·ä½¿ç”¨,å®ƒèƒ½å¤Ÿå°†ä¸€åˆ—æ•°æ®æ‹†æˆå¤šè¡Œæ•°æ®,åœ¨æ­¤åŸºç¡€ä¸Šå¯ä»¥å¯¹æ‹†åˆ†åçš„æ•°æ®è¿›è¡Œèšåˆ.

##### 6.7.4.1 æ•°æ®å‡†å¤‡
```
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    æ‚¬ç–‘,åŠ¨ä½œ,ç§‘å¹»,å‰§æƒ…
ã€ŠLietomeã€‹     æ‚¬ç–‘,è­¦åŒª,åŠ¨ä½œ,å¿ƒç†,å‰§æƒ…
ã€Šæˆ˜ç‹¼2ã€‹       æˆ˜äº‰,åŠ¨ä½œ,ç¾éš¾
```
##### 6.7.4.2 éœ€æ±‚
> å°†ç”µå½±åˆ†ç±»ä¸­çš„æ•°ç»„æ•°æ®å±•å¼€,ç»“æœå¦‚ä¸‹
##### 6.7.4.3 åˆ›å»ºæœ¬åœ°æ–‡ä»¶,å¯¼å…¥æ•°æ®
> vim movie.txt
```
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    æ‚¬ç–‘,åŠ¨ä½œ,ç§‘å¹»,å‰§æƒ…
ã€ŠLietomeã€‹     æ‚¬ç–‘,è­¦åŒª,åŠ¨ä½œ,å¿ƒç†,å‰§æƒ…
ã€Šæˆ˜ç‹¼2ã€‹       æˆ˜äº‰,åŠ¨ä½œ,ç¾éš¾
```
##### 6.7.4.4 åˆ›å»ºhiveè¡¨å¹¶å¯¼å…¥æ•°æ®
```
hive (default)> create table movie_info (movie string,category array<string>)
              > row format delimited fields terminated by '\t'  
              > collection items terminated by ',';
OK
Time taken: 0.378 seconds
hive (default)> load data local inpath '/opt/module/datas/movie.txt' into table movie_info;
Loading data to table default.movie_info
Table default.movie_info stats: [numFiles=1, totalSize=131]
OK
Time taken: 0.451 seconds
hive (default)>
```
##### 6.7.4.5 æŒ‰éœ€æ±‚æŸ¥è¯¢æ•°æ®
```
hive (default)> select movie,category_name from movie_info LATERAL VIEW EXPLODE(category) mv as category_name; 
OK
movie   category_name
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    æ‚¬ç–‘
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    åŠ¨ä½œ
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    ç§‘å¹»
ã€Šç–‘çŠ¯è¿½è¸ªã€‹    å‰§æƒ…
ã€ŠLietomeã€‹     æ‚¬ç–‘
ã€ŠLietomeã€‹     è­¦åŒª
ã€ŠLietomeã€‹     åŠ¨ä½œ
ã€ŠLietomeã€‹     å¿ƒç†
ã€ŠLietomeã€‹     å‰§æƒ…
ã€Šæˆ˜ç‹¼2ã€‹       æˆ˜äº‰
ã€Šæˆ˜ç‹¼2ã€‹       åŠ¨ä½œ
ã€Šæˆ˜ç‹¼2ã€‹       ç¾éš¾
Time taken: 0.137 seconds, Fetched: 12 row(s)
hive (default)> 
```


#### 6.7.5 çª—å£å‡½æ•°
##### 6.7.5.0 ç›¸å…³å‡½æ•°è¯´æ˜
> `OVER()` : æŒ‡å®šåˆ†æå‡½æ•°å·¥ä½œçš„æ•°æ®çª—å£å¤§å°,è¿™ä¸ªæ•°æ®çª—å£å¤§å°å¯èƒ½ä¼šéšç€è¡Œçš„å˜åŒ–è€Œå˜åŒ–.
> 
> `CURRENT ROW` : å½“å‰è¡Œ.
> 
> `PRECEDINGn` : å¾€å‰nè¡Œæ•°æ®.
> 
> `FOLLOWINGn` : å¾€ånè¡Œæ•°æ®.
> 
> `UNBOUNDED` : `UNBOUNDED  PRECEDING` è¡¨ç¤ºä»å‰é¢çš„èµ·ç‚¹,è€Œ`UNBOUNDED FOLLOWING`è¡¨ç¤ºåˆ°åé¢çš„ç»ˆç‚¹.
> 
> `LAG(col,n)` : å¾€å‰ç¬¬nè¡Œæ•°æ®.
> 
> `LEAD(col,n)` : å¾€åç¬¬nè¡Œæ•°æ®.
> 
> `NTILE(n)` : æŠŠæœ‰åºåˆ†åŒºä¸­çš„è¡Œåˆ†å‘åˆ°æŒ‡å®šæ•°æ®çš„ç»„ä¸­,å„ä¸ªç»„æœ‰ç¼–å·,ç¼–å·ä»1å¼€å§‹,å¯¹äºæ¯ä¸€è¡Œ,NTILEè¿”å›æ­¤è¡Œæ‰€å±çš„ç»„çš„ç¼–å·,æ³¨æ„:nå¿…é¡»ä¸ºintç±»å‹.

##### 6.7.5.1 æ•°æ®å‡†å¤‡
```
jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94
```
##### 6.7.5.2 éœ€æ±‚
> 1.æŸ¥è¯¢åœ¨2017å¹´4æœˆä»½è´­ä¹°è¿‡çš„é¡¾å®¢åŠæ€»äººæ•°.
> 2.æŸ¥è¯¢é¡¾å®¢çš„è´­ä¹°æ˜ç»†åŠæœˆè´­ä¹°æ€»é¢.
> 3.ä¸Šè¿°çš„åœºæ™¯,è¦å°†costæŒ‰ç…§æ—¥æœŸè¿›è¡Œç´¯åŠ .
> 4.æŸ¥è¯¢é¡¾å®¢ä¸Šæ¬¡çš„è´­ä¹°æ—¶é—´.
> 5.æŸ¥è¯¢å‰20%æ—¶é—´çš„è®¢å•ä¿¡æ¯.
##### 6.7.5.3 åˆ›å»ºæœ¬åœ°æ–‡ä»¶,å¯¼å…¥æ•°æ®
> vim business.txt
```
jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94
```
##### 6.7.5.4 åˆ›å»ºhiveè¡¨å¹¶å¯¼å…¥æ•°æ®
```
hive (default)> create table business(name string,orderdate string,cost int)
              > row format delimited fields terminated by ',';
OK
Time taken: 0.096 seconds
hive (default)> 
hive (default)> load data local inpath '/opt/module/datas/business.txt' into table business;
Loading data to table default.business
Table default.business stats: [numFiles=1, totalSize=266]
OK
Time taken: 0.475 seconds
hive (default)> 
```
##### 6.7.5.5 æŒ‰éœ€æ±‚æŸ¥è¯¢æ•°æ®
###### 6.7.5.5.1 æŸ¥è¯¢åœ¨2017å¹´4æœˆä»½è´­ä¹°è¿‡çš„é¡¾å®¢åŠæ€»äººæ•°
```
hive (default)> select name,count(*)over() from business where substring(orderdate,1,7)='2017-04' group by name;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.72 sec   HDFS Read: 7287 HDFS Write: 140 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.32 sec   HDFS Read: 6076 HDFS Write: 14 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 40 msec
OK
name    count_window_0
mart    2
jack    2
Time taken: 65.934 seconds, Fetched: 2 row(s)
hive (default)> 
```
###### 6.7.5.5.2 æŸ¥è¯¢é¡¾å®¢çš„è´­ä¹°æ˜ç»†åŠæœˆè´­ä¹°æ€»é¢
```
hive (default)> select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.62 sec   HDFS Read: 9051 HDFS Write: 319 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 620 msec
OK
name    orderdate       cost    sum_window_0
jack    2017-01-01      10      205
jack    2017-01-08      55      205
tony    2017-01-07      50      205
jack    2017-01-05      46      205
tony    2017-01-04      29      205
tony    2017-01-02      15      205
jack    2017-02-03      23      23
mart    2017-04-13      94      341
jack    2017-04-06      42      341
mart    2017-04-11      75      341
mart    2017-04-09      68      341
mart    2017-04-08      62      341
neil    2017-05-10      12      12
neil    2017-06-12      80      80
Time taken: 31.257 seconds, Fetched: 14 row(s)
hive (default)> 
```
###### 6.7.5.5.3 ä¸Šè¿°åœºæ™¯ä¸­,è¦å°†costæŒ‰ç…§æ—¥æœŸè¿›è¡Œç´¯åŠ 
```
hive (default)>  select *,sum(cost) over(sort by orderdate rows between UNBOUNDED PRECEDING and CURRENT ROW) from business;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.32 sec   HDFS Read: 8890 HDFS Write: 319 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 320 msec
OK
business.name   business.orderdate      business.cost   sum_window_0
jack    2017-01-01      10      10
tony    2017-01-02      15      25
tony    2017-01-04      29      54
jack    2017-01-05      46      100
tony    2017-01-07      50      150
jack    2017-01-08      55      205
jack    2017-02-03      23      228
jack    2017-04-06      42      270
mart    2017-04-08      62      332
mart    2017-04-09      68      400
mart    2017-04-11      75      475
mart    2017-04-13      94      569
neil    2017-05-10      12      581
neil    2017-06-12      80      661
Time taken: 31.629 seconds, Fetched: 14 row(s)
hive (default)> 
```
###### 6.7.5.5.4 æŸ¥çœ‹é¡¾å®¢ä¸Šæ¬¡çš„è´­ä¹°æ—¶é—´
```
hive (default)> select name,orderdate,cost,
              > lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate) as time1,
              > lag(orderdate,2) over(partition by name order by orderdate) as time2
              > from business;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.9 sec   HDFS Read: 9382 HDFS Write: 510 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 900 msec
OK
name    orderdate       cost    time1   time2
jack    2017-01-01      10      1900-01-01      NULL
jack    2017-01-05      46      2017-01-01      NULL
jack    2017-01-08      55      2017-01-05      2017-01-01
jack    2017-02-03      23      2017-01-08      2017-01-05
jack    2017-04-06      42      2017-02-03      2017-01-08
mart    2017-04-08      62      1900-01-01      NULL
mart    2017-04-09      68      2017-04-08      NULL
mart    2017-04-11      75      2017-04-09      2017-04-08
mart    2017-04-13      94      2017-04-11      2017-04-09
neil    2017-05-10      12      1900-01-01      NULL
neil    2017-06-12      80      2017-05-10      NULL
tony    2017-01-02      15      1900-01-01      NULL
tony    2017-01-04      29      2017-01-02      NULL
tony    2017-01-07      50      2017-01-04      2017-01-02
Time taken: 29.147 seconds, Fetched: 14 row(s)
hive (default)>
```
###### 6.7.5.5.5 æŸ¥è¯¢å‰20%æ—¶é—´çš„è®¢å•ä¿¡æ¯
```
hive (default)> select * from(
              > select name,orderdate,cost,ntile(5) over(order by orderdate) sorted
              > from business
              > ) t
              > where sorted = 1;
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.92 sec   HDFS Read: 8941 HDFS Write: 63 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 920 msec
OK
t.name  t.orderdate     t.cost  t.sorted
jack    2017-01-01      10      1
tony    2017-01-02      15      1
tony    2017-01-04      29      1
Time taken: 32.417 seconds, Fetched: 3 row(s)
hive (default)> 
```

#### 6.7.6 Rnak
##### 6.7.6.1 å‡½æ•°è¯´æ˜
> `RNAK()` æ’åºç›¸åŒæ—¶ä¼šé‡å¤,æ€»æ•°ä¸ä¼šå˜.
> `DENSE_RNAK()` æ’åºç›¸åŒæ—¶ä¼šé‡å¤,æ€»æ•°ä¼šå‡å°‘
> `ROW_NUMBER()` æ ¹æ®é¡ºåºè®¡ç®—
##### 6.7.6.2 æ•°æ®å‡†å¤‡
```
TestUser001     Language        87
TestUser001     Mathematics     95
TestUser001      English        68
TestUser002     Language        94
TestUser002     Mathematics     56
TestUser002      English        84
TestUser003     Language        64
TestUser003     Mathematics     86
TestUser003      English        84
TestUser004     Language        65
TestUser004     Mathematics     85
TestUser004      English        78
```
##### 6.7.6.3 éœ€æ±‚
> è®¡ç®—æ¯é—¨å­¦ç§‘æˆç»©æ’å.

##### 6.7.6.4 åˆ›å»ºæœ¬åœ°æ–‡ä»¶,å¯¼å…¥æ•°æ®
```
[root@systemhub711 datas]# vim score.txt
```
```
TestUser001     Language        87
TestUser001     Mathematics     95
TestUser001      English        68
TestUser002     Language        94
TestUser002     Mathematics     56
TestUser002      English        84
TestUser003     Language        64
TestUser003     Mathematics     86
TestUser003      English        84
TestUser004     Language        65
TestUser004     Mathematics     85
TestUser004      English        78
```

##### 6.7.6.5 åˆ›å»ºhiveè¡¨å¹¶å¯¼å…¥æ•°æ®
```
hive (default)> create table score(name string,subject string,score int)
              > row format delimited fields terminated by '\t';
OK
Time taken: 0.182 seconds
hive (default)> 
hive (default)> load data local inpath '/opt/module/datas/score.txt' into table score;
Loading data to table default.score
Table default.score stats: [numFiles=1, totalSize=388]
OK
Time taken: 0.268 seconds
hive (default)> 
```

##### 6.7.6.6 æŒ‰éœ€æ±‚æŸ¥è¯¢æ•°æ®
```
hive (default)> select name,subject,
              > RANK() OVER(partition by subject order by score desc),
              > DENSE_RANK() OVER(partition by subject order by score desc),
              > ROW_NUMBER() OVER(partition by subject order by score desc)
              > from score;
Query ID = root_20190331222827_cfb301fb-9df9-4531-aa71-9c4414112721
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.65 sec   HDFS Read: 9985 HDFS Write: 336 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 650 msec
OK
name    subject rank_window_0   dense_rank_window_1     row_number_window_2
TestUser003      English        1       1       1
TestUser002      English        1       1       2
TestUser004      English        3       2       3
TestUser001      English        4       3       4
TestUser002     Language        1       1       1
TestUser001     Language        2       2       2
TestUser004     Language        3       3       3
TestUser003     Language        4       4       4
TestUser001     Mathematics     1       1       1
TestUser003     Mathematics     2       2       2
TestUser004     Mathematics     3       3       3
TestUser002     Mathematics     4       4       4
Time taken: 32.982 seconds, Fetched: 12 row(s)
hive (default)> 
```

## 7. å‡½æ•°
### 7.1 ç³»ç»Ÿå‡½æ•°
#### 7.1.1 æŸ¥çœ‹ç³»ç»Ÿè‡ªå¸¦å‡½æ•°
```
hive (default)> show functions;
OK
tab_name
!
!=
%
&
*
+
-
/
<
<=
<=>
<>
=
==
>
>=
^
abs
acos
add_months
and
array
array_contains
ascii
asin
assert_true
atan
avg
base64
between
bin
case
cbrt
ceil
ceiling
coalesce
collect_list
collect_set
compute_stats
concat
concat_ws
context_ngrams
conv
corr
cos
count
covar_pop
covar_samp
create_union
cume_dist
current_database
current_date
current_timestamp
current_user
date_add
date_format
date_sub
datediff
day
dayofmonth
decode
degrees
dense_rank
div
e
elt
encode
ewah_bitmap
ewah_bitmap_and
ewah_bitmap_empty
ewah_bitmap_or
exp
explode
factorial
field
find_in_set
first_value
floor
format_number
from_unixtime
from_utc_timestamp
get_json_object
greatest
hash
hex
histogram_numeric
hour
if
in
in_file
index
initcap
inline
instr
isnotnull
isnull
java_method
json_tuple
lag
last_day
last_value
lcase
lead
least
length
levenshtein
like
ln
locate
log
log10
log2
lower
lpad
ltrim
map
map_keys
map_values
matchpath
max
min
minute
month
months_between
named_struct
negative
next_day
ngrams
noop
noopstreaming
noopwithmap
noopwithmapstreaming
not
ntile
nvl
or
parse_url
parse_url_tuple
percent_rank
percentile
percentile_approx
pi
pmod
posexplode
positive
pow
power
printf
radians
rand
rank
reflect
reflect2
regexp
regexp_extract
regexp_replace
repeat
reverse
rlike
round
row_number
rpad
rtrim
second
sentences
shiftleft
shiftright
shiftrightunsigned
sign
sin
size
sort_array
soundex
space
split
sqrt
stack
std
stddev
stddev_pop
stddev_samp
str_to_map
struct
substr
substring
sum
tan
to_date
to_unix_timestamp
to_utc_timestamp
translate
trim
trunc
ucase
unbase64
unhex
unix_timestamp
upper
var_pop
var_samp
variance
weekofyear
when
windowingtablefunction
xpath
xpath_boolean
xpath_double
xpath_float
xpath_int
xpath_long
xpath_number
xpath_short
xpath_string
year
|
~
Time taken: 0.023 seconds, Fetched: 216 row(s)
hive (default)> 
```
#### 7.1.2 æ˜¾ç¤ºè‡ªå¸¦å‡½æ•°ç”¨æ³•
```
hive (default)> desc function upper;
OK
tab_name
upper(str) - Returns str with all characters changed to uppercase
Time taken: 0.015 seconds, Fetched: 1 row(s)
hive (default)> 
```
#### 7.1.3 è¯¦ç»†æ˜¾ç¤ºè‡ªå¸¦å‡½æ•°ç”¨æ³•
```
hive (default)> desc function extended upper;
OK
tab_name
upper(str) - Returns str with all characters changed to uppercase
Synonyms: ucase
Example:
  > SELECT upper('Facebook') FROM src LIMIT 1;
  'FACEBOOK'
Time taken: 0.02 seconds, Fetched: 5 row(s)
hive (default)> 
```

### 7.2 è‡ªå®šä¹‰å‡½æ•°
> Hiveè‡ªå¸¦äº†ä¸€äº›å‡½æ•°,æ¯”å¦‚ : `max`/`min`ç­‰,ä½†æ˜¯æ•°é‡æœ‰é™,è‡ªå·±å¯ä»¥é€šè¿‡è‡ªå®šä¹‰UDFæ¥æ–¹ä¾¿çš„æ‰©å±•.
> 
> å½“Hiveæä¾›çš„å†…ç½®å‡½æ•°æ— æ³•æ»¡è¶³å¼€å‘è€…ä¸šåŠ¡å¤„ç†éœ€è¦æ—¶,æ­¤æ—¶å°±å¯ä»¥è€ƒè™‘ä½¿ç”¨`(UDF : User-Defined Function)`è‡ªå®šä¹‰å‡½æ•°.

#### 7.2.1 è‡ªå®šä¹‰å‡½æ•°ç±»åˆ«
> æ ¹æ®è‡ªå®šä¹‰å‡½æ•°ç±»åˆ«åˆ†ä¸ºä»¥ä¸‹ä¸‰ç§
##### 7.2.1.1 UDF (User-Defined-Function) 
> ä¸€è¿›ä¸€å‡º
##### 7.2.1.2 UDAF (User-Defined Aggregation Function)
> èšé›†å‡½æ•°,å¤šè¿›ä¸€å‡º,ç±»ä¼¼äº:count/max/min
##### 7.2.1.3 UDTF (User-Defined Table-Generating Functions)
> ä¸€è¿›å¤šå‡º,å¦‚lateral view explore()

#### 7.2.2 å®˜æ–¹æ–‡æ¡£åœ°å€
> [cwiki.apache.org/confluence/display/Hive/HivePlugins](https://cwiki.apache.org/confluence/display/Hive/HivePlugins)

#### 7.2.3 ç¼–ç¨‹æ­¥éª¤
> ç»§æ‰¿org.apache.hadoop.hive.ql.UDF
> éœ€è¦å®ç°evaluateå‡½æ•°,evaluateå‡½æ•°æ”¯æŒé‡è½½.
> åœ¨hiveçš„å‘½ä»¤è¡Œçª—å£åˆ›å»ºå‡½æ•°.
> æ·»åŠ jar / `add jar linux_jar_path`
> åˆ›å»ºfunction / `create [temporary] function [dbname.]function_name AS class_name;`
> åœ¨hiveçš„å‘½ä»¤è¡Œçª—å£åˆ é™¤å‡½æ•°.
> `Drop [temporary] function [if exists] [dbname.]function_name;`
> æ³¨æ„äº‹é¡¹ : UDFå¿…é¡»è¦æœ‰è¿”å›ç±»å‹,å¯ä»¥è¿”å›null,ä½†æ˜¯è¿”å›ç±»å‹ä¸èƒ½ä¸ºvoid.


## 7.3 è‡ªå®šä¹‰UDFå‡½æ•°
### 7.3.1 JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
### 7.3.2 é…ç½® Maven pom.xml 
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub</groupId>
    <artifactId>hive</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <!--https://mvnrepository.com/artifact/org.apache.hive/hive-exec -->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.1</version>
        </dependency>
    </dependencies>

</project>
```
### 7.3.3 Create HiveUdf.class
``` java
package com.geekparkhub.hive.hiveudf;

import org.apache.hadoop.hive.ql.exec.UDF;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * HiveUdf
 * <p>
 */

public class HiveUdf extends UDF {
    public String evaluate(final String s) {
        if (s == null) {
            return null;
        }
        return s.toLowerCase();
    }
}
```
### 7.3.4 æ‰“åŒ…ä¸Šä¼ 
> æ‰“æˆjaråŒ…ä¸Šä¼ åˆ°æœåŠ¡å™¨/opt/module/jars/udf.jar
> ![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_005.jpg)
```
[root@systemhub711 ~]# cd /opt/module/datas/jars/
[root@systemhub711 jars]# ll
total 4
-rw-r--r--. 1 root root 2449 Apr  1 12:57 udf.jar
[root@systemhub711 jars]# pwd
/opt/module/datas/jars
[root@systemhub711 jars]# 
```
### 7.3.4 å°†jaråŒ…æ·»åŠ åˆ°hiveçš„classpath
```
hive (default)> add jar /opt/module/datas/jars/udf.jar;
Added [/opt/module/datas/jars/udf.jar] to class path
Added resources: [/opt/module/datas/jars/udf.jar]
hive (default)> 
```

### 7.3.5 åˆ›å»ºå¹¶å…³è”ä¸´æ—¶å‡½æ•°
> åˆ›å»ºä¸´æ—¶å‡½æ•°å¹¶ä¸HiveUdf.classç›¸äº’å…³è”.
```
hive (default)> create temporary function udf_lower as "com.geekparkhub.hive.hiveudf.HiveUdf";
OK
Time taken: 0.892 seconds
hive (default)> 
```
### 7.3.6 ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•° å®ç°å¤§å†™è½¬æ¢å°å†™
> æœªè½¬æ¢å‰
```
hive (default)> select emp.ename from emp;
OK
emp.ename
SMITH
ALLTE
WAROS
JOSSS
SOCTD
ADAMS
JAMSK
FOESS
KINGS
Time taken: 0.092 seconds, Fetched: 9 row(s)
hive (default)>
```
> è‡ªå®šä¹‰å‡½æ•°,è½¬æ¢å
```
hive (default)> select udf_lower(ename) from emp;
OK
_c0
smith
allte
waros
josss
soctd
adams
jamsk
foess
kings
Time taken: 1.305 seconds, Fetched: 9 row(s)
hive (default)> 
```

## 8. å‹ç¼© & å­˜å‚¨
### 8.1 Hadoopæºç ç¼–è¯‘æ”¯æŒSnappyå‹ç¼©
#### 8.1.1 å·¥å…·å‡†å¤‡
##### 8.1.1.1 CentOSè”ç½‘
> é…ç½®CentOSèƒ½è¿æ¥å¤–ç½‘,Linuxè™šæ‹Ÿæœºping www.baidu.comç•…é€šå³å¯.
> æ³¨æ„: é‡‡ç”¨rootè§’è‰²ç¼–è¯‘,å‡å°‘æ–‡ä»¶å¤¹æƒé™å‡ºç°é—®é¢˜
##### 8.1.1.2 jaråŒ…å‡†å¤‡(hadoopæºç /JDK/Maven/Protobuf)
> `hadoop-2.7.2-src.tar.gz` | [å¿«é€Ÿä¸‹è½½é€šé“](https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/)
`jdk-8u144-linux-x64.tar.gz`  | [å¿«é€Ÿä¸‹è½½é€šé“](https://www.oracle.com/technetwork/java/javase/documentation/8u-relnotes-2225394.html)
`snappy-1.1.3.tar.gz`   | [å¿«é€Ÿä¸‹è½½é€šé“](https://github.com/google/snappy/releases/download/1.1.3/snappy-1.1.3.tar.gz)
`apache-maven-3.0.5-bin.tar.gz`  | [å¿«é€Ÿä¸‹è½½é€šé“](http://archive.apache.org/dist/maven/maven-3/3.0.5/binaries/)
`protobuf-2.5.0.tar.gz` (åºåˆ—åŒ–æ¡†æ¶)  | [å¿«é€Ÿä¸‹è½½é€šé“](https://files.pythonhosted.org/packages/3f/ad/c8221a0778cc04197047f0f6ddee683ef1a0851976a4bd4ad17af19d22ec/protobuf-2.5.0.tar.gz)

#### 8.1.2 jaråŒ…å®‰è£…
> æ³¨æ„: æ‰€æœ‰æ“ä½œå¿…é¡»åœ¨rootç”¨æˆ·ä¸‹å®Œæˆ.

##### JDK
> JDKè§£å‹ã€é…ç½®ç¯å¢ƒå˜é‡JAVA_HOMEå’ŒPATH,éªŒè¯java-version(å¦‚ä¸‹éƒ½éœ€è¦éªŒè¯æ˜¯å¦é…ç½®æˆåŠŸ.
> 
> éªŒè¯å‘½ä»¤ ï¼šjava -version
```
[root@systemhub611 ~]# java -version
java version "1.8.0_162"
Java(TM) SE Runtime Environment (build 1.8.0_162-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.162-b12, mixed mode)
[root@systemhub611 ~]# 
```

##### Maven
> è§£å‹taråŒ…åˆ°æŒ‡å®šç›®å½•
``` powershell
[root@systemhub611 software]# tar -zvxf apache-maven-3.0.5-bin.tar.gz -C /opt/module/
```
> é‡å‘½å
``` powershell
[root@systemhub611 module]# mv apache-maven-3.0.5 maven
[root@systemhub611 module]# ll
total 16
drwxr-xr-x.  6 root   root  4096 Feb  4  2018 ant
drwxr-xr-x. 15  10011 10011 4096 Jan 31 13:52 hadoop
drwxr-xr-x.  6 root   root  4096 Feb  3 14:54 maven
[root@systemhub611 module]# 
```
> é…ç½®ç¯å¢ƒå˜é‡
``` powershell
[root@systemhub611 ~]# cd /opt/module/maven/
[root@systemhub611 maven]# pwd
/opt/module/maven
[root@systemhub611 maven]# vim /etc/profile
```
``` powershell
## MAVEN_HOME
export MAVEN_HOME=/opt/module/maven
export PATH=$PATH:$MAVEN_HOME/bin
```
``` powershell
[root@systemhub611 maven]# source /etc/profile
[root@systemhub611 maven]# mvn -version
Apache Maven 3.0.5 (r01de14724cdef164cd33c7c8c2fe155faf9602da; 2013-02-19 21:51:28+0800)
Maven home: /opt/module/maven
Java version: 1.8.0_162, vendor: Oracle Corporation
Java home: /opt/devtool/jdk1.8.0_162/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "2.6.32-754.10.1.el6.x86_64", arch: "amd64", family: "unix"
[root@corehub-001 maven]# 
```

##### Protobuf
> è§£å‹taråŒ…åˆ°æŒ‡å®šç›®å½•
``` powershell
[root@systemhub611 software]# tar -zvxf protobuf-2.5.0.tar.gz -C /opt/module/
```
> é‡å‘½å
``` powershell
[root@systemhub611 module]# mv protobuf-2.5.0 protobuf
[root@systemhub611 module]# ll
total 16
drwxr-xr-x.  6 root   root  4096 Feb  4  2018 ant
drwxr-xr-x. 15  10011 10011 4096 Jan 31 13:52 hadoop
drwxr-xr-x.  6 root   root  4096 Feb  3 14:54 maven
drwxr-x---.  4 109965  5000 4096 Feb 28  2013 protobuf
[root@corehub-001 module]# 
```
> é…ç½®ç¯å¢ƒå˜é‡
``` powershell
[root@systemhub611 ~]# cd /opt/module/protobuf/
[root@systemhub611 protobuf]# pwd
/opt/module/protobuf
[root@systemhub611 protobuf]# vim /etc/profile
```
``` powershell
## PROTOBUF_HOME
export PROTOBUF_HOME=/opt/module/protobuf
export PATH=$PATH:$PROTOBUF_HOME/bin
```
``` powershell
[root@systemhub611 protobuf]# source /etc/profile
```

#### 8.1.3 ç¼–è¯‘æºç 
##### 8.1.1 å‡†å¤‡ç¼–è¯‘ç¯å¢ƒ
###### 8.1.1.1 yum install svn
``` powershell
[root@systemhub611 module]# yum install svn
Loaded plugins: fastestmirror, refresh-packagekit, security
Determining fastest mirrors
 * base: ap.stykers.moe
 * extras: mirror.jdcloud.com
 * updates: mirrors.neusoft.edu.cn
base                                                                                      | 3.7 kB     00:00     
extras                                                                                    
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================
 Package                    Arch                   Version                            Repository            Size
=================================================================================================================
Installing:
 subversion                 x86_64                 1.6.11-15.el6_7                    base                 2.3 M
Installing for dependencies:
 perl-URI                   noarch                 1.40-2.el6                         base                 117 k

Transaction Summary
=================================================================================================================
Install       2 Package(s)

Total download size: 2.4 M
Installed size: 12 M
Is this ok [y/N]: y
Downloading Packages:
(1/2): perl-URI-1.40-2.el6.noarch.rpm                                                     | 117 kB     00:00     
(2/2): subversion-1.6.11-15.el6_7.x86_64.rpm                                              | 2.3 MB     00:00     
-----------------------------------------------------------------------------------------------------------------
Total                                                                            4.6 MB/s | 2.4 MB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : perl-URI-1.40-2.el6.noarch                                                                    1/2 
  Installing : subversion-1.6.11-15.el6_7.x86_64                                                             2/2 
  Verifying  : perl-URI-1.40-2.el6.noarch                                                                    1/2 
  Verifying  : subversion-1.6.11-15.el6_7.x86_64                                                             2/2 

Installed:
  subversion.x86_64 0:1.6.11-15.el6_7                                                                            

Dependency Installed:
  perl-URI.noarch 0:1.40-2.el6                                                                                   

Complete!
[root@systemhub611 module]#
```
###### 8.1.1.2 yum install autoconf automake libtool cmake
``` powershell
[root@systemhub611 module]# yum install autoconf automake libtool cmake
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
 * base: ap.stykers.moe
 * extras: mirror.jdcloud.com
 * updates: mirrors.neusoft.edu.cn
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package autoconf.noarch 0:2.63-5.1.el6 will be installed
---> Package automake.noarch 0:1.11.1-4.el6 will be installed
---> Package cmake.x86_64 0:2.8.12.2-4.el6 will be installed
---> Package libtool.x86_64 0:2.2.6-15.5.el6 will be installed
--> Processing Dependency: gcc = 4.4.4 for package: libtool-2.2.6-15.5.el6.x86_64
--> Running transaction check
---> Package gcc.x86_64 0:4.4.7-23.el6 will be installed
--> Processing Dependency: libgomp = 4.4.7-23.el6 for package: gcc-4.4.7-
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================
 Package                    Arch                    Version                          Repository             Size
=================================================================================================================
Installing:
 autoconf                   noarch                  2.63-5.1.el6                     base                  781 k
 automake                   noarch                  1.11.1-4.el6                     base                  550 k
 cmake                      x86_64                  2.8.12.2-4.el6                   base                  8.0 M

Transaction Summary
=================================================================================================================
Install       9 Package(s)
Upgrade       2 Package(s)

Total download size: 25 M
Is this ok [y/N]: y
Downloading Packages:
http://ap.stykers.moe/centos/6.10/os/x86_64/Packages/autoconf-2.63-5.1.el6.noarch.rpm: [Errno 14] PYCURL ERROR 6 - "Couldn't resolve host 'ap.stykers.moe'"
Trying other mirror.
http://mirrors.neusoft.edu.cn/centos/6.10/os/x86_64/Packages/autoconf-2.63-5.1.el6.noarch.rpm: [Errno 14] PYCURL ERROR 6 - "Couldn't resolve host 'mirrors.neusoft.edu.cn'"
Trying other mirror.
(1/11): autoconf-2.63-5.1.el6.noarch.rpm                                                  | 781 kB     00:02     
(2/11): automake-1.11.1-4.el6.noarch.rpm                                                  | 550 kB     00:00                                                      | 1.3 MB     00:00     
-----------------------------------------------------------------------------------------------------------------
Total                                                                            485 kB/s |  25 MB     00:53     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Updating   : libgcc-4.4.7-23.el6.x86_64                                                                   1/13 
  Installing : autoconf-2.63-5.1.el6.noarch                                                                 2/13 
  Installing : automake-1.11.1-4.el6.noarch                                                                 3/13 

Installed:
  autoconf.noarch 0:2.63-5.1.el6        automake.noarch 0:1.11.1-4.el6       cmake.x86_64 0:2.8.12.2-4.el6      
  libtool.x86_64 0:2.2.6-15.5.el6      

Dependency Installed:
  cloog-ppl.x86_64 0:0.15.7-1.2.el6         cpp.x86_64 0:4.4.7-23.el6          gcc.x86_64 0:4.4.7-23.el6        
  mpfr.x86_64 0:2.4.1-6.el6                 ppl.x86_64 0:0.10.2-11.el6        

Dependency Updated:
  libgcc.x86_64 0:4.4.7-23.el6                           libgomp.x86_64 0:4.4.7-23.el6                          

Complete!
[root@systemhub611 module]# 
```
###### 8.1.1.3 yum install ncurses-devel
```
[root@systemhub611 module]# yum install ncurses-devel
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
 * base: ap.stykers.moe
 * extras: mirror.jdcloud.com
 * updates: mirrors.neusoft.edu.cn
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package ncurses-base.x86_64 0:5.7-4.20090207.el6 will be an update
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================
 Package                      Arch                  Version                            Repository           Size
=================================================================================================================
Installing:
 ncurses-devel                x86_64                5.7-4.20090207.el6                 base                641 k
Updating for dependencies:
 ncurses-base                 x86_64                5.7-4.20090207.el6                 base                 61 k
 ncurses-libs                 x86_64                5.7-4.20090207.el6                 base                245 k

Transaction Summary
=================================================================================================================
Install       1 Package(s)
Upgrade       2 Package(s)

Total download size: 947 k
Is this ok [y/N]: y
Downloading Packages:
(1/3): ncurses-base-5.7-4.20090207.el6.x86_64.rpm                                         |  61 kB     00:00     
(2/3): ncurses-devel-5.7-4.20090207.el6.x86_64.rpm                                        | 641 kB     00:00     
(3/3): ncurses-libs-5.7-4.20090207.el6.x86_64.rpm                                         | 245 kB     00:00     
-----------------------------------------------------------------------------------------------------------------
Total                                                                            2.3 MB/s | 947 kB     00:00     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Updating   : ncurses-base-5.7-4.20090207.el6.x86_64                                                        1/5 
Installed:
  ncurses-devel.x86_64 0:5.7-4.20090207.el6                                                                      

Dependency Updated:
  ncurses-base.x86_64 0:5.7-4.20090207.el6                ncurses-libs.x86_64 0:5.7-4.20090207.el6               

Complete!
[root@systemhub611 module]#
```
###### 8.1.1.4 yum install openssl-devel
```
[root@systemhub611 module]# yum install openssl-devel
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
 * base: ap.stykers.moe
 * extras: mirror.jdcloud.com
 * updates: mirrors.neusoft.edu.cn
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package openssl.x86_64 0:1.0.1e-15.el6 will be updated
---> Package openssl.x86_64 0:1.0.1e-57.el6 will be an update
---> Package zlib-devel.x86_64 0:1.2.3-29.el6 will be installed
--> Running transaction check
---> Package keyutils-libs-devel.x86_64 0:1.4-5.el6 will be installed
--> Processing Dependency: keyutils-libs = 1.4-5.el6 for package: keyutils-libs-devel-1.4-5.el6.x86_64
---> Package krb5-libs.x86_64 0:1.10.3-10.el6_4.6 will be updated
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================
 Package                           Arch                 Version                         Repository          Size
=================================================================================================================
Installing:
 openssl-devel                     x86_64               1.0.1e-57.el6                   base               1.2 M
Installing for dependencies:
 keyutils-libs-devel               x86_64               1.4-5.el6                       base                29 k
 krb5-devel                        x86_64               1.10.3-65.el6                  

Transaction Summary
=================================================================================================================
Install       8 Package(s)
Upgrade      12 Package(s)

Total download size: 6.3 M
Is this ok [y/N]: y
Downloading Packages:
(1/20): e2fsprogs-1.41.12-24.el6.x86_64.rpm                                               | 554 kB     00:00     
(2/20): e2fsprogs-libs-1.41.12-24.el6.x86_64.rpm                                          | 121 kB     00:00     
(3/20): keyutils-1.4-5.el6.x86_64.rpm                                                                                                         9/32 
  Verifying  : keyutils-1.4-5.el6.x86_64                                                                   10/32 
  Verifying  : e2fsprogs-libs-1.41.12-24.el6.x86_64                                                        11/32 
  Verifying  : libselinux-python-2.0.94-7.el6.x86_64                                                       12/32 
Installed:
  openssl-devel.x86_64 0:1.0.1e-57.el6                                                                           

Dependency Installed:
  keyutils-libs-devel.x86_64 0:1.4-5.el6                    krb5-devel.x86_64 0:1.10.3-65.el6                    
  libcom_err-devel.x86_64 0:1.41.12-24.el6                                 
Complete!
[root@systemhub611 module]# 
```
###### 8.1.1.5 yum install gcc*
```
[root@systemhub611 module]# yum install gcc*
Loaded plugins: fastestmirror, refresh-packagekit, security
Loading mirror speeds from cached hostfile
---> Package java_cup.x86_64 1:0.10k-5.el6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

=================================================================================================================
 Package                        Arch                  Version                          Repository           Size
=================================================================================================================
Installing:
 gcc-c++                        x86_64                4.4.7-23.el6                     base                4.7 M
 gcc-gfortran                   x86_64                4.4.7-23.el6                     base                4.7 M
 gcc-gnat                       x86_64                4.4.7-23.el6                                      
Transaction Summary
=================================================================================================================
Install      17 Package(s)
Upgrade       1 Package(s)

Total download size: 61 M
Is this ok [y/N]: y
Downloading Packages:
(1/18): ecj-4.5.2-3.el6.x86_64.rpm                                                        | 3.9 MB     00:00     
(2/18): gcc-c++-4.4.7-23.el6.x86_64.rpm                                                                                    | 139 kB     00:00     
(9/18): java_cup-0.10k-5.el6.x86_64.rpm                                                   
-----------------------------------------------------------------------------------------------------------------
Total                                                                            6.3 MB/s |  61 MB     00:09     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : libgcj-4.4.7-23.el6.x86_64                                                                   1/19 
  Installing : libgnat-4.4.7-23.el6.x86_64                                                                  2/19 
  Updating   : libstdc++-4.4.7-23.el6.x86_64                                                                3/19 
  Installing : libstdc++-devel-4.4.7-23.el6.x86_64                                                          4/19 
  Installing : gcc-c++-4.4.7-23.el6.x86_64                                                                  5/19 
  Installing : libgnat-devel-4.4.7-23.el6.x86_64                                                            6/19                                                     14/19 
  Installing : gcc-objc++-4.4.7-23.el6.x86_64                                                              15/19 
  Installing : gcc-gfortran-4.4.7-23.el6.x86_64                                                            16/19 
  Installing : gcc-java-4.4.7-23.el6.x86_64                                                                17/19 
  Installing : gcc-gnat-4.4.7-23.el6.x86_64                                                                18/19 
  Cleanup    : libstdc++-4.4.7-4.el6.x86_64                                                                19/19 
  Verifying  : libobjc-4.4.7-23.el6.x86_64                                                                  1/19 
  Verifying  : gcc-java-4.4.7-23.el6.x86_64                                                                 2/19 
  Verifying  : libgfortran-4.4.7-23.el6.x86_64                                                              3/19                                                      18/19 
  Verifying  : libstdc++-4.4.7-4.el6.x86_64                                                                19/19 

Installed:
  gcc-c++.x86_64 0:4.4.7-23.el6      gcc-gfortran.x86_64 0:4.4.7-23.el6     gcc-gnat.x86_64 0:4.4.7-23.el6      
  gcc-java.x86_64 0:4.4.7-23.el6     gcc-objc.x86_64 0:4.4.7-23.el6         gcc-objc++.x86_64 0:4.4.7-23.el6    

Dependency Installed:
  ecj.x86_64 1:4.5.2-3.el6                             java-1.5.0-gcj.x86_64 0:1.5.0.0-29.1.el6                  
  java_cup.x86_64 1:0.10k-5.el6                        libgcj.x86_64 0:4.4.7-23.el6                                                  
Dependency Updated:
  libstdc++.x86_64 0:4.4.7-23.el6                                                                                
Complete!
[root@systemhub611 module]#
```

##### 8.1.12 ç¼–è¯‘å®‰è£… snappy
```
[root@systemhub611 software]# tar -zvxf snappy-1.1.3.tar.gz -C /opt/module/
snappy-1.1.3/
snappy-1.1.3/snappy-sinksource.cc
snappy-1.1.3/configure
snappy-1.1.3/config.guess
```
```
[root@systemhub611 snappy]# ./configure
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking how to print strings... printf
checking for style of include used by make... GNU
checking for gcc... gcc
checking whether the C compiler works... yes
checking for C compiler default output file name... a.out
checking for suffix of executables... 
checking whether we are cross compiling... no
checking for suffix of object files... o
```
```
[root@systemhub611 snappy]# make
make  all-am
make[1]: Entering directory `/opt/module/snappy'
/bin/sh ./libtool  --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I.     -g -O2 -MT snappy.lo -MD -MP -MF .deps/snappy.Tpo -c -o snappy.lo snappy.cc
libtool: compile:  g++ -DHAVE_CONFIG_H -I. -g -O2 -MT snappy.lo -MD -MP -MF .deps/snappy.Tpo -c snappy.cc  -fPIC -DPIC -o .libs/snappy.o
libtool: compile:  g++ -DHAVE_CONFIG_H -I. -g -O2 -MT snappy.lo -MD -MP -MF .deps/snappy.Tpo -c snappy.cc -o snappy.o >/dev/null 2>&1
```
```
[root@systemhub611 snappy]# make install
make[1]: Entering directory `/opt/module/snappy'
 /bin/mkdir -p '/usr/local/lib'
 /bin/sh ./libtool   --mode=install /usr/bin/install -c   libsnappy.la '/usr/local/lib'
libtool: install: /usr/bin/install -c .libs/libsnappy.so.1.3.0 /usr/local/lib/libsnappy.so.1.3.0
libtool: install: (cd /usr/local/lib && { ln -s -f libsnappy.so.1.3.0 libsnappy.so.1 || { rm -f libsnappy.so.1 && ln -s libsnappy.so.1.3.0 libsnappy.so.1; }; })
```
> æŸ¥çœ‹snappyåº“æ–‡ä»¶
```
[root@systemhub611 snappy]# ls -lh /usr/local/lib |grep snappy
-rw-r--r--. 1 root root 462K Apr  1 16:45 libsnappy.a
-rwxr-xr-x. 1 root root  955 Apr  1 16:45 libsnappy.la
lrwxrwxrwx. 1 root root   18 Apr  1 16:45 libsnappy.so -> libsnappy.so.1.3.0
lrwxrwxrwx. 1 root root   18 Apr  1 16:45 libsnappy.so.1 -> libsnappy.so.1.3.0
-rwxr-xr-x. 1 root root 223K Apr  1 16:45 libsnappy.so.1.3.0
[root@systemhub611 snappy]# 
```

##### 8.1.13 ç¼–è¯‘å®‰è£… protobuf
```
[root@systemhub611 software]# tar -zvxf protobuf-2.5.0.tar.gz -C /opt/module/
protobuf-2.5.0/
protobuf-2.5.0/protobuf.egg-info/
protobuf-2.5.0/protobuf.egg-info/namespace_packages.txt
protobuf-2.5.0/protobuf.egg-info/top_level.txt
```
```
[root@systemhub611 protobuf]# ./configure
checking whether to enable maintainer-specific portions of Makefiles... yes
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking target system type... x86_64-unknown-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
```
```
[root@systemhub611 protobuf]# make
make  all-recursive
make[1]: Entering directory `/opt/module/protobuf'
Making all in .
make[2]: Entering directory `/opt/module/protobuf'
make[2]: Leaving directory `/opt/module/protobuf'
Making all in src
make[2]: Entering directory `/opt/module/protobuf/src'
```
```
[root@systemhub611 protobuf]# make install
Making install in .
make[1]: Entering directory `/opt/module/protobuf'
make[2]: Entering directory `/opt/module/protobuf'
make[2]: Nothing to be done for `install-exec-am'.
```
> æŸ¥çœ‹protobufç‰ˆæœ¬ä»¥æµ‹è¯•æ˜¯å¦å®‰è£…æˆåŠŸ.
```
[root@systemhub611 protobuf]# protoc --version
libprotoc 2.5.0
[root@systemhub611 protobuf]# 
```
##### 8.1.14 ç¼–è¯‘hadoop native
```
[root@systemhub611 software]# tar -zxvf hadoop-2.7.2-src.tar.gz
hadoop-2.7.2/
hadoop-2.7.2/libexec/
hadoop-2.7.2/libexec/yarn-config.sh
hadoop-2.7.2/libexec/hadoop-config.sh
hadoop-2.7.2/libexec/mapred-config.cmd
```
```
[root@systemhub611 native]# cp ./* /opt/module/hadoop/lib/native/
[root@systemhub611 native]# hadoop checknative
19/04/01 20:50:03 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
19/04/01 20:50:03 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/module/hadoop/lib/native/libhadoop.so
zlib:    true /lib64/libz.so.1
snappy:  true /opt/module/hadoop/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   false 
openssl: true /usr/lib64/libcrypto.so
[root@systemhub611 native]# 
```

### 8.2 Hadoopå‹ç¼©é…ç½®
#### 8.28.2.1 MRæ”¯æŒå‹ç¼©ç¼–ç 
| å‹ç¼©æ ¼å¼ | å·¥å…· | ç®—æ³• | æ–‡ä»¶æ‰©å±•å | æ˜¯å¦å¯åˆ‡åˆ† |
| :--------: | :--------:| :------: | :------: | :------: |
| DEFAULT  | æ—  | DEFAULT | .default | å¦ |
| Gzip  | gzip | DEFAULT | .gz | å¦ |
| bzip2  | bzip2 | bzip2 | .bz2 | æ˜¯ |
| LZO  | lzop | LZO | .lzo | æ˜¯ |
| Snappy  | æ—  | Snappy | .snappy | å¦ |

> ä¸ºäº†æ”¯æŒå¤šç§å‹ç¼©/è§£å‹ç¼©ç®—æ³•,Hadoopå¼•å…¥äº†ç¼–ç /è§£ç å™¨
| å‹ç¼©æ ¼å¼ | å¯¹åº”çš„ç¼–ç /è§£ç å™¨ |
| :-------- | --------:|
| DEFLATE | org.apache.hadoop.io.compress.DefaultCodec |
| gzip    | org.apache.hadoop.io.compress.BZip2Codec |
| bzip2    |org.apache.hadoop.io.compress.BZip2Codec|
| LZO    | com.hadoop.compression.lzo.LzopCodec |
| Snappy    | org.apache.hadoop.io.compress.SnappyCodec |

> å‹ç¼©æ€§èƒ½æ¯”è¾ƒ
| å‹ç¼©ç®—æ³• | åŸå§‹æ–‡ä»¶å¤§å°| å‹ç¼©æ–‡ä»¶å¤§å°| å‹ç¼©é€Ÿåº¦ | è§£å‹é€Ÿåº¦ |
| :-------- | --------:| ------: | :------: | :------: |
| gzip   |   8.3GB |  1.8GB  | 17.5MB/s | 58MB/s   |
| bzip2  |   8.3GB |  1.1GB  | 2.4MB/s  | 9.5MB/s  |
| LZO    |   8.3GB |  2.9GB  | 49.3MB/s | 74.6MB/s |
| snappy |   8.3GB |  1.5GB  | 250MB/s | 500MB/s |


#### 8.2.2 å‹ç¼©å‚æ•°é…ç½®
> è¦åœ¨Hadoopä¸­å¯ç”¨å‹ç¼©,å¯ä»¥é…ç½®å¦‚ä¸‹å‚æ•°ï¼š
| å‚æ•°      |     é»˜è®¤å€¼ |   é˜¶æ®µ   |   å»ºè®®   |
| :--------: | :--------:| :------: | :------: |
| io.compression.codecs (åœ¨core-site.xmlä¸­é…ç½®) | org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec  | è¾“å…¥å‹ç¼© | Hadoopä½¿ç”¨æ–‡ä»¶æ‰©å±•ååˆ¤æ–­æ˜¯å¦æ”¯æŒæŸç§ç¼–è§£ç å™¨ |
| mapreduce.map.output.compress (åœ¨mapred-site.xmlä¸­é…ç½®) | false  | mapperè¾“å‡º |  è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼© |
| mapreduce.map.output.compress.codec (åœ¨core-site.xmlä¸­é…ç½®) | field2 | field3 |  field3 |
| io.compression.codecs (åœ¨mapred-site.xmlä¸­é…ç½®) | org.apache.hadoop.io.compress.DefaultCodec | mapperè¾“å‡º | ä½¿ç”¨LZOæˆ–snappyç¼–è§£ç å™¨åœ¨æ­¤é˜¶æ®µå‹ç¼©æ•°æ® |
| mapreduce.output.fileoutputformat.compress (åœ¨mapred-site.xmlä¸­é…ç½®) | false | reducerè¾“å‡º |  è¿™ä¸ªå‚æ•°è®¾ä¸ºtrueå¯ç”¨å‹ç¼© |
| mapreduce.output.fileoutputformat.compress.codec(åœ¨mapred-site.xmlä¸­é…ç½®) | org.apache.hadoop.io.compress. DefaultCodec | reducerè¾“å‡º |  ä½¿ç”¨æ ‡å‡†å·¥å…·æˆ–è€…ç¼–è§£ç å™¨,å¦‚gzipå’Œbzip2 |
| mapreduce.output.fileoutputformat.compress.type (åœ¨mapred-site.xmlä¸­é…ç½®) | RECORD | reducerè¾“å‡º |  SequenceFileè¾“å‡ºä½¿ç”¨çš„å‹ç¼©ç±»å‹:NONEå’ŒBLOCK  |

### 8.3 å¼€å¯Mapè¾“å‡ºé˜¶æ®µå‹ç¼©
> å¼€å¯mapè¾“å‡ºé˜¶æ®µå‹ç¼©å¯ä»¥å‡å°‘jobä¸­mapå’ŒReduce taské—´æ•°æ®ä¼ è¾“é‡.
> 
> å¼€å¯hiveä¸­é—´ä¼ è¾“æ•°æ®å‹ç¼©åŠŸèƒ½.
```
hive (default)> set hive.exec.compress.intermediate=true;

hive (default)> set hive.exec.compress.intermediate;
hive.exec.compress.intermediate=true
hive (default)> 
```
> å¼€å¯mapreduceä¸­mapè¾“å‡ºå‹ç¼©åŠŸèƒ½
```
hive (default)> set mapreduce.map.output.compress=true;

hive (default)> set mapreduce.map.output.compress;
mapreduce.map.output.compress=true
hive (default)> 
```
> è®¾ç½®mapreduceä¸­mapè¾“å‡ºæ•°æ®çš„å‹ç¼©æ–¹å¼.
```
hive (default)> set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;

hive (default)> set mapreduce.map.output.compress.codec;
mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec
hive (default)> 
```
> æ‰§è¡ŒæŸ¥è¯¢è¯­å¥
```
hive (default)> select count(ename) name from emp;
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.21 sec   HDFS Read: 7685 HDFS Write: 2 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 210 msec
OK
ename
9
Time taken: 39.62 seconds, Fetched: 1 row(s)
hive (default)> 
```

### 8.4 å¼€å¯Reduceè¾“å‡ºé˜¶æ®µå‹ç¼©
> å½“Hiveå°†è¾“å‡ºå†™å…¥åˆ°è¡¨ä¸­æ—¶,è¾“å‡ºå†…å®¹åŒæ ·å¯ä»¥è¿›è¡Œå‹ç¼©,å±æ€§`hive.exec.compress.output`æ§åˆ¶ç€è¿™ä¸ªåŠŸèƒ½,å¼€å‘è€…å¯èƒ½éœ€è¦ä¿æŒé»˜è®¤è®¾ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼false,è¿™æ ·é»˜è®¤çš„è¾“å‡ºå°±æ˜¯éå‹ç¼©çš„çº¯æ–‡æœ¬æ–‡ä»¶äº†,ä¹Ÿå¯ä»¥é€šè¿‡åœ¨æŸ¥è¯¢è¯­å¥æˆ–æ‰§è¡Œè„šæœ¬ä¸­è®¾ç½®è¿™ä¸ªå€¼ä¸ºtrue,æ¥å¼€å¯è¾“å‡ºç»“æœå‹ç¼©åŠŸèƒ½.
> 
> å¼€å¯hiveæœ€ç»ˆè¾“å‡ºæ•°æ®å‹ç¼©åŠŸèƒ½
```
hive (default)> set hive.exec.compress.output=true;
hive (default)> set hive.exec.compress.output;
hive.exec.compress.output=true
hive (default)> 
```
> å¼€å¯mapreduceæœ€ç»ˆè¾“å‡ºæ•°æ®å‹ç¼©
```
hive (default)> set mapreduce.output.fileoutputformat.compress=true;
hive (default)> set mapreduce.output.fileoutputformat.compress;
mapreduce.output.fileoutputformat.compress=true
hive (default)> 
```
> è®¾ç½®mapreduceæœ€ç»ˆæ•°æ®è¾“å‡ºå‹ç¼©æ–¹å¼
```
hive (default)> set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
hive (default)> set mapreduce.output.fileoutputformat.compress.codec;
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec
hive (default)> 
```
> è®¾ç½®mapreduceæœ€ç»ˆæ•°æ®è¾“å‡ºå‹ç¼©ä¸ºå—å‹ç¼©
```
hive (default)> set mapreduce.output.fileoutputformat.compress.type=BLOCK;
hive (default)> set mapreduce.output.fileoutputformat.compress.type;
mapreduce.output.fileoutputformat.compress.type=BLOCK
```
> æµ‹è¯•è¾“å‡ºç»“æœæ˜¯å¦æ˜¯å‹ç¼©æ–‡ä»¶
```
hive (default)> insert overwrite local directory '/opt/module/datas/distribute-result' select * from emp distribute by deptno sort by empno desc;
[root@systemhub711 datas]# cat distribute-result/
cat: distribute-result/: Is a directory
[root@systemhub711 datas]# cd distribute-result/
[root@systemhub711 distribute-result]# ll
total 4
-rw-r--r--. 1 root root 424 Apr  1 22:07 000000_0.snappy
[root@systemhub711 distribute-result]# cat 000000_0.snappy 
ï¿½ï¿½ï¿½ï¿½<7939KINGSCLADDJHEW75661993-07-123000.020.0\N
7788FOES4(EDFDFD76984$4-09-1795.3       3T2JAMSKKIHNGSEHN77693$1-06-23116gRï¿½ADAMSJUSHHWESD45521985-05-1625524.0ï¿½.ï¿½@654SOCTDMANSJUS855j86-02-142852.3ï¿½ï¿½`JOSSSJDHYHDSDS45451874j(52894.252iT521WAROSSJDHHJDJX78ï¿½D84-06-121250.185ï¿½H30
7499ALLTESALESï¿½
/k5X369SMITHCLERKSKLD790%7ï¿½2316
T0-12-17800.020.0\N
[root@systemhub711 distribute-result]# 
```

### 8.5 æ–‡ä»¶å­˜å‚¨æ ¼å¼
> Hiveæ”¯æŒçš„å­˜å‚¨æ•°çš„æ ¼å¼ä¸»è¦æœ‰ : `TEXTFILE` / `SEQUENCEFILE` / `ORC` / `PARQUET`.
> 
> `TEXTFILE`å’Œ`SEQUENCEFILE`å­˜å‚¨æ ¼å¼æ˜¯åŸºäºè¡Œå¼å­˜å‚¨.
> 
> `ORC`å’Œ`PARQUET`æ˜¯åŸºäºåˆ—å¼å­˜å‚¨.

#### 8.5.1 åˆ—å¼å­˜å‚¨ & è¡Œå¼å­˜å‚¨
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_006.jpg)
> å›¾ç‰‡ä¸­å·¦è¾¹ä¸ºé€»è¾‘è¡¨,å³è¾¹ç¬¬ä¸€ä¸ªä¸ºè¡Œå¼å­˜å‚¨,ç¬¬äºŒä¸ªä¸ºåˆ—å¼å­˜å‚¨.
> 
> è¡Œå­˜å‚¨çš„ç‰¹ç‚¹ : æŸ¥è¯¢æ»¡è¶³æ¡ä»¶çš„ä¸€æ•´è¡Œæ•°æ®çš„æ—¶å€™,åˆ—å­˜å‚¨åˆ™éœ€è¦å»æ¯ä¸ªèšé›†çš„å­—æ®µæ‰¾åˆ°å¯¹åº”çš„æ¯ä¸ªåˆ—çš„å€¼,è¡Œå­˜å‚¨åªéœ€è¦æ‰¾åˆ°å…¶ä¸­ä¸€ä¸ªå€¼,å…¶ä½™çš„å€¼éƒ½åœ¨ç›¸é‚»åœ°æ–¹,æ‰€ä»¥æ­¤æ—¶è¡Œå­˜å‚¨æŸ¥è¯¢çš„é€Ÿåº¦æ›´å¿«.
> 
> åˆ—å­˜å‚¨çš„ç‰¹ç‚¹ : å› ä¸ºæ¯ä¸ªå­—æ®µçš„æ•°æ®èšé›†å­˜å‚¨,åœ¨æŸ¥è¯¢åªéœ€è¦å°‘æ•°å‡ ä¸ªå­—æ®µçš„æ—¶å€™,èƒ½å¤§å¤§å‡å°‘è¯»å–çš„æ•°æ®é‡,æ¯ä¸ªå­—æ®µçš„æ•°æ®ç±»å‹ä¸€å®šæ˜¯ç›¸åŒçš„,åˆ—å¼å­˜å‚¨å¯ä»¥é’ˆå¯¹æ€§çš„è®¾è®¡æ›´å¥½çš„è®¾è®¡å‹ç¼©ç®—æ³•.

#### 8.5.2 TextFileæ ¼å¼
> é»˜è®¤æ ¼å¼,æ•°æ®ä¸åšå‹ç¼©,ç£ç›˜å¼€é”€å¤§,æ•°æ®è§£æå¼€é”€å¤§,å¯ç»“åˆGzipã€Bzip2ä½¿ç”¨,ä½†ä½¿ç”¨Gzipè¿™ç§æ–¹å¼,hiveä¸ä¼šå¯¹æ•°æ®è¿›è¡Œåˆ‡åˆ†,ä»è€Œæ— æ³•å¯¹æ•°æ®è¿›è¡Œå¹¶è¡Œæ“ä½œ.

#### 8.5.3 Orcæ ¼å¼
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_007.jpg)

> Orc(Optimized Row Columnar )Hive 0.11ç‰ˆé‡Œå¼•å…¥çš„æ–°çš„å­˜å‚¨æ ¼å¼.
> 
> å¯ä»¥çœ‹åˆ°æ¯ä¸ªOrcæ–‡ä»¶ç”±1ä¸ªæˆ–å¤šä¸ªstripeç»„æˆ,æ¯ä¸ªstripe250MBå¤§å°,è¿™ä¸ªStripeå®é™…ç›¸å½“äºRowGroupæ¦‚å¿µ,ä¸è¿‡å¤§å°ç”±4MB->250MB,è¿™æ ·åº”è¯¥èƒ½æå‡é¡ºåºè¯»çš„ååç‡,æ¯ä¸ªStripeé‡Œæœ‰ä¸‰éƒ¨åˆ†ç»„æˆ,åˆ†åˆ«æ˜¯Index Data,Row Data,Stripe Footer:
> 
> Index Data : ä¸€ä¸ªè½»é‡çº§çš„index,é»˜è®¤æ˜¯æ¯éš”1Wè¡Œåšä¸€ä¸ªç´¢å¼•,è¿™é‡Œåšçš„ç´¢å¼•åº”è¯¥åªæ˜¯è®°å½•æŸè¡Œçš„å„å­—æ®µåœ¨Row Dataä¸­çš„offset.
> Row Data : å­˜çš„æ˜¯å…·ä½“çš„æ•°æ®,å…ˆå–éƒ¨åˆ†è¡Œ,ç„¶åå¯¹è¿™äº›è¡ŒæŒ‰åˆ—è¿›è¡Œå­˜å‚¨,å¯¹æ¯ä¸ªåˆ—è¿›è¡Œäº†ç¼–ç ,åˆ†æˆå¤šä¸ªStreamæ¥å­˜å‚¨.
> 
> Stripe Footer : å­˜çš„æ˜¯å„ä¸ªStreamçš„ç±»å‹,é•¿åº¦ç­‰ä¿¡æ¯.
> 
> æ¯ä¸ªæ–‡ä»¶æœ‰ä¸€ä¸ªFile Footer,è¿™é‡Œé¢å­˜çš„æ˜¯æ¯ä¸ªStripeçš„è¡Œæ•°,æ¯ä¸ªColumnçš„æ•°æ®ç±»å‹ä¿¡æ¯ç­‰,æ¯ä¸ªæ–‡ä»¶çš„å°¾éƒ¨æ˜¯ä¸€ä¸ªPostScript,è¿™é‡Œé¢è®°å½•äº†æ•´ä¸ªæ–‡ä»¶çš„å‹ç¼©ç±»å‹ä»¥åŠFileFooterçš„é•¿åº¦ä¿¡æ¯ç­‰,åœ¨è¯»å–æ–‡ä»¶æ—¶,ä¼šseekåˆ°æ–‡ä»¶å°¾éƒ¨è¯»PostScript,ä»é‡Œé¢è§£æåˆ°File Footeré•¿åº¦,å†è¯»FileFooter,ä»é‡Œé¢è§£æåˆ°å„ä¸ªStripeä¿¡æ¯,å†è¯»å„ä¸ªStripe,å³ä»åå¾€å‰è¯».

#### 8.5.4 Parquetæ ¼å¼
> Parquetæ–‡ä»¶çš„æ ¼å¼å¦‚ä¸‹å›¾æ‰€ç¤º
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hive/start_008.jpg)
> Parquetæ˜¯é¢å‘åˆ†æå‹ä¸šåŠ¡çš„åˆ—å¼å­˜å‚¨æ ¼å¼,ç”±Twitterå’ŒClouderaåˆä½œå¼€å‘,2015å¹´5æœˆä»Apacheçš„å­µåŒ–å™¨é‡Œæ¯•ä¸šæˆä¸ºApacheé¡¶çº§é¡¹ç›®.
> 
> Parquetæ–‡ä»¶æ˜¯ä»¥äºŒè¿›åˆ¶æ–¹å¼å­˜å‚¨çš„,æ‰€ä»¥æ˜¯ä¸å¯ä»¥ç›´æ¥è¯»å–çš„,æ–‡ä»¶ä¸­åŒ…æ‹¬è¯¥æ–‡ä»¶çš„æ•°æ®å’Œå…ƒæ•°æ®,å› æ­¤Parquetæ ¼å¼æ–‡ä»¶æ˜¯è‡ªè§£æ.
> 
> é€šå¸¸æƒ…å†µä¸‹,åœ¨å­˜å‚¨Parquetæ•°æ®çš„æ—¶å€™ä¼šæŒ‰ç…§Blockå¤§å°è®¾ç½®è¡Œç»„çš„å¤§å°,ç”±äºä¸€èˆ¬æƒ…å†µä¸‹æ¯ä¸€ä¸ªMapperä»»åŠ¡å¤„ç†æ•°æ®çš„æœ€å°å•ä½æ˜¯ä¸€ä¸ªBlock,è¿™æ ·å¯ä»¥æŠŠæ¯ä¸€ä¸ªè¡Œç»„ç”±ä¸€ä¸ªMapperä»»åŠ¡å¤„ç†,å¢å¤§ä»»åŠ¡æ‰§è¡Œå¹¶è¡Œåº¦.
> 
> ä¸Šå›¾å±•ç¤ºäº†ä¸€ä¸ªParquetæ–‡ä»¶çš„å†…å®¹,ä¸€ä¸ªæ–‡ä»¶ä¸­å¯ä»¥å­˜å‚¨å¤šä¸ªè¡Œç»„,æ–‡ä»¶çš„é¦–ä½éƒ½æ˜¯è¯¥æ–‡ä»¶çš„Magic  Code,ç”¨äºæ ¡éªŒå®ƒæ˜¯å¦æ˜¯ä¸€ä¸ªParquetæ–‡ä»¶,Footer  lengthè®°å½•äº†æ–‡ä»¶å…ƒæ•°æ®çš„å¤§å°,é€šè¿‡è¯¥å€¼å’Œæ–‡ä»¶é•¿åº¦å¯ä»¥è®¡ç®—å‡ºå…ƒæ•°æ®çš„åç§»é‡,æ–‡ä»¶çš„å…ƒæ•°æ®ä¸­åŒ…æ‹¬æ¯ä¸€ä¸ªè¡Œç»„çš„å…ƒæ•°æ®ä¿¡æ¯å’Œè¯¥æ–‡ä»¶å­˜å‚¨æ•°æ®çš„Schemaä¿¡æ¯,é™¤äº†æ–‡ä»¶ä¸­æ¯ä¸€ä¸ªè¡Œç»„çš„å…ƒæ•°æ®,æ¯ä¸€é¡µçš„å¼€å§‹éƒ½ä¼šå­˜å‚¨è¯¥é¡µçš„å…ƒæ•°æ®,åœ¨Parquetä¸­,æœ‰ä¸‰ç§ç±»å‹çš„é¡µï¼šæ•°æ®é¡µã€å­—å…¸é¡µå’Œç´¢å¼•é¡µ,æ•°æ®é¡µç”¨äºå­˜å‚¨å½“å‰è¡Œç»„ä¸­è¯¥åˆ—çš„å€¼,å­—å…¸é¡µå­˜å‚¨è¯¥åˆ—å€¼çš„ç¼–ç å­—å…¸,æ¯ä¸€ä¸ªåˆ—å—ä¸­æœ€å¤šåŒ…å«ä¸€ä¸ªå­—å…¸é¡µ,ç´¢å¼•é¡µç”¨æ¥å­˜å‚¨å½“å‰è¡Œç»„ä¸‹è¯¥åˆ—çš„ç´¢å¼•,ç›®å‰Parquetä¸­è¿˜ä¸æ”¯æŒç´¢å¼•é¡µ.

### 8.6 å­˜å‚¨å’Œå‹ç¼©ç»“åˆ
#### 8.6.1 ä¿®æ”¹Hadoopé›†ç¾¤å…·æœ‰Snappyå‹ç¼©æ–¹å¼
> 1.æŸ¥çœ‹hadoop checknativeå‘½ä»¤ä½¿ç”¨.
```
[root@systemhub511 native]# hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  checknative [-a|-h]  check native hadoop and compression libraries 
Most commands print help when invoked w/o parameters.
[root@systemhub511 native]# 

```
> 2.æŸ¥çœ‹hadoopæ”¯æŒçš„å‹ç¼©æ–¹å¼.
```
[root@systemhub511 native]# hadoop checknative
19/04/01 23:14:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Native library checking:
hadoop:  false 
zlib:    false 
snappy:  false 
lz4:     false 
bzip2:   false 
openssl: false 
19/04/01 23:14:07 INFO util.ExitUtil: Exiting with status 1
[root@systemhub511 native]# 
```
> 3.å°†ç¼–è¯‘å¥½æ”¯æŒSnappyå‹ç¼©hadoop-2.7.2.tar.gzåŒ…å¯¼å…¥åˆ°/opt/softwareä¸­.
> 4.å°†hadoop-2.7.2.tar.gzè§£å‹åˆ°å½“å‰è·¯å¾„.
```
[root@systemhub611 software]# tar -zxvf hadoop-2.7.2.-src.tar.gz
hadoop-2.7.2/
hadoop-2.7.2/libexec/
hadoop-2.7.2/libexec/yarn-config.sh
hadoop-2.7.2/libexec/hadoop-config.sh
hadoop-2.7.2/libexec/mapred-config.cmd
```
> 5.è¿›å…¥åˆ°/opt/software/hadoop-2.7.2/lib/nativeè·¯å¾„å¯ä»¥çœ‹åˆ°æ”¯æŒSnappyå‹ç¼©åŠ¨æ€é“¾æ¥åº“.
```
[root@systemhub611 opt]# cd software/
[root@systemhub611 software]# cd hadoop-2.7.2/
[root@systemhub611 hadoop-2.7.2]# cd lib/native/
[root@systemhub611 native]# pwd
/opt/software/hadoop-2.7.2/lib/native
[root@systemhub611 native]# ll
total 5188
-rw-r--r--. 1 root root 1210260 Sep  1  2017 libhadoop.a
-rw-r--r--. 1 root root 1487268 Sep  1  2017 libhadooppipes.a
lrwxrwxrwx. 1 root root      18 Apr  1 19:30 libhadoop.so -> libhadoop.so.1.0.0
-rwxr-xr-x. 1 root root  716316 Sep  1  2017 libhadoop.so.1.0.0
-rw-r--r--. 1 root root  582048 Sep  1  2017 libhadooputils.a
-rw-r--r--. 1 root root  364860 Sep  1  2017 libhdfs.a
lrwxrwxrwx. 1 root root      16 Apr  1 19:30 libhdfs.so -> libhdfs.so.0.0.0
-rwxr-xr-x. 1 root root  229113 Sep  1  2017 libhdfs.so.0.0.0
-rw-r--r--. 1 root root  472950 Sep  1  2017 libsnappy.a
-rwxr-xr-x. 1 root root     955 Sep  1  2017 libsnappy.la
lrwxrwxrwx. 1 root root      18 Apr  1 19:30 libsnappy.so -> libsnappy.so.1.3.0
lrwxrwxrwx. 1 root root      18 Apr  1 19:30 libsnappy.so.1 -> libsnappy.so.1.3.0
-rwxr-xr-x. 1 root root  228177 Sep  1  2017 libsnappy.so.1.3.0
[root@systemhub611 native]#
```
> 6.å°†/opt/software/hadoop-2.7.2/lib/nativeç›®å½•ä¸‹æ‰€æœ‰å†…å®¹æ‹·è´åˆ°å¼€å‘é›†ç¾¤ çš„/opt/module/hadoop-2.7.2/lib/nativeè·¯å¾„ä¸­.
```
[root@systemhub611 native]# cp ./* /opt/module/hadoop/lib/native/
```
> 7.åˆ†å‘é›†ç¾¤
```
[root@systemhub611 lib]# rsync -rvl /opt/module/hadoop/lib/native/ root@systemhub511:/opt/module/hadoop/lib/native/
sending incremental file list
libhadoop.a
libhadoop.so
libhadoop.so.1.0.0
libhadooppipes.a
libhadooputils.a
libhdfs.a
libhdfs.so
libhdfs.so.0.0.0
libsnappy.a
libsnappy.la
libsnappy.so
libsnappy.so.1
libsnappy.so.1.3.0

sent 6617750 bytes  received 30829 bytes  4432386.00 bytes/sec
total size is 6693730  speedup is 1.01
[root@systemhub611 lib]# 
[root@systemhub611 lib]# rsync -rvl /opt/module/hadoop/lib/native/ root@systemhub711:/opt/module/hadoop/lib/native/
sending incremental file list
libhadoop.a
libhadoop.so
libhadoop.so.1.0.0
libhadooppipes.a
libhadooputils.a
libhdfs.a
libhdfs.so
libhdfs.so.0.0.0
libsnappy.a
libsnappy.la
libsnappy.so
libsnappy.so.1
libsnappy.so.1.3.0

sent 6613502 bytes  received 38665 bytes  4434778.00 bytes/sec
total size is 6693730  speedup is 1.01
[root@systemhub611 lib]# 
```
> 8.å†æ¬¡æŸ¥çœ‹hadoopé›†ç¾¤æ”¯æŒå‹ç¼©ç±»å‹.
> systemhub511
```
[root@systemhub511 hadoop]# hadoop checknative
19/04/01 23:24:45 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
19/04/01 23:24:45 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/module/hadoop/lib/native/libhadoop.so
zlib:    true /lib64/libz.so.1
snappy:  true /opt/module/hadoop/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   false 
openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!
[root@systemhub511 hadoop]# 
```
> systemhub611
```
[root@systemhub611 lib]# hadoop checknative
19/04/01 23:25:02 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
19/04/01 23:25:02 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/module/hadoop/lib/native/libhadoop.so
zlib:    true /lib64/libz.so.1
snappy:  true /opt/module/hadoop/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   false 
openssl: true /usr/lib64/libcrypto.so
[root@systemhub611 lib]# 
```
> systemhub711
```
[root@systemhub711 hive]# hadoop checknative
19/04/01 23:25:28 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
19/04/01 23:25:28 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/module/hadoop/lib/native/libhadoop.so
zlib:    true /lib64/libz.so.1
snappy:  true /opt/module/hadoop/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   false 
openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!
[root@systemhub711 hive]# 
```
> 9.é‡æ–°å¯åŠ¨hadoopé›†ç¾¤å’Œhive.

#### 8.6.2 æµ‹è¯•å­˜å‚¨ & å‹ç¼©
> [LanguageManual+ORC æ–‡æ¡£](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC)

> ORCå­˜å‚¨æ–¹å¼å‹ç¼©

| Key      |     Default |   Notes   |
| :--------: | :--------:| :------: |
| orc.compress    |   ZLIB |  high level compression (one of NONE,ZLIB, SNAPPY)  |
| orc.compress.size    |   262,144 |  number of bytes in each compression chunk  |
| orc.stripe.size    |   67,108,864 |  number of bytes in each stripe  |
| orc.row.index.stride    |   10,000 |  number of rows between index entries(must be >= 1000)  |
| orc.create.index    |   true |  whether to create row indexes  |
| orc.bloom.filter.columns    |   "" |  comma separated list of column names for which bloom filter should be created  |
| orc.bloom.filter.fpp    |   0.05 |  false  positive  probability  for  bloom  filter(must >0.0 and <1.0)  |


## 9. ä¼ä¸šçº§è°ƒä¼˜
### 9.1 FetchæŠ“å–
> FetchæŠ“å–æ˜¯æŒ‡:Hiveä¸­å¯¹æŸäº›æƒ…å†µçš„æŸ¥è¯¢å¯ä»¥ä¸å¿…ä½¿ç”¨MapReduceè®¡ç®—.
> 
> ä¾‹å¦‚ : `SELECT * FROM employees;`åœ¨è¿™ç§æƒ…å†µä¸‹,Hiveå¯ä»¥ç®€å•åœ°è¯»å–employeeå¯¹åº”çš„å­˜å‚¨ç›®å½•ä¸‹çš„æ–‡ä»¶,ç„¶åè¾“å‡ºæŸ¥è¯¢ç»“æœåˆ°æ§åˆ¶å°.
> 
> åœ¨hive-default.xml.templateæ–‡ä»¶ä¸­`hive.fetch.task.conversion`é»˜è®¤æ˜¯more,è€ç‰ˆæœ¬hiveé»˜è®¤æ˜¯minimal,è¯¥å±æ€§ä¿®æ”¹ä¸ºmoreä»¥å,åœ¨å…¨å±€æŸ¥æ‰¾ã€å­—æ®µæŸ¥æ‰¾ã€limitæŸ¥æ‰¾ç­‰éƒ½ä¸ä¼šæ‰§è¡ŒMapReduce.
```xml
<property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
    <description>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have
      any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
    </description>
  </property>
```
> 1.å°†hive.fetch.task.conversionå±æ€§è®¾ç½®æˆnone,ç„¶åæ‰§è¡ŒæŸ¥è¯¢è¯­å¥,éƒ½ä¼šæ‰§è¡ŒMapReduceç¨‹åº.
> 
> å½“å‰é»˜è®¤å±æ€§æ˜¯more.
```
hive (default)> set hive.fetch.task.conversion;
hive.fetch.task.conversion=more
hive (default)> 
```
> è®¾ç½®å±æ€§ä¸ºnone.
```
hive (default)> set hive.fetch.task.conversion=none;
hive (default)> set hive.fetch.task.conversion;
hive.fetch.task.conversion=none
hive (default)> 
```
> æŸ¥è¯¢æ•°æ®è¡¨,ç»“æœæ¯æ¬¡æŸ¥è¯¢éƒ½ä¼šæ‰§è¡ŒMapReduceç¨‹åº.
```
hive (default)> select * from emp;
Query ID = root_20190401234234_12968ad3-7c1c-4e6b-a06c-7e06deabe984
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 1.55 sec   HDFS Read: 4192 HDFS Write: 472 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 550 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 35.844 seconds, Fetched: 9 row(s)
hive (default)> 
```
> 2.å°†hive.fetch.task.conversionå±æ€§è®¾ç½®æˆmore,ç„¶åæ‰§è¡ŒæŸ¥è¯¢è¯­å¥,å¦‚ä¸‹æŸ¥è¯¢æ–¹å¼éƒ½ä¸ä¼šæ‰§è¡ŒMapReduceç¨‹åº.
```
hive (default)> set hive.fetch.task.conversion=more;
hive (default)> select * from emp;
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
Time taken: 0.134 seconds, Fetched: 9 row(s)
hive (default)> 
```

### 9.2 æœ¬åœ°æ¨¡å¼
> å¤§å¤šæ•°Hadoop Jobæ˜¯éœ€è¦Hadoopæä¾›çš„å®Œæ•´çš„å¯æ‰©å±•æ€§æ¥å¤„ç†å¤§æ•°æ®é›†.
> 
> ä¸è¿‡,æœ‰æ—¶Hiveçš„è¾“å…¥æ•°æ®é‡æ˜¯éå¸¸å°çš„,åœ¨è¿™ç§æƒ…å†µä¸‹,ä¸ºæŸ¥è¯¢è§¦å‘æ‰§è¡Œä»»åŠ¡æ—¶æ¶ˆè€—å¯èƒ½ä¼šæ¯”å®é™…jobçš„æ‰§è¡Œæ—¶é—´è¦å¤šçš„å¤š,å¯¹äºå¤§å¤šæ•°è¿™ç§æƒ…å†µ,Hiveå¯ä»¥é€šè¿‡æœ¬åœ°æ¨¡å¼åœ¨å•å°æœºå™¨ä¸Šå¤„ç†æ‰€æœ‰çš„ä»»åŠ¡,å¯¹äºå°æ•°æ®é›†,æ‰§è¡Œæ—¶é—´å¯ä»¥æ˜æ˜¾è¢«ç¼©çŸ­.
> 
> å¼€å‘è€…å¯ä»¥é€šè¿‡è®¾ç½®`hive.exec.mode.local.auto`çš„å€¼ä¸º`true`,æ¥è®©Hiveåœ¨é€‚å½“çš„æ—¶å€™è‡ªåŠ¨å¯åŠ¨è¿™ä¸ªä¼˜åŒ–.
```
// å¼€å¯æœ¬åœ° MapReduce
hive (default)> set hive.exec.mode.local.auto=true;
// è®¾ç½®LocalMapReduceçš„æœ€å¤§è¾“å…¥æ•°æ®é‡,å½“è¾“å…¥æ•°æ®é‡å°äºè¿™ä¸ªå€¼æ—¶é‡‡ç”¨LocalMapReduceæ–¹å¼,é»˜è®¤ä¸º134217728,å³128M.
hive (default)> set hive.exec.mode.local.auto.inputbytes.max=50000000;
// è®¾ç½®LocalMapReduceæœ€å¤§è¾“å…¥æ–‡ä»¶ä¸ªæ•°,å½“è¾“å…¥æ–‡ä»¶ä¸ªæ•°å°äºè¿™ä¸ªå€¼æ—¶é‡‡ç”¨LocalMapReduceæ–¹å¼,é»˜è®¤ä¸º4.
hive (default)> set hive.exec.mode.local.auto.input.files.max=10;
```
> å¼€å¯æœ¬åœ°æ¨¡å¼,æ‰§è¡ŒæŸ¥è¯¢è¯­å¥,å¹¶æŸ¥çœ‹è®¡ç®—è¿è¡Œæ—¶é—´,æ‰€ç”¨æŸ¥è¯¢æ—¶é—´ä¸º2.716ç§’
```
hive (default)> set hive.exec.mode.local.auto=true;
hive (default)> select * from emp cluster by deptno;
Automatically selecting local only mode for query
Query ID = root_20190401235135_caee5452-243d-4299-b7a2-37ecf31c53c4
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 2724 HDFS Write: 41613642 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
Time taken: 2.716 seconds, Fetched: 9 row(s)
hive (default)> 
```
> å…³é—­æœ¬åœ°æ¨¡å¼,æ‰§è¡ŒæŸ¥è¯¢è¯­å¥,å¹¶æŸ¥çœ‹è®¡ç®—è¿è¡Œæ—¶é—´,æ‰€ç”¨æŸ¥è¯¢æ—¶é—´ä¸º35.145ç§’.
```
hive (default)> set hive.exec.mode.local.auto=flase;
hive (default)> select * from emp cluster by deptno;
Query ID = root_20190401235235_043bf057-0655-46f1-8003-0c0a842009ca
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.31 sec   HDFS Read: 8480 HDFS Write: 472 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 310 msec
OK
emp.empno       emp.ename       emp.job emp.mgr emp.hiredate    emp.sal emp.comm        emp.deptno
7939    KINGS   CLADDJHEW       7566    1993-07-12      3000.0  20.0    NULL
7788    FOESS   CLAEDFDFD       7698    1994-09-17      950.0   30.0    NULL
7782    JAMSK   KIHNGSEHN       7769    1991-06-23      1100.0  20.0    NULL
7698    ADAMS   JUSHHWESD       4552    1985-05-16      25524.02        30.0    NULL
7654    SOCTD   MANSJUSSD       4855    1996-02-14      2852.3  30.0    NULL
7566    JOSSS   JDHYHDSDS       4545    1874-05-15      2894.25 20.0    NULL
7369    SMITH   CLERKSKLD       7902    1980-12-17      800.0   20.0    NULL
7521    WAROS   SJDHHJDJX       7869    1984-06-12      1250.18 500.0   30
7499    ALLTE   SALESMANS       7689    1987-02-23      1600.0  300.0   30
Time taken: 35.145 seconds, Fetched: 9 row(s)
hive (default)>
```

### 9.3 æ•°æ®è¡¨ä¼˜åŒ–
#### 9.3.1 å°è¡¨/å¤§è¡¨Join
> å°†keyç›¸å¯¹åˆ†æ•£,å¹¶ä¸”æ•°æ®é‡å°çš„è¡¨æ”¾åœ¨joinçš„å·¦è¾¹,è¿™æ ·å¯ä»¥æœ‰æ•ˆå‡å°‘å†…å­˜æº¢å‡ºé”™è¯¯å‘ç”Ÿçš„å‡ ç‡,å†è¿›ä¸€æ­¥,å¯ä»¥ä½¿ç”¨Groupè®©å°çš„ç»´åº¦è¡¨(1000æ¡ä»¥ä¸‹çš„è®°å½•æ¡æ•°)è¿›å†…å­˜,åœ¨mapç«¯å®Œæˆreduce.
> 
> å®é™…æµ‹è¯•å‘ç° : æ–°ç‰ˆæœ¬hiveå·²ç»å¯¹å°è¡¨JOINå¤§è¡¨å’Œå¤§è¡¨JOINå°è¡¨è¿›è¡Œäº†ä¼˜åŒ–,å°è¡¨æ”¾åœ¨å·¦è¾¹å’Œå³è¾¹å·²ç»æ²¡æœ‰æ˜æ˜¾åŒºåˆ«.
> 
#### 9.3.2 å¤§è¡¨Joinå¤§è¡¨
> 1.ç©ºKEYè¿‡æ»¤
> æœ‰æ—¶joinè¶…æ—¶æ˜¯å› ä¸ºæŸäº›keyå¯¹åº”çš„æ•°æ®å¤ªå¤š,è€Œç›¸åŒkeyå¯¹åº”çš„æ•°æ®éƒ½ä¼šå‘é€åˆ°ç›¸åŒçš„reducerä¸Š,ä»è€Œå¯¼è‡´å†…å­˜ä¸å¤Ÿ,æ­¤æ—¶åº”è¯¥ä»”ç»†åˆ†æè¿™äº›å¼‚å¸¸çš„key,å¾ˆå¤šæƒ…å†µä¸‹,è¿™äº›keyå¯¹åº”çš„æ•°æ®æ˜¯å¼‚å¸¸æ•°æ®,éœ€è¦åœ¨SQLè¯­å¥ä¸­è¿›è¡Œè¿‡æ»¤,ä¾‹å¦‚keyå¯¹åº”çš„å­—æ®µä¸ºç©º.
#### 9.3.3 MapJoin
> å¦‚æœä¸æŒ‡å®šMapJoinæˆ–è€…ä¸ç¬¦åˆMapJoinçš„æ¡ä»¶,é‚£ä¹ˆHiveè§£æå™¨ä¼šå°†Joinæ“ä½œè½¬æ¢æˆCommon  Join,å³åœ¨Reduceé˜¶æ®µå®Œæˆjoin,å®¹æ˜“å‘ç”Ÿæ•°æ®å€¾æ–œ,å¯ä»¥ç”¨MapJoinæŠŠå°è¡¨å…¨éƒ¨åŠ è½½åˆ°å†…å­˜åœ¨mapç«¯è¿›è¡Œjoin,é¿å…reducerå¤„ç†.
#### 9.3.4 Group By
> é»˜è®¤æƒ…å†µä¸‹,Mapé˜¶æ®µåŒä¸€Keyæ•°æ®åˆ†å‘ç»™ä¸€ä¸ªreduce,å½“ä¸€ä¸ªkeyæ•°æ®è¿‡å¤§æ—¶å°±å€¾æ–œäº†.
> å¹¶ä¸æ˜¯æ‰€æœ‰çš„èšåˆæ“ä½œéƒ½éœ€è¦åœ¨Reduceç«¯å®Œæˆ,å¾ˆå¤šèšåˆæ“ä½œéƒ½å¯ä»¥å…ˆåœ¨Mapç«¯è¿›è¡Œéƒ¨åˆ†èšåˆ,æœ€ååœ¨Reduceç«¯å¾—å‡ºæœ€ç»ˆç»“æœ.
#### 9.3.5 Count(Distinct) å»é‡ç»Ÿè®¡
> æ•°æ®é‡å°çš„æ—¶å€™æ— æ‰€è°“,æ•°æ®é‡å¤§çš„æƒ…å†µä¸‹,ç”±äºCOUNT DISTINCTæ“ä½œéœ€è¦ç”¨ä¸€ä¸ªReduce Taskæ¥å®Œæˆ,è¿™ä¸€ä¸ªReduceéœ€è¦å¤„ç†çš„æ•°æ®é‡å¤ªå¤§,å°±ä¼šå¯¼è‡´æ•´ä¸ªJobå¾ˆéš¾å®Œæˆ,ä¸€èˆ¬COUNT DISTINCTä½¿ç”¨å…ˆGROUP BYå†COUNTçš„æ–¹å¼æ›¿æ¢.
#### 9.3.6 ç¬›å¡å°”ç§¯
> å°½é‡é¿å…ç¬›å¡å°”ç§¯,joinçš„æ—¶å€™ä¸åŠ onæ¡ä»¶,æˆ–è€…æ— æ•ˆçš„onæ¡ä»¶,Hiveåªèƒ½ä½¿ç”¨1ä¸ªreduceræ¥å®Œæˆç¬›å¡å°”ç§¯.
#### 9.3.7 è¡Œåˆ—è¿‡æ»¤
> åˆ—å¤„ç† : åœ¨SELECTä¸­,åªæ‹¿éœ€è¦çš„åˆ—,å¦‚æœæœ‰,å°½é‡ä½¿ç”¨åˆ†åŒºè¿‡æ»¤,å°‘ç”¨SELECT *
> 
> è¡Œå¤„ç† : åœ¨åˆ†åŒºå‰ªè£ä¸­,å½“ä½¿ç”¨å¤–å…³è”æ—¶,å¦‚æœå°†å‰¯è¡¨çš„è¿‡æ»¤æ¡ä»¶å†™åœ¨Whereåé¢,é‚£ä¹ˆå°±ä¼šå…ˆå…¨è¡¨å…³è”,ä¹‹åå†è¿‡æ»¤.

#### 9.3.8 åŠ¨æ€åˆ†åŒºè°ƒæ•´
> å…³ç³»å‹æ•°æ®åº“ä¸­,å¯¹åˆ†åŒºè¡¨Insertæ•°æ®æ—¶å€™,æ•°æ®åº“è‡ªåŠ¨ä¼šæ ¹æ®åˆ†åŒºå­—æ®µçš„å€¼,å°†æ•°æ®æ’å…¥åˆ°ç›¸åº”çš„åˆ†åŒºä¸­,Hiveä¸­ä¹Ÿæä¾›äº†ç±»ä¼¼çš„æœºåˆ¶,å³åŠ¨æ€åˆ†åŒº(Dynamic Partition),åªä¸è¿‡,ä½¿ç”¨Hiveçš„åŠ¨æ€åˆ†åŒº,éœ€è¦è¿›è¡Œç›¸åº”çš„é…ç½®.


## ğŸ”’ å°šæœªè§£é” æ­£åœ¨å­¦ä¹ æ¢ç´¢ä¸­... å°½æƒ…æœŸå¾… Blogæ›´æ–°! ğŸ”’

### 9.4 æ•°æ®å€¾æ–œ
#### 9.4.1 åˆç†è®¾ç½®Mapæ•°
#### 9.4.2 å°æ–‡ä»¶è¿›è¡Œåˆå¹¶
#### 9.4.3 å¤æ‚æ–‡ä»¶å¢åŠ Mapæ•°
#### 9.4.4 åˆç†è®¾ç½®Reduceæ•°

### 9.5 å¹¶è¡Œæ‰§è¡Œ
### 9.6 ä¸¥æ ¼æ¨¡å¼
### 9.7 JVMé‡ç”¨
### 9.8 æ¨æµ‹æ‰§è¡Œ
### 9.10 æ‰§è¡Œè®¡åˆ’ (Explain)

## 10. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------