
# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Hadoop Blog

@(2019-01-22)[Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Language:Hadoop|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg)|GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub)]

##  ğŸ˜ Hadoop Technology ä¿®ä»™ä¹‹é“ ç‚¼ç²¾åŒ–æ°” ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/hadoop.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------

[TOC]



## 1. å¤§æ•°æ® ç®€ä»‹
> å¤§æ•°æ®æ˜¯æŒ‡æ— æ³•åœ¨ä¸€å®šæ—¶é—´å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·å¯¹å…¶å†…å®¹è¿›è¡ŒæŠ“å–ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆã€‚å¤§æ•°æ®æŠ€æœ¯,æ˜¯æŒ‡ä»å„ç§å„æ ·ç±»å‹çš„æ•°æ®ä¸­,å¿«é€Ÿè·å¾—æœ‰ä»·å€¼ä¿¡æ¯çš„èƒ½åŠ›ã€‚é€‚ç”¨äºå¤§æ•°æ®çš„æŠ€æœ¯,åŒ…æ‹¬å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†(MPP)æ®åº“,æ•°æ®æŒ–æ˜ç”µç½‘,åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,åˆ†å¸ƒå¼æ•°æ®åº“,äº‘è®¡ç®—å¹³å°,äº’è”ç½‘,å’Œå¯æ‰©å±•çš„å­˜å‚¨ç³»ç»Ÿ,å¤§æ•°æ®ç”±å·¨å‹æ•°æ®é›†ç»„æˆ,è¿™äº›æ•°æ®é›†å¤§å°å¸¸è¶…å‡ºäººç±»åœ¨å¯æ¥å—æ—¶é—´ä¸‹çš„æ”¶é›†ã€åº‹ç”¨ã€ç®¡ç†å’Œå¤„ç†èƒ½åŠ›,å¤§æ•°æ®çš„å¤§å°ç»å¸¸æ”¹å˜,æˆªè‡³2012å¹´,å•ä¸€æ•°æ®é›†çš„å¤§å°ä»æ•°å¤ªå­—èŠ‚(TB)åå…†äº¿å­—èŠ‚(PB)ç­‰.   â€”â€” [MBAæ™ºåº“ç™¾ç§‘](https://wiki.mbalib.com/wiki/%E5%A4%A7%E6%95%B0%E6%8D%AE)


## 2. å¤§æ•°æ® æ¦‚è®º

### å¤§æ•°æ® æ¦‚å¿µ
> **`å¤§æ•°æ®(BigData)`**æ˜¯æŒ‡**`æ— æ³•åœ¨ä¸€å®šæ—¶é—´èŒƒå›´`**å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·è¿›è¡Œæ•æ‰ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆ,æ˜¯éœ€è¦æ–°å¤„ç†æ¨¡å¼æ‰èƒ½å…·æœ‰æ›´å¼ºçš„å†³ç­–åŠ›ã€æ´å¯Ÿå‘ç°åŠ›å’Œæµç¨‹ä¼˜åŒ–èƒ½åŠ›çš„æµ·é‡ã€é«˜å¢é•¿ç‡å’Œå¤šæ ·åŒ–çš„ä¿¡æ¯èµ„äº§.
> 
> å¤§æ•°æ®ä¸»è¦è§£å†³:æµ·é‡æ•°æ®çš„**`å­˜å‚¨`**å’Œæµ·é‡æ•°æ®çš„**`åˆ†æè®¡ç®—`**é—®é¢˜.
> 
> æ•°æ®å­˜å‚¨å•ä½: **`bit`** / **`Byte`** / **`KB`** / **`MB`** / **`GB`** / **`TB`**  / **`PB`** / **`EB`** / **`ZB`** / **`YB`** / **`BB`** / **`NB`** / **`DB`**
> 
> **`1 Byte = 8bit`**
> **`1 KB = 1024Byte`**
> **`1 MB = 1024KB`**
> **`1 GB = 1024MB`**
> **`1 TB = 1024GB`**
> **`1 PB = 1024TB`**
> **`1 EB = 1024PB`**
> **`1 ZB = 1024EB`**
> **`1 YB = 1024ZB`**
> **`1 BB = 1024YB`**
> **`1 NB = 1024BB`**
> **`1 DB = 1024NB`**

### å¤§æ•°æ® ç‰¹ç‚¹(4V)
#### 1.Volume (å¤§é‡)
> æˆªæ­¢ç›®å‰,äººç±»ç”Ÿäº§çš„æ‰€æœ‰å°åˆ·ææ–™çš„æ•°æ®é‡æ˜¯200PB(1PB=1024TB),è€Œå†å²ä¸Šå…¨äººç±»æ€»å…±è¯´è¿‡çš„è¯çš„æ•°é‡å¤§çº¦æ˜¯5EB(1EB=1024PB),å½“å‰å…¸å‹ä¸ªäººè®¡ç®—æœºç¡¬ç›˜çš„å®¹é‡ä¸ºTBé‡çº§,è€Œä¸€äº›å¤§ä¼ä¸šçš„æ•°æ®é‡å·²ç»è¿‘EBé‡çº§.
> 
#### 2.Velocity (é«˜é€Ÿ)
> è¿™æ˜¯å¤§æ•°æ®åŒºåˆ†äºä¼ ç»Ÿæ•°æ®æŒ–æ˜çš„æœ€æ˜¾è‘—ç‰¹å¾,æ ¹æ®IDCçš„"æ•°å­—å®‡å®™"æŠ¥å‘Š,é¢„è®¡2020å¹´,å…¨çƒæ•°æ®ä½¿ç”¨é‡å°†è¾¾åˆ°35.2ZB(1 ZB = 1024EB),åœ¨å¦‚æ­¤æµ·é‡çš„æ•°æ®é¢å‰,å¤„ç†æ•°æ®çš„æ•ˆç‡å°±æ˜¯ä¼ä¸šçš„ç”Ÿå‘½.

#### 3.Variety (å¤šæ ·)
> è¿™ç§ç±»å‹çš„å¤šæ ·æ€§ä¹Ÿè®©æ•°æ®è¢«åˆ†ä¸ºç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–æ•°æ®,ç›¸å¯¹äºä»¥å¾€ä¾¿äºå­˜å‚¨çš„ä»¥æ•°æ®åº“&æ–‡æœ¬ä¸ºä¸»çš„ç»“æ„åŒ–æ•°æ®,éç»“æ„åŒ–æ•°æ®è¶Šæ¥è¶Šå¤š,åŒ…æ‹¬ç½‘ç»œæ—¥å¿—,éŸ³é¢‘,è§†é¢‘,å›¾ç‰‡,åœ°ç†ä½ç½®ä¿¡æ¯ç­‰,è¿™äº›å¤šç±»å‹çš„æ•°æ®å¯¹æ•°æ®çš„å¤„ç†èƒ½åŠ›æå‡ºäº†æ›´é«˜è¦æ±‚.

#### 4.Value (ä½ä»·å€¼å¯†åº¦)
> ä»·å€¼å¯†åº¦çš„é«˜ä½ä¸æ•°æ®æ€»é‡çš„å¤§å°æˆåæ¯”,å¦‚ä½•å¿«é€Ÿå¯¹æœ‰ä»·å€¼æ•°æ®"æçº¯",æˆä¸ºç›®å‰å¤§æ•°æ®èƒŒæ™¯ä¸‹å¾…è§£å†³çš„éš¾é¢˜.

### å¤§æ•°æ® åº”ç”¨åœºæ™¯
> **`ç‰©æµä»“å‚¨`**:å¤§æ•°æ®åˆ†æç³»ç»ŸåŠ©åŠ›å•†å®¶ç²¾ç»†åŒ–è¿è¥,æå‡é”€é‡,èŠ‚çº¦æˆæœ¬.
> 
> **`é›¶å”®`**:åˆ†æç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯,ä¸ºç”¨æˆ·è´­ä¹°å•†å“æä¾›æ–¹ä¾¿,ä»è€Œæå‡å•†å“å”®é‡,ç»å…¸æ•…äº‹æ¡ˆä¾‹ -ã€Šçº¸å°¿å¸ƒ+å•¤é…’ã€‹.
> 
> **`æ—…æ¸¸`**:æ·±åº¦ç»“åˆå¤§æ•°æ®èƒ½åŠ›ä¸æ—…æ¸¸è¡Œä¸šéœ€æ±‚,å…±å»ºæ—…æ¸¸äº§ä¸šæ™ºæ…§ç®¡ç†,æ™ºæ…§æœåŠ¡,æ™ºæ…§è¥é”€çš„æœªæ¥.
> 
> **`å•†å“å¹¿å‘Šæ¨è`**:ä¸ºç”¨æˆ·æ¨èå¯èƒ½å–œæ¬¢çš„å•†å“.
> 
> **`ä¿é™©`**:æµ·é‡æ•°æ®æŒ–æ˜åŠé£é™©é¢„æµ‹,åŠ©åŠ›ä¿é™©è¡Œä¸šç²¾å‡†è¥é”€,æå‡ç²¾ç»†åŒ–å®šä»·èƒ½åŠ›.
> 
> **`é‡‘è`**:å¤šç»´åº¦ä½“ç°ç”¨æˆ·ç‰¹å¾,å¸®åŠ©é‡‘èæœºæ„æ¨èä¼˜è´¨å®¢æˆ·,é˜²èŒƒæ¬ºè¯ˆé£é™©.
> 
> **`æˆ¿äº§`**:å¤§æ•°æ®å…¨é¢åŠ©åŠ›æˆ¿åœ°äº§è¡Œä¸š,æ‰“é€ ç²¾å‡†æŠ•ç­–ä¸è¥é”€,æŒ‘é€‰å‡ºæ›´åˆé€‚çš„åœ°åŸŸ.

### å¤§æ•°æ®éƒ¨é—¨ä¸šåŠ¡æµç¨‹åˆ†æ
``` sequence
äº§å“äººå‘˜->æ•°æ®éƒ¨é—¨:æå‡ºéœ€æ±‚(ç»Ÿè®¡æ€»ç”¨æˆ·æ•°,æ—¥æ´»è·ƒç”¨æˆ·æ•°,å›æµç”¨æˆ·æ•°ç­‰)
æ•°æ®éƒ¨é—¨-->æ•°æ®å¯è§†åŒ–/æŠ¥è¡¨å±•ç¤º/é‚®ä»¶å‘é€/å¤§å±å¹•å±•ç¤ºç­‰:æ­å»ºæ•°æ®å¹³å°,åˆ†ææ•°æ®æŒ‡æ ‡
```
### å¤§æ•°æ®éƒ¨é—¨ç»„ç»‡ç»“æ„(é‡ç‚¹)
**å¤§æ•°æ®éƒ¨é—¨ç»„ç»‡ç»“æ„**
| æ‰€åœ¨ç»„    |  æ‰€åœ¨ç»„å·¥ä½œèŒè´£ |
| :-------- | --------:|
| å¹³å°ç»„  | Hadoop,Flume,Kafka,Hbase,Sparkç­‰æ¡†æ¶å¹³å°æ­å»º,é›†ç¾¤æ€§èƒ½ç›‘æ§,é›†ç¾¤æ€§èƒ½è°ƒä¼˜ |
| æ•°æ®ä»“åº“ç»„  | ETLå·¥ç¨‹å¸ˆ-æ•°æ®æ¸…æ´—,Hiveå·¥ç¨‹å¸ˆ-æ•°æ®åˆ†æ,æ•°æ®ä»“åº“å»ºæ¨¡ |
| æ•°æ®æŒ–æ˜ç»„  | ç®—æ³•å·¥ç¨‹å¸ˆ æ¨èç³»ç»Ÿå·¥ç¨‹å¸ˆ ç”¨æˆ·ç”»åƒå·¥ç¨‹å¸ˆ |
| æ•°æ®æŠ¥è¡¨å¼€å‘ç»„  | JAVAEEå·¥ç¨‹å¸ˆ |

## 3. æ¢ç´¢Hadoopæ¡†æ¶ å¤§æ•°æ®ç”Ÿæ€

### Hadoop ç®€ä»‹

> Apache Hadoopæ˜¯ä¸€æ¬¾æ”¯æŒæ•°æ®å¯†é›†å‹åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºå¹¶ä»¥Apache 2.0è®¸å¯åè®®å‘å¸ƒçš„å¼€æºè½¯ä»¶æ¡†æ¶,å®ƒæ”¯æŒåœ¨å•†å“ç¡¬ä»¶æ„å»ºçš„å¤§å‹é›†ç¾¤ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åº,Hadoopæ˜¯æ ¹æ®è°·æ­Œå…¬å¸å‘è¡¨çš„MapReduceå’ŒGoogleæ–‡ä»¶ç³»ç»Ÿçš„è®ºæ–‡è‡ªè¡Œå®ç°è€Œæˆã€‚æ‰€æœ‰çš„Hadoopæ¨¡å—éƒ½æœ‰ä¸€ä¸ªåŸºæœ¬å‡è®¾,å³ç¡¬ä»¶æ•…éšœæ˜¯å¸¸è§æƒ…å†µ,åº”è¯¥ç”±æ¡†æ¶è‡ªåŠ¨å¤„ç†ã€‚
> 
> Hadoopæ¡†æ¶é€æ˜åœ°ä¸ºåº”ç”¨æä¾›å¯é æ€§å’Œæ•°æ®ç§»åŠ¨,å®ƒå®ç°äº†åä¸ºMapReduceçš„ç¼–ç¨‹èŒƒå¼:åº”ç”¨ç¨‹åºè¢«åˆ†å‰²æˆè®¸å¤šå°éƒ¨åˆ†,è€Œæ¯ä¸ªéƒ¨åˆ†éƒ½èƒ½åœ¨é›†ç¾¤ä¸­çš„ä»»æ„èŠ‚ç‚¹ä¸Šè¿è¡Œæˆ–é‡æ–°è¿è¡Œã€‚æ­¤å¤–,Hadoopè¿˜æä¾›äº†åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,ç”¨ä»¥å­˜å‚¨æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹çš„æ•°æ®,è¿™ä¸ºæ•´ä¸ªé›†ç¾¤å¸¦æ¥äº†éå¸¸é«˜çš„å¸¦å®½ã€‚MapReduceå’Œåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿçš„è®¾è®¡,ä½¿å¾—æ•´ä¸ªæ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨å¤„ç†èŠ‚ç‚¹æ•…éšœ,å®ƒä½¿åº”ç”¨ç¨‹åºä¸æˆåƒä¸Šä¸‡çš„ç‹¬ç«‹è®¡ç®—çš„è®¡ç®—æœºå’ŒPBçº§çš„æ•°æ®è¿æ¥èµ·æ¥,ç°åœ¨æ™®éè®¤ä¸ºæ•´ä¸ªApache Hadoop"å¹³å°"åŒ…æ‹¬Hadoopå†…æ ¸ã€MapReduceã€Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ(HDFS)ä»¥åŠä¸€äº›ç›¸å…³é¡¹ç›®,æœ‰Apache Hiveå’ŒApache HBaseç­‰ç­‰.   â€”â€” [ç»´åŸºç™¾ç§‘](https://zh.wikipedia.org/wiki/Apache_Hadoop)

### Hadoop æ˜¯ä»€ä¹ˆ
> Hadoopæ˜¯ç”±ApacheåŸºé‡‘ä¼šæ‰€å¼€å‘çš„åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€æ¶æ„.
> 
> Hadoopä¸»è¦è§£å†³:æµ·é‡**`æ•°æ®çš„å­˜å‚¨`**å’Œæµ·é‡**`æ•°æ®çš„åˆ†æè®¡ç®—`**é—®é¢˜
> 
> å¹¿ä¹‰ä¸Šæ¥è®²,Hadoopé€šå¸¸æ˜¯æŒ‡ä¸€ä¸ªæ›´å¹¿æ³›çš„æ¦‚å¿µ --- Hadoopç”Ÿæ€åœˆ

### Hadoop å‘å±•å†å²
> 1.Hadoopåˆ›å§‹äºº:Doug Cutting
> 
> Luceneæ¡†æ¶æ˜¯Doug Cuttingå¼€åˆ›çš„å¼€æºè½¯ä»¶,ä½¿ç”¨javaç¼–ç¨‹è¯­è¨€å¼€å‘,å®ç°ä¸Googleç±»ä¼¼çš„å…¨æ–‡æœç´¢åŠŸèƒ½,å®ƒæä¾›äº†å…¨æ–‡æ£€ç´¢å¼•æ“çš„æ¶æ„,åŒ…æ‹¬å®Œæ•´çš„æŸ¥è¯¢å¼•æ“å’Œç´¢å¼•å¼•æ“.
> 
> 2.2001å¹´å¹´åº•Luceneæˆä¸ºApacheåŸºé‡‘ä¼šçš„ä¸€ä¸ªå­é¡¹ç›®.
> 
> 3.å¯¹äºæµ·é‡æ•°æ®çš„åœºæ™¯,Luceneé¢å¯¹ä¸GoogleåŒæ ·çš„å›°éš¾,å­˜å‚¨æ•°æ®å›°éš¾,æ£€ç´¢é€Ÿåº¦æ…¢.
> 
> 4.å­¦ä¹ å’Œæ¨¡ä»¿Googleè§£å†³è¿™äº›é—®é¢˜çš„åŠæ³•:(Luceneçš„å‡çº§ç‰ˆ) Nutch.
> 
> 5.å¯ä»¥è¯´Googleæ˜¯Hadoopçš„æ€æƒ³ä¹‹æº(Googleåœ¨å¤§æ•°æ®æ–¹é¢çš„ä¸‰ç¯‡è®ºæ–‡)
> 
> GFS ---> HDFS
> Map-MapReduce ---> MR
> BigTable ---> Hbase
> 
> 6.2003è‡³2004å¹´,Googleå…¬å¼€äº†éƒ¨åˆ†GFSå’ŒMapReduceæ€æƒ³ç»†èŠ‚,ä»¥æ­¤ä¸ºåŸºç¡€Doug Cuttingç­‰å¼€å‘è€…ç”¨äº†2å¹´ä¸šä½™æ—¶é—´å®ç°äº†DFSå’ŒMapReduceæœºåˆ¶,ä½¿Nutchæ€§èƒ½é£™å‡.
> 
> 7.2005å¹´Hadoopä½œä¸ºLuceneçš„å­é¡¹ç›®,Nutchçš„ä¸€éƒ¨åˆ†æ­£å¼å¼•å…¥ApacheåŸºé‡‘ä¼š.
> 
> 8.2006å¹´3æœˆä»½,Map-Reduceå’ŒNDFS(Nutch Distributed File System),åˆ†åˆ«è¢«çº³å…¥ç§°ä¸ºHadoopçš„é¡¹ç›®ä¸­.
> 
> 9.Hadoopåå­—æ¥æºäºDoug Cuttingå­©å­çš„ç©å…·å¤§è±¡.
> 
> 10.Hadoopå°±æ­¤è¯ç”Ÿå¹¶è¿…é€Ÿå‘å±•,æ ‡å¿—ç€å¤§æ•°æ®æ—¶ä»£æ¥ä¸´.

### Hadoop ä¸‰å¤§å‘è¡Œç‰ˆæœ¬
> Hadoop ä¸‰å¤§å‘è¡Œç‰ˆæœ¬ **`Apache`** | **`Cloudera`** | **`Hortonworks`**

#### Apache Hadoop
> Apacheç‰ˆæœ¬æœ€æœ€åŸå§‹(æœ€åŸºç¡€)ç‰ˆæœ¬,å¯¹äºå…¥é—¨å­¦ä¹ æœ€ä½³.
> 
> å®˜ç½‘åœ°å€ : http://hadoop.apache.org/releases.html
> 
> ä¸‹è½½åœ°å€ : https://archive.apache.org/dist/hadoop/common/

#### Cloudera Hadoop
> Clouderaåœ¨å¤§å‹äº’è”ç½‘ä¼ä¸šä¸­åº”ç”¨åœºæ™¯è¾ƒå¤š.
> 
> å®˜ç½‘åœ°å€ : https://www.cloudera.com/downloads/cdh/5-10-0.html
> 
> ä¸‹è½½åœ°å€ : http://archive.cloudera.com/cdh5/cdh/5/
> 
> 2008å¹´æˆç«‹çš„Clouderaæ˜¯æœ€æ—©å°†Hadoopå•†ç”¨å…¬å¸,ä¸ºåˆä½œä¼™ä¼´æä¾›Hadoopçš„å•†ä¸šè§£å†³æ–¹æ¡ˆ,ä¸»è¦æ˜¯åŒ…æ‹¬æ”¯æŒ,å’¨è¯¢æœåŠ¡,åŸ¹è®­.
> 
> 2009å¹´Hadoopåˆ›å§‹äººDoug Cuttingä¹ŸåŠ ç›Ÿäº†Clouderaå…¬å¸,Clouderaäº§å“ä¸»è¦ä¸ºCDH,Cloudera Manager | Cloudera Support.
> 
> CDHæ˜¯Clouderaçš„Hadoopå‘è¡Œç‰ˆ,å®Œå…¨å¼€æº,æ¯”Apache Hadoopåœ¨å…¼å®¹æ€§,å®‰å…¨æ€§,ç¨³å®šæ€§ä¸Šæœ‰æ‰€å¢å¼º.
> 
> Cloudera Manageræ˜¯é›†ç¾¤çš„è½¯ä»¶åˆ†å‘åŠç®¡ç†ç›‘æ§å¹³å°,å¯ä»¥å†å‡ ä¸ªå°æ—¶å†…éƒ¨éƒ¨ç½²å¥½ä¸€ä¸ªHadoopé›†ç¾¤,å¹¶å¯¹é›†ç¾¤èŠ‚ç‚¹åŠæœåŠ¡
è¿›è¡Œå®æ—¶ç›‘æ§,Cloudera Supportå³æ˜¯å¯¹Hadoopçš„æŠ€æœ¯æ”¯æŒ.
> 
> Clouderaçš„æ ‡ä»·ä¸ºæ¯å¹´æ¯ä¸ªèŠ‚ç‚¹4000ç¾å…ƒ,Clouderaå¼€å‘å¹¶è´¡çŒ®äº†å¯å®æ—¶å¤„ç†å¤§æ•°æ®çš„Impalaé¡¹ç›®.

#### Hortonworks Hadoop
> Hortonworksæ–‡æ¡£è¾ƒå¥½.
> 
> å®˜ç½‘åœ°å€ : https://hortonworks.com/products/data-center/hdp/
> 
> ä¸‹è½½åœ°å€ : https://hortonworks.com/downloads/#data-platform
> 
> 2011å¹´æˆç«‹çš„Hortonworksæ˜¯é›…è™ä¸ç¡…è°·é£æŠ•å…¬å¸Benchmark Capitalåˆèµ„ç»„å»º.
> 
> å…¬å¸æˆç«‹ä¹‹åˆå°±å¸çº³äº†å¤§çº¦25åè‡³30åä¸“é—¨ç ”ç©¶Hadoopçš„é›…è™å·¥ç¨‹å¸ˆ,ä¸Šè¿°å·¥ç¨‹å¸ˆå‡åœ¨2005å¹´å¼€å§‹ååŠ©é›…è™å¼€å‘Hadoop,å¹¶è´¡çŒ®äº†80%çš„Hadoopä»£ç .
> 
> é›…è™å·¥ç¨‹å‰¯æ€»è£,é›…è™Hadoopå¼€å‘å›¢é˜Ÿè´Ÿè´£äººEric Baldeschwielerå‡ºä»»Hortonworksçš„é¦–å¸­æ‰§è¡Œå®˜.
> 
> Hortonworksä¸»æ‰“äº§å“æ˜¯Hortonworks Data Platform(HDP),ä¹ŸåŒæ ·æ˜¯100%å¼€æºäº§å“,HDPé™¤å¸¸è§çš„é¡¹ç›®å¤–è¿˜åŒ…æ‹¬Ambari,ä¸€æ¬¾å¼€æºçš„å®‰è£…å’Œç®¡ç†ç³»ç»Ÿ.
> 
> HCatalog ä¸€ä¸ªå…ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ,HCatalogç°å·²é›†æˆåˆ°Facebookå¼€æºçš„Hiveä¸­,Hortonworksçš„Stingerå¼€åˆ›æ€§çš„æå¤§çš„ä¼˜åŒ–äº†Hiveé¡¹ç›®,Hortonworksä¸ºå…¥é—¨æä¾›ä¸€ä¸ªéå¸¸å¥½çš„æ˜“äºä½¿ç”¨çš„æ²™ç›’.
> 
> Hortonworkså¼€å‘äº†å¾ˆå¤šå¢å¼ºç‰¹æ€§å¹¶æäº¤è‡³æ ¸å¿ƒä¸»å¹²,è¿™ä½¿å¾—Apache Hadoopèƒ½å¤Ÿåœ¨åŒ…æ‹¬Window Serverå’ŒWindowns Azureåœ¨å†…çš„Microsoft Windowså¹³å°ä¸Šæœ¬åœ°è¿è¡Œ,å®šä»·ä»¥é›†ç¾¤ä¸ºåŸºç¡€,æ¯10ä¸ªèŠ‚ç‚¹æ¯å¹´ä¸º12500ç¾å…ƒ.

### Hadoop ä¼˜åŠ¿ (4é«˜)
#### 1.é«˜å¯é æ€§
> Hadoopåº•å±‚ç»´æŠ¤å¤šä¸ªæ•°æ®å‰¯æœ¬,æ‰€ä»¥å³ä½¿HadoopæŸä¸ªè®¡ç®—å…ƒç´ æˆ–å­˜å‚¨å‡ºç°æ•…éšœ,ä¹Ÿä¸ä¼šå¯¼è‡´æ•°æ®çš„ä¸¢å¤±.
#### 2.é«˜æ‰©å±•æ€§
> åœ¨é›†ç¾¤é—´åˆ†é…ä»»åŠ¡æ•°æ®,å¯æ–¹ä¾¿çš„æ‰©å±•æ•°ä»¥åƒè®¡çš„èŠ‚ç‚¹.
#### 3.é«˜æ•ˆæ€§
> åœ¨MapReduceçš„æ€æƒ³ä¸‹,Hadoopæ˜¯å¹¶è¡Œå·¥ä½œ,ä»¥åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦.
#### 4.é«˜å®¹é”™æ€§
> èƒ½å¤Ÿè‡ªåŠ¨å°†å¤±è´¥çš„ä»»åŠ¡é‡æ–°åˆ†é….



### Hadoop ç»„æˆ(é¢è¯•é‡ç‚¹)
#### Hadoop1.xä¸Hadoop2.x åŒºåˆ«
> Hadoop1.xç»„æˆ : **`MapReduce(è®¡ç®—+èµ„æºè°ƒåº¦)`** | **`HDFS(æ•°æ®å­˜å‚¨)`** | **`Common(è¾…åŠ©å·¥å…·)`**
> 
> Hadoop2.xç»„æˆ : **`MapReduce(è®¡ç®—)`** | **`Yarn(èµ„æºè°ƒåº¦)`** | **`HDFS(æ•°æ®å­˜å‚¨)`** | **`Common(è¾…åŠ©å·¥å…·)`**
> 
> åœ¨Hadoop1.xæ—¶ä»£,Hadoopä¸­çš„MapReduceåŒæ—¶å¤„ç†ä¸šåŠ¡é€»è¾‘è¿ç®—å’Œèµ„æºçš„è°ƒåº¦,æ‰€ä»¥è€¦åˆæ€§è¾ƒå¤§.
> 
> åœ¨Hadoop2.xæ—¶ä»£,å¢åŠ äº†Yarn,Yarnåªè´Ÿè´£èµ„æºçš„è°ƒåº¦,MapReduceåªè´Ÿè´£è¿ç®—.

#### HDFS æ¶æ„æ¦‚è¿°
HDFS (Hadoop Distributed File System) | **`ä¸‰å¤§ç»„ä»¶ nn / dn / 2nn`**
> 1.NameNode(nn) : å­˜å‚¨æ–‡ä»¶çš„å…ƒæ•°æ®,å¦‚æ–‡ä»¶å,æ–‡ä»¶ç›®å½•ç»“æ„,æ–‡ä»¶å±æ€§(ç”Ÿæˆæ—¶é—´,å‰¯æœ¬æ•°,æ–‡ä»¶æƒé™,),ä»¥åŠæ¯ä¸ªæ–‡ä»¶çš„å—åˆ—è¡¨å’Œå—æ‰€åœ¨çš„DataNodeç­‰.
> 
> 2.DataNode(dn) : åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨æ–‡ä»¶å—æ•°æ®,ä»¥åŠå—æ•°æ®çš„æ ¡éªŒå’Œ.
> 
> 3.Secondary NameNode(2nn):ç”¨æ¥ç›‘æ§HDFSçŠ¶æ€çš„è¾…åŠ©åå°ç¨‹åº,æ¯éš”ä¸€æ®µæ—¶é—´è·å–HDFSå…ƒæ•°æ®çš„å¿«ç…§.

#### YARN æ¶æ„æ¦‚è¿°
**`å››å¤§ç»„ä»¶ | RM / NM / AM / Container`**
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_001.jpg)

#### MapReduce æ¶æ„æ¦‚è¿°
> **`ä¸¤å¤§é˜¶æ®µ | Map / Reduce`**
> 
> MapReduceå°†è®¡ç®—è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ:Map å’Œ Reduce
> 
> Mapé˜¶æ®µå¹¶è¡Œå¤„ç†è¾“å…¥æ•°æ® | Reduceé˜¶æ®µå¯¹Mapç»“æœè¿›è¡Œæ±‡æ€»
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_002.jpg)

### å¤§æ•°æ®æŠ€æœ¯ ç”Ÿæ€ä½“ç³»
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_003.jpg)

### æ¨è ç³»ç»Ÿæ¡†æ¶å›¾
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_004.jpg)


## 4. Hadoop è¿è¡Œç¯å¢ƒæ­å»º(å¼€å‘é‡ç‚¹)
### è™šæ‹Ÿæœºç¯å¢ƒ å‡†å¤‡
#### 1.å…‹éš†è™šæ‹Ÿæœº
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_005.jpg)
#### 2.ä¿®æ”¹å…‹éš†è™šæ‹Ÿæœºçš„é™æ€IP
> ä½¿ç”¨rootç”¨æˆ·ç™»å½•
**`vim /etc/udev/rules.d/70-persistent-net.rules`**

æºä»£ç 
``` bash
# This file was automatically generated by the /lib/udev/write_net_rules
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.

# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:a3:d8:a7", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"

# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:67:b3:77", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"
```

å°†NAME="eth1"æ›´æ”¹ä¸ºNAME="eth0",å¹¶å¤åˆ¶00:0c:29:67:b3:77åœ°å€
``` bash
# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:67:b3:77", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

ä¿®æ”¹ç½‘ç»œé…ç½®
ç²˜è´´ä¸Šä¸€æ­¥åœ°å€,ä¿®æ”¹HWADDRå±æ€§
**`vim /etc/sysconfig/network-scripts/ifcfg-eth0`**
```bash
DEVICE=eth0
HWADDR=00:0c:29:67:b3:77
TYPE=Ethernet
UUID=b75136b3-4a81-41b5-9ebd-bfc1831d0df7
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static

IPADDR=192.168.177.131
GATEWAY=192.168.177.2
DNS1=192.168.177.2
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

#### 3.ä¿®æ”¹ä¸»æœºå
`vim /etc/sysconfig/network`
``` bash
NETWORKING=yes
HOSTNAME=corehub-004
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

#### 4.å…³é—­é˜²ç«å¢™
æš‚æ—¶æ€§å…³é—­é˜²ç«å¢™:`service iptables stop`

#### 5.åˆ›å»ºç”¨æˆ·
`useradd username`

#### 6.é…ç½®ç”¨æˆ·å…·æœ‰rootæƒé™
`vim /etc/sudoers`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_006.jpg)
æ›´æ”¹å®Œæ¯•,`:wq!`ä¿å­˜é€€å‡º

#### 7.åœ¨/optç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹
> 1.åˆ›å»ºsoftware,moduleæ–‡ä»¶å¤¹
> software ç”¨äºæ—¥åå­˜å‚¨çš„ç¨‹åºå®‰è£…åŒ…
> module ç”¨äºæ—¥åå­˜å‚¨è§£æåçš„ç¨‹åºjaråŒ…
```
sudo mkdir software
sudo mkdir module
```
``` powershell
[geek-developer@corehub-001 ~]$ cd /opt/
[geek-developer@corehub-001 opt]$ ll
total 8
drwxr-xr-x. 6 root root 4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 root root 4096 Oct  4  2017 rh
[geek-developer@corehub-001 opt]$ sudo mkdir software
[sudo] password for geek-developer: 
[geek-developer@corehub-001 opt]$ sudo mkdir module
[geek-developer@corehub-001 opt]$ ll
total 16
drwxr-xr-x. 6 root root 4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 root root 4096 Jan 24 20:12 module
drwxr-xr-x. 2 root root 4096 Oct  4  2017 rh
drwxr-xr-x. 2 root root 4096 Jan 24 20:11 software
[geek-developer@corehub-001 opt]$ 
```
2.ä¿®æ”¹software,moduleæ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…
``` powershell
[geek-developer@corehub-001 opt]# chown geek-developer:geek-developer software/ module/
[geek-developer@corehub-001 opt]# ll
total 16
drwxr-xr-x. 6 root           root           4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 geek-developer geek-developer 4096 Jan 24 20:12 module
drwxr-xr-x. 2 root           root           4096 Oct  4  2017 rh
drwxr-xr-x. 2 geek-developer geek-developer 4096 Jan 24 20:11 software
[geek-developer@corehub-001 opt]# 
```

### å®‰è£… Hadoop
> åœ¨å®‰è£…Hadoopå‰ææ˜¯éœ€è¦å…ˆå®‰è£…JAVAå¹¶é…ç½®ç¯å¢ƒå˜é‡å³å¯
> 
> Apache Hadoopå®˜æ–¹åœ°å€ : https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/
> 
> é€šè¿‡è¿œç¨‹å·¥å…·,å°†hadoop-2.7.2.tar.gzä¼ è¾“åˆ°/opt/software/ç›®å½•ä¸‹
> 

å°†hadoop-2.7.2.tar.gzè§£å‹/opt/module/ç›®å½•ä¸‹
```
tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module
```
å°†è§£å‹å®Œæˆhadoop-2.7.2é‡å‘½åä¸ºhadoop
``` powershell
[root@corehub-001 software]# cd ..
[root@corehub-001 opt]# cd module/
[root@corehub-001 module]# ll
total 4
drwxr-xr-x. 9 10011 10011 4096 Jan 26  2016 hadoop-2.7.2
[root@corehub-001 module]# mv hadoop-2.7.2 hadoop
[root@corehub-001 module]# ll
total 4
drwxr-xr-x. 9 10011 10011 4096 Jan 26  2016 hadoop
[root@corehub-001 module]# 
```
é…ç½®hadoopç¯å¢ƒå˜é‡
```
[root@corehub-001 module]# cd hadoop/
[root@corehub-001 hadoop]# pwd
/opt/module/hadoop
[root@corehub-001 hadoop]#
```
```
[root@corehub-001 geek-developer]# vim /etc/profile
```
```
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```
å®Œæˆç¯å¢ƒå˜é‡,`:wq`ä¿å­˜é€€å‡º
`source /etc/profile` æ›´æ–°é…ç½®æ–‡ä»¶æŒ‡ä»¤
```
[root@corehub-001 geek-developer]# source /etc/profile
[root@corehub-001 geek-developer]# hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
[root@corehub-001 geek-developer]# 
```


### Hadoop ç›®å½•ç»“æ„
> **`binç›®å½•`** :  HadoopæœåŠ¡è„šæœ¬.
> 
> **`etcç›®å½•`** :  Hadoopçš„é…ç½®æ–‡ä»¶ç›®å½•,å­˜æ”¾Haoopé…ç½®æ–‡ä»¶.
> 
> **`libç›®å½•`** : å­˜æ”¾Hadoopæœ¬åœ°åº“,(å¯¹æ•°æ®è¿›è¡Œå‹ç¼©è§£å‹åŠŸèƒ½).
> 
> **`sbinç›®å½•`** : å­˜æ”¾å¯åŠ¨æˆ–åœæ­¢Hadoopç›¸å…³æœåŠ¡è„šæœ¬.
> 
> **`shareç›®å½•`** : å­˜æ”¾Hadoopä¾èµ–jaråŒ…,æ–‡æ¡£,å®˜æ–¹æ¡ˆä¾‹.


## 5. Hadoop è¿è¡Œæ¨¡å¼
> Hadoopè¿è¡Œæ¨¡å¼åŒ…æ‹¬ : **`æœ¬åœ°è¿è¡Œ`** / **`ä¼ªåˆ†å¸ƒå¼è¿è¡Œ`** / **`å®Œå…¨åˆ†å¸ƒå¼è¿è¡Œ`**
### ğŸ‰ğŸ‰ æœ¬åœ°è¿è¡Œæ¨¡å¼ ğŸ‰ğŸ‰
#### ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’» Grep å®˜æ–¹æ¡ˆä¾‹ ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’»
> å®˜æ–¹æ¡ˆä¾‹åœ°å€ : [Standalone Operation](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SingleCluster.html)
> 
> By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.
> 
> The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.
> ```
> $ mkdir input
> $ cp etc/hadoop/*.xml input
> $ bin/hadoop jar s
> ```

##### 1.å¿«é€Ÿå¼€å§‹,åœ¨hadoopç›®å½•ä¸‹ åˆ›å»ºinputæ–‡ä»¶å¤¹
``` powershell
[geek-developer@corehub-001 ~]$ cd /opt/module/hadoop/
[geek-developer@corehub-001 hadoop]$ ll
total 52
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[geek-developer@corehub-001 hadoop]$ sudo mkdir input
[geek-developer@corehub-001 hadoop]$ ll
total 56
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:23 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[geek-developer@corehub-001 hadoop]$ 
```
##### 2.åœ¨hadoopç›®å½•ä¸­,å°†etcæ–‡ä»¶å¤¹å†… ä»¥.xmlä¸ºåç¼€çš„é…ç½®æ–‡ä»¶æ‹·è´åˆ°inputæ–‡ä»¶å¤¹é‡Œ
```
[geek-developer@corehub-001 hadoop]$ sudo cp etc/hadoop/*.xml input/
[geek-developer@corehub-001 hadoop]$ ls input/
capacity-scheduler.xml  hadoop-policy.xml  httpfs-site.xml  kms-site.xml
core-site.xml           hdfs-site.xml      kms-acls.xml     yarn-site.xml
[geek-developer@corehub-001 hadoop]$ 
```

##### 3.æ‰§è¡Œshareç›®å½•ä¸‹çš„hadoop-mapreduce-examples-2.7.2.jaråŒ…,å¹¶æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºè·¯å¾„,ä»¥ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼å¹¶ç»Ÿè®¡ä¸ªæ•°
`dfs[a-z.]+` ä»¥dfså¼€å¤´,ä»¥aåˆ°zä»»æ„å­—ç¬¦ä»¥.è¿‡æ»¤æ‰ - - å­—ç¬¦
``` powershell
[root@corehub-001 geek-developer]# cd /opt/module/hadoop/
##########æ‰§è¡Œshareç›®å½•ä¸‹çš„hadoop-mapreduce-examples-2.7.2.jaråŒ…,å¹¶æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºè·¯å¾„#############
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input/ output 'dfs[a-z.]+'
19/01/24 22:43:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/24 22:43:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 22:43:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/24 22:43:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1034400674_0001_r_000000_0' to file:/opt/module/hadoop/grep-temp-1632689888/_temporary/0/task_local1034400674_0001_r_000000
19/01/24 22:43:50 INFO mapred.LocalJobRunner: reduce > reduce
19/01/24 22:43:50 INFO mapred.Task: Task 'attempt_local1034400674_0001_r_000000_0' done.
19/01/24 22:43:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local1034400674_0001_r_000000_0
19/01/24 22:43:50 INFO mapred.LocalJobRunner: reduce task executor complete.
19/01/24 22:43:50 INFO mapreduce.Job: Job job_local1034400674_0001 running in uber mode : false
19/01/24 22:43:50 INFO mapreduce.Job:  map 100% reduce 100%
19/01/24 22:43:50 INFO mapreduce.Job: Job job_local1034400674_0001 completed successfully
19/01/24 22:43:50 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=2691317
		FILE: Number of bytes written=5002436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=745
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=67
		Input split bytes=877
		Combine input records=1
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=67
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =8
		Failed Shuffles=0
		Merged Map outputs=8
		GC time elapsed (ms)=147
		Total committed heap usage (bytes)=2574778368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=26007
	File Output Format Counters 
		Bytes Written=123
[root@corehub-001 hadoop]# ll
total 60
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[root@corehub-001 hadoop]# ll output/
total 4
-rw-r--r--. 1 root root 11 Jan 24 22:43 part-r-00000
-rw-r--r--. 1 root root  0 Jan 24 22:43 _SUCCESS
############cd è¿›å…¥outputç›®å½•ä¸‹############
[root@corehub-001 hadoop]# cd output/
############æœ€åæŸ¥çœ‹ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼å¹¶ç»Ÿè®¡ä¸ªæ•°############
[root@corehub-001 output]# cat part-r-00000 
1	dfsadmin
[root@corehub-001 output]#
```

#### ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’» WordCount å®˜æ–¹æ¡ˆä¾‹ ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’»
##### 1.åœ¨hadoopç›®å½•ä¸‹,åˆ›å»ºä¸€ä¸ªwcinputæ–‡ä»¶å¤¹
``` powershell
[root@corehub-001 hadoop]# mkdir wcinput
[root@corehub-001 hadoop]# ll
total 64
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:07 wcinput
[root@corehub-001 hadoop]# 
```
##### 2.åœ¨wcinputæ–‡ä»¶ä¸‹åˆ›å»ºä¸€ä¸ªwc.inputæ–‡ä»¶
```
[root@corehub-001 hadoop]# cd wcinput/
[root@corehub-001 wcinput]# touch wc.input
[root@corehub-001 wcinput]# ll
total 0
-rw-r--r--. 1 root root 0 Jan 24 23:08 wc.input
[root@corehub-001 wcinput]# 
```
##### 3.ç¼–è¾‘wc.inputæ–‡ä»¶å¹¶åœ¨æ–‡ä»¶ä¸­è¾“å…¥å†…å®¹,è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º
```
[root@corehub-001 wcinput]# vim wc.input
```
```
hello-world
hello-world
java
python
php
golang
hadoop yarn
hadoop mapreduce
hive
spark
java
springcloud
springboot
geek
geekpark
geekparkhub
geekparkhub
geek-developer
jeep-711
jeep-711
github
~                                           
~   
```
##### 4.è¿”å›hadoopç›®å½•
```
[root@corehub-001 wcinput]# cd ..
[root@corehub-001 hadoop]# 
```
##### 5.æ‰§è¡Œç¨‹åº
``` powershell
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput
19/01/24 23:20:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/24 23:20:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 23:20:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/24 23:20:50 INFO input.FileInputFormat: Total input paths to process : 1
19/01/24 23:20:50 INFO mapreduce.JobSubmitter: number of splits:1
19/01/24 23:20:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local450202257_0001
19/01/24 23:20:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
19/01/24 23:20:51 INFO mapreduce.Job: Running job: job_local450202257_0001
19/01/24 23:20:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local450202257_0001_r_000000_0' to file:/opt/module/hadoop/wcoutput/_temporary/0/task_local450202257_0001_r_000000
19/01/24 23:20:51 INFO mapred.LocalJobRunner: reduce > reduce
19/01/24 23:20:51 INFO mapred.Task: Task 'attempt_local450202257_0001_r_000000_0' done.
19/01/24 23:20:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local450202257_0001_r_000000_0
19/01/24 23:20:51 INFO mapred.LocalJobRunner: reduce task executor complete.
19/01/24 23:20:52 INFO mapreduce.Job: Job job_local450202257_0001 running in uber mode : false
19/01/24 23:20:52 INFO mapreduce.Job:  map 100% reduce 100%
19/01/24 23:20:52 INFO mapreduce.Job: Job job_local450202257_0001 completed successfully
19/01/24 23:20:52 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=547482
		FILE: Number of bytes written=1105096
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=6
		Map output bytes=77
		Map output materialized bytes=77
		Input split bytes=105
		Combine input records=6
		Combine output records=5
		Reduce input groups=5
		Reduce shuffle bytes=77
		Reduce input records=5
		Reduce output records=5
		Spilled Records=10
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=397410304
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=53
	File Output Format Counters 
		Bytes Written=63
[root@corehub-001 hadoop]# 
```
##### 6.æŸ¥çœ‹ç»Ÿè®¡å•è¯çš„ä¸ªæ•°ç»“æœ | ğŸ˜ğŸ˜ æ·±æ·±æ„Ÿå—åˆ°å¤§æ•°æ®çš„é­…åŠ›æ‰€åœ¨ ğŸ˜ğŸ˜
``` powershell
[root@corehub-001 hadoop]# ll
total 68
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:33 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 24 23:34 wcoutput
[root@corehub-001 hadoop]# ll wcoutput/
total 4
-rw-r--r--. 1 root root 184 Jan 24 23:34 part-r-00000
-rw-r--r--. 1 root root   0 Jan 24 23:34 _SUCCESS
[root@corehub-001 hadoop]# cd wcoutput/
[root@corehub-001 wcoutput]# cat part-r-00000 
geek	1
geek-developer	1
geekpark	1
geekparkhub	2
github	1
golang	1
hadoop	2
hello-world	2
hive	1
java	2
jeep-711	2
mapreduce	1
php	1
python	1
spark	1
springboot	1
springcloud	1
yarn	1
[root@corehub-001 wcoutput]# 
```

###  ğŸ‰ğŸ‰ ä¼ªåˆ†å¸ƒå¼ è¿è¡Œæ¨¡å¼ ğŸ‰ğŸ‰
#### å¯åŠ¨HDFSå¹¶è¿è¡ŒMapReduceç¨‹åº
> åˆ†æ : é…ç½®é›†ç¾¤,å¯åŠ¨æµ‹è¯•é›†ç¾¤å¢åˆ æŸ¥,æ‰§è¡ŒWordCountæ¡ˆä¾‹
##### é…ç½®é›†ç¾¤
###### é…ç½® hadoop-env.sh,è·å–jdkå®‰è£…è·¯å¾„
è·å–å¹¶å¤åˆ¶JAVA_HOMEè·¯å¾„
```
[root@corehub-001 hadoop]# echo $JAVA_HOME
/opt/jdk1.8.0_162
```
é…ç½®hadoop-env.sh
```
[root@corehub-001 hadoop]# vim etc/hadoop/hadoop-env.sh
```
``` powershell
# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=/opt/jdk1.8.0_162
```

###### é…ç½® **`core-site.xml`**
> core-site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [core-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml)

```
[root@corehub-001 hadoop]# vim etc/hadoop/core-site.xml
```
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šHDFSä¸­çš„NameNodeåœ°å€ -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://corehub-001:9000</value>
  </property>
<!-- æŒ‡å®šHadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜å‚¨ç›®å½• -->
   <property>
     <name>hadoop.tmp.dir</name>
     <value>/opt/module/hadoop/data/tmp</value>
   </property>
</configuration>
```
è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º

###### é…ç½® **`hdfs.site.xml`**
> hdfs.site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [hdfs-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)
```
[root@corehub-001 hadoop]# vim etc/hadoop/hdfs-site.xml
```
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šHDFSå‰¯æœ¬æ•°é‡ -->
  <property>
   <name>dfs.replication</name>
     <value>1</value>
  </property>
</configuration>
```
è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º

##### å¯åŠ¨é›†ç¾¤
###### æ ¼å¼åŒ– NameNode
> (ç¬¬ä¸€æ¬¡åˆå§‹åŒ–å¯åŠ¨éœ€è¦æ ¼å¼åŒ–,åªéœ€åœ¨é¦–æ¬¡å¯åŠ¨å‰æ ¼å¼åŒ–)
```
[root@corehub-001 hadoop]# bin/hdfs namenode -format
19/01/25 12:59:38 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = corehub-001/192.168.152.130
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = /opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/module/hadoop/share/hadoop/common/lib/jsch-
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41; compiled by 'jenkins' on 2016-01-26T00:08Z
STARTUP_MSG:   java = 1.8.0_162
************************************************************/
19/01/25 12:59:38 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
19/01/25 12:59:38 INFO namenode.NameNode: createNameNode [-format]
19/01/25 12:59:38 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
19/01/25 12:59:38 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
19/01/25 12:59:38 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
19/01/25 12:59:38 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Jan 25 12:59:38
19/01/25 12:59:39 INFO util.GSet: Computing capacity for map NameNodeRetryCache
19/01/25 12:59:39 INFO util.GSet: VM type       = 64-bit
19/01/25 12:59:39 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
19/01/25 12:59:39 INFO util.GSet: capacity      = 2^15 = 32768 entries
19/01/25 12:59:39 INFO namenode.FSImage: Allocated new BlockPoolId: BP-169105537-192.168.152.130-1548449979185
19/01/25 12:59:39 INFO common.Storage: Storage directory /opt/module/hadoop/data/tmp/dfs/name has been successfully formatted.
19/01/25 12:59:39 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
19/01/25 12:59:39 INFO util.ExitUtil: Exiting with status 0
19/01/25 12:59:39 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at corehub-001/192.168.152.130
************************************************************/
[root@corehub-001 hadoop]# ll
total 72
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 bin
drwxr-xr-x. 3 root  root   4096 Jan 25 12:59 data
drwxr-xr-x. 3 10011 10011  4096 Jan 25  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 25 09:44 input
drwxr-xr-x. 3 10011 10011  4096 Jan 25  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 25  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 25  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 25 09:45 output
-rw-r--r--. 1 10011 10011  1366 Jan 25  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 25  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 25 09:47 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 25 09:48 wcoutput
[root@corehub-001 hadoop]# 
```
###### å¯åŠ¨ NameNode
```
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-corehub-001.out
[root@corehub-001 hadoop]# jps
3153 Jps
3022 NameNode
```

###### å¯åŠ¨ DataNode
```
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-corehub-001.out
[root@corehub-001 hadoop]# jps
3696 DataNode
3858 Jps
3022 NameNode
[root@corehub-001 hadoop]# 
```

##### æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ
> å¯é€šè¿‡hadoopæä¾›websiteå›¾å½¢åŒ–ç•Œé¢ æŸ¥çœ‹å¯åŠ¨ç»“æœ
> 
> é€šè¿‡ LinuxHostName:50070ç«¯å£å·å½¢å¼è®¿é—® æˆ– é€šè¿‡ Linux IPaddr:50070ç«¯å£å·è®¿é—®
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_007.jpg)
##### ä»¥hdfsæŒ‡ä»¤ åˆ›å»ºå¤šçº§ç›®å½•
> ç±»ä¼¼äºLinuxç›®å½•æ ‘ç»“æ„ä¸€è‡´,å¯è§ç†Ÿç»ƒæŒæ¡LInuxå‘½ä»¤å°¤ä¸ºé‡è¦
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_008.jpg)

```
[root@corehub-001 hadoop]# bin/hdfs dfs -mkdir -p /user/geekparkhub/input
19/01/25 14:41:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```
```
[root@corehub-001 hadoop]# bin/hdfs dfs -ls -R /
19/01/25 14:44:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user/geekparkhub
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user/geekparkhub/input
[root@corehub-001 hadoop]# 
```
##### åœ¨hadoopç›®å½•ä¸‹,å°†wcinputç›®å½•åŠå…ƒæ•°æ®æ–‡ä»¶ä¸Šä¼ åˆ°/user/geekparkhub/inputç›®å½•ä¸­
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_009.jpg)
```
[root@corehub-001 hadoop]# bin/hdfs dfs -put wcinput/wc.input /user/geekparkhub/input
19/01/25 15:02:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@corehub-001 hadoop]#
```

##### æ‰§è¡Œå¹¶è¯»å–HDFSç¨‹åº
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_010.jpg)
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
19/01/25 15:16:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/25 15:16:05 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/25 15:16:05 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/25 15:16:05 INFO input.FileInputFormat: Total input paths to process : 1
19/01/25 15:16:08 INFO mapreduce.Job:  map 100% reduce 100%
19/01/25 15:16:08 INFO mapreduce.Job: Job job_local169102714_0001 completed successfully
19/01/25 15:16:08 INFO mapreduce.Job: Counters: 35
        File System Counters
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=471
        File Output Format Counters 
                Bytes Written=503
[root@corehub-001 hadoop]#
```

#### å¯åŠ¨YARNä¸Šè¿è¡ŒMapReduceç¨‹åº
> åˆ†æ : é…ç½®é›†ç¾¤åœ¨Yarnä¸Šè¿è¡ŒMR,å¯åŠ¨æµ‹è¯•é›†ç¾¤å¢åˆ æŸ¥,åœ¨Yarnä¸Šæ‰§è¡ŒWordcountæ¡ˆä¾‹
##### é…ç½®é›†ç¾¤
###### é…ç½®yarn-env.sh
`vim etc/hadoop/yarn-env.sh`
``` powershell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# User for YARN daemons
export HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}

# resolve links - $0 may be a softlink
export YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"

# some Java parameters
export JAVA_HOME=/opt/jdk1.8.0_162
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi

if [ "$JAVA_HOME" = "" ]; then
  echo "Error: JAVA_HOME is not set."
  exit 1
fi

JAVA=$JAVA_HOME/bin/java
JAVA_HEAP_MAX=-Xmx1000m
```
###### é…ç½®yarn-site.xml
> yarn-site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [yarn-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)
```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-site.xml
```
``` xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
<!-- Site specific YARN configuration properties -->
<!-- Reducerè·å–æ•°æ®æ–¹å¼ -->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
<!-- æŒ‡å®šYarnçš„ResourceManageråœ°å€-->
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>corehub-001</value>
    </property>
</configuration>
```

###### é…ç½®mapred-env.sh
`vim etc/hadoop/mapred-env.sh`
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-env.sh
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

export JAVA_HOME=/opt/jdk1.8.0_162

export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000

export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA

#export HADOOP_JOB_HISTORYSERVER_OPTS=
#export HADOOP_MAPRED_LOG_DIR="" # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.
#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.
#export HADOOP_MAPRED_PID_DIR= # The pid files are stored. /tmp by default.
#export HADOOP_MAPRED_IDENT_STRING= #A string representing this instance of hadoop. $USER by default
#export HADOOP_MAPRED_NICENESS= #The scheduling priority for daemons. Defaults to 0.
```

###### é…ç½®mapred-site.xml
> mapred-site.xml.template é‡å‘½åä¸ºmapred-site.xml
```
[root@corehub-001 hadoop]# mv etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
```
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-site.xml
```
``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
<!-- æŒ‡å®šMRè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
</configuration>
```
##### å¯åŠ¨é›†ç¾¤
> å¯åŠ¨å‰å¿…é¡»ä¿è¯NameNodeå’ŒDataNodeå·²ç»å¯åŠ¨
###### å¯åŠ¨ResourceManager
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
39653 ResourceManager
9353 DataNode
9066 NameNode
40171 Jps
[root@corehub-001 hadoop]# 
```
###### å¯åŠ¨NodeManager
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
40880 Jps
40769 NodeManager
39653 ResourceManager
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
###### æ‰§è¡Œwordcountç¨‹åºå¹¶æŸ¥çœ‹è¿è¡Œç»“æœ
> å¯é€šè¿‡hadoopæä¾›websiteå›¾å½¢åŒ–ç•Œé¢ æŸ¥çœ‹å¯åŠ¨ç»“æœ
> 
> é€šè¿‡ LinuxHostName:8088ç«¯å£å·å½¢å¼è®¿é—® æˆ– é€šè¿‡ Linux IPaddr:8088ç«¯å£å·è®¿é—®
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_011.jpg)
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
^H19/01/27 19:37:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 19:37:37 INFO client.RMProxy: Connecting to ResourceManager at corehub-001/192.168.177.130:8032
19/01/27 19:37:38 INFO input.FileInputFormat: Total input paths to process : 1
19/01/27 19:37:38 INFO mapreduce.JobSubmitter: number of splits:1
19/01/27 19:37:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1548588180141_0001
19/01/27 19:37:39 INFO impl.YarnClientImpl: Submitted application application_1548588180141_0001
19/01/27 19:37:39 INFO mapreduce.Job: The url to track the job: http://corehub-001:8088/proxy/application_1548588180141_0001/
19/01/27 19:37:39 INFO mapreduce.Job: Running job: job_1548588180141_0001
19/01/27 19:37:53 INFO mapreduce.Job: Job job_1548588180141_0001 running in uber mode : false
19/01/27 19:37:53 INFO mapreduce.Job:  map 0% reduce 0%
19/01/27 19:38:01 INFO mapreduce.Job:  map 100% reduce 0%
19/01/27 19:38:08 INFO mapreduce.Job:  map 100% reduce 100%
19/01/27 19:38:08 INFO mapreduce.Job: Job job_1548588180141_0001 completed successfully
19/01/27 19:38:08 INFO mapreduce.Job: Counters: 49
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=5298
                Total time spent by all reduces in occupied slots (ms)=4839
                Total time spent by all map tasks (ms)=5298
                Total time spent by all reduce tasks (ms)=4839
                Total vcore-milliseconds taken by all map tasks=5298
                Total vcore-milliseconds taken by all reduce tasks=4839
                Total megabyte-milliseconds taken by all map tasks=5425152
                Total megabyte-milliseconds taken by all reduce tasks=4955136
        Map-Reduce Framework
                Map input records=24
                Map output records=23
                Map output bytes=285
                Map output materialized bytes=262
                Input split bytes=120
                Combine input records=23
                Combine output records=18
                Reduce input groups=18
                Reduce shuffle bytes=262
                Reduce input records=18
                Reduce output records=18
                Spilled Records=36
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=164
                CPU time spent (ms)=1830
                Physical memory (bytes) snapshot=416026624
                Virtual memory (bytes) snapshot=4163923968
                Total committed heap usage (bytes)=275775488
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=196
        File Output Format Counters 
                Bytes Written=184
```


#### é…ç½®å†å²æœåŠ¡å™¨
> ä¸ºäº†æŸ¥çœ‹ç¨‹åºå†å²è¿è¡Œæƒ…å†µ,éœ€è¦é…ç½®ä¸€ä¸‹å†å²æœåŠ¡å™¨
##### é…ç½®mapred-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-site.xml
```
``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šMRè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
<!-- å†å²æœåŠ¡å™¨åœ°å€ -->
    <property>
       <name>mapreduce.jobhistory.address</name>
       <value>corehub-001:10020</value>
    </property>
<!-- å†å²æœåŠ¡å™¨WEBåœ°å€ -->
    <property>
       <name>mapreduce.jobhistory.webapp.address</name>
       <value>corehub-001:19888</value>
    </property>
</configuration>
```
##### å¯åŠ¨å†å²æœåŠ¡å™¨å¹¶æŸ¥çœ‹æŸ¥çœ‹å†å²æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨ä»¥åŠJobHistoryè¿è¡ŒçŠ¶æ€
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop/logs/mapred-root-historyserver-corehub-001.out
[root@corehub-001 hadoop]# jps
40769 NodeManager
66818 JobHistoryServer
39653 ResourceManager
66948 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_012.jpg)



#### é…ç½®æ—¥å¿—èšé›†
> æ—¥å¿—èšé›†æ¦‚å¿µ:åº”ç”¨è¿è¡Œå®Œæˆä»¥å,å°†ç¨‹åºè¿è¡Œæ—¥å¿—ä¿¡æ¯ä¸Šä¼ åˆ°HDFSç³»ç»Ÿä¸Š.
> æ—¥å¿—èšé›†åŠŸèƒ½ä¼˜åŠ¿:å¯ä»¥æ–¹ä¾¿æŸ¥çœ‹ç¨‹åºè¿è¡Œè¯¦æƒ…,æ–¹ä¾¿å¼€å‘è°ƒè¯•.
> æ³¨æ„:å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½,éœ€è¦**`é‡æ–°å¯åŠ¨NodeManager,ResourceManager,HistoryManageræ­¤ä¸‰é¡¹æœåŠ¡`**.

##### åˆ†åˆ«åœæ­¢HistoryManageræœåŠ¡,NodeManageræœåŠ¡,ResourceManageræœåŠ¡

###### åœæ­¢HistoryManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[root@corehub-001 hadoop]# jps
40769 NodeManager
39653 ResourceManager
94488 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
###### åœæ­¢NodeManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh stop nodemanager
stopping nodemanager
[root@corehub-001 hadoop]# jps
39653 ResourceManager
9353 DataNode
9066 NameNode
96078 Jps
[root@corehub-001 hadoop]# 
```
###### åœæ­¢ResourceManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh stop resourcemanager
stopping resourcemanager
[root@corehub-001 hadoop]# jps
98388 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
##### é…ç½®yarn-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-site.xml
```
```
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
<!-- Site specific YARN configuration properties -->
<!-- Reducerè·å–æ•°æ®æ–¹å¼ -->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
<!-- æŒ‡å®šYarnçš„ResourceManageråœ°å€-->
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>corehub-001</value>
    </property>
<!-- æ—¥å¿—èšé›†åŠŸèƒ½ä½¿ç”¨ -->
    <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
    </property>
<!-- æ—¥å¿—ä¿ç•™æ—¶é—´è®¾ç½®ä¸º7å¤© æ ¹æ®å¼€å‘æƒ…å†µ,æ—¶é—´å¯è‡ªå®šä¹‰-->
<!-- ä¸€å¤©=3600ç§’ 3600*24*7=604800 -->
    <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>604800</value>
    </property>
</configuration>
```
##### åˆ†åˆ«å¼€å¯HistoryManageræœåŠ¡,NodeManageræœåŠ¡,ResourceManageræœåŠ¡
###### å¼€å¯ResourceManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
113380 ResourceManager
113463 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]#
```
###### å¼€å¯NodeManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
114081 NodeManager
113380 ResourceManager
9353 DataNode
9066 NameNode
114159 Jps
[root@corehub-001 hadoop]# 
```
###### å¼€å¯HistoryManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop/logs/mapred-root-historyserver-corehub-001.out
[root@corehub-001 hadoop]# jps
114081 NodeManager
115184 JobHistoryServer
113380 ResourceManager
9353 DataNode
9066 NameNode
115263 Jps
[root@corehub-001 hadoop]# 
```
##### åˆ é™¤HDFSä¸Šå·²ç»å­˜åœ¨çš„è¾“å‡ºç›®å½•
```
[root@corehub-001 hadoop]# bin/hdfs dfs -rm -r /user/geekparkhub/output
19/01/27 22:26:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 22:26:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/geekparkhub/output
[root@corehub-001 hadoop]# 
```
##### æ‰§è¡ŒWordCountç¨‹åº
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
19/01/27 22:32:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 22:32:30 INFO client.RMProxy: Connecting to ResourceManager at corehub-001/192.168.177.130:8032
19/01/27 22:32:33 INFO input.FileInputFormat: Total input paths to process : 1
19/01/27 22:32:33 INFO mapreduce.JobSubmitter: number of splits:1
19/01/27 22:32:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1548598949012_0001
19/01/27 22:32:35 INFO impl.YarnClientImpl: Submitted application application_1548598949012_0001
19/01/27 22:32:35 INFO mapreduce.Job: The url to track the job: http://corehub-001:8088/proxy/application_1548598949012_0001/
19/01/27 22:32:35 INFO mapreduce.Job: Running job: job_1548598949012_0001
19/01/27 22:33:14 INFO mapreduce.Job: Job job_1548598949012_0001 running in uber mode : false
19/01/27 22:33:14 INFO mapreduce.Job:  map 0% reduce 0%
19/01/27 22:33:23 INFO mapreduce.Job:  map 100% reduce 0%
19/01/27 22:33:32 INFO mapreduce.Job:  map 100% reduce 100%
19/01/27 22:33:33 INFO mapreduce.Job: Job job_1548598949012_0001 completed successfully
19/01/27 22:33:34 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=262
                FILE: Number of bytes written=235459
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=316
                HDFS: Number of bytes written=184
                HDFS: Number of read operations=6
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters 
                Map-Reduce Framework
                Map input records=24
                Map output records=23
                Map output bytes=285
                Map output materialized bytes=262
                Input split bytes=120
                Combine input records=23
                Combine output records=18
                Reduce input groups=18
                Reduce shuffle bytes=262
                Reduce input records=18
                Reduce output records=18
                Spilled Records=36
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=220
                CPU time spent (ms)=2130
                Physical memory (bytes) snapshot=399134720
                Virtual memory (bytes) snapshot=4166119424
                Total committed heap usage (bytes)=276824064
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=196
        File Output Format Counters 
                Bytes Written=184
```
##### æŸ¥çœ‹æ—¥å¿—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_013.jpg)

æŸ¥çœ‹æ—¥å¿—æ–¹å¼ ä¹Ÿå¯ä»¥é€šè¿‡è¿›å…¥logæ–‡ä»¶å¤¹è¿›è¡ŒæŸ¥çœ‹
```
[root@corehub-001 hadoop]# ll
total 76
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 root  root   4096 Jan 27 18:47 data
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
drwxr-xr-x. 3 root  root   4096 Jan 27 22:23 logs
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:48 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 24 23:34 wcoutput
[root@corehub-001 hadoop]# ll logs/
total 472
-rw-r--r--. 1 root root  51669 Jan 27 22:36 hadoop-root-datanode-corehub-001.log
-rw-r--r--. 1 root root    715 Jan 27 18:48 hadoop-root-datanode-corehub-001.out
-rw-r--r--. 1 root root  59522 Jan 27 22:36 hadoop-root-namenode-corehub-001.log
-rw-r--r--. 1 root root   4960 Jan 27 18:55 hadoop-root-namenode-corehub-001.out
-rw-r--r--. 1 root root  53574 Jan 27 22:42 mapred-root-historyserver-corehub-001.log
-rw-r--r--. 1 root root   1484 Jan 27 22:24 mapred-root-historyserver-corehub-001.out
-rw-r--r--. 1 root root   1477 Jan 27 20:53 mapred-root-historyserver-corehub-001.out.1
-rw-r--r--. 1 root root      0 Jan 27 18:47 SecurityAuth-root.audit
drwxr-xr-x. 3 root root   4096 Jan 27 22:42 userlogs
-rw-r--r--. 1 root root 126215 Jan 27 22:33 yarn-root-nodemanager-corehub-001.log
-rw-r--r--. 1 root root   1515 Jan 27 22:23 yarn-root-nodemanager-corehub-001.out
-rw-r--r--. 1 root root   1508 Jan 27 19:24 yarn-root-nodemanager-corehub-001.out.1
-rw-r--r--. 1 root root 125846 Jan 27 22:38 yarn-root-resourcemanager-corehub-001.log
-rw-r--r--. 1 root root   1531 Jan 27 22:22 yarn-root-resourcemanager-corehub-001.out
-rw-r--r--. 1 root root   1524 Jan 27 19:23 yarn-root-resourcemanager-corehub-001.out.1
[root@corehub-001 hadoop]# 
```

#### é…ç½®æ–‡ä»¶è¯´æ˜
> Hadoop é…ç½®æ–‡ä»¶åˆ†ä¸ºä¸¤ç±»:é»˜è®¤é…ç½®æ–‡ä»¶å’Œè‡ªå®šä¹‰é…ç½®æ–‡ä»¶,åªæœ‰å¼€å‘è€…æƒ³ä¿®æ”¹æŸä¸€é»˜è®¤é…ç½®æ—¶,æ‰éœ€è¦ä¿®æ”¹è‡ªå®šä¹‰é…ç½®æ–‡ä»¶,æ›´æ”¹ç›¸åº”å±æ€§å€¼.
##### 1.é»˜è®¤é…ç½®æ–‡ä»¶
  

| è¦è·å–çš„é»˜è®¤æ–‡ä»¶ | æ–‡ä»¶å­˜æ”¾åœ¨Hadoopçš„jaråŒ…ä¸­çš„ä½ç½® | å¸¸ç”¨é…ç½®ä¿¡æ¯
| :-------- | --------:| --------:|
| [core-default.xml]    | hadoop-common-2.7.2.jar/core-default.xml | NameNodeå±æ€§å’Œç«¯å£å·,æ•°æ®å­˜å‚¨è·¯å¾„ |
| [hdfs-default.xml]    | hadoop-hdfs-2.7.2.jar/hdfs-default.xml | å‰¯æœ¬æ•° |
| [yarn-default.xml]    | hadoop-yarn-common-2.7.2.jar/yarn-default.xml | ResourceManager&NodeManagerå±æ€§ |
| [mapred-default.xml]    | hadoop-mapred-client-core-2.7.2.jar/mapred-default.xml | åœ¨Yarnè¿è¡Œç¨‹åº,é»˜è®¤æ˜¯åœ¨æœ¬æœºè¿è¡Œ |

##### 2.è‡ªå®šä¹‰é…ç½®æ–‡
> å››ä¸ªé…ç½®æ–‡ä»¶å­˜æ”¾åœ¨**`$HADOOP_HOME/etc/hadoop`**è·¯å¾„ä¸­,å¼€å‘è€…å¯ä»¥æ ¹æ®é¡¹ç›®éœ€æ±‚é‡æ–°è¿›è¡Œä¿®æ”¹é…ç½®
> 
> **`core-site.xml`** | **`hdfs-site.xml`**
> **`yarn-site.xml`** | **`mapred-site.xml`**


###  ğŸ‰ğŸ‰ å®Œå…¨åˆ†å¸ƒå¼ è¿è¡Œæ¨¡å¼ (å¼€å‘é‡ç‚¹) ğŸ‰ğŸ‰
> åˆ†æ:å‡†å¤‡ä¸‰å°æœåŠ¡å™¨ (å…³é—­é˜²ç«å¢™,è®¾ç½®é™æ€IPåœ°å€,ä¸»æœºåç§°)
> å®‰è£…JavaJDK | é…ç½®Javaç¯å¢ƒå˜é‡
> å®‰è£…Hadoop | é…ç½®hadoopç¯å¢ƒå˜é‡
> é…ç½®é›†ç¾¤ | å•ç‚¹å¯åŠ¨
> é…ç½®SSH | ç¾¤èµ·å¹¶æµ‹è¯•é›†ç¾¤

#### è™šæ‹Ÿæœºå‡†å¤‡
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_005.jpg)

##### scp(secure copy)å®‰å…¨æ‹·è´
> 1.scpå®šä¹‰
> scpå¯ä»¥å®ç°æœåŠ¡å™¨ä¸æœåŠ¡å™¨ä¹‹é—´çš„æ•°æ®æ‹·è´,(from server1 to server2)
> 
> 2.scpåŸºæœ¬è¯­æ³•
> **`scp     -r      $pdir/$fname`**            **`$user@corehub$host:$pdir/$fname`**
> æŒ‡ä»¤    é€’å½’    æºæ•°æ®æ–‡ä»¶è·¯å¾„/åç§°                   ç›®çš„ç”¨æˆ·å@ä¸»æœºåç§°:ç›®çš„è·¯å¾„/åç§°
> 
> 3.scpå®æ“æ¡ˆä¾‹
> åœ¨corehub-001ä¸Š,å°†corehub-001ä¸­çš„/opt/moduleç›®å½•ä¸‹çš„è½¯ä»¶æ‹·è´åˆ°corehub-002ä¸Š
```
[root@corehub-001 ~]# cd /opt/
[root@corehub-001 opt]# ll
total 408824
-rwxrw-rw-.  1 root root   9621331 Jan 13 17:36 apache-tomcat-8.5.33.tar.gz
drwxr-xr-x.  8 uucp  143      4096 Dec 19  2017 jdk1.8.0_162
-rwxrw-rw-.  1 root root 189815615 Jan 13 18:22 jdk-8u162-linux-x64.tar.gz
drwxr-xr-x.  3 root root      4096 Jan 25 09:23 module
drwxr-xr-x. 13 root root      4096 Jan 13 23:07 mysql
-rwxrw-rw-.  1 root root 184122460 Jan 13 18:21 mysql-5.5.35-linux2.6-x86_64.tar.gz
drwxr-xr-x.  2 root root      4096 Nov 22  2013 rh
drwxr-xr-x.  2 root root      4096 Jan 25 09:20 software
drwxr-xr-x.  9 root root      4096 Jan 13 23:06 tomcat
drwxr-xr-x. 11 1001 1001      4096 Jan 17 22:48 zookeeper
-rw-r--r--.  1 root root  35042811 Jan 17 17:11 zookeeper-3.4.10.tar.gz
[root@corehub-001 opt]# scp -r module/ root@corehub-002:/opt/module/
The authenticity of host 'corehub-002 (192.168.152.135)' can't be established.
RSA key fingerprint is 63:9d:81:a7:3d:83:7f:04:19:32:8f:c8:97:9d:07:d8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'corehub-002,192.168.152.135' (RSA) to the list of known hosts.
root@corehub-002's password:
hdfs-config.sh                                                                      100% 1427     1.4KB/s   00:00    
mapred-config.sh                                                                    100% 2223     2.2KB/s   00:00    
httpfs-config.sh                                                                    100% 5749     5.6KB/s   00:00    
mapred-config.cmd                                                                   100% 1640     1.6KB/s   00:00    
yarn-config.cmd                                                                     100% 2131     2.1KB/s   00:00    
kms-config.sh                                                                       100% 5431     5.3KB/s   00:00    
yarn-config.sh                                                                      100% 2134     2.1KB/s   00:00    
hadoop-config.cmd                                                                   100% 8270     8.1KB/s   00:00    
[root@corehub-001 opt]#
```
> æ•°æ®å·²ä»corehub-001æœåŠ¡å™¨åŒæ­¥æ¨é€åˆ°corehub-002æœåŠ¡å™¨
```
[root@corehub-002 ~]# cd /opt/
[root@corehub-002 opt]# ll
total 408824
-rwxrw-rw-.  1 root root   9621331 Jan 13 17:36 apache-tomcat-8.5.33.tar.gz
drwxr-xr-x.  8 uucp  143      4096 Dec 19  2017 jdk1.8.0_162
-rwxrw-rw-.  1 root root 189815615 Jan 13 18:22 jdk-8u162-linux-x64.tar.gz
drwxr-xr-x.  4 root root      4096 Jan 29 06:08 module
drwxr-xr-x. 13 root root      4096 Jan 13 23:07 mysql
-rwxrw-rw-.  1 root root 184122460 Jan 13 18:21 mysql-5.5.35-linux2.6-x86_64.tar.gz
drwxr-xr-x.  2 root root      4096 Nov 22  2013 rh
drwxr-xr-x.  2 root root      4096 Jan 25 10:20 software
drwxr-xr-x.  9 root root      4096 Jan 13 23:06 tomcat
drwxr-xr-x. 11 1001 1001      4096 Jan 19 18:51 zookeeper
-rw-r--r--.  1 root root  35042811 Jan 17 17:11 zookeeper-3.4.10.tar.gz
[root@corehub-002 opt]# cd module/
[root@corehub-002 module]# ll
total 4
drwxr-xr-x. 15 root root 4096 Jan 29 06:09 hadoop
[root@corehub-002 module]# 
```

> åœ¨corehub-003æœåŠ¡å™¨ä¸Š,æ‹‰å–corehub-001æœåŠ¡å™¨ä¸Šæ•°æ®
```
[root@corehub-003 ~]# scp -r root@corehub-001:/opt/module /opt
root@corehub-001's password: 
hadoop-policy.xml                                                                   100% 9683     9.5KB/s   00:00    
yarn-site.xml                                                                       100%  690     0.7KB/s   00:00    
hdfs-site.xml                                                                       100%  775     0.8KB/s   00:00    
core-site.xml                                                                       100%  774     0.8KB/s   00:00    
httpfs-site.xml                                                                     100%  620     0.6KB/s   00:00    
capacity-scheduler.xml
mapred-config.cmd                                                                   100% 1640     1.6KB/s   00:00    
yarn-config.cmd                                                                     100% 2131     2.1KB/s   00:00    
kms-config.sh                                                                       100% 5431     5.3KB/s   00:00    
yarn-config.sh                                                                      100% 2134     2.1KB/s   00:00    
hadoop-config.cmd                                                                   100% 8270     8.1KB/s   00:00    
[root@corehub-003 ~]# 
```
> 4. å°†corehub-001é…ç½®æ–‡ä»¶åˆ†å‘æ¨é€åˆ°corehub-002,corehub-003æœåŠ¡å™¨ä¸Š,æ¨é€å®Œæ¯•åæ›´æ–°é…ç½®å³å¯ç”Ÿæ•ˆ
```
[root@corehub-001 ~]# scp -r /etc/profile root@corehub-002:/etc/profile
root@corehub-002's password: 
profile                                                                             100% 2073     2.0KB/s   00:00    
[root@corehub-001 ~]# 
```
```
[root@corehub-001 ~]# scp -r /etc/profile root@corehub-003:/etc/profile
The authenticity of host 'corehub-003 (192.168.152.136)' can't be established.
RSA key fingerprint is 63:9d:81:a7:3d:83:7f:04:19:32:8f:c8:97:9d:07:d8.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'corehub-003,192.168.152.136' (RSA) to the list of known hosts.
root@corehub-003's password: 
profile                                                                             100% 2073     2.0KB/s   00:00    
[root@corehub-001 ~]# 
```
```
source /etc/profile
```

##### rsync è¿œç¨‹åŒæ­¥å·¥å…·
> rsyncä¸»è¦ç”¨äºå¤‡ä»½å’Œé•œåƒ,å…·æœ‰é€Ÿåº¦å¿«,é¿å…å¤åˆ¶ç›¸åŒå†…å®¹å’Œæ”¯æŒç¬¦å·é“¾æ¥çš„ä¼˜ç‚¹.
> rsyncä¸scpåŒºåˆ«:ç”¨rsyncåšæ–‡ä»¶çš„å¤åˆ¶è¦æ¯”scpé€Ÿåº¦å¿«,rsyncåªå¯¹å·®å¼‚æ–‡ä»¶åšæ›´æ–°,scpæ˜¯æŠŠæ‰€æœ‰æ–‡ä»¶å¤åˆ¶çš„è¿‡ç¨‹.
> 


åŸºæœ¬è¯­æ³•
**`rsync -rVl $pdir$fname $user@corehub$host:$pdir/$fname`**
æŒ‡ä»¤    é€‰é¡¹å‚æ•° æºæ–‡ä»¶è·¯å¾„/åç§°  ç›®çš„ç”¨æˆ·å@ä¸»æœºåç§°:ç›®çš„è·¯å¾„/åç§°

| é€‰é¡¹      |     åŠŸèƒ½ |
| :-------- | --------:|
| -r    |   é€’å½’ |
| -v    |   æ˜¾ç¤ºå¤åˆ¶è¿‡ç¨‹ |
| -l    |   æ‹·è´ç¬¦å·è¿æ¥ |

rsyncå®æ“æ¡ˆä¾‹
> å°†corehub-001æœåŠ¡å™¨ä¸Šçš„/opt/softwareç›®å½•åŒæ­¥åˆ°corehub-002æœåŠ¡å™¨çš„rootç”¨æˆ·ç›®å½•ä¸‹
```
[root@corehub-001 ~]# rsync -rvl /opt/software/ root@corehub-002:/opt/software/
root@corehub-002's password: 
sending incremental file list
created directory /opt/software
./
hadoop-2.7.2.tar.gz
sent 212072761 bytes  received 34 bytes  12852896.67 bytes/sec
total size is 212046774  speedup is 1.00
[root@corehub-001 ~]# 
```

#### ç¼–å†™é›†ç¾¤åˆ†å‘è„šæœ¬xsync
> éœ€æ±‚:å¾ªç¯å¤åˆ¶æ–‡ä»¶åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„ç›¸åŒç›®å½•ä¸‹

éœ€æ±‚åˆ†æ:
rsyncæŒ‡ä»¤ åŸå§‹æ‹·è´
rsync -rvl /opt/module root@corehub-002:/opt/
æœŸæœ›è„šæœ¬:å°†sxyncè¦è¦åŒæ­¥çš„æ–‡ä»¶åç§°
è¯´æ˜:åœ¨/home/geek-developer/bin/æ­¤ç›®å½•ä¸‹å­˜æ”¾è„šæœ¬,geek-developerç”¨æˆ·å¯ä»¥åœ¨ç³»ç»Ÿä»»ä½•åœ°æ–¹ç›´æ¥æ‰§è¡Œ

è„šæœ¬å®ç°
> åˆ›å»ºbinç›®å½• mkdir bin
> ```
> [root@corehub-001 ~]# mkdir bin
> [root@corehub-001 ~]# ll
> total 100
> -rw-------. 1 root root  3362 Jan 18 04:54 anaconda-ks.cfg
> drwxr-xr-x. 2 root root  4096 Jan 30 18:00 bin
> drwxr-xr-x. 2 root root  4096 Jan 24 19:40 Desktop
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Documents
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Downloads
> -rw-r--r--. 1 root root 41364 Jan 18 04:54 install.log
> -rw-r--r--. 1 root root  9154 Jan 18 04:52 install.log.syslog
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Music
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Pictures
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Public
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Templates
> drwxr-xr-x. 2 root root  4096 Jan 18 05:51 Videos
> [root@corehub-001 ~]#
> ```

> è¿›å…¥binç›®å½• cd bin/
> ```
> [root@corehub-001 ~]# cd bin/
> [root@corehub-001 bin]# ll
> total 0
> [root@corehub-001 bin]# 
> ```

> åˆ›å»ºxsyncæ–‡ä»¶ touch xsync
> ```
> [root@corehub-001 bin]# touch xsync
> [root@corehub-001 bin]# ll
> total 0
> -rw-r--r--. 1 root root 0 Jan 30 18:05 xsync
> [root@corehub-001 bin]# 
> ```

> ç¼–è¾‘xsync vim xsync
> ```
> #!/bin/bash
> # 1.è·å–è¾“å…¥å‚æ•°ä¸ªæ•°,å¦‚æœæ²¡æœ‰å‚æ•°,ç›´æ¥é€€å‡º
> pcount=$#
> if((pcount==0)); then
> echo no args;
> exit
> fi
> 
> # 2.è·å–æ–‡ä»¶åç§°
> p1=$1
> fname=`basename $p1`
> echo fname=$fname
> 
> # 3.è·å–ä¸Šçº§ç›®å½•åˆ°æ®å¯¹è·¯å¾„
> pdri=`cd -P $(dirname $p1); pwd`
> echo pdir=$pdri
> 
> # 4.è·å–å½“å‰ç”¨æˆ·åç§°
> user=`whoami`
> 
> # 5.å¾ªç¯éå†
> for((host=103;host<105;host++)); do
> echo -------corehub$host-------
> rsync -rvl $pdir/$fname $user@corehub$host:$pdri
> done
> ```





#### é›†ç¾¤é…ç½®
1.é›†ç¾¤éƒ¨ç½²è§„åˆ’

| linuxæœåŠ¡å™¨ | corehub-001 | corehub-002 | corehub-003 |
| :-------- | --------:| :------: | :------: |
| HDFS    | NameNode,DataNode | DataNode | SecondaryNameNode,DataNode |
| YARN    | NodeManager |  ResourceManager,NodeManager  | NodeManager |


2.é…ç½®é›†ç¾¤
é…ç½®core-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/core-site.xml
```
åœ¨è¯¥æ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹é…ç½®
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
<!-- æŒ‡å®šHDFSä¸­çš„NameNodeåœ°å€ -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://corehub-001:9000</value>
  </property>
<!-- æŒ‡å®šHadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜å‚¨ç›®å½• -->
   <property>
     <name>hadoop.tmp.dir</name>
     <value>/opt/module/hadoop/data/tmp</value>
   </property>
</configuration>
```
HDFSé…ç½®æ–‡ä»¶
é…ç½®hadoop-env.sh
```
[root@corehub-001 hadoop]# vim etc/hadoop/hadoop-env.sh
```
``` bash
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Set Hadoop-specific environment variables here.

# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.
# The java implementation to use.
export JAVA_HOME=/opt/devtool/jdk1.8.0_162
```
é…ç½®hdfs-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/hdfs-site.xml
```
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
<!-- æŒ‡å®šHDFSå‰¯æœ¬æ•°é‡ -->
  <property>
   <name>dfs.replication</name>
     <value>3</value>
  </property>
  <!-- æŒ‡å®šHadoopè¾…åŠ©åç§°èŠ‚ç‚¹ä¸»æœºé…ç½® -->
  <property>
   <name>dfs.namenode.secondary.http-address</name>
     <value>corehub-003:50090</value>
  </property>
</configuration>
```
YARNé…ç½®æ–‡ä»¶
é…ç½®yarn-env.sh
```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-env.sh
```
```
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# User for YARN daemons
export HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}

# resolve links - $0 may be a softlink
export YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"

# some Java parameters
export JAVA_HOME=/opt/devtool/jdk1.8.0_162
```
é…ç½®yarn-site.xml
åœ¨è¯¥æ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹é…ç½®
``` xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
<!-- Site specific YARN configuration properties -->
<!-- Reducerè·å–æ•°æ®æ–¹å¼ -->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
<!-- æŒ‡å®šYarnçš„ResourceManageråœ°å€-->
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>corehub-002</value>
    </property>
</configuration>
```

MapReduceé…ç½®æ–‡ä»¶
é…ç½®mapred-env.sh
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-env.sh
```

```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-site.xml
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
export JAVA_HOME=/opt/devtool/jdk1.8.0_16
```
é…ç½®mapred-site.xml
åœ¨è¯¥æ–‡ä»¶ä¸­ç¼–å†™å¦‚ä¸‹é…ç½®
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-site.xml
```
``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
<!-- æŒ‡å®šMRè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
</configuration>
```
åˆ†åˆ«åˆ é™¤001,002,003å·æœåŠ¡å™¨ä¸Šçš„ log,dataæ–‡ä»¶
åˆ é™¤å‰æ,å…ˆä¿è¯æ²¡æœ‰jpsåœ¨è¿è¡Œä¸­,å¦åˆ™ä¼šå¯¼è‡´æ‚²å‰§å‘ç”Ÿ
åˆ é™¤001å·æœåŠ¡å™¨
```
[root@corehub-001 hadoop]# rm -rf data/ logs/
[root@corehub-001 hadoop]# ll
total 68
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:48 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 24 23:34 wcoutput
[root@corehub-001 hadoop]#
```
åˆ é™¤002å·æœåŠ¡å™¨
```
[root@corehub-002 hadoop]# rm -rf data/ logs/
[root@corehub-002 hadoop]# ll
total 68
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 bin
drwxr-xr-x. 3 root root  4096 Jan 31 13:33 etc
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 include
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 input
drwxr-xr-x. 3 root root  4096 Jan 31 13:34 lib
drwxr-xr-x. 2 root root  4096 Jan 31 13:33 libexec
-rw-r--r--. 1 root root 15429 Jan 31 13:33 LICENSE.txt
-rw-r--r--. 1 root root   101 Jan 31 13:34 NOTICE.txt
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 output
-rw-r--r--. 1 root root  1366 Jan 31 13:33 README.txt
drwxr-xr-x. 2 root root  4096 Jan 31 13:33 sbin
drwxr-xr-x. 4 root root  4096 Jan 31 13:34 share
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 wcinput
drwxr-xr-x. 2 root root  4096 Jan 31 13:34 wcoutput
[root@corehub-002 hadoop]# 

```
åˆ é™¤003å·æœåŠ¡å™¨
```
[root@corehub-003 hadoop]# rm -rf data/ logs/
[root@corehub-003 hadoop]# ll
total 68
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 bin
drwxr-xr-x. 3 root root  4096 Jan 31 13:37 etc
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 include
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 input
drwxr-xr-x. 3 root root  4096 Jan 31 13:37 lib
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 libexec
-rw-r--r--. 1 root root 15429 Jan 31 13:37 LICENSE.txt
-rw-r--r--. 1 root root   101 Jan 31 13:37 NOTICE.txt
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 output
-rw-r--r--. 1 root root  1366 Jan 31 13:37 README.txt
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 sbin
drwxr-xr-x. 4 root root  4096 Jan 31 13:37 share
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 wcinput
drwxr-xr-x. 2 root root  4096 Jan 31 13:37 wcoutput
[root@corehub-003 hadoop]# 
```
æœ€å æ ¼å¼åŒ– 001æœåŠ¡å™¨æ•°æ®
```
[root@corehub-001 hadoop]# bin/hdfs namenode -format
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = corehub-001/192.168.177.130
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = /opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/module/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/module/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/module/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/module/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/module/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/module/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/module/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar
19/01/31 13:49:10 INFO common.Storage: Storage directory /opt/module/hadoop/data/tmp/dfs/name has been successfully formatted.
19/01/31 13:49:10 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
19/01/31 13:49:10 INFO util.ExitUtil: Exiting with status 0
19/01/31 13:49:10 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at corehub-001/192.168.177.130
************************************************************/
```



#### é›†ç¾¤å•ç‚¹å¯åŠ¨
å¯åŠ¨001å·æœåŠ¡
```
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-corehub-001.out
[root@corehub-001 hadoop]# jps
94401 NameNode
94539 Jps
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-corehub-001.out
[root@corehub-001 hadoop]# jps
94401 NameNode
94789 DataNode
95017 Jps
[root@corehub-001 hadoop]# 
```
å¯åŠ¨002å·æœåŠ¡
```
[root@corehub-002 hadoop]# sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-corehub-002.out
[root@corehub-002 hadoop]# jps
63289 DataNode
63405 Jps
[root@corehub-002 hadoop]# 
```
å¯åŠ¨003å·æœåŠ¡
```
[root@corehub-003 hadoop]# sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-corehub-003.out
[root@corehub-003 hadoop]# jps
67184 DataNode
67332 Jps
[root@corehub-003 hadoop]# 
```

#### SSHæ— å¯†ç é…ç½®
SSHæœ‰å¯†ç æ¼”ç¤º
```
[root@corehub-001 ~]# ssh corehub-002
root@corehub-002's password: 
Last login: Thu Jan 31 14:22:32 2019 from 192.168.177.2
[root@corehub-002 ~]# hostname
corehub-002
[root@corehub-002 ~]# exit
logout
Connection to corehub-002 closed.
[root@corehub-001 ~]# 
```
å…ç™»å½•åŸç†
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_014.jpg)

ls -alæŒ‡ä»¤ grepæŒ‡ä»¤æŸ¥æ‰¾åˆ°.sshæ–‡ä»¶
```
[root@corehub-001 ~]# ls -al | grep .ssh
drwx------.  2 root root  4096 Jan 31 13:24 .ssh
[root@corehub-001 ~]# 
```
cdè¿›å…¥.sshç›®å½•,åˆ›å»ºå…¬é’¥ç§é’¥,è¾“å…¥æŒ‡ä»¤åè¿ç»­è¾“å…¥ä¸‰æ¬¡å›è½¦å³å¯å®Œæˆåˆ›å»º
``` bash
[root@corehub-001 .ssh]# ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
42:52:6e:8d:a2:3b:55:fb:d8:bf:dd:d1:de:d4:c3:21 root@corehub-001
The key's randomart image is:
+--[ RSA 2048]----+
|      .          |
|     o o         |
|    o * .        |
|   . * .         |
|  . . o S    E . |
|   o   =      o.o|
|  o   . o     .o+|
|   .     . . . +o|
|          o.. . o|
+-----------------+
[root@corehub-001 .ssh]# ll
total 12
-rw-------. 1 root root 1675 Jan 31 14:42 id_rsa
-rw-r--r--. 1 root root  398 Jan 31 14:42 id_rsa.pub
-rw-r--r--. 1 root root  409 Jan 31 13:24 known_hosts
[root@corehub-001 .ssh]# 
```
å°†001å·æœåŠ¡å™¨å…¬é’¥æ‹·è´åˆ°è‡ªèº«æœåŠ¡å™¨
```
[root@corehub-001 ~]# ssh corehub-001
The authenticity of host 'corehub-001 (192.168.177.130)' can't be established.
RSA key fingerprint is 99:b3:c1:16:af:d9:de:79:5f:cf:53:25:63:f1:30:1d.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'corehub-001,192.168.177.130' (RSA) to the list of known hosts.
root@corehub-001's password: 
Last login: Thu Jan 31 14:22:12 2019 from 192.168.177.2
[root@corehub-001 ~]# 
```
å°†001å·æœåŠ¡å™¨å…¬é’¥æ‹·è´åˆ°002æœåŠ¡å™¨
```
[root@corehub-001 .ssh]# ssh-copy-id corehub-002
root@corehub-002's password: 
Now try logging into the machine, with "ssh 'corehub-002'", and check in:
  .ssh/authorized_keys
to make sure we haven't added extra keys that you weren't expecting.
[root@corehub-001 .ssh]# 
```
å°†001å·æœåŠ¡å™¨å…¬é’¥æ‹·è´åˆ°003æœåŠ¡å™¨
```
[root@corehub-001 .ssh]# ssh-copy-id corehub-003
The authenticity of host 'corehub-003 (192.168.177.132)' can't be established.
RSA key fingerprint is 99:b3:c1:16:af:d9:de:79:5f:cf:53:25:63:f1:30:1d.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'corehub-003,192.168.177.132' (RSA) to the list of known hosts.
root@corehub-003's password: 
Now try logging into the machine, with "ssh 'corehub-003'", and check in:
  .ssh/authorized_keys
to make sure we haven't added extra keys that you weren't expecting.
[root@corehub-001 .ssh]# 
```
æ‹·è´å®Œæ¯•,æµ‹è¯•æ˜¯å¦å¯ä»¥å…ç™»å½•
å…ç™»å½•002æœåŠ¡å™¨
```
[root@corehub-001 ~]# ssh corehub-002
Last login: Thu Jan 31 14:22:33 2019 from corehub-001
[root@corehub-002 ~]# exit
logout
Connection to corehub-002 closed.
[root@corehub-001 ~]# 
```
å…ç™»å½•003æœåŠ¡å™¨
```
[root@corehub-001 ~]# ssh corehub-003
Last login: Thu Jan 31 14:22:32 2019 from 192.168.177.2
[root@corehub-003 ~]# exit
logout
Connection to corehub-003 closed.
[root@corehub-001 ~]# 
```
sshæ–‡ä»¶åŠŸèƒ½è¯´æ˜
**`Known hosts`**:è®°å½•sshè®¿é—®è¿‡è®¡ç®—æœºå…¬é’¥(public key)
**`id rsa`**:ç”Ÿæˆçš„ç§é’¥
**`id_rsa.pub`**:ç”Ÿæˆçš„å…¬é’¥
**`authorized_keys`**:å­˜æ”¾æˆæƒè¿‡å¾—æ— å¯†ç ç™»å½•æœåŠ¡å™¨å…¬é’¥

#### ç¾¤èµ·é›†ç¾¤
1.é…ç½®slaves
```
[root@corehub-001 hadoop]# vim etc/hadoop/slaves
```
åœ¨è¯¥æ–‡ä»¶ä¸­æ·»åŠ ä¸€ä¸‹å†…å®¹
æ³¨æ„:è¯¥æ–‡ä»¶æ·»åŠ çš„å†…å®¹ç»“å°¾ä¸å…è®¸æœ‰ç©ºæ ¼,æ–‡ä»¶ä¸­éƒ¨å…è®¸æœ‰ç©ºæ ¼
```
corehub-001
corehub-002
corehub-003
```
æµ‹è¯•ç¾¤èµ·
```
[root@corehub-001 hadoop]# sbin/start-dfs.sh
Starting namenodes on [corehub-001]
root@corehub-001's password: 
corehub-001: namenode running as process 39894. Stop it first.
root@corehub-001's password: corehub-002: datanode running as process 9007. Stop it first.
corehub-003: datanode running as process 12654. Stop it first.
root@corehub-001's password: corehub-001: Permission denied, please try again.
corehub-001: Permission denied, please try again.
root@corehub-001's password: 
corehub-001: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).
Starting secondary namenodes [corehub-003]
corehub-003: secondarynamenode running as process 18212. Stop it first.
```
å¯¹ç…§ é›†ç¾¤éƒ¨ç½²è§„åˆ’ æŸ¥çœ‹001å·è¿›ç¨‹ æ˜¯å¦æ­£å¸¸å¯åŠ¨
```
[root@corehub-001 hadoop]# jps
39894 NameNode
47978 Jps
46235 DataNode
[root@corehub-001 hadoop]#
```
å¯¹ç…§ é›†ç¾¤éƒ¨ç½²è§„åˆ’ æŸ¥çœ‹002å·è¿›ç¨‹ æ˜¯å¦æ­£å¸¸å¯åŠ¨
```
[root@corehub-002 hadoop]# jps
19375 Jps
9007 DataNode
[root@corehub-002 hadoop]# 
```
å¯¹ç…§ é›†ç¾¤éƒ¨ç½²è§„åˆ’ æŸ¥çœ‹003å·è¿›ç¨‹ æ˜¯å¦æ­£å¸¸å¯åŠ¨
```
[root@corehub-003 hadoop]# jps
18212 SecondaryNameNode
23335 Jps
12654 DataNode
[root@corehub-003 hadoop]# 
```
åœ¨002æœåŠ¡å™¨å¯åŠ¨YARN ResourceManager
```
[root@corehub-002 hadoop]# sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-002.out
corehub-001: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
corehub-003: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-003.out
corehub-002: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-002.out
[root@corehub-002 hadoop]# jps
22144 Jps
22052 NodeManager
9007 DataNode
21935 ResourceManager
[root@corehub-002 hadoop]# 
```
3.é›†ç¾¤åŸºæœ¬æµ‹è¯•
a.ä¸Šä¼ æ–‡ä»¶åˆ°é›†ç¾¤
ä¸Šä¼ å°æ–‡ä»¶
```
[root@corehub-001 hadoop]# bin/hdfs dfs -mkdir -p /user/geekparkhub/input
```
```
[root@corehub-001 hadoop]# bin/hdfs dfs -put wcinput/wc.input /user/geekparkhub/input
```
ä¸Šä¼ å¤§æ–‡ä»¶
```
[root@corehub-001 hadoop]# bin/hdfs dfs -put /opt/software/hadoop-2.7.2.tar.gz /user/geekparkhub/input
```

b.ä¸Šä¼ æ–‡ä»¶æŸ¥çœ‹æ–‡ä»¶å­˜æ”¾ä½ç½®
æŸ¥çœ‹HDFSæ–‡ä»¶å­˜å‚¨è·¯å¾„
```
[root@corehub-001 subdir0]# pwd
/opt/module/hadoop/data/tmp/dfs/data/current/BP-1162876294-192.168.177.130-1548913750188/current/finalized/subdir0/subdir0
[root@corehub-001 subdir0]# ll
total 405008
-rw-r--r--. 1 root root       196 Jan 31 16:16 blk_1073741827
-rw-r--r--. 1 root root        11 Jan 31 16:16 blk_1073741827_1003.meta
-rw-r--r--. 1 root root 134217728 Jan 31 16:21 blk_1073741830
-rw-r--r--. 1 root root   1048583 Jan 31 16:21 blk_1073741830_1006.meta
-rw-r--r--. 1 root root  77829046 Jan 31 16:22 blk_1073741831
-rw-r--r--. 1 root root    608047 Jan 31 16:22 blk_1073741831_1007.meta
-rw-r--r--. 1 root root   9621331 Jan 31 16:31 blk_1073741832
-rw-r--r--. 1 root root     75175 Jan 31 16:31 blk_1073741832_1008.meta
-rw-r--r--. 1 root root 134217728 Jan 31 16:32 blk_1073741833
-rw-r--r--. 1 root root   1048583 Jan 31 16:32 blk_1073741833_1009.meta
-rw-r--r--. 1 root root  55597887 Jan 31 16:32 blk_1073741834
-rw-r--r--. 1 root root    434367 Jan 31 16:32 blk_1073741834_1010.meta
[root@corehub-001 subdir0]# 
```

#### é›†ç¾¤å¯åŠ¨/å…³é—­æ–¹å¼æ€»ç»“
1.å„ä¸ªæœåŠ¡ç»„ä»¶é€ä¸€å¯åŠ¨å’Œå…³é—­
åˆ†åˆ«å¯åŠ¨/å…³é—­ HDFSç»„ä»¶
`hadoop-daemon.sh start/stop namenode/datanode/secondarynamenode`
å¯åŠ¨/å…³é—­ YARN
`yarn-daemon.sh start/stop resourcemanager/nodemanager`

2.å„ä¸ªæ¨¡å—åˆ†å¼€å¯åŠ¨å’Œå…³é—­(å‰ææ˜¯é…ç½®å¥½sshå…ç™»å½•)å¸¸ç”¨
1.æ•´ä½“å¯åŠ¨å…³é—­YARN
`start-dfs.sh / stop-dfs.sh`
2.æ•´ä½“å¯åŠ¨å…³é—­YARN
`start-yarn.sh / stop-yarn.sh`


#### é›†ç¾¤æ—¶é—´åŒæ­¥
> æ—¶é—´åŒæ­¥æ–¹å¼:æ‰¾ä¸€å°æœºå™¨ä½œä¸ºæ—¶é—´æœåŠ¡å™¨,æ‰€æœ‰æœºå™¨ä¸è¿™å°é›†ç¾¤æ—¶é—´è¿›è¡Œå®šæ—¶çš„åŒæ­¥,æ¯”å¦‚æ¯éš”ååˆ†é’Ÿ,åŒæ­¥ä¸€æ¬¡æ—¶é—´

é…ç½®æ—¶é—´åŒæ­¥å®ç°æ­¥éª¤
1.æ—¶é—´æœåŠ¡é…ç½®(å¿…é¡»æ˜¯rootç”¨æˆ·)
æ£€æŸ¥ntpæ˜¯å¦å®‰è£…
```
[root@corehub-002 ~]# rpm -qa|grep ntp
fontpackages-filesystem-1.41-1.1.el6.noarch
ntpdate-4.2.6p5-15.el6.centos.x86_64
ntp-4.2.6p5-15.el6.centos.x86_64
[root@corehub-002 ~]# 
```
ä¿®æ”¹ntpé…ç½®æ–‡ä»¶
`vim /etc/ntp.conf`
ä¿®æ”¹å†…å®¹å¦‚ä¸‹:
ä¿®æ”¹ (æˆæƒ`192.168.177.2`-`192.168.177.255`ç½‘æ®µä¸Šæ‰€æœ‰çš„æœºå™¨å¯ä»¥ä»è¿™å°æœºå™¨ä¸ŠæŸ¥è¯¢å’ŒåŒæ­¥æ—¶é—´)
``` bash
[root@corehub-002 ~]# vim /etc/ntp.conf
# For more information about this file, see the man pages
# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).

driftfile /var/lib/ntp/drift

# Permit time synchronization with our time source, but do not
# permit the source to query or modify the service on this system.
restrict default kod nomodify notrap nopeer noquery
restrict -6 default kod nomodify notrap nopeer noquery

# Permit all access over the loopback interface.  This could
# be tightened as well, but to do so would effect some of
# the administrative functions.
restrict 127.0.0.1
restrict -6 ::1

# Hosts on local network are less restricted.
restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
```

ä¿®æ”¹(é›†ç¾¤åœ¨å±€åŸŸç½‘ä¸­,ä¸ä½¿ç”¨å…¶ä»–äº’è”ç½‘ä¸Šçš„æ—¶é—´)
```
# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
# server 0.centos.pool.ntp.org iburst
# server 1.centos.pool.ntp.org iburst
# server 2.centos.pool.ntp.org iburst
# server 3.centos.pool.ntp.org iburst
```
æ·»åŠ (å½“è¯¥èŠ‚ç‚¹ä¸¢å¤±ç½‘ç»œè¿æ¥,ä¾ç„¶å¯ä»¥é‡‡ç”¨æœ¬åœ°æ—¶é—´ä½œä¸ºæ—¶é—´æœåŠ¡å™¨ä¸ºé›†ç¾¤ä¸­çš„å…¶ä»–èŠ‚ç‚¹æä¾›æ—¶é—´åŒæ­¥)
```
# å½“è¯¥èŠ‚ç‚¹ä¸¢å¤±ç½‘ç»œè¿æ¥,ä¾ç„¶å¯ä»¥é‡‡ç”¨æœ¬åœ°æ—¶é—´ä½œä¸ºæ—¶é—´æœåŠ¡å™¨ä¸ºé›†ç¾¤ä¸­çš„å…¶ä»–èŠ‚ç‚¹æä¾›æ—¶é—´åŒæ­¥
server 127.127.1.0
fudge 127.127.1.0 stratum 10
```
ä¿®æ”¹/etc/sysconfig/ntpdæ–‡ä»¶
è®©ç¡¬ä»¶ä¸ç³»ç»Ÿæ—¶é—´åŒæ­¥
`vim /etc/sysconfig/ntpd`
```
SYNC_HWCLOCK=yes
```
é‡æ–°å¯åŠ¨ntpdæœåŠ¡
```
[root@corehub-002 geek-developer]# service ntpd start
Starting ntpd:                                             [  OK  ]
[root@corehub-002 geek-developer]# service ntpd status
ntpd (pid  2871) is running...
[root@corehub-002 geek-developer]# 
```
è®¾ç½®ntpdæœåŠ¡å¼€æœºè‡ªå¯
```
[root@corehub-002 geek-developer]# chkconfig ntpd on
[root@corehub-002 geek-developer]# 
```
å…¶ä»–æœºå™¨é…ç½®(å¿…é¡»rootç”¨æˆ·)
åœ¨å…¶ä»–æœºå™¨é…ç½®10åˆ†é’Ÿä¸æ—¶é—´æœåŠ¡å™¨åŒæ­¥ä¸€æ¬¡
åˆæ­¥æµ‹è¯•
```
[root@corehub-001 ~]# date -s "2018-11-11 11:11:11"
Sun Nov 11 11:11:11 CST 2018
[root@corehub-001 ~]# date
Sun Nov 11 11:11:12 CST 2018
[root@corehub-001 ~]# /usr/sbin/ntpdate corehub-002
 3 Feb 12:58:56 ntpdate[6473]: step time server 192.168.177.131 offset 7264060.505383 sec
[root@corehub-001 ~]# date
Sun Feb  3 12:59:43 CST 2019
[root@corehub-001 ~]# 
```

ç¼–å†™å®šæ—¶ä»»åŠ¡å¦‚ä¸‹:
```
[root@corehub-003 hadoop]# crontab -e
```
```
*/1 * * * * /usr/sbin/ntpdate corehub-002
~                                                                               
~                                                                      
~                                                                               
"/tmp/crontab.phnH6Y" 1L, 42C
```

ä¿®æ”¹ä»»æ„æœºå™¨æ—¶é—´
date -s "2019-7-12 41:55:23"
ä¸€åˆ†é’ŸåæŸ¥çœ‹æœºå™¨æ˜¯å¦ä¸æ—¶é—´åº¦æœåŠ¡å™¨åŒæ­¥
```
[root@corehub-003 ~]# date -s "2018-11-11 11:11:11"
[root@corehub-003 ~]# date
Sun Nov 11 11:11:12 CST 2018
[root@corehub-003 ~]# date
Sun Feb  3 13:04:23 CST 2019
```

## 6. Hadoop ç¼–è¯‘æºç 
### å‰æœŸå‡†å¤‡å·¥ä½œ
#### 1.centosè”ç½‘
é…ç½®centosèƒ½å¤Ÿè¿æ¥å¤–ç½‘,linuxè™šæ‹Ÿæœº æµ‹è¯• `ping www.baidu.com` æ˜¯å¦ç•…é€š
æ³¨æ„:é‡‡ç”¨rootè§’è‰²ç¼–è¯‘,å‡å°‘æ–‡ä»¶æƒé™å‡ºç°çš„é—®é¢˜
#### 2.jaråŒ…å‡†å¤‡
`hadoop-2.7.2-src.tar.gz` | [å¿«é€Ÿä¸‹è½½é€šé“](https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/)
`jdk-8u144-linux-x64.tar.gz`  | [å¿«é€Ÿä¸‹è½½é€šé“](https://www.oracle.com/technetwork/java/javase/documentation/8u-relnotes-2225394.html)
`apache-ant-1.9.10-bin.tar.gz` (build tool æ‰“åŒ…å·¥å…·)  | [å¿«é€Ÿä¸‹è½½é€šé“](https://archive.apache.org/dist/ant/binaries/apache-ant-1.9.10-bin.tar.gz)
`apache-maven-3.0.5-bin.tar.gz`  | [å¿«é€Ÿä¸‹è½½é€šé“](http://archive.apache.org/dist/maven/maven-3/3.0.5/binaries/)
`protobuf-2.5.0.tar.gz` (åºåˆ—åŒ–æ¡†æ¶)  | [å¿«é€Ÿä¸‹è½½é€šé“](https://files.pythonhosted.org/packages/3f/ad/c8221a0778cc04197047f0f6ddee683ef1a0851976a4bd4ad17af19d22ec/protobuf-2.5.0.tar.gz)

### jaråŒ…å®‰è£…
#### mavenå®‰è£…
è§£å‹taråŒ…åˆ°æŒ‡å®šç›®å½•
```
[root@corehub-001 software]# tar -zvxf apache-maven-3.0.5-bin.tar.gz -C /opt/module/
```
é‡å‘½å
```
[root@corehub-001 module]# mv apache-maven-3.0.5 maven
[root@corehub-001 module]# ll
total 16
drwxr-xr-x.  6 root   root  4096 Feb  4  2018 ant
drwxr-xr-x. 15  10011 10011 4096 Jan 31 13:52 hadoop
drwxr-xr-x.  6 root   root  4096 Feb  3 14:54 maven
[root@corehub-001 module]# 
```
é…ç½®ç¯å¢ƒå˜é‡
```
[root@corehub-001 ~]# cd /opt/module/maven/
[root@corehub-001 maven]# pwd
/opt/module/maven
[root@corehub-001 maven]# vim /etc/profile
```
```
##MAVEN_HOME
export MAVEN_HOME=/opt/module/maven
export PATH=$PATH:$MAVEN_HOME/bin
```
```
[root@corehub-001 maven]# source /etc/profile
[root@corehub-001 maven]# mvn -version
Apache Maven 3.0.5 (r01de14724cdef164cd33c7c8c2fe155faf9602da; 2013-02-19 21:51:28+0800)
Maven home: /opt/module/maven
Java version: 1.8.0_162, vendor: Oracle Corporation
Java home: /opt/devtool/jdk1.8.0_162/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "2.6.32-754.10.1.el6.x86_64", arch: "amd64", family: "unix"
[root@corehub-001 maven]# 
```


#### antå®‰è£…
è§£å‹taråŒ…åˆ°æŒ‡å®šç›®å½•
```
[root@corehub-001 software]# tar -zvxf apache-ant-1.9.10-bin.tar.gz -C /opt/module/
```
é‡å‘½å
```
[root@corehub-001 module]# mv apache-ant-1.9.10 ant
[root@corehub-001 module]# ll
total 8
drwxr-xr-x.  6 root  root  4096 Feb  4  2018 ant
drwxr-xr-x. 15 10011 10011 4096 Jan 31 13:52 hadoop
[root@corehub-001 module]# 
```
é…ç½®ç¯å¢ƒå˜é‡
```
[root@corehub-001 ~]# cd /opt/module/ant/
[root@corehub-001 ant]# pwd
/opt/module/ant
[root@corehub-001 ant]# vim /etc/profile
```
```
##ANT_HOME
export ANT_HOME=/opt/module/ant
export PATH=$PATH:$ANT_HOME/bin
```
```
[root@corehub-001 ant]# source /etc/profile
[root@corehub-001 ant]# ant -version
Apache Ant(TM) version 1.9.10 compiled on February 3 2018
[root@corehub-001 ant]# 
```
#### å®‰è£…glibc-headers ä¸ g++
```
yum install glibc-headers
```
```
[root@corehub-001 geek-developer]# yum install gcc-c++
Loaded plugins: fastestmirror, refresh-packagekit, security
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: ftp.sjtu.edu.cn
 * extras: centos.ustc.edu.cn
 * updates: mirror.bit.edu.cn
Resolving Dependencies
--> Running transaction check
---> Package gcc-c++.x86_64 0:4.4.7-23.el6 will be installed
--> Processing Dependency: libstdc++-devel = 4.4.7-23.el6 for package: gcc-c++-4.4.7-23.el6.x86_64
--> Running transaction check
---> Package libstdc++-devel.x86_64 0:4.4.7-23.el6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package                 Arch           Version              Repository    Size
================================================================================
Installing:
 gcc-c++                 x86_64         4.4.7-23.el6         base         4.7 M
Installing for dependencies:
 libstdc++-devel         x86_64         4.4.7-23.el6         base         1.6 M

Transaction Summary
================================================================================
Install       2 Package(s)

Total size: 6.3 M
Total download size: 4.7 M
Installed size: 20 M
Is this ok [y/N]: y
Downloading Packages:
gcc-c++-4.4.7-23.el6.x86_64.rpm                          | 4.7 MB     00:03     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : libstdc++-devel-4.4.7-23.el6.x86_64                          1/2 
  Installing : gcc-c++-4.4.7-23.el6.x86_64                                  2/2 
  Verifying  : libstdc++-devel-4.4.7-23.el6.x86_64                          1/2 
  Verifying  : gcc-c++-4.4.7-23.el6.x86_64                                  2/2 

Installed:
  gcc-c++.x86_64 0:4.4.7-23.el6                                                 

Dependency Installed:
  libstdc++-devel.x86_64 0:4.4.7-23.el6                                         

Complete!
[root@corehub-001 geek-developer]#
```

#### å®‰è£…makeä¸cmake
```
yum install make
```

```
[root@corehub-001 geek-developer]# yum install cmake
Loaded plugins: fastestmirror, refresh-packagekit, security
Setting up Install Process
Loading mirror speeds from cached hostfile
 * base: ftp.sjtu.edu.cn
 * extras: centos.ustc.edu.cn
 * updates: mirror.bit.edu.cn
Resolving Dependencies
--> Running transaction check
---> Package cmake.x86_64 0:2.8.12.2-4.el6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package         Arch             Version                  Repository      Size
================================================================================
Installing:
 cmake           x86_64           2.8.12.2-4.el6           base           8.0 M

Transaction Summary
================================================================================
Install       1 Package(s)

Total download size: 8.0 M
Installed size: 28 M
Is this ok [y/N]: y
Downloading Packages:
cmake-2.8.12.2-4.el6.x86_64.rpm                          | 8.0 MB     00:05     
Running rpm_check_debug
Running Transaction Test
Transaction Test Succeeded
Running Transaction
  Installing : cmake-2.8.12.2-4.el6.x86_64                                  1/1 
  Verifying  : cmake-2.8.12.2-4.el6.x86_64                                  1/1 

Installed:
  cmake.x86_64 0:2.8.12.2-4.el6                                                 

Complete!
[root@corehub-001 geek-developer]# 
```


#### protobufå®‰è£…
è§£å‹taråŒ…åˆ°æŒ‡å®šç›®å½•
```
[root@corehub-001 software]# tar -zvxf protobuf-2.5.0.tar.gz -C /opt/module/
```
é‡å‘½å
```
[root@corehub-001 module]# mv protobuf-2.5.0 protobuf
[root@corehub-001 module]# ll
total 16
drwxr-xr-x.  6 root   root  4096 Feb  4  2018 ant
drwxr-xr-x. 15  10011 10011 4096 Jan 31 13:52 hadoop
drwxr-xr-x.  6 root   root  4096 Feb  3 14:54 maven
drwxr-x---.  4 109965  5000 4096 Feb 28  2013 protobuf
[root@corehub-001 module]# 
```
é…ç½®ç¯å¢ƒå˜é‡
```
[root@corehub-001 ~]# cd /opt/module/protobuf/
[root@corehub-001 protobuf]# pwd
/opt/module/protobuf
[root@corehub-001 protobuf]# vim /etc/profile
```
```
##PROTOBUF_HOME
export PROTOBUF_HOME=/opt/module/protobuf
export PATH=$PATH:$PROTOBUF_HOME/bin
```
```
[root@corehub-001 protobuf]# source /etc/profile
```

### ç¼–è¯‘æºç 

## 7. HDFS æ¦‚è¿°
### HDFSäº§ç”ŸèƒŒæ™¯ä»¥åŠå®šä¹‰
#### HDFSäº§ç”ŸèƒŒæ™¯
> éšç€æ•°æ®é‡è¶Šæ¥è¶Šå¤§,åœ¨ä¸€ä¸ªæ“ä½œç³»ç»Ÿå­˜ä¸ä¸‹æ‰€æœ‰çš„æ•°æ®,é‚£ä¹ˆå°±åˆ†é…åˆ°æ›´å¤šçš„æ“ä½œç³»ç»Ÿç®¡ç†çš„ç£ç›˜ä¸­,ä½†æ˜¯ä¸æ–¹ä¾¿ç®¡ç†å’Œç»´æŠ¤,è¿«åˆ‡éœ€è¦ä¸€ç§ç³»ç»Ÿæ¥ç®¡ç†å¤šå°æœºå™¨ä¸Šçš„æ–‡ä»¶,è¿™å°±æ˜¯åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†ç³»ç»Ÿ,HDFSåªæ˜¯åˆ†å¸ƒå¼æ–‡ä»¶ç®¡ç†ç³»ç»Ÿä¸­çš„ä¸€ç§.

#### HDFSå®šä¹‰
> HDFS(Hadoop Distributed File System) å®ƒæ˜¯ä¸€ä¸ªæ–‡ä»¶ç³»ç»Ÿ,ç”¨äºå­˜å‚¨æ–‡ä»¶,é€šè¿‡ç›®å½•æ ‘æ¥å®šä½æ–‡ä»¶,å…¶æ¬¡,å®ƒæ˜¯åˆ†å¸ƒå¼çš„,ç”±å¾ˆå¤šæœåŠ¡å™¨è”åˆèµ·æ¥å®ç°å…¶åŠŸèƒ½,é›†ç¾¤ä¸­çš„æœåŠ¡å™¨æœ‰å„è‡ªçš„è§’è‰².
> 
> HDFSä½¿ç”¨åœºæ™¯: é€‚åˆä¸€æ¬¡å†™å…¥,å¤šæ¬¡è¯»å–çš„åœºæ™¯,ä¸”ä¸æ”¯æŒæ–‡ä»¶çš„ä¿®æ”¹,é€‚åˆç”¨æ¥åšæ•°æ®åˆ†æ,å¹¶ä¸é€‚åˆç”¨æ¥åšç½‘ç›˜åº”ç”¨.

#### HDFSä¼˜ç‚¹ç¼ºç‚¹ | æŠ€æœ¯é€‰å‹çŸ¥è¯†ç‚¹
> **`ä¼˜ç‚¹`**
1.`é«˜å®¹é”™æ€§`:æ•°æ®è‡ªåŠ¨ä¿å­˜å¤šä¸ªå‰¯æœ¬,å®ƒé€šè¿‡å¢åŠ å‰¯æœ¬çš„å½¢å¼,æä¾›å®¹é”™æ€§.æŸä¸€ä¸ªå‰¯æœ¬ä¸¢å¤±ä»¥å,å®ƒå¯ä»¥è‡ªåŠ¨æ¢å¤.
2.`é€‚åˆå¤„ç†å¤§æ•°æ®`:
`æ•°æ®è§„æ¨¡`:èƒ½å¤Ÿå¤„ç†æ•°æ®è§„æ¨¡è¾¾åˆ°GB,TB,ç”šè‡³PBçº§åˆ«æ•°æ®.
`æ–‡ä»¶è§„æ¨¡`:èƒ½å¤Ÿå¤„ç†ç™¾ä¸‡è§„æ¨¡ä»¥ä¸Šçš„æ–‡ä»¶æ•°é‡,æ•°é‡ç›¸å½“ä¹‹å¤§.
3.`å¯æ„å»ºåˆ°å»‰ä»·æœºå™¨ä¸Š`,é€šè¿‡å¤šä¸ªå‰¯æœ¬æœºåˆ¶,æé«˜å¯é æ€§.

> **`ç¼ºç‚¹`**
1.`ä¸é€‚åˆä½å»¶æ—¶æ•°æ®è®¿é—®`,æ¯”å¦‚æ¯«ç§’çº§çš„å­˜å‚¨æ•°æ®,æ˜¯åšä¸åˆ°çš„.
2.`æ— æ³•é«˜æ•ˆçš„å¯¹å¤§é‡çš„å°æ–‡ä»¶è¿›è¡Œå­˜å‚¨`:å­˜å‚¨å¤§é‡å°æ–‡ä»¶çš„è¯,å®ƒä¼šå ç”¨NameNodeå¤§é‡çš„å†…å­˜æ¥å­˜å‚¨æ–‡ä»¶ç›®å½•å’Œå—ä¿¡æ¯,è¿™æ ·æ˜¯ä¸å¯å–çš„,å› ä¸ºNameNodeçš„å†…å­˜æ€»æ˜¯æœ‰é™çš„.å°æ–‡ä»¶å­˜å‚¨çš„å¯»å€æ—¶é—´ä¼šè¶…è¿‡è¯»å–æ—¶é—´,å®ƒè¿åäº†HDFSè®¾è®¡ç›®æ ‡
3.`ä¸æ”¯æŒå¹¶å‘å†™å…¥`,æ–‡ä»¶éšæœºä¿®æ”¹.
4.`ä»…æ”¯æŒæ•°æ®çš„è¿½åŠ `,ä¸æ”¯æŒæ–‡ä»¶çš„éšæœºä¿®æ”¹.

#### HDFSæ¶æ„ç»„æˆ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_015.jpg)

##### 1.NameNode(nn):
> Masert,å®ƒæ˜¯ä¸€ä¸ªä¸»ç®¡,ç®¡ç†è€….
> ç®¡ç†HDFSåç§°ç©ºé—´,é…ç½®å‰¯æœ¬ç­–ç•¥,ç®¡ç†æ•°æ®å—(Block)æ˜ å°„ä¿¡æ¯,å¤„ç†å®¢æˆ·ç«¯è¯»å†™è¯·æ±‚.

##### 2.DataNode(dn):
> Slave,NameNodeä¸‹è¾¾å‘½ä»¤,DataNodeæ‰§è¡Œå®é™…æ“ä½œ.
> å­˜å‚¨å®é™…æ•°æ®å—,æ‰§è¡Œæ•°æ®å—çš„è¯»å†™æ“ä½œ.

##### 3.Clientå®¢æˆ·ç«¯:
> æ–‡ä»¶åˆ‡åˆ†,æ–‡ä»¶ä¸Šä¼ HDFSæ—¶,Clientå°†æ–‡ä»¶åˆ‡åˆ†æˆä¸€ä¸ªä¸€ä¸ªçš„Block,ç„¶ååœ¨è¿›è¡Œä¸Šä¼ .
> ä¸NameNodeäº¤äº’,è·å–æ–‡ä»¶çš„ä½ç½®ä¿¡æ¯.
> ä¸DataNodeäº¤äº’,è¯»å–æˆ–å†™å…¥æ•°æ®.
> Clientæä¾›ä¸€äº›å‘½ä»¤æ¥ç®¡ç†HDFS,æ¯”å¦‚NameNodeæ ¼å¼åŒ–.
> Clientå¯ä»¥æä¾›ä¸€äº›å‘½ä»¤æ¥è®¿é—®HDFS,æ¯”å¦‚å¯¹HDFSå¢åˆ æ”¹æŸ¥(CURD)æ“ä½œ.

##### 4.SecondaryNameNode:
> å¹¶éNameNodeçš„çƒ­å¤‡,å½“NameNodeæŒ‚æ‰æ—¶,å®ƒå¹¶ä¸èƒ½é©¬ä¸Šæ›¿æ¢NameNodeå¹¶æä¾›æœåŠ¡.
> è¾…åŠ©NameNode,åˆ†æ‹…å…¶å·¥ä½œé‡,æ¯”å¦‚å®šæœŸåˆå¹¶Fsimageå’ŒEdis,å¹¶æ¨é€ç»™NameNode.
> åœ¨ç´§æ€¥æƒ…å†µä¸‹,å¯è¾…åŠ©æ¢å¤NameNode.


#### HDFSæ–‡ä»¶å—å¤§å°(é¢è¯•é‡ç‚¹)
> HDFSä¸­çš„æ–‡ä»¶åœ¨ç‰©ç†ä¸Šæ˜¯åˆ†å—å­˜å‚¨(Block),å—çš„å¤§å°å¯ä»¥é€šè¿‡é…ç½®(dfs.blocksize)å‚æ•°æ¥è§„å®š,é»˜è®¤å¤§å°åœ¨Hadoop2.xç‰ˆæœ¬ä¸­æ˜¯128M,è€ç‰ˆæœ¬1.xä¸­æ˜¯64M.
> 
> Q&A
> ä¸ºä»€ä¹ˆå—çš„å¤§å°ä¸èƒ½è®¾ç½®å¤ªå°?ä¹Ÿä¸èƒ½è®¾ç½®å¤ªå¤§?
> 
> HDFSçš„å—è®¾ç½®å¤ªå°,ä¼šå¢åŠ å¯»å€æ—¶é—´,ç¨‹åºä¸€ç›´åœ¨æ‰¾å—çš„å¼€å§‹ä½ç½®.
> 
>å¦‚æœå—è®¾ç½®çš„å¤ªå¤§,ä»ç£ç›˜ä¼ è¾“æ•°æ®çš„æ—¶é—´ä¼šæ˜æ˜¾å¤§äºå®šä½è¿™ä¸ªå—å¼€å§‹ä½ç½®æ‰€éœ€çš„æ—¶é—´,å¯¼è‡´ç¨‹åºåœ¨å¤„ç†å—æ•°æ®æ—¶ä¼šéå¸¸æ…¢.
> 
> **`HDFSå—çš„å¤§å°è®¾ç½®ä¸»è¦å–å†³äºç£ç›˜ä¼ è¾“é€Ÿç‡.`**

### 7.1 HDFS Shellæ“ä½œ(å¼€å‘é‡ç‚¹)

#### 1.åŸºæœ¬è¯­æ³•
> **`bin/hadoop fs å…·æœ‰æŒ‡ä»¤`** OR **`bin/hdfs dfs å…·ä½“æŒ‡ä»¤`**
> dfsæ˜¯fsçš„å®ç°ç±»,dfsç›¸å½“äºå­ç±»

#### 2.å¯åŠ¨é›†ç¾¤
> å¯åŠ¨001å·æœåŠ¡å™¨(å¯åŠ¨dfsæœåŠ¡)å¹¶æŸ¥çœ‹è¿›ç¨‹
``` powershell
[root@corehub-001 hadoop]# sbin/start-dfs.sh
19/02/13 22:58:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [corehub-001]
root@corehub-001's password: 
corehub-001: namenode running as process 84816. Stop it first.
root@corehub-001's password: corehub-003: datanode running as process 85244. Stop it first.
corehub-002: datanode running as process 86146. Stop it first
corehub-003: secondarynamenode running as process 101469. Stop it first.
19/02/13 22:58:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@corehub-001 hadoop]# jps
84816 NameNode
102134 Jps
101695 DataNode
[root@corehub-001 hadoop]# 
```
> å¯åŠ¨002å·æœåŠ¡å™¨(å¯åŠ¨yarnæœåŠ¡)å¹¶æŸ¥çœ‹è¿›ç¨‹
``` powershell
[root@corehub-002 hadoop]# sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-002.out
corehub-001: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
corehub-003: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-003.out
corehub-002: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-002.out
[root@corehub-002 hadoop]# jps
105555 Jps
86146 DataNode
105307 ResourceManager
105421 NodeManager
[root@corehub-002 hadoop]# 
```
> æŸ¥çœ‹003å·æœåŠ¡å™¨è¿›ç¨‹
``` powershell
[root@corehub-003 hadoop]# jps
104626 NodeManager
107159 Jps
101469 SecondaryNameNode
85244 DataNode
You have new mail in /var/spool/mail/root
[root@corehub-003 hadoop]# 
```

#### 3.hadoop fså‘½ä»¤å¤§å…¨
```
[root@corehub-001 hadoop]# hadoop fs
Usage: hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
        [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] <path> ...]
        [-cp [-f] [-p | -p[topax]] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] <path> ...]
        [-expunge]
        [-find <path> ... <expression> ...]
        [-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] <src> <localdst>]
        [-help [cmd ...]]
        [-ls [-d] [-h] [-R] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] <file>]
        [-test -[defsz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|resourcemanager:port>    specify a ResourceManager
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

[root@corehub-001 hadoop]# 
```
#### 4.å¸¸ç”¨å‘½ä»¤å®æ“
> 1. å¯åŠ¨Hadoopé›†ç¾¤
> **`sbin/start-dfs.sh`**
> **`sbin/start-yarn.sh`**
> 
> 2. -help å¸®åŠ©ä¿¡æ¯
> **`hadoop fs -help rm`**
> 
> 3. -ls æ˜¾ç¤ºç›®å½•ä¿¡æ¯
> **`hadoop fs -ls /`**
> 
> 4. -mkdir åœ¨HDFSä¸Šåˆ›å»ºç›®å½•
> **`hadoop fs -mkdir -p /group/geekparkhub`**
> 
> 5. -moveFromLocal ä»æœ¬åœ°å‰ªåˆ‡ç²˜è´´åˆ°HDFS
> touch test.txt
> **`hadoop fs -moveFromLocal ./test.txt /group/geekparkhub`**
> 
> 6. -appendToFile è¿½åŠ ä¸€ä¸ªæ–‡ä»¶åˆ°å·²å­˜åœ¨çš„æ–‡ä»¶æœ«å°¾
> touch test001.txt
> vim test001.txt
> è¾“å…¥ 123
> **`hadoop fs -appendToFile ./test001.txt /group/geekparkhub/test.txt`**
> 
> 7. -cat æ˜¾ç¤ºæ–‡ä»¶å†…å®¹
> **`hadoop fs -cat /group/geekparkhub/test.txt`**
> 
> 8. -chgrp,-chmod,-chown,linuxæ–‡ä»¶ç³»ç»Ÿä¸­ç”¨æ³•ä¸€è‡´,ä¿®æ”¹æ–‡ä»¶æ‰€å±æƒé™
> 
> 9. -copyFromLocal ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­æ‹·è´åˆ°HDFSä¸­
> **`hadoop fs -copyFromLocal test001.txt /group/geekparkhub/`**
> 
> 10. -copyToLocal ä»HDFSä¸Šæ‹·è´åˆ°æœ¬åœ°
> **`hadoop fs -copyToLocal /group/geekparkhub/test.txt ./`**
> 
> 11. -cp ä»HDFSè·¯å¾„æ‹·è´åˆ°HDFSå¦ä¸€ä¸ªè·¯å¾„
> **`hadoop fs -cp /group/geekparkhub/test.txt /user/geekparkhub/`**
> 
> 12. -mv åœ¨HDFSç›®å½•ä¸­ç§»åŠ¨æ–‡ä»¶
> **`hadoop fs -mv /group/geekparkhub/test001.txt /user/geekparkhub/`**
> 
> 13. -get ç­‰åŒäºcopyToLocal ä»HDFSä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
> **`hadoop fs -get /group/geekparkhub/test001.txt ./`**
> 
> 14. -getmerge åˆå¹¶ä¸‹è½½å¤šä¸ªæ–‡ä»¶,æ¯”å¦‚HDFSç›®å½• /log/ä¸‹æœ‰å¤šä¸ªæ–‡ä»¶æ—¥å¿—æ–‡ä»¶,log1,log3,log3
> **`hadoop fs -getmerge /user/geekparkhub/* ./list.txt`**
> 
> 15. -put ç­‰åŒäºcopyFromLocal
> **`hadoop fs -put ./list.txt /user/geekparkhub`**
> 
> 16. -tail æ˜¾ç¤ºä¸€ä¸ªæ–‡ä»¶çš„æœ«å°¾
> **`hadoop fs -tail /group/geekparkhub/test.txt`**
> 
> 17. -rm åˆ é™¤æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
> **`hadoop fs -rm /user/geekparkhub/list.txt`**
> 
> 18. -rmdir åˆ é™¤ç©ºç›®å½•
> **`hadoop fs -rmdir /user/testfile/`**
> 
> 19. -du ç»Ÿè®¡æ–‡ä»¶å¤¹çš„å¤§å°ä¿¡æ¯
> **`hadoop fs -du -s -h /`**
> 
> 20. -setrep è®¾ç½®HDFSä¸­æ–‡ä»¶çš„å‰¯æœ¬æ•°é‡
> **`hadoop fs -setrep 10 /group/geekparkhub/test.txt`**


### 7.2 HDFSå®¢æˆ·ç«¯æ“ä½œ(å¼€å‘é‡ç‚¹)
#### HDFSå®¢æˆ·ç«¯ç¯å¢ƒå‡†å¤‡ ä»¥Winç‰ˆæœ¬ ä¸ºä¾‹
##### 1.æ ¹æ®è‡ªèº«ç”µè„‘æ“ä½œç³»ç»Ÿæ‹·è´å¯¹åº”ç¼–è¯‘åçš„hadoop jaråŒ…åˆ°è‹±æ–‡è·¯å¾„
##### 2.Winç‰ˆæœ¬ é…ç½®HADOOP_HOMEç¯å¢ƒå˜é‡
```
HADOOP_HOME = D:\J2EE\Hadoop\hadoop
```
##### 3.Winç‰ˆæœ¬ é…ç½®Pathç¯å¢ƒå˜é‡
```
Path = %HADOOP_HOME%\bin
```
##### 4.JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥

##### 5.åˆ›å»ºHDFSå®¢æˆ·ç«¯
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException {
        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.åœ¨HDFSä¸­åˆ›å»ºè·¯å¾„
         * Create a path in HDFS
         */
        fs.mkdirs(new Path("/hdfstest/files"));

        /**
         * 3.å…³é—­HDFSèµ„æº
         * Turn off HDFS resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-æ‰§è¡Œç»“æŸ!");
    }
}
```
##### 6.æŸ¥çœ‹æµ‹è¯•ç»“æœ

#### HDFS APIæ“ä½œ
> å‚æ•°ä¼˜å…ˆçº§
> å‚æ•°ä¼˜å…ˆçº§æ’åº:
> 1.å®¢æˆ·ç«¯ä»£ç ä¸­è®¾ç½®çš„å€¼
> 2.ClassPathä¸‹ç”¨æˆ·è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
> 3.æœåŠ¡å™¨é»˜è®¤é…ç½®æ–‡ä»¶

##### HDFSæ–‡ä»¶ä¸Šä¼ (æµ‹è¯•)
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }
    
	/**
    * æ–‡ä»¶ä¸Šä¼ 
    * File Upload
    * @throws URISyntaxException
    * @throws IOException
    * @throws InterruptedException
    */
    @Test
    public void testCopyFromLocalFile() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.æ‰§è¡Œä¸Šä¼ API
         * Execute the upload API
         */
        fs.copyFromLocalFile(new Path("D:/J2EE/md5/rfc1321.txt"), new Path("/hdfs/client/files/rfc1321.txt"));

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-æ–‡ä»¶æ‹·è´æ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶ä¸‹è½½
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }

    /**
     * æ–‡ä»¶ä¸‹è½½
     * file download
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void testCopyToLocalFile() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.æ‰§è¡Œä¸‹è½½API
         * Execute the download API
         */
        fs.copyToLocalFile(new Path("/group/geekparkhub/input/hadoop-2.7.2.tar.gz"), new Path("F:/Demo/hadoop-2.7.2.tar.gz"));

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-æ–‡ä»¶ä¸‹è½½æˆåŠŸ-æ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶å¤¹åˆ é™¤
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }

    @Test
    public void testDelete() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"),conf,"root");

        /**
         * 2.æ‰§è¡Œåˆ é™¤API
         * Execute the download API
         */
        fs.delete(new Path("/hdfs/client/files/001"),true);

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-åˆ é™¤æˆåŠŸ-æ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶åæ›´æ”¹
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }

    /**
     * æ–‡ä»¶æ›´å
     * File rename
     *
     * @throws IOException
     * @throws URISyntaxException
     * @throws InterruptedException
     */
    @Test
    public void testReName() throws IOException, URISyntaxException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.æ‰§è¡Œä¿®æ”¹API
         * Execute the modification API
         */
        fs.rename(new Path("/hdfs/client/files/001/test/bootmgr.exe.mui"), new Path("/hdfs/client/files/001/test/hub.exe.mui"));

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-ä¿®æ”¹æˆåŠŸ-æ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶è¯¦æƒ…æŸ¥çœ‹
> æŸ¥çœ‹æ–‡ä»¶åç§°,æƒé™,é•¿åº¦,å—ä¿¡æ¯
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }

    /**
     * æŸ¥çœ‹æ–‡ä»¶è¯¦æƒ…
     * View file details
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void testListFile() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.æ‰§è¡ŒæŸ¥çœ‹API
         * Execute view API
         */
        RemoteIterator<LocatedFileStatus> iterator = fs.listFiles(new Path("/"), true);

        while (iterator.hasNext()) {
            LocatedFileStatus fileStatus = iterator.next();
            /**
             * æŸ¥çœ‹æ–‡ä»¶åç§°,æƒé™,é•¿åº¦,å—ä¿¡æ¯
             */
            // æ–‡ä»¶åç§°
            log.info("æ–‡ä»¶åç§°ï¼š" + fileStatus.getPath().getName());
            // æ–‡ä»¶æƒé™
            log.info("æ–‡ä»¶æƒé™ï¼š" + fileStatus.getPermission());
            // æ–‡ä»¶é•¿åº¦
            log.info("æ–‡ä»¶é•¿åº¦ï¼š" + fileStatus.getLen());
            // æ–‡ä»¶å—ä¿¡æ¯
            BlockLocation[] blockLocation = fileStatus.getBlockLocations();
            for (BlockLocation blockLocations : blockLocation) {
                String[] hosts = blockLocations.getHosts();
                for (String host : hosts) {
                    log.info("å—ä¿¡æ¯ï¼š" + host);
                }
                log.info("-------------------------------");
            }
        }

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-æ–‡ä»¶æŸ¥çœ‹æˆåŠŸ-æ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ¤æ–­
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS å®¢æˆ·ç«¯
 */
public class HdfsClient {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HdfsClient.class);

    public static void main(String[] args) {
    }

    /**
     * åˆ¤æ–­æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void testListStatus() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.æ‰§è¡Œåˆ¤æ–­API
         * Execution judgment API
         */
        FileStatus[] statuses = fs.listStatus(new Path("/"));
        for (FileStatus status : statuses) {
            if (status.isFile()) {
                // æ–‡ä»¶
                log.info("Fileï¼š" + status.getPath().getName());
            } else {
                // æ–‡ä»¶å¤¹
                log.info("File Directoryï¼š" + status.getPath().getName());
            }
        }

        /**
         * 3.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        fs.close();

        /**
         * 4.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æµ‹è¯•ç¨‹åº-æ–‡ä»¶åˆ¤æ–­æˆåŠŸ-æ‰§è¡Œç»“æŸ!");
    }
}
```
#### HDFS I/Oæµæ“ä½œ
> å¦‚è‡ªè¡Œå®ç°ä¸Šè¿°APIæ“ä½œ,å¯ä»¥é‡‡ç”¨I/Oæµæ–¹å¼å®ç°æ•°æ®ä¸Šä¼ ä¸‹è½½
##### HDFSæ–‡ä»¶ä¸Šä¼ 
> éœ€æ±‚: å°†C:\Windows\Web\4K\Wallpaper\Windows\img0_3840x2160.jpgæ–‡ä»¶ä¸Šä¼ åˆ°HDFSç›®å½•ä¸­
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS IO
 */
public class HDFSIO {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HDFSIO.class);

    /**
     * å°†C:\Windows\Web\4K\Wallpaper\Windows\img0_3840x2160.jpgæ–‡ä»¶ä¸Šä¼ åˆ°HDFSç›®å½•ä¸­
     * Upload the C:\Windows\Web\4 K\Wallpaper\Windows\img 0_3840 x 2160.jpg file to the HDFS directory
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void putFileToHDFS() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.è·å–è¾“å…¥æµ
         * Get the input stream
         */
        FileInputStream fileInputStream = new FileInputStream(new File("C:/Windows/Web/4K/Wallpaper/Windows/img0_3840x2160.jpg"));

        /**
         * 3.è·å–è¾“å‡ºæµ
         * Get the output stream
         */
        FSDataOutputStream fsDataOutputStream = fs.create(new Path("/hdfs/client/files/img0_3840x2160.jpg"));

        /**
         * 4.æµæ•°æ®å¯¹æ‹·
         * Stream data copy
         */
        IOUtils.copyBytes(fileInputStream, fsDataOutputStream, conf);

        /**
         * 5.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        IOUtils.closeStream(fsDataOutputStream);
        IOUtils.closeStream(fileInputStream);
        fs.close();

        /**
         * 5.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ-ç¨‹åºæ‰§è¡Œç»“æŸ!");
    }
}
```
##### HDFSæ–‡ä»¶ä¸‹è½½
> éœ€æ±‚: ä»HDFSä¸Šä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ç›˜ç¬¦ä¸‹
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS IO
 */
public class HDFSIO {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HDFSIO.class);

    /**
     * ä»HDFSä¸Šä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ç›˜ç¬¦ä¸‹
     * Download files from HDFS to local drive letter
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void getFileFromHDFS() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.è·å–è¾“å…¥æµ
         * Get the input stream
         */
        FSDataInputStream fsDataInputStream = fs.open(new Path("/hdfs/client/files/img0_3840x2160.jpg"));

        /**
         * 3.è·å–è¾“å‡ºæµ
         * Get the output stream
         */
        FileOutputStream fileInputStream = new FileOutputStream(new File("d:/Downloads/img0_3840x2160.jpg"));

        /**
         * 4.æµæ•°æ®å¯¹æ‹·
         * Stream data copy
         */
        IOUtils.copyBytes(fsDataInputStream, fileInputStream, conf);

        /**
         * 5.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        IOUtils.closeStream(fsDataInputStream);
        IOUtils.closeStream(fileInputStream);
        fs.close();

        /**
         * 5.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("æ–‡ä»¶ä¸‹è½½æˆåŠŸ-ç¨‹åºæ‰§è¡Œç»“æŸ!");
    }
}
```
##### å®šä½æ–‡ä»¶è¯»å–
> éœ€æ±‚: åˆ†å—è¯»å–HDFSä¸Šçš„å¤§æ–‡ä»¶,æ¯”å¦‚HDFSç›®å½•ä¸‹çš„hadoop-2.7.2.tar.gz
``` java
package com.geekparkhub.hdfs;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.log4j.Logger;
import org.junit.Test;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * HDFS IO
 */
public class HDFSIO {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(HDFSIO.class);

    /**
     * ç¬¬ä¸€å—æ–‡ä»¶ ä¸‹è½½
     * First file download
     *
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void readFileSeek1() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.è·å–è¾“å…¥æµ
         * Get the input stream
         */
        FSDataInputStream fsDataInputStream = fs.open(new Path("/user/geekparkhub/input/hadoop-2.7.2.tar.gz"));

        /**
         * 3.è·å–è¾“å‡ºæµ
         * Get the output stream
         */
        FileOutputStream fileInputStream = new FileOutputStream(new File("d:/Downloads/hadoop-2.7.2.tar.gz.part1"));

        /**
         * 4.æµæ•°æ®å¯¹æ‹· åªæ‹·è´å•å—128M
         * Stream data copy Copy only a single block of 128 M
         */
        byte[] bytes = new byte[1024];
        for (int i = 0; i < 1024 * 128; i++) {
            fsDataInputStream.read(bytes);
            fileInputStream.write(bytes);
        }

        /**
         * 5.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        IOUtils.closeStream(fsDataInputStream);
        IOUtils.closeStream(fileInputStream);
        fs.close();

        /**
         * 5.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("ç¬¬ä¸€å—æ–‡ä»¶ä¸‹è½½æˆåŠŸ-ç¨‹åºæ‰§è¡Œç»“æŸ!");
    }

    /**
     * ç¬¬äºŒå—æ–‡ä»¶ ä¸‹è½½
     * Second file download
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    @Test
    public void readFileSeek2() throws URISyntaxException, IOException, InterruptedException {

        /**
         * 1.è·å–HDFSå®¢æˆ·ç«¯å®ä¾‹
         *  Obtain an HDFS client instance
         */
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(new URI("hdfs://corehub-001:9000"), conf, "root");

        /**
         * 2.è·å–è¾“å…¥æµ
         * Get the input stream
         */
        FSDataInputStream fsDataInputStream = fs.open(new Path("/user/geekparkhub/input/hadoop-2.7.2.tar.gz"));

        /**
         * 3.è®¾ç½®æŒ‡å®šè¯»å–èµ·ç‚¹
         * Set the specified reading start point
         */
        fsDataInputStream.seek(1024 * 1024 * 128);

        /**
         * 4.è·å–è¾“å‡ºæµ
         * Get the output stream
         */
        FileOutputStream fileInputStream = new FileOutputStream(new File("d:/Downloads/hadoop-2.7.2.tar.gz.part2"));

        /**
         * 5.æµæ•°æ®å¯¹æ‹·
         * Stream data copy
         */
        IOUtils.copyBytes(fsDataInputStream, fileInputStream, conf);

        /**
         * 6.å…³é—­æ•°æ®èµ„æº
         * Close data resources
         */
        IOUtils.closeStream(fsDataInputStream);
        IOUtils.closeStream(fileInputStream);
        fs.close();

        /**
         * 7.æ—¥å¿—æ‰“å°
         * Log printing
         */
        log.info("ç¬¬äºŒå—æ–‡ä»¶ä¸‹è½½æˆåŠŸ-ç¨‹åºæ‰§è¡Œç»“æŸ!");
    }
}
```
ä½¿ç”¨winæŒ‡ä»¤,å°†part1+part2 æ•°æ®æ‹¼æ¥
``` powershell
D:\Downloads>type hadoop-2.7.2.tar.gz.part2 >> hadoop-2.7.2.tar.gz.part1
```
            
### 7.3 HDFSæ•°æ®æµ(é¢è¯•é‡ç‚¹)
#### HDFSå†™æ•°æ®æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_016.jpg)
##### å‰–ææ–‡ä»¶å†™å…¥
> 1.å®¢æˆ·ç«¯é€šè¿‡Distributed File Systemæ¨¡å—å‘NameNodeè¯·æ±‚ä¸Šä¼ æ–‡ä»¶,NameNodeæ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨,çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨.
> 2.NameNodeè¿”å›æ˜¯å¦å¯ä»¥ä¸Šä¼ .
> 3.å®¢æˆ·ç«¯è¯·æ±‚ç¬¬ä¸€ä¸ªblockä¸Šä¼ åˆ°å“ªä¸ªDatanodeæœåŠ¡å™¨.
> 4.NameNodeè¿”å›3ä¸ªDataNodeèŠ‚ç‚¹,åˆ†åˆ«é—®dn1,dn2,dn3.
> 5.å®¢æˆ·ç«¯é€šè¿‡FSDataOutputStreamæ¨¡å—è¯·æ±‚dh1ä¸Šä¼ æ•°æ®,dn1æ”¶åˆ°è¯·æ±‚ä¼šç»§ç»­è°ƒç”¨dn2,ç„¶ådn2è°ƒç”¨dn3,å°†è¿™ä¸ªé€šä¿¡ç®¡é“å»ºç«‹å®Œæˆ.
> 6.dn1,dn2,dn3é€çº§åº”ç­”å®¢æˆ·ç«¯.
> 7.å®¢æˆ·ç«¯å¼€å§‹æƒ³dn1ä¸Šä¼ ç¬¬ä¸€ä¸ªblock(å…ˆä»ç£ç›˜è¯»å–æ•°æ®æ”¾åˆ°ä¸€ä¸ªæœ¬åœ°å†…å­˜ç¼“å­˜),ä»¥packetä¸ºå•ä½,dn1æ”¶åˆ°ä¸€ä¸ªpacketå°±ä¼šä¼ ç»™dn2,dn2ä¼ ç»™dn3,dn1æ¯ä¼ ä¸€ä¸ªpacketä¼šæ”¾å…¥ä¸€ä¸ªåº”ç­”é˜Ÿåˆ—ç­‰å¾…åº”ç­”.
> 8.å½“ä¸€ä¸ªblockä¼ è¾“å®Œæˆä¹‹å,å®¢æˆ·ç«¯å†æ¬¡è¯·æ±‚NameNodeä¸Šä¼ ç¬¬äºŒä¸ªblockçš„æœåŠ¡å™¨(é‡å¤æ‰§è¡Œ3-7æ­¥éª¤).

#### HDFSè¯»æ•°æ®æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_018.jpg)
> 1.å®¢æˆ·ç«¯é€šè¿‡Distributed File Systemå‘NameNodeè¯·æ±‚ä¸‹è½½æ–‡ä»¶,NameNodeé€šè¿‡æŸ¥è¯¢å…ƒæ•°æ®,æ‰¾åˆ°æ–‡ä»¶å—æ‰€åœ¨çš„DataNodeåœ°å€.
> 2.æŒ‘é€‰ä¸€å°DataNode(å°±è¿‘åŸåˆ™,ç„¶åéšæœº)æœåŠ¡å™¨,è¯·æ±‚è¯»å–æ•°æ®.
> 3.DataNodeå¼€å§‹ä¼ è¾“æ•°æ®ç»™å®¢æˆ·ç«¯(ä»ç£ç›˜é‡Œé¢è¯»å–æ•°æ®è¾“å…¥æµ,ä»¥packetä¸ºå•ä½æ¥åšæ ¡éªŒ).
> 4.å®¢æˆ·ç«¯ä»¥packetä¸ºå•ä½æ¥æ”¶,å…ˆåœ¨æœ¬åœ°ç¼“å­˜,ç„¶åå†™å…¥ç›®æ ‡æ–‡ä»¶.

##### ç½‘ç»œæ‹“å±•-èŠ‚ç‚¹è·ç¦»è®¡ç®—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_017.jpg)
> åœ¨HDFSå†™æ•°æ®çš„è¿‡ç¨‹ä¸­,NameNodeä¼šé€‰æ‹©è·ç¦»å¾…ä¸Šä¼ æ•°æ®æœ€è¿‘è·ç¦»çš„DataNodeæ¥æ”¶æ•°æ®,é‚£ä¹ˆè¿™ä¸ªæœ€è¿‘è·ç¦»æ€ä¹ˆè®¡ç®—?
> èŠ‚ç‚¹è·ç¦»:ä¸¤ä¸ªèŠ‚ç‚¹åˆ°è¾¾æœ€è¿‘çš„å…±åŒç¥–å…ˆçš„è·ç¦»æ€»å’Œ.

### 7.4 NameNodeå’ŒSecondayNameNodeå·¥ä½œæœºåˆ¶(é¢è¯•é‡ç‚¹)
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_019.jpg)

> ç¬¬ä¸€é˜¶æ®µï¼šNameNodeå¯åŠ¨
> (1) ç¬¬ä¸€æ¬¡å¯åŠ¨ NameNodeæ ¼å¼åŒ–å,åˆ›å»º fsimage å’Œ edits æ–‡ä»¶,å¦‚æœä¸æ˜¯ç¬¬ä¸€æ¬¡å¯åŠ¨,ç›´æ¥åŠ è½½ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶åˆ°å†…å­˜.
> (2) å®¢æˆ·ç«¯å¯¹å…ƒæ•°æ®è¿›è¡Œå¢åˆ æ”¹çš„è¯·æ±‚.
> (3) NameNode è®°å½•æ“ä½œæ—¥å¿—ï¼Œæ›´æ–°æ»šåŠ¨æ—¥å¿—.
> (4) NameNode åœ¨å†…å­˜ä¸­å¯¹æ•°æ®è¿›è¡Œå¢åˆ æ”¹æŸ¥.
> 
> ç¬¬äºŒé˜¶æ®µï¼šSecondary NameNodeå·¥ä½œ
> (1) Secondary NameNodeè¯¢é—®NameNode æ˜¯å¦éœ€è¦ checkpoint,ç›´æ¥å¸¦å› NameNodeæ˜¯å¦æ£€æŸ¥ç»“æœ.
> (2) Secondary NameNode è¯·æ±‚æ‰§è¡Œ checkpoint.
> (3) NameNode æ»šåŠ¨æ­£åœ¨å†™çš„ edits æ—¥å¿—.
> (4) å°†æ»šåŠ¨å‰çš„ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶æ‹·è´åˆ° Secondary NameNode.
> (5) Secondary NameNode åŠ è½½ç¼–è¾‘æ—¥å¿—å’Œé•œåƒæ–‡ä»¶åˆ°å†…å­˜ï¼Œå¹¶åˆå¹¶.
> (6) ç”Ÿæˆæ–°çš„é•œåƒæ–‡ä»¶ fsimage.chkpoint.
> (7) æ‹·è´ fsimage.chkpoint åˆ° NameNode.
> (8) NameNode å°† fsimage.chkpoint é‡æ–°å‘½åæˆ fsimage.

#### Fsimageå’ŒEditsè§£æ
> 1. æ¦‚å¿µ
> namenodeè¢«æ ¼å¼åŒ–ä¹‹å,å°†åœ¨/opt/module/hadoop/data/tmp/dfs/name/currentç›®å½•ä¸­äº§ç”Ÿå¦‚ä¸‹æ–‡ä»¶
``` powershell
[root@corehub-001 hadoop]# cd data/tmp/dfs/name/current/
[root@corehub-001 current]# ll
total 3120
-rw-r--r--. 1 root root 1048576 Feb 17 02:14 edits_0000000000000000001-0000000000000000040
-rw-r--r--. 1 root root    1335 Feb 17 19:40 edits_0000000000000000041-0000000000000000061
-rw-r--r--. 1 root root      42 Feb 17 20:40 edits_0000000000000000062-0000000000000000063
-rw-r--r--. 1 root root     280 Feb 17 21:40 edits_0000000000000000064-0000000000000000068
-rw-r--r--. 1 root root      42 Feb 17 22:40 edits_0000000000000000069-0000000000000000070
-rw-r--r--. 1 root root      42 Feb 17 23:40 edits_0000000000000000071-0000000000000000072
-rw-r--r--. 1 root root 1048576 Feb 17 23:40 edits_0000000000000000073-0000000000000000073
-rw-r--r--. 1 root root      42 Feb 19 19:19 edits_0000000000000000074-0000000000000000075
-rw-r--r--. 1 root root 1048576 Feb 19 19:19 edits_inprogress_0000000000000000076
-rw-r--r--. 1 root root    1361 Feb 19 19:18 fsimage_0000000000000000073
-rw-r--r--. 1 root root      62 Feb 19 19:18 fsimage_0000000000000000073.md5
-rw-r--r--. 1 root root    1361 Feb 19 19:19 fsimage_0000000000000000075
-rw-r--r--. 1 root root      62 Feb 19 19:19 fsimage_0000000000000000075.md5
-rw-r--r--. 1 root root       3 Feb 19 19:19 seen_txid
-rw-r--r--. 1 root root     207 Feb 19 19:18 VERSION
```
> 1.Fsimage æ–‡ä»¶ï¼šHDFSæ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®çš„ä¸€ä¸ªæ°¸ä¹…æ€§çš„æ£€æŸ¥ç‚¹,å…¶ä¸­åŒ…å«HDFSæ–‡ä»¶ç³»ç»Ÿçš„æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶idnodeçš„åºåˆ—åŒ–ä¿¡æ¯.
> 2.Edits æ–‡ä»¶ï¼šå­˜æ”¾HDFSæ–‡ä»¶ç³»ç»Ÿçš„æ‰€æœ‰æ›´æ–°æ“ä½œçš„è·¯å¾„,æ–‡ä»¶ç³»ç»Ÿå®¢æˆ·ç«¯æ‰§è¡Œçš„æ‰€æœ‰å†™æ“ä½œé¦–å…ˆä¼šè¢«è®°å½•åˆ°editsæ–‡ä»¶ä¸­.
> 3.seen_txidæ–‡ä»¶ä¿å­˜çš„æ˜¯ä¸€ä¸ªæ•°å­—,å°±æ˜¯æœ€åä¸€ä¸ª edits_çš„æ•°å­—.
> 4.æ¯æ¬¡ NameNodeå¯åŠ¨çš„æ—¶å€™éƒ½ä¼šå°† fsimage æ–‡ä»¶è¯»å…¥å†…å­˜,å¹¶ä» 00001 å¼€å§‹åˆ°seen_txid ä¸­è®°å½•çš„æ•°å­—ä¾æ¬¡æ‰§è¡Œæ¯ä¸ª edits é‡Œé¢çš„æ›´æ–°æ“ä½œ,ä¿è¯å†…å­˜ä¸­çš„å…ƒæ•°æ®ä¿¡æ¯æ˜¯æœ€æ–°çš„ã€åŒæ­¥çš„,å¯ä»¥çœ‹æˆ NameNode å¯åŠ¨çš„æ—¶å€™å°±å°† fsimage å’Œ edits æ–‡ä»¶è¿›è¡Œäº†åˆå¹¶.
> 
> 2 oivæŒ‡ä»¤ æŸ¥çœ‹ fsimage æ–‡ä»¶
> 
> 1.æŸ¥çœ‹ oiv å’Œ oev å‘½ä»¤
```
[root@corehub-001 current]$ hdfs
oiv apply the offline fsimage viewer to an fsimage
oev apply the offline edits viewer to an edits file
```
> 2.åŸºæœ¬è¯­æ³•
> hdfs oiv -p æ–‡ä»¶ç±»å‹ -i é•œåƒæ–‡ä»¶ -o è½¬æ¢åæ–‡ä»¶è¾“å‡ºè·¯å¾„
> 
> 3.æ¡ˆä¾‹å®æ“
``` powershell
[root@corehub-001 current]# ll
total 3120
-rw-r--r--. 1 root root 1048576 Feb 17 02:14 edits_0000000000000000001-0000000000000000040
-rw-r--r--. 1 root root    1335 Feb 17 19:40 edits_0000000000000000041-0000000000000000061
-rw-r--r--. 1 root root      42 Feb 17 20:40 edits_0000000000000000062-0000000000000000063
-rw-r--r--. 1 root root     280 Feb 17 21:40 edits_0000000000000000064-0000000000000000068
-rw-r--r--. 1 root root      42 Feb 17 22:40 edits_0000000000000000069-0000000000000000070
-rw-r--r--. 1 root root      42 Feb 17 23:40 edits_0000000000000000071-0000000000000000072
-rw-r--r--. 1 root root 1048576 Feb 17 23:40 edits_0000000000000000073-0000000000000000073
-rw-r--r--. 1 root root      42 Feb 19 19:19 edits_0000000000000000074-0000000000000000075
-rw-r--r--. 1 root root 1048576 Feb 19 19:19 edits_inprogress_0000000000000000076
-rw-r--r--. 1 root root    1361 Feb 19 19:18 fsimage_0000000000000000073
-rw-r--r--. 1 root root      62 Feb 19 19:18 fsimage_0000000000000000073.md5
-rw-r--r--. 1 root root    1361 Feb 19 19:19 fsimage_0000000000000000075
-rw-r--r--. 1 root root      62 Feb 19 19:19 fsimage_0000000000000000075.md5
-rw-r--r--. 1 root root       3 Feb 19 19:19 seen_txid
-rw-r--r--. 1 root root     207 Feb 19 19:18 VERSION
[root@corehub-001 current]# hdfs oiv -p XML -i fsimage_0000000000000000073 -o fs-073.xml
```
> å°†æ˜¾ç¤ºçš„ xml æ–‡ä»¶å†…å®¹æ‹·è´åˆ° eclipse ä¸­åˆ›å»ºçš„ xml æ–‡ä»¶ä¸­,å¹¶æ ¼å¼åŒ–ã€‚éƒ¨åˆ†æ˜¾ç¤ºç»“æœå¦‚ä¸‹.
``` xml
<?xml version="1.0"?>
<fsimage>
    <NameSection>
        <genstampV1>1000</genstampV1>
        <genstampV2>1010</genstampV2>
        <genstampV1Limit>0</genstampV1Limit>
        <lastAllocatedBlockId>1073741834</lastAllocatedBlockId>
        <txid>73</txid>
    </NameSection>
    <INodeSection>
        <lastInodeId>16401</lastInodeId>
        <inode>
            <id>16385</id>
            <type>DIRECTORY</type>
            <name></name>
            <mtime>1550340875095</mtime>
            <permission>root:supergroup:rwxr-xr-x</permission>
            <nsquota>9223372036854775807</nsquota>
            <dsquota>-1</dsquota>
        </inode>
        <inode>
            <id>16386</id>
            <type>DIRECTORY</type>
            <name>user</name>
            <mtime>1550339935918</mtime>
            <permission>root:supergroup:rwxr-xr-x</permission>
            <nsquota>-1</nsquota>
            <dsquota>-1</dsquota>
        </inode>
        <inode>
            <id>16393</id>
            <type>FILE</type>
            <name>wc.input</name>
            <replication>3</replication>
            <mtime>1550340172723</mtime>
            <atime>1550340172398</atime>
            <perferredBlockSize>134217728</perferredBlockSize>
            <permission>root:supergroup:rw-r--r--</permission>
            <blocks>
                <block>
                    <id>1073741826</id>
                    <genstamp>1002</genstamp>
                    <numBytes>196</numBytes>
                </block>
            </blocks>
        </inode>
        <inode>
            <id>16396</id>
            <type>DIRECTORY</type>
            <name>hdfs</name>
            <mtime>1550340875095</mtime>
            <permission>root:supergroup:rwxr-xr-x</permission>
            <nsquota>-1</nsquota>
            <dsquota>-1</dsquota>
        </inode>
    </INodeSection>
    <INodeReferenceSection></INodeReferenceSection>
    <SnapshotSection>
        <snapshotCounter>0</snapshotCounter>
    </SnapshotSection>
    <INodeDirectorySection>
        <directory>
            <parent>16396</parent>
            <inode>16397</inode>
        </directory>
        <directory>
            <parent>16397</parent>
            <inode>16398</inode>
        </directory>
        <directory>
            <parent>16398</parent>
            <inode>16400</inode>
        </directory>
    </INodeDirectorySection>
    <FileUnderConstructionSection></FileUnderConstructionSection>
    <SnapshotDiffSection>
        <diff>
            <inodeid>16385</inodeid>
        </diff>
    </SnapshotDiffSection>
    <SecretManagerSection>
        <currentId>0</currentId>
        <tokenSequenceNumber>0</tokenSequenceNumber>
    </SecretManagerSection>
</fsimage>
```
> 3 oev æŸ¥çœ‹ edits æ–‡ä»¶
> 
> 1.åŸºæœ¬è¯­æ³•
> hdfs oev -p æ–‡ä»¶ç±»å‹ -i ç¼–è¾‘æ—¥å¿— -o è½¬æ¢åæ–‡ä»¶è¾“å‡ºè·¯å¾„
> 
> 2.æ¡ˆä¾‹å®æ“
``` powershell
[root@corehub-001 current]# hdfs oev -p XML -i edits_0000000000000000073-0000000000000000073 -o edits-073.xml
```
> å°†æ˜¾ç¤ºçš„ xml æ–‡ä»¶å†…å®¹æ‹·è´åˆ° eclipse ä¸­åˆ›å»ºçš„ xml æ–‡ä»¶ä¸­,å¹¶æ ¼å¼åŒ–,æ˜¾ç¤ºç»“æœå¦‚ä¸‹.
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<EDITS>
  <EDITS_VERSION>-63</EDITS_VERSION>
  <RECORD>
    <OPCODE>OP_START_LOG_SEGMENT</OPCODE>
    <DATA>
      <TXID>73</TXID>
    </DATA>
  </RECORD>
</EDITS>
```
#### checkpoint æ—¶é—´è®¾ç½®
> 1.é€šå¸¸æƒ…å†µä¸‹,SecondaryNameNodeæ¯éš”ä¸€å°æ—¶æ‰§è¡Œ
> hdfs-default.xml
> ``` xml
> <property>
>  <name>dfs.namenode.checkpoint.period</name>
>  <value>3600</value>
> </property>
> ```
> 2.ä¸€åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°,å½“æ“ä½œæ¬¡æ•°è¾¾åˆ°1ç™¾ä¸‡æ—¶,SecondaryNameNodeæ‰§è¡Œä¸€æ¬¡
> ``` xml
> <property>
>  <name>dfs.namenode.checkpoint.txns</name>
>  <value>1000000</value>
> <description>æ“ä½œåŠ¨ä½œæ¬¡æ•°</description>
> </property>
> <property>
>  <name>dfs.namenode.checkpoint.check.period</name>
>  <value>60</value>
> <description> 1 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ“ä½œæ¬¡æ•°</description>
> </property>
> ```
#### NameNode æ•…éšœå¤„ç†
> NameNode æ•…éšœåï¼Œå¯ä»¥é‡‡ç”¨å¦‚ä¸‹ä¸¤ç§æ–¹æ³•æ¢å¤æ•°æ®
> 
> æ–¹æ³•ä¸€ï¼šå°† SecondaryNameNodeä¸­æ•°æ®æ‹·è´åˆ°NameNodeå­˜å‚¨æ•°æ®çš„ç›®å½•.
> 1.kill -9 namenodeè¿›ç¨‹
> 
> 2.åˆ é™¤NameNodeå­˜å‚¨çš„æ•°æ®(/opt/module/hadoop/data/tmp/dfs/name)
``` powershell
[root@corehub-001 hadoop]$ rm -rf 
/opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
```
> 3.æ‹·è´SecondaryNameNodeä¸­æ•°æ®åˆ°åŸNameNodeå­˜å‚¨æ•°æ®ç›®å½•
``` powershell
[root@corehub-001 name]$ scp -r root@corehub-003:/opt/module/hadoop/data/tmp/dfs/namesecondary/* ./
```
> 4.é‡æ–°å¯åŠ¨namenode
``` powershell
[root@corehub-001 hadoop]$ sbin/hadoop-daemon.sh start namenode
```

> æ–¹ æ³• äºŒ ï¼š ä½¿ ç”¨ -importCheckpointé€‰é¡¹å¯åŠ¨NameNodeå®ˆæŠ¤è¿›ç¨‹,ä»è€Œå°†SecondaryNameNodeä¸­æ•°æ®æ‹·è´åˆ° NameNodeç›®å½•ä¸­
> 1.ä¿®æ”¹ hdfs-site.xml ä¸­çš„é…ç½®ä¿¡æ¯
``` xml
<property>
 <name>dfs.namenode.checkpoint.period</name>
 <value>120</value>
</property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/opt/module/hadoop-2.7.2/data/tmp/dfs/name</value>
</property>
```
> 2.kill -9 namenode è¿›ç¨‹
> 3.åˆ é™¤ NameNodeå­˜å‚¨çš„æ•°æ®(/opt/module/hadoop/data/tmp/dfs/name)
```
[root@corehub-001 hadoop]$ rm -rf /opt/module/hadoop/data/tmp/dfs/name/*
```
> 4.å¦‚æœ SecondaryNameNodeä¸å’Œ NameNodeåœ¨ä¸€ä¸ªä¸»æœºèŠ‚ç‚¹ä¸Š,éœ€è¦å°†SecondaryNameNodeå­˜å‚¨æ•°æ®çš„ç›®å½•æ‹·è´åˆ° NameNodeå­˜å‚¨æ•°æ®çš„å¹³çº§ç›®å½•å¹¶åˆ é™¤in_use.lock æ–‡ä»¶.
``` powershell
[root@corehub-001 dfs]$ scp -r root@corehub-003:/opt/module/hadoop/data/tmp/dfs/namesecondary ./
[root@corehub-001 namesecondary]$ rm -rf in_use.lock
[root@corehub-001 dfs]$ pwd
/opt/module/hadoop-2.7.2/data/tmp/dfs
[root@corehub-001 dfs]$ ls
data name namesecondary
```
> 5.å¯¼å…¥æ£€æŸ¥ç‚¹æ•°æ®(ç­‰å¾…ä¸€ä¼š ctrl+c ç»“æŸ)
```
 [root@corehub-001 hadoop]$ bin/hdfs namenode -importCheckpoint
```
> 6.å¯åŠ¨ namenode
```
[root@corehub-001 hadoop]$ sbin/haddaemon.sh start namenode
```

#### é›†ç¾¤å®‰å…¨æ¨¡å¼
> 1.æ¦‚è¿°
> 
> 1.NameNodeå¯åŠ¨æ—¶
> NameNode å¯åŠ¨æ—¶,é¦–å…ˆå°†æ˜ åƒæ–‡ä»¶(fsimage)å…¥å†…å­˜,å¹¶æ‰§è¡Œç¼–è¾‘æ—¥å¿—(edits)çš„å„é¡¹æ“ä½œ,ä¸€æ—¦åœ¨å†…å­˜ä¸­æˆåŠŸå»ºç«‹æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®çš„æ˜ åƒ,åˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„(fsimage)æ–‡ä»¶å’Œä¸€ä¸ªç©ºçš„ç¼–è¾‘æ—¥å¿—,æ­¤æ—¶,NameNodeå¼€å§‹ç›‘å¬DataNodeè¯·æ±‚,ä½†æ˜¯æ­¤åˆ»,NameNodeè¿è¡Œåœ¨å®‰å…¨æ¨¡å¼,å³NameNodeçš„æ–‡ä»¶ç³»ç»Ÿå¯¹äºå®¢æˆ·ç«¯æ¥è¯´æ˜¯åªè¯»çš„.
> 
> 2.DataNodeå¯åŠ¨æ—¶
> ç³»ç»Ÿä¸­çš„æ•°æ®å—çš„ä½ç½®å¹¶ä¸æ˜¯ç”±NameNodeç»´æŠ¤çš„,è€Œæ˜¯ä»¥å—åˆ—è¡¨çš„å½¢å¼å­˜å‚¨åœ¨DataNodeä¸­,åœ¨ç³»ç»Ÿçš„æ­£å¸¸æ“ä½œæœŸé—´,NameNodeä¼šåœ¨å†…å­˜ä¸­ä¿ç•™æ‰€æœ‰å—ä½ç½®çš„æ˜ å°„ä¿¡æ¯,åœ¨å®‰å…¨æ¨¡å¼ä¸‹,å„ä¸ªDataNodeä¼šå‘NameNodeå‘é€æœ€æ–°çš„å—åˆ—è¡¨ä¿¡æ¯,NameNodeäº†è§£åˆ°è¶³å¤Ÿå¤šçš„å—ä½ç½®ä¿¡æ¯ä¹‹å,å³å¯é«˜æ•ˆè¿è¡Œæ–‡ä»¶ç³»ç»Ÿã€‚
> 
> 3.å®‰å…¨æ¨¡å¼é€€å‡ºåˆ¤æ–­
> å¦‚æœæ»¡è¶³"æœ€å°å‰¯æœ¬æ¡ä»¶",NameNodeä¼šåœ¨30ç§’é’Ÿä¹‹åå°±é€€å‡ºå®‰å…¨æ¨¡å¼,æ‰€è°“çš„æœ€å°å‰¯æœ¬æ¡ä»¶æŒ‡çš„æ˜¯åœ¨æ•´ä¸ªæ–‡ä»¶ç³»ç»Ÿä¸­ 99.9%çš„å—æ»¡è¶³æœ€å°å‰¯æœ¬çº§åˆ«(é»˜è®¤å€¼ ï¼šdfs.replication.min=1),åœ¨å¯åŠ¨ä¸€ä¸ªåˆšåˆšæ ¼å¼åŒ–çš„ HDFSé›†ç¾¤æ—¶,å› ä¸ºç³»ç»Ÿä¸­è¿˜æ²¡æœ‰ä»»ä½•å—,æ‰€ä»¥NameNodeä¸ä¼šè¿›å…¥å®‰å…¨æ¨¡å¼.

> 2.åŸºæœ¬è¯­æ³•
> é›†ç¾¤å¤„äºå®‰å…¨æ¨¡å¼,ä¸èƒ½æ‰§è¡Œé‡è¦æ“ä½œ(å†™æ“ä½œ),ç¾¤å¯åŠ¨å®Œæˆå,è‡ªåŠ¨é€€å‡ºå®‰å…¨æ¨¡å¼ã€‚
> æŸ¥çœ‹å®‰å…¨æ¨¡å¼çŠ¶æ€
> ```
> [root@corehub-001 hadoop]# bin/hdfs dfsadmin -safemode get
> Safe mode is OFF
> [root@corehub-001 hadoop]# 
> ```
> è¿›å…¥å®‰å…¨æ¨¡å¼çŠ¶æ€
> ```
> [root@corehub-001 hadoop]# bin/hdfs dfsadmin -safemode enter
> Safe mode is ON
> [root@corehub-001 hadoop]# 
> ```
> ç¦»å¼€å®‰å…¨æ¨¡å¼çŠ¶æ€
> ```
> [root@corehub-001 hadoop]# bin/hdfs dfsadmin -safemode  leave
> Safe mode is OFF
> [root@corehub-001 hadoop]# 
> ```
> ç­‰å¾…å®‰å…¨æ¨¡å¼çŠ¶æ€
> ```
> [root@corehub-001 hadoop]# bin/hdfs dfsadmin -safemode wait
> ```

#### NameNode å¤šç›®å½•é…ç½®
> 1.NameNodeçš„æœ¬åœ°ç›®å½•å¯ä»¥é…ç½®æˆå¤šä¸ª,ä¸”æ¯ä¸ªç›®å½•å­˜æ”¾å†…å®¹ç›¸åŒ,å¢åŠ äº†å¯é æ€§.
> 2.å…·ä½“é…ç½®å¦‚ä¸‹:
> åœ¨hdfs-site.xmlæ–‡ä»¶ä¸­å¢åŠ å¦‚ä¸‹å†…å®¹
``` xml
<property>
 <name>dfs.namenode.name.dir</name>
 <value>file:///${hadoop.tmp.dir}/dfs/name1,file:///${hadoop.tmp.dir}/dfs/name2</v
alue>
</property>
```
> 3.åœæ­¢é›†ç¾¤,åˆ é™¤ data å’Œ logs ä¸­æ‰€æœ‰æ•°æ®
``` powershell
[root@corehub-001 hadoop]$ rm -rf data/ logs/
[root@corehub-002 hadoop]$ rm -rf data/ logs/
[root@corehub-003 hadoop]$ rm -rf data/ logs/
```
> 4.æ ¼å¼åŒ–é›†ç¾¤å¹¶å¯åŠ¨.
``` powershell
[root@corehub-001 hadoop]$ bin/hdfs namenode â€“format
[root@corehub-001 hadoop]$ sbin/start-dfs.sh
```
> 5.æŸ¥çœ‹ç»“æœ
``` powershell
[root@corehub-001 dfs]$ ll
æ€»ç”¨é‡ 12
drwx------. 3 root root 4096 02 æœˆ 11 08:03 data
drwxrwxr-x. 3 root root 4096 02 æœˆ 11 08:03 name1
drwxrwxr-x. 3 root root 4096 02 æœˆ 11 08:03 nam
```


### 7.5 DataNode(é¢è¯•å¼€å‘é‡ç‚¹)
#### DataNodeå·¥ä½œæœºåˆ¶
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_020.jpg)
> 1.ä¸€ä¸ªæ•°æ®å—åœ¨DataNodeä¸Šä»¥æ–‡ä»¶å½¢å¼å­˜å‚¨åœ¨ç£ç›˜ä¸Š,åŒ…æ‹¬ä¸¤ä¸ªæ–‡ä»¶,ä¸€ä¸ªæ˜¯æ•°æ®æœ¬èº«,ä¸€ä¸ªæ˜¯å…ƒæ•°æ®åŒ…æ‹¬æ•°æ®å—çš„é•¿åº¦,å—æ•°æ®çš„æ ¡éªŒå’Œ,ä»¥åŠæ—¶é—´æˆ³.
> 
> 2.DataNodeå¯åŠ¨åå‘NameNodeæ³¨å†Œ,é€šè¿‡å,å‘¨æœŸæ€§(1 å°æ—¶)å‘NameNodeä¸ŠæŠ¥æ‰€æœ‰çš„å—ä¿¡æ¯.
> 
> 3.å¿ƒè·³æ˜¯æ¯3ç§’ä¸€æ¬¡,å¿ƒè·³è¿”å›ç»“æœå¸¦æœ‰NameNodeç»™è¯¥DataNodeçš„å‘½ä»¤å¦‚å¤åˆ¶å—æ•°æ®åˆ°å¦ä¸€å°æœºå™¨,æˆ–åˆ é™¤æŸä¸ªæ•°æ®å—ã€‚å¦‚æœè¶…è¿‡10åˆ†é’Ÿæ²¡æœ‰æ”¶åˆ°æŸä¸ªDataNode çš„å¿ƒè·³,åˆ™è®¤ä¸ºè¯¥èŠ‚ç‚¹ä¸å¯ç”¨.
> 
> 4.é›†ç¾¤è¿è¡Œä¸­å¯ä»¥å®‰å…¨åŠ å…¥å’Œé€€å‡ºä¸€äº›æœºå™¨.

#### æœå½¹æ–°æ•°æ®èŠ‚ç‚¹
> éšç€å…¬å¸ä¸šåŠ¡çš„å¢é•¿,æ•°æ®é‡è¶Šæ¥è¶Šå¤§,åŸæœ‰çš„æ•°æ®èŠ‚ç‚¹çš„å®¹é‡å·²ç»ä¸èƒ½æ»¡è¶³å­˜å‚¨æ•°æ®çš„éœ€æ±‚,éœ€è¦åœ¨åŸæœ‰é›†ç¾¤åŸºç¡€ä¸ŠåŠ¨æ€æ·»åŠ æ–°çš„æ•°æ®èŠ‚ç‚¹.
> 
> 1.ç¯å¢ƒå‡†å¤‡
> (1) å…‹éš†ä¸€å°è™šæ‹Ÿæœº
> (2) ä¿®æ”¹ipåœ°å€å’Œä¸»æœºåç§°
> (3) ä¿®æ”¹xsyncæ–‡ä»¶,å¢åŠ æ–°å¢èŠ‚ç‚¹çš„sshæ— å¯†ç™»å½•é…ç½®
> (4) åˆ é™¤åŸæ¥ HDFS æ–‡ä»¶ç³»ç»Ÿç•™å­˜çš„æ–‡ä»¶/opt/module/hadoop/data
> 
> 2.æœå½¹æ–°èŠ‚ç‚¹å…·ä½“æ­¥éª¤
> (1) åœ¨namenodeçš„/opt/module/hadoop/etc/hadoopç›®å½•ä¸‹åˆ›å»º dfs.hosts æ–‡ä»¶
```
[root@corehub-004 hadoop]$ pwd
/opt/module/hadoop/etc/hadoop
[root@corehub-004 hadoop]$ touch dfs.hosts
[root@corehub-004 hadoop]$ vi dfs.hosts
```
> æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§°ï¼ˆåŒ…å«æ–°æœå½¹çš„èŠ‚ç‚¹ï¼‰
corehub-001
corehub-002
corehub-003
corehub-004

> (2) åœ¨namenodeçš„hdfs-site.xmlé…ç½®æ–‡ä»¶ä¸­å¢åŠ dfs.hostså±æ€§
``` xml
<property>
<name>dfs.hosts</name>
 <value>/opt/module/hadoop/etc/hadoop/dfs.hosts</value>
</property>
```

> (3) åˆ·æ–° namenode
```
[root@corehub-001 hadoop]$ hdfs dfsadmin -refreshNodes Refresh nodes successful
```

> (4) æ›´æ–° resourcemanager èŠ‚ç‚¹
```
[root@corehub-001 hadoop$ yarn rmadmin -refreshNodes
19/02/19 14:17:11 INFO client.RMProxy: Connecting to ResourceManager at corehub-003/192.168.1.103:8033
```

> (5) åœ¨ NameNode çš„ slaves æ–‡ä»¶ä¸­å¢åŠ æ–°ä¸»æœºåç§°
å¢åŠ  004
corehub-001
corehub-002
corehub-003
corehub-004

> (6) å•ç‹¬å‘½ä»¤å¯åŠ¨æ–°çš„æ•°æ®èŠ‚ç‚¹å’ŒèŠ‚ç‚¹ç®¡ç†å™¨
```
[root@corehub-004 hadoop]$ sbin/hadoop-daemon.sh start datanode
starting datanode, logging to 
/opt/module/hadoop/logs/hadoop-atguigu-datanode-corehub-004.out
[root@corehub-004 hadoop]$ sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to 
/opt/module/hadoop/logs/yarn-atguigu-nodemanager-corehub-004.out
```
> (7) åœ¨ web æµè§ˆå™¨ä¸Šæ£€æŸ¥æ˜¯å¦ ok
> å¦‚æœæ•°æ®ä¸å‡è¡¡,å¯ä»¥ç”¨å‘½ä»¤å®ç°é›†ç¾¤çš„å†å¹³è¡¡
```
[root@corehub-001 sbin]$ ./start-balancer.sh
starting balancer, logging to 
/opt/module/hadoop/logs/hadoop-atguigu-balancer-corehub-001.out
Time Stamp Iteration# Bytes Already Moved Bytes Left To Move 
Bytes Being Moved
```


#### é€€å½¹æ—§æ•°æ®èŠ‚ç‚¹
> 1.åœ¨ namenode çš„/opt/module/hadoop/etc/hadoop ç›®å½•ä¸‹åˆ›å»º dfs.hosts.ex
```
[root@corehub-001 hadoop]$ pwd
/opt/module/hadoop/etc/hadoop
[root@corehub-001 hadoop]$ touch dfs.hosts.exclude
[root@corehub-001 hadoop]$ vi dfs.hosts.exclude
```
> æ·»åŠ å¦‚ä¸‹ä¸»æœºåç§° (è¦é€€å½¹çš„èŠ‚ç‚¹)
> corehub-004
> 2.åœ¨namenodeçš„hdfs-site.xml é…ç½®æ–‡ä»¶ä¸­å¢åŠ  dfs.hosts.exclude å±æ€§
``` xml
<property>
<name>dfs.hosts.exclude</name>
 <value>/opt/module/hadoop/etc/hadoop/dfs.hosts.exclude</value>
</property>
```
> 3.åˆ·æ–°namenodeã€åˆ·æ–° resourcemanager
```
[root@corehub-001 hadoop]$ hdfs dfsadmin -refreshNodes Refresh nodes successful
[root@corehub-001 hadoop]$ yarn rmadmin -refreshNodes
19/02/19 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at 
corehub-001/192.168.1.103:8033
```

> 4.æ£€æŸ¥ web æµè§ˆå™¨,é€€å½¹èŠ‚ç‚¹çš„çŠ¶æ€ä¸º decommission in progress(é€€å½¹ä¸­)è¯´æ˜æ•°æ®èŠ‚ç‚¹æ­£åœ¨å¤åˆ¶å—åˆ°å…¶ä»–èŠ‚ç‚¹.
> 
> 5.ç­‰å¾…é€€å½¹èŠ‚ç‚¹çŠ¶æ€ä¸º decommissioned(æ‰€æœ‰å—å·²ç»å¤åˆ¶å®Œæˆ)åœæ­¢è¯¥èŠ‚ç‚¹åŠèŠ‚ç‚¹èµ„æºç®¡ç†å™¨ã€‚æ³¨æ„ï¼šå¦‚æœå‰¯æœ¬æ•°æ˜¯3,æœå½¹çš„èŠ‚ç‚¹å°äºç­‰äº3,æ˜¯ä¸èƒ½é€€å½¹æˆåŠŸçš„,éœ€è¦ä¿®æ”¹å‰¯æœ¬æ•°åæ‰èƒ½é€€å½¹
```
[root@corehub-004 hadoop]$ sbin/hadoop-daemon.sh stop datanode
stopping datanode
[root@corehub-004 hadoop]$ sbin/yarn-daemon.sh stop nodemanager
stopping nodemanager
```
> 6.ä»includeæ–‡ä»¶ä¸­åˆ é™¤é€€å½¹èŠ‚ç‚¹,å†è¿è¡Œåˆ·æ–°èŠ‚ç‚¹çš„å‘½ä»¤
> (1) ä»namenodeçš„dfs.hostsæ–‡ä»¶ä¸­åˆ é™¤é€€å½¹èŠ‚ç‚¹corehub-004
corehub-001
corehub-002
corehub-003
> (2) åˆ·æ–° namenode,åˆ·æ–° resourcemanager
```
[root@corehub-001 hadoop]$ hdfs dfsadmin -refreshNodes
Refresh nodes successful
[root@corehub-001 hadoop]$ yarn rmadmin -refreshNodes
19/02/19 14:55:56 INFO client.RMProxy: Connecting to ResourceManager at 
corehub-002/192.168.1.103:8033
```
> 7.ä» namenodeçš„slaveæ–‡ä»¶ä¸­åˆ é™¤é€€å½¹èŠ‚ç‚¹corehub-004
corehub-001
corehub-002
corehub-003
> 8.å¦‚æœæ•°æ®ä¸å‡è¡¡ï¼Œå¯ä»¥ç”¨å‘½ä»¤å®ç°é›†ç¾¤çš„å†å¹³è¡¡
```
[root@corehub-001 hadoop]$ sbin/start-balancer.sh 
starting balancer, logging to 
/opt/module/hadoop/logs/hadoop-atguigu-balancer-corehub-001.out
Time Stamp Iteration# Bytes Already Moved Bytes Left To Move 
Bytes Being Moved
```

#### DataNodeå¤šç›®å½•é…ç½®
> 1.datanodeä¹Ÿå¯ä»¥é…ç½®æˆå¤šä¸ªç›®å½•,æ¯ä¸ªç›®å½•å­˜å‚¨çš„æ•°æ®ä¸ä¸€æ ·, å³:æ•°æ®ä¸æ˜¯å‰¯æœ¬.
> 
> 2.å…·ä½“é…ç½®å¦‚ä¸‹ï¼š
> hdfs-site.xml
> ``` xml
> <property>
>  <name>dfs.datanode.data.dir</name>
> <value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>
> </property>
> ```


### 7.6 HDFS 2.Xæ–°ç‰¹æ€§
#### é›†ç¾¤é—´æ•°æ®æ‹·è´
> 1.scpå®ç°ä¸¤ä¸ªè¿œç¨‹ä¸»æœºä¹‹é—´æ–‡ä»¶å¤åˆ¶
> ```
> æ¨ push
> scp -r hello.txt root@corehub-002:/user/geekparkhub/hello.txt
> ```
> ```
> æ‹‰å– pull
> scp -r root@corehub-002:/user/geekparkhub/hello.txt /hello.txt
> ```
> 2.é‡‡ç”¨distcpæŒ‡ä»¤ å®ç°ä¸¤ä¸ªhadoopé›†ç¾¤ä¹‹é—´é€’å½’æ•°æ®å¤åˆ¶
> ```
> bin/hadoop distcp hdfs://corehub-001:9000/user/geekparkhub/hello.txt hdfs://corehub-002:9000/user/geekparkhub/hello.txt
> ```
#### Hadoopå­˜æ¡£
> å°æ–‡ä»¶å­˜æ¡£
> 
> 1.HDFSå­˜å‚¨å°æ–‡ä»¶å¼Šç«¯
> æ¯ä¸ªæ–‡ä»¶å‡æŒ‰å—å‚¨å­˜,æ¯ä¸ªå—çš„å…ƒæ•°æ®åœ¨NameNodeçš„å†…å­˜ä¸­,å› æ­¤HDFSå­˜å‚¨å°æ–‡ä»¶ä¼šéå¸¸ä½æ•ˆ,å› ä¸ºå¤§é‡çš„å°æ–‡ä»¶ä¼šè€—å°½NameNodeä¸­çš„å¤§éƒ¨åˆ†å†…å­˜,ä½†æ³¨æ„,å‚¨å­˜å°æ–‡ä»¶æ‰€éœ€è¦çš„ç£ç›˜å®¹é‡å’Œæ•°æ®å—çš„å¤§å°æ— å…³,ä¾‹å¦‚,ä¸€ä¸ª1MBçš„æ–‡ä»¶è®¾ç½®ä¸º128MBçš„å—å­˜å‚¨,å®é™…ä½¿ç”¨çš„æ˜¯1MBçš„ç£ç›˜ç©ºé—´,è€Œä¸æ˜¯128MB.
> 
> 2.è§£å†³å­˜å‚¨å°æ–‡ä»¶åŠæ³•ä¹‹ä¸€
> HDFSå­˜æ¡£æ–‡ä»¶æˆ–HARæ–‡ä»¶,æ˜¯ä¸€ä¸ªæ›´é«˜æ•ˆçš„æ–‡ä»¶å­˜æ¡£å·¥å…·,å®ƒå°†æ–‡ä»¶å­˜å…¥HDFSå—,åœ¨å‡å°‘NameNodeå†…å­˜ä½¿ç”¨çš„åŒæ—¶,å…è®¸å¯¹æ–‡ä»¶è¿›è¡Œé€æ˜è®¿é—®,å…·ä½“è¯´æ¥,HDFSå­˜æ¡£æ–‡ä»¶å¯¹å†…è¿˜æ˜¯ä¸€ä¸ªä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶,å¯¹NameNodeè€Œè¨€å´æ˜¯ä¸€ä¸ªæ•´ä½“,å‡å°‘äº†NameNodeçš„å†…å­˜.
> 
> 3.æ¡ˆä¾‹å®æ“
> å½’æ¡£æ–‡ä»¶
> å°†/user/geekparkhub/inputç›®å½•é‡Œé¢çš„æ‰€æœ‰æ–‡ä»¶å½’æ¡£æˆä¸€ä¸ªå« myhar.har çš„å½’æ¡£æ–‡ä»¶,å¹¶æŠŠå½’æ¡£åæ–‡ä»¶å­˜å‚¨åˆ°/user/geekparkhub/outputè·¯å¾„ä¸‹
``` powershell
[root@corehub-001 hadoop]# hadoop archive -archiveName input.har -p /user/geekparkhub/input /user/geekparkhub/output
19/02/20 00:21:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/02/20 00:21:30 INFO client.RMProxy: Connecting to ResourceManager at corehub-002/192.168.152.135:8032
19/02/20 00:21:33 INFO client.RMProxy: Connecting to ResourceManager at corehub-002/192.168.152.135:8032
19/02/20 00:21:33 INFO client.RMProxy: Connecting to ResourceManager at corehub-002/192.168.152.135:8032
19/02/20 00:21:33 INFO mapreduce.JobSubmitter: number of splits:1
19/02/20 00:21:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1550666045505_0001
19/02/20 00:21:35 INFO impl.YarnClientImpl: Submitted application application_1550666045505_0001
19/02/20 00:21:35 INFO mapreduce.Job: The url to track the job: http://corehub-002:8088/proxy/application_1550666045505_0001/
19/02/20 00:21:35 INFO mapreduce.Job: Running job: job_1550666045505_0001
19/02/20 00:21:53 INFO mapreduce.Job: Job job_1550666045505_0001 running in uber mode : false
19/02/20 00:21:53 INFO mapreduce.Job:  map 0% reduce 0%
19/02/20 00:22:07 INFO mapreduce.Job:  map 32% reduce 0%
19/02/20 00:22:28 INFO mapreduce.Job:  map 43% reduce 0%
19/02/20 00:22:57 INFO mapreduce.Job:  map 55% reduce 0%
19/02/20 00:23:30 INFO mapreduce.Job:  map 100% reduce 0%
19/02/20 00:23:41 INFO mapreduce.Job:  map 100% reduce 100%
19/02/20 00:23:41 INFO mapreduce.Job: Job job_1550666045505_0001 completed successfully
19/02/20 00:23:41 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=535
                FILE: Number of bytes written=238807
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=585985943
                HDFS: Number of bytes written=585985840
                HDFS: Number of read operations=21
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=7
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Other local map tasks=1
                Total time spent by all maps in occupied slots (ms)=94659
                Total time spent by all reduces in occupied slots (ms)=7959
                Total time spent by all map tasks (ms)=94659
                Total time spent by all reduce tasks (ms)=7959
                Total vcore-milliseconds taken by all map tasks=94659
                Total vcore-milliseconds taken by all reduce tasks=7959
                Total megabyte-milliseconds taken by all map tasks=96930816
                Total megabyte-milliseconds taken by all reduce tasks=8150016
        Map-Reduce Framework
                Map input records=5
                Map output records=5
                Map output bytes=518
                Map output materialized bytes=535
                Input split bytes=116
                Combine input records=0
                Combine output records=0
                Reduce input groups=5
                Reduce shuffle bytes=535
                Reduce input records=5
                Reduce output records=0
                Spilled Records=10
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=1297
                CPU time spent (ms)=15680
                Physical memory (bytes) snapshot=414289920
                Virtual memory (bytes) snapshot=4127817728
                Total committed heap usage (bytes)=251527168
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=507
        File Output Format Counters 
                Bytes Written=0
[root@corehub-001 hadoop]# 
```
> è§£ææŸ¥çœ‹å½’æ¡£
```
[root@corehub-001 hadoop]# hadoop fs -ls -R har:///user/geekparkhub/output/input.har
19/02/20 00:28:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-rw-r--r--   3 root supergroup  212046774 2019-02-14 17:33 har:///user/geekparkhub/output/input.har/hadoop-2.7.2.tar.gz
-rw-r--r--   3 root supergroup  189815615 2019-02-14 17:34 har:///user/geekparkhub/output/input.har/jdk-8u162-linux-x64.tar.gz
-rw-r--r--   3 root supergroup  184122460 2019-02-14 17:35 har:///user/geekparkhub/output/input.har/mysql-5.5.35-linux2.6-x86_64.tar.gz
-rw-r--r--   3 root supergroup        471 2019-02-14 17:32 har:///user/geekparkhub/output/input.har/wc.input
[root@corehub-001 hadoop]# 
```
#### å›æ”¶ç«™
> å¼€å¯å›æ”¶ç«™åŠŸèƒ½,å¯ä»¥å°†åˆ é™¤çš„æ–‡ä»¶åœ¨ä¸è¶…æ—¶çš„æƒ…å†µä¸‹,æ¢å¤å…ƒæ•°æ®,èµ·åˆ°é˜²æ­¢è¯¯åˆ é™¤,å¤‡ä»½ç­‰ä½œç”¨.
> 
> 1.å›æ”¶ç«™å‚æ•°è®¾ç½®åŠå·¥ä½œæœºåˆ¶
> å¼€å¯å›æ”¶ç«™åŠŸèƒ½å‚æ•°è¯´æ˜
> 1.é»˜è®¤å€¼ fs.trash.interval=0    0 è¡¨ç¤ºç¦ç”¨å›æ”¶ç«™,å¯ä»¥è®¾ç½®åˆ é™¤æ–‡ä»¶çš„å­˜æ´»æ—¶é—´.
> 2.é»˜è®¤å€¼ fs.trash.checkpoint.interval=0,æ£€æŸ¥å›æ”¶ç«™çš„é—´éš”æ—¶é—´
> å¦‚æœè¯¥å€¼ä¸º 0,åˆ™è¯¥å€¼è®¾ç½®å’Œ fs.trash.interval çš„å‚æ•°å€¼ç›¸ç­‰,è¦æ±‚ fs.trash.checkpoint.interval<=fs.trash.interval
> 2.å¯ç”¨å›æ”¶ç«™
> ä¿®æ”¹core-site.xml é…ç½®åƒåœ¾å›æ”¶æ—¶é—´ä¸º1åˆ†é’Ÿ
``` xml
<property>
	<name>fs.trash.interval</name>
	<value>1</value>
</property>
```
> 3.æŸ¥çœ‹å›æ”¶ç«™
> å›æ”¶ç«™åœ¨é›†ç¾¤ä¸­è·¯å¾„: /user/geekparkhub/.Trash/ .....
> 
> 4.ä¿®æ”¹è®¿é—®åƒåœ¾å›æ”¶ç«™ç”¨æˆ·åç§°
> è¿›å…¥åƒåœ¾å›æ”¶ç«™ç”¨æˆ·åç§°,é»˜è®¤æ˜¯dr.who ä¿®æ”¹ä¸ºrootç”¨æˆ·
> ä¿®æ”¹core-site.xml
``` xml
<property>
	<name>hadoop.http.staticuser.user</name>
	<value>root</value>
</property>
```
> 5.é€šè¿‡ç¨‹åºåˆ é™¤çš„æ–‡ä»¶ä¸ä¼šç»è¿‡å›æ”¶ç«™,éœ€è¦è°ƒç”¨moveToTrash()æ‰è¿›å…¥å›æ”¶ç«™
```
Trash trash = New Trash(conf);
trash.moveToTrash(path);
```
> 6.æ¢å¤å›æ”¶ç«™æ•°æ®
```
hadoop fs -mv /user/geekparkhub/.Trash/Current/user/geekparkhub/input /user/geekparkhub/input
```
> 7.æ¸…ç©ºå›æ”¶ç«™
```
hadoop fs -expunge
```

#### å¿«ç…§ç®¡ç†
> å¿«ç…§ç›¸å½“äºå¯¹ç›®å½•åšä¸€ä¸ªå¤‡ä»½,å¹¶ä¸ä¼šç«‹å³å¤åˆ¶æ‰€æœ‰æ–‡ä»¶,è€Œæ˜¯æŒ‡å‘åŒä¸€ä¸ªæ–‡ä»¶,å½“å†™å…¥å‘ç”Ÿæ—¶,æ‰ä¼šäº§ç”Ÿæ–°æ–‡ä»¶.
> 
> 1.åŸºæœ¬è¯­æ³•
> (1) `hdfs dfsadmin -allowSnapshot è·¯å¾„` (åŠŸèƒ½æè¿°ï¼šå¼€å¯æŒ‡å®šç›®å½•çš„å¿«ç…§åŠŸèƒ½)
> (2) `hdfs dfsadmin -disallowSnapshot è·¯å¾„` (åŠŸèƒ½æè¿°ï¼šç¦ç”¨æŒ‡å®šç›®å½•çš„å¿«ç…§åŠŸèƒ½,é»˜è®¤æ˜¯ç¦ç”¨)
> (3) `hdfs dfs -createSnapshot è·¯å¾„` (åŠŸèƒ½æè¿°ï¼šå¯¹ç›®å½•åˆ›å»ºå¿«ç…§)
> (4) `hdfs dfs -createSnapshot è·¯å¾„ åç§°` (åŠŸèƒ½æè¿°ï¼šæŒ‡å®šåç§°åˆ›å»ºå¿«ç…§)
> (5) `hdfs dfs -renameSnapshot è·¯å¾„ æ—§åç§° æ–°åç§°` (åŠŸèƒ½æè¿°ï¼šé‡å‘½åå¿«ç…§)
> (6) `hdfs lsSnapshottableDir` (åŠŸèƒ½æè¿°ï¼šåˆ—å‡ºå½“å‰ç”¨æˆ·æ‰€æœ‰å¯å¿«ç…§ç›®å½•)
> (7) `hdfs snapshotDiff è·¯å¾„ 1 è·¯å¾„ 2` (åŠŸèƒ½æè¿°ï¼šæ¯”è¾ƒä¸¤ä¸ªå¿«ç…§ç›®å½•çš„ä¸åŒä¹‹å¤„)
> (8) `hdfs dfs -deleteSnapshot <path> <snapshotName>` (åŠŸèƒ½æè¿°ï¼šåˆ é™¤å¿«ç…§)

### 7.7 MapReduce æ¦‚è¿°
#### MapReduce å®šä¹‰
> MapReduceæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è¿ç®—ç¨‹åºçš„ç¼–ç¨‹æ¡†æ¶,æ˜¯ç”¨æˆ·å¼€å‘''åŸºäºHadoopçš„æ•°æ®åˆ†æåº”ç”¨''çš„æ ¸å¿ƒæ¡†æ¶.
> 
> MapReduceæ ¸å¿ƒåŠŸèƒ½æ˜¯å°†ç”¨æˆ·ç¼–å†™çš„ä¸šåŠ¡é€»è¾‘ä»£ç å’Œè‡ªå¸¦é»˜è®¤ç»„ä»¶æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„åˆ†å¸ƒå¼è¿ç®—ç¨‹åº,å¹¶å‘è¿è¡Œåœ¨ä¸€ä¸ªHadoopé›†ç¾¤ä¸Š.
> 
#### MapReduce ä¼˜ç¼ºç‚¹
> `ä¼˜ç‚¹`
> 
> MapReduce æ˜“äºç¼–ç¨‹
> å®ƒç®€å•çš„å®ç°ä¸€äº›æ¥å£,å°±å¯ä»¥å®Œæˆä¸€ä¸ªåˆ†å¸ƒå¼ç¨‹åº,è¿™ä¸ªåˆ†å¸ƒå¼ç¨‹åºå¯ä»¥ä»¥åˆ†å¸ƒåˆ°å¤§é‡å»‰ä»·çš„PCæœºå™¨ä¸Šè¿è¡Œ,ä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªåˆ†å¸ƒå¼ç¨‹åº,è·Ÿå†™ä¸€ä¸ªç®€å•çš„ä¸²è¡Œç¨‹åºæ˜¯ä¸€æ¨¡ä¸€æ ·çš„,å°±æ˜¯å› ä¸ºè¿™ä¸ªç‰¹ç‚¹ä½¿å¾—MapReduceç¼–ç¨‹å˜å¾—éå¸¸æµè¡Œ.
> 
> è‰¯å¥½çš„æ‰©å±•æ€§
> å½“è®¡ç®—èµ„æºä¸èƒ½å¾—åˆ°æ»¡è¶³çš„æ—¶å€™,å¯ä»¥é€šè¿‡ç®€å•çš„å¢åŠ æœºå™¨æ¥æ‰©å±•å®ƒçš„è®¡ç®—èƒ½åŠ›.
> 
> é«˜å®¹é”™æ€§
> MapReduceè®¾è®¡çš„åˆè¡·å°±æ˜¯ä½¿ç”¨ç¨‹åºèƒ½å¤Ÿéƒ¨ç½²åœ¨å»‰ä»·çš„PCæœºå™¨ä¸Š,è¿™å°±è¦æ±‚å®ƒå…·æœ‰å¾ˆé«˜çš„å®¹é”™æ€§,æ¯”å¦‚å…¶ä¸­ä¸€å°æœºå™¨å®•æœºäº†,å®ƒå¯ä»¥æŠŠä¸Šé¢çš„è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œ,ä¸è‡³äºè¿™ä¸ªä»»åŠ¡è¿è¡Œå¤±è´¥,è€Œä¸”è¿™ä¸ªç¨‹åºä¸éœ€è¦äººå·¥å‚ä¸,è€Œå®Œå…¨æ˜¯ç”±Hadoopå†…éƒ¨å®Œæˆ.
> 
> é€‚åˆPBçº§åˆ«ä»¥ä¸Šæµ·é‡æ•°æ®çš„ç¦»çº¿å¤„ç†
> å¯æ˜¯å®ç°ä¸Šåƒå°æœåŠ¡å™¨é›†ç¾¤å¹¶å‘å·¥ä½œ,æå‡æ•°æ®å¤„ç†èƒ½åŠ›.
> 
>  `ç¼ºç‚¹`
>  
>  ä¸æ“…é•¿ å®æ—¶è®¡ç®—
>  MapReduceæ— æ³•å‘MYSQLä¸€æ ·,åœ¨æ¯«ç§’æˆ–ç§’çº§åˆ«å†…è¿”å›ç»“æœ.
>  
>  ä¸æ“…é•¿ æµå¼è®¡ç®—
>  æµå¼è®¡ç®—çš„è¾“å…¥æ•°æ®æ˜¯åŠ¨æ€çš„,è€ŒMapReduceçš„è¾“å…¥æ•°æ®é›†æ˜¯é™æ€çš„,ä¸èƒ½åŠ¨æ€å˜åŒ–,è¿™æ˜¯å› ä¸ºMapReduceè‡ªèº«çš„è®¾è®¡ç‰¹ç‚¹å†³å®šäº†æ•°æ®æºå¿…é¡»æ˜¯é™æ€çš„.
>  
>  ä¸æ“…é•¿ DAG (æœ‰å‘å›¾è®¡ç®—)
>  å¤šä¸ªåº”ç”¨ç¨‹åºå­˜åœ¨ä¾èµ–å…³ç³»,åä¸€ä¸ªåº”ç”¨ç¨‹åºçš„è¾“å…¥ä¸ºå‰ä¸€ä¸ªçš„è¾“å‡º,åœ¨è¿™ç§æƒ…å†µä¸‹,MapReduceå¹¶ä¸æ˜¯ä¸èƒ½åš,è€Œæ˜¯ä½¿ç”¨å,æ¯ä¸ªMaoReduceä½œä¸šçš„è¾“å‡ºç»“æœéƒ½ä¼šå†™å…¥åˆ°ç£ç›˜,ä¼šé€ æˆå¤§é‡çš„ç£ç›˜I/O,å¯¼è‡´æ€§èƒ½éå¸¸çš„ä½.

#### MapReduce æ ¸å¿ƒç¼–ç¨‹æ€æƒ³
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_021.jpg)

> 1.åˆ†å¸ƒå¼çš„è¿ç®—ç¨‹åºå¾€å¾€éœ€è¦åˆ†æˆè‡³å°‘2ä¸ªé˜¶æ®µ.
> 
> 2.ç¬¬ä¸€ä¸ªé˜¶æ®µçš„Maptaskå¹¶å‘å®ä¾‹,å®Œå…¨å¹¶è¡Œè¿è¡Œ,äº’ä¸ç›¸å¹².
> 
> 3.ç¬¬äºŒä¸ªé˜¶æ®µçš„Reduce taskå¹¶å‘å®ä¾‹äº’ä¸ç›¸å¹²,ä½†æ˜¯ä»–ä»¬çš„æ•°æ®ä¾èµ–äºä¸Šä¸€ä¸ªé˜¶æ®µçš„æ‰€æœ‰MapTaskå¹¶å‘å®ä¾‹çš„è¾“å‡º.
> 
> 4.MapReduceç¼–ç¨‹æ¨¡å‹åªèƒ½åŒ…å«ä¸€ä¸ªMapé˜¶æ®µå’Œä¸€ä¸ªReduceé˜¶æ®µ,å¦‚æœç”¨æˆ·çš„ä¸šåŠ¡é€»è¾‘éå¸¸å¤æ‚,é‚£å°±åªèƒ½å¤šä¸ªMapReduceç¨‹åº,ä¸²è¡Œè¿è¡Œ.

#### MapReduce è¿›ç¨‹
> ä¸€ä¸ªå®Œæ•´çš„MapReduceç¨‹åºåœ¨åˆ†å¸ƒå¼è¿è¡Œæ—¶æœ‰ä¸‰å¤§å®ä¾‹è¿›ç¨‹
> MrAppMaster è´Ÿè´£æ•´ä¸ªç¨‹åºçš„è¿‡ç¨‹è°ƒåº¦åŠçŠ¶æ€åè°ƒ.
> MapTask è´Ÿè´£Mapé˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹.
> ReduceTask è´Ÿè´£Reduceé˜¶æ®µçš„æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹.

#### WordCount å®˜æ–¹æºç 
> é‡‡ç”¨åç¼–è¯‘å·¥å…·ç¼–è¯‘æºç ,å‘ç°WordCountæ¡ˆä¾‹æœ‰Mapç±»,Reduceç±»å’Œé©±åŠ¨ç±»,ä¸”æ•°æ®çš„ç±»å‹æ˜¯Hadoopè‡ªèº«å°è£…çš„åºåˆ—åŒ–ç±»å‹.

#### å¸¸ç”¨æ•°æ®åº“ åºåˆ—åŒ–ç±»å‹
> å¸¸ç”¨æ•°æ®ç±»å‹å¯¹åº” Hadoopæ•°æ®åºåˆ—åŒ–ç±»å‹
| Javaæ•°æ®ç±»å‹ |Hadoop Writable |
| :-------- | --------:| 
| boolean    |   BooleanWritable |
| byte    |   ByteWritable | 
| int    |   IntWritable | 
| float    |   FloatWritable | 
| long    |   LongWritable | 
| double    |   DoubleWritable | 
| **`String`**    |   **`Text`** | 
| map    |   MapWritable | 
| array    |   ArrayWritable | 

#### MapReduce ç¼–ç¨‹èŒƒå¼
> å¼€å‘è€…ç¼–å†™ç¨‹åºåˆ†æˆä¸‰ä¸ªéƒ¨åˆ† : **`Mapper`** / **`Reduce`** / **`Driver`**
> 
> **`Mapperé˜¶æ®µ`**
> 1.å¼€å‘è€…è‡ªå®šä¹‰çš„Mapperè¦ç»§æ‰¿è‡ªå·±çš„çˆ¶ç±».
> 2.Mapperçš„è¾“å…¥æ•°æ®æ˜¯K/V(é”®å€¼å¯¹)çš„å½¢å¼,(K/Vçš„ç±»å‹å¯ä»¥è‡ªå®šä¹‰).
> 3.Mapperä¸­çš„ä¸šåŠ¡é€»è¾‘å†™åœ¨map()æ–¹æ³•ä¸­.
> 4.Mapperçš„è¾“å‡ºæ•°æ®æ˜¯K/V(é”®å€¼å¯¹)çš„å½¢å¼,(K/Vçš„ç±»å‹å¯ä»¥è‡ªå®šä¹‰).
> 5.map()æ–¹æ³•(MapTaskè¿›ç¨‹)å¯¹æ¯ä¸€ä¸ª<K,V>è°ƒç”¨ä¸€æ¬¡.
> 
> **`Reduceé˜¶æ®µ`**
> 1.å¼€å‘è€…è‡ªå®šä¹‰çš„Reducerè¦ç»§æ‰¿è‡ªå·±çš„çˆ¶ç±».
> 2.Reducerçš„è¾“å…¥æ•°æ®ç±»å‹å¯¹åº”Mapperçš„è¾“å‡ºæ•°æ®ç±»å‹,ä¹Ÿå°±æ˜¯K/V.
> 3.Reducerçš„ä¸šåŠ¡é€»è¾‘å†™åœ¨reduce()æ–¹æ³•ä¸­.
> 4.ReduceTaskè¿›ç¨‹å¯¹æ¯ä¸€ç»„ç›¸åŒçš„Kçš„<K,V>ç»„è°ƒç”¨ä¸€æ¬¡reduce()æ–¹æ³•.
> 
> **`Driveré˜¶æ®µ`**
> ç›¸å½“äºYarné›†ç¾¤çš„å®¢æˆ·ç«¯,ç”¨äºæäº¤å¼€å‘è€…æ•´ä¸ªç¨‹åºåˆ°Yarné›†ç¾¤,æäº¤çš„æ˜¯å°è£…äº†MapReduceç¨‹åºç›¸å…³è¿è¡Œå‚æ•°çš„jobå¯¹è±¡.

#### WordCount æ¡ˆä¾‹å®æ“
##### 1. æ¡ˆä¾‹éœ€æ±‚ : åœ¨ç»™å®šçš„æ–‡æœ¬æ–‡ä»¶ä¸­ç»Ÿè®¡è¾“å‡ºæ¯ä¸€ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°.
> 
##### 2. åˆ›å»ºdemo.txt è¾“å…¥æ•°æ®æºæ–‡ä»¶
> 
##### 3. ç¼–å†™ æœŸæœ›è¾“å‡ºæ•°æ®,ä¾‹å¦‚
> geek geek
> geekparkhub
> hackerparkhub hackerparkhub hackerparkhub hackerparkhub hackerparkhub
> hadoop hadoop hadoop
> test
> helloworld helloworld
> 
##### 4. éœ€æ±‚åˆ†æ
> æŒ‰ç…§MapReduceç¼–ç¨‹è§„èŒƒ,åˆ†åˆ«ç¼–å†™Mapper,Reducer,Driver,å¦‚å›¾æ‰€ç¤º
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_022.jpg)
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_023.jpg)

##### 5. JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
##### 6. é…ç½® maven pom.xmlæ–‡ä»¶
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.geekparkhub.mapreduce</groupId>
    <artifactId>mapreduce</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>RELEASE</version>
        </dependency>
        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.8.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>2.7.2</version>
        </dependency>
    </dependencies>
</project>
```
##### 7. é…ç½® log4j.propertiesæ–‡ä»¶
``` prolog
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/corehub.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```
##### 8. åˆ›å»º Mapé˜¶æ®µWordcountMapper.class
``` java
package com.geekparkhub.hadoop.mapreduce;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * Map é˜¶æ®µ
 * <p>
 * KEYIN è¾“å…¥æ•°æ®çš„key
 * VALUEIN è¾“å…¥æ•°æ®çš„value
 * KEYOUT è¾“å‡ºæ•°æ®çš„key
 * VALUEOUT è¾“å‡ºæ•°æ®çš„value
 * @author system
 */

public class WordcountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    Text k = new Text();
    IntWritable v = new IntWritable(1);

    /**
     * Rewrite the map() method
     * é‡å†™map()æ–¹æ³•
     *
     * @param key
     * @param value
     * @param context
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        /**
         * 1. Get the first row of data, assuming the data is: geek geek
         * 1. è·å–ç¬¬ä¸€è¡Œæ•°æ®,å‡è®¾æ•°æ®æ˜¯:geek geek
         */
        String line = value.toString();

        /**
         * 2. Cutting data
         * 2. åˆ‡å‰²ç©ºæ ¼æ•°æ®
         */
        String[] words = line.split(" ");

        /**
         * 3. Loop through the data
         * 3. å¾ªç¯éå†æ•°æ®
         */
        for (String word : words) {
            k.set(word);
            context.write(k, v);
        }
    }
}
```
##### 9. åˆ›å»º Reduceé˜¶æ®µWordcountReducer.class
``` java
package com.geekparkhub.hadoop.mapreduce;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * Reducer é˜¶æ®µ
 * <p>
 * KEYIN æ—¢æ˜¯mapé˜¶æ®µè¾“å‡ºçš„key
 * VALUEIN æ—¢æ˜¯mapé˜¶æ®µè¾“å‡ºçš„value
 * @author system
 */

public class WordcountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    /**
     * Rewrite the reduce() method
     * é‡å†™reduce()æ–¹æ³•
     *
     * @param key
     * @param values
     * @param context
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        IntWritable v = new IntWritable();
        /**
         * 1. Accumulate summation
         * 1. ç´¯åŠ æ±‚å’Œ
         */
        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }
        v.set(sum);

        /**
         * 2. Output data
         * 2. è¾“å‡ºæ•°æ®
         */
        context.write(key, v);
    }
}
```
##### 10. åˆ›å»º Driveré˜¶æ®µWordcountDriver.class
``` java
package com.geekparkhub.hadoop.mapreduce;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.log4j.Logger;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * GeekDeveloper : JEEP-711
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * <p>
 * Driver é˜¶æ®µ
 *
 * @author system
 */

public class WordcountDriver {

    /**
     * Statement Logger
     */
    private static org.apache.log4j.Logger log = Logger.getLogger(WordcountDriver.class);

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        /**
         * 1. Get the Job object
         * 1. è·å–Jobå¯¹è±¡
         */
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        /**
         * 2. Set the jar storage location
         * 2. è®¾ç½®jarå­˜å‚¨ä½ç½®
         */
        job.setJarByClass(WordcountDriver.class);

        /**
         * 3. Associate Map and Reduce classes
         * 3. å…³è”Mapå’ŒReduceç±»
         */
        job.setMapperClass(WordcountMapper.class);
        job.setReducerClass(WordcountReducer.class);

        /**
         * 4. Set the key and value types of the output data in the Mapper stage.
         * 4. è®¾ç½®Mapperé˜¶æ®µè¾“å‡ºæ•°æ®çš„keyä¸valueç±»å‹
         */
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        /**
         * 5. Set the key and value types for the final data output
         * 5. è®¾ç½®æœ€ç»ˆæ•°æ®è¾“å‡ºçš„keyä¸valueç±»å‹
         */
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        /**
         * 6. Set the input path and output path
         * 6. è®¾ç½®è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
         */
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        /**
         * 7. Submit the Job
         * 7. æäº¤Job
         */
        boolean result = job.waitForCompletion(true);

        /**
         * 8. Log printing
         * 8. æ—¥å¿—æ‰“å°
         */
        System.exit(result ? 0 : 1);
    }
}
```

##### 11. è¿è¡Œç»“æœ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_024.jpg)

``` powershell
bogon:resources system$ ls -ll
total 48
drwxrwxrwx  1 system  staff  8192  3  2 17:00 input
-rwxrwxrwx  1 system  staff   436  1 21 22:15 log4j.properties
drwxrwxrwx  1 system  staff  8192  3  2 17:21 output
bogon:resources system$ cd output/
._SUCCESS.crc      _SUCCESS           
.part-r-00000.crc  part-r-00000       
bogon:resources system$ cat output/part-r-00000
geek	2
geekparkhub	1
hackerparkhub	5
hadoop	3
helloworld	2
test	1
bogon:resources system$ 
```

##### 12.é›†ç¾¤æµ‹è¯• WordCount
###### åœ¨ä¸­pom.xmlæ·»åŠ ä¾èµ–,ä½¿ç”¨maven install å°†WordCountç¨‹åºæ‰“åŒ…æˆjaråŒ…
``` xml
<build> 
  <plugins> 
    <plugin> 
      <artifactId>maven-compiler-plugin</artifactId>  
      <version>2.3.2</version>  
      <configuration> 
        <source>1.8</source>  
        <target>1.8</target> 
      </configuration> 
    </plugin>  
    <plugin> 
      <artifactId>maven-assembly-plugin</artifactId>  
      <configuration> 
        <descriptorRefs> 
          <descriptorRef>jar-with-dependencies</descriptorRef> 
        </descriptorRefs>  
        <archive> 
          <manifest> 
          <mainClass>com.geekparkhub.hadoop.mapreduce.WordcountDriver</mainClass> 
          </manifest> 
        </archive> 
      </configuration>  
      <executions> 
        <execution> 
          <id>make-assembly</id>  
          <phase>package</phase>  
          <goals> 
            <goal>single</goal> 
          </goals> 
        </execution> 
      </executions> 
    </plugin> 
  </plugins> 
</build>
```

###### è¿è¡ŒWordCount jaråŒ…ç¨‹åº
``` powershell
[root@systemhub511 hadoop]# hadoop jar mapreduce.jar com.geekparkhub.hadoop.mapreduce.WordcountDriver /user/geekparkhub/input /user/geekparkhub/output
19/03/04 21:12:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/04 21:12:57 INFO client.RMProxy: Connecting to ResourceManager at systemhub611/172.16.168.131:8032
19/03/04 21:12:57 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
19/03/04 21:12:58 INFO input.FileInputFormat: Total input paths to process : 1
19/03/04 21:12:58 INFO mapreduce.JobSubmitter: number of splits:1
19/03/04 21:12:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1551704437826_0003
19/03/04 21:12:58 INFO impl.YarnClientImpl: Submitted application application_1551704437826_0003
19/03/04 21:12:58 INFO mapreduce.Job: The url to track the job: http://systemhub611:8088/proxy/application_1551704437826_0003/
19/03/04 21:12:58 INFO mapreduce.Job: Running job: job_1551704437826_0003
19/03/04 21:13:08 INFO mapreduce.Job: Job job_1551704437826_0003 running in uber mode : false
19/03/04 21:13:08 INFO mapreduce.Job:  map 0% reduce 0%
19/03/04 21:13:14 INFO mapreduce.Job:  map 100% reduce 0%
19/03/04 21:13:22 INFO mapreduce.Job:  map 100% reduce 100%
19/03/04 21:13:22 INFO mapreduce.Job: Job job_1551704437826_0003 completed successfully
19/03/04 21:13:22 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=230
                FILE: Number of bytes written=235407
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=261
                HDFS: Number of bytes written=66
                HDFS: Number of read operations=6
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=3817
                Total time spent by all reduces in occupied slots (ms)=4512
                Total time spent by all map tasks (ms)=3817
                Total time spent by all reduce tasks (ms)=4512
                Total vcore-milliseconds taken by all map tasks=3817
                Total vcore-milliseconds taken by all reduce tasks=4512
                Total megabyte-milliseconds taken by all map tasks=3908608
                Total megabyte-milliseconds taken by all reduce tasks=4620288
        Map-Reduce Framework
                Map input records=6
                Map output records=14
                Map output bytes=196
                Map output materialized bytes=230
                Input split bytes=121
                Combine input records=0
                Combine output records=0
                Reduce input groups=6
                Reduce shuffle bytes=230
                Reduce input records=14
                Reduce output records=6
                Spilled Records=28
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=161
                CPU time spent (ms)=1130
                Physical memory (bytes) snapshot=289771520
                Virtual memory (bytes) snapshot=4118065152
                Total committed heap usage (bytes)=139399168
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=140
        File Output Format Counters 
                Bytes Written=66
[root@systemhub511 hadoop]# 
```
###### ä½¿ç”¨hadoop fs -cat æŒ‡ä»¤æŸ¥çœ‹ WordCountç»Ÿè®¡ç»“æœ
```powershell
[root@systemhub511 hadoop]# hadoop fs -cat /user/geekparkhub/output/part-r-00000
19/03/04 21:24:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
geek    2
geekparkhub     1
hackerparkhub   5
hadoop  3
helloworld      2
test    1
[root@systemhub511 hadoop]# 
```



### 7.7.1 Hadoop åºåˆ—åŒ–
#### åºåˆ—åŒ– æ¦‚è¿°
##### ä»€ä¹ˆæ˜¯åºåˆ—åŒ–
> åºåˆ—åŒ–ï¼šå°±æ˜¯æŠŠå†…å­˜ä¸­çš„å¯¹è±¡,è½¬æ¢æˆå­—èŠ‚åºåˆ—,(æˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®)ä»¥ä¾¿äºå­˜å‚¨åˆ°ç£ç›˜(æŒä¹…åŒ–)å’Œç½‘ç»œä¼ è¾“.
> 
> ååºåˆ—åŒ–ï¼šå°±æ˜¯å°†æ”¶åˆ°å­—èŠ‚åºåˆ—,(æˆ–å…¶ä»–æ•°æ®ä¼ è¾“åè®®)æˆ–è€…æ˜¯ç£ç›˜çš„æŒä¹…åŒ–æ•°æ®,è½¬æ¢æˆå†…å­˜ä¸­çš„å¯¹è±¡.
> 
##### ä¸ºä»€ä¹ˆè¦åºåˆ—åŒ–
> ä¸€èˆ¬æ¥è®²,''å­˜æ´»''å¯¹è±¡åªèƒ½ç”Ÿå­˜åœ¨å†…å­˜é‡Œ,å…³æœºæ–­ç”µå°±æ²¡æœ‰äº†,è€Œä¸”''å­˜æ´»''å¯¹è±¡åªèƒ½ç”±æœ¬åœ°çš„è¿›ç¨‹ä½¿ç”¨,ä¸èƒ½è¢«å‘é€åˆ°ç½‘ç»œä¸Šçš„å¦ä¸€å°è®¡ç®—æœº,ç„¶è€Œåºåˆ—åŒ–å¯ä»¥å­˜å‚¨''å­˜æ´»''å¯¹è±¡,å¯ä»¥å°†''å­˜æ´»''å¯¹è±¡å‘é€åˆ°è¿œç¨‹è®¡ç®—æœº.
> 
##### ä¸ºä»€ä¹ˆä¸ä½¿ç”¨javaåºåˆ—åŒ–
> javaåºåˆ—åŒ–æ˜¯ä¸€ä¸ªé‡é‡çº§åºåˆ—åŒ–æ¡†æ¶(Serializable),ä¸€ä¸ªå¯¹è±¡è¢«åºåˆ—åŒ–å,ä¼šé™„å¸¦å¾ˆå¤šé¢å¤–çš„ä¿¡æ¯(å„ç§æ ¡éªŒä¿¡æ¯,Header,ç»§æ‰¿ä½“ç³»ç­‰),ä¸ä¾¿äºåœ¨ç½‘ç»œä¸Šé«˜æ•ˆä¼ è¾“,æ‰€ä»¥Hadoopè‡ªå·±å¼€å‘äº†ä¸€å¥—åºåˆ—åŒ–æœºåˆ¶(Writable).
> 
##### Hadoopåºåˆ—åŒ–ç‰¹ç‚¹
> ç´§å‡‘ï¼šé«˜æ•ˆä½¿ç”¨å­˜å‚¨ç©ºé—´.
> å¿«é€Ÿï¼šè¯»å†™æ•°æ®çš„é¢å¤–å¼€é”€å°.
> å¯æ‰©å±•æ€§ï¼šéšç€é€šè®¯åè®®çš„å‡çº§è€Œå‡çº§.
> äº’æ“ä½œæ€§ï¼šæ”¯æŒå¤šè¯­è¨€äº¤äº’.

#### è‡ªå®šä¹‰beanå¯¹è±¡ å®ç°åºåˆ—åŒ–æ¥å£
> åœ¨ä¼ä¸šå¼€å‘ä¸­å¾€å¾€å¸¸ç”¨çš„åŸºæœ¬åºåˆ—åŒ–ç±»å‹ä¸èƒ½æ»¡è¶³æ‰€æœ‰éœ€æ±‚,æ¯”å¦‚åœ¨Hadoopæ¡†æ¶å†…éƒ¨ä¼ é€’ä¸€ä¸ªbeanå¯¹è±¡,é‚£ä¹ˆè¯¥å¯¹è±¡å°±éœ€è¦å®ç°åºåˆ—åŒ–æ¥å£.
> 
> å…·ä½“å®ç°beanå¯¹è±¡åºåˆ—åŒ– ä¸ƒæ­¥èµ°
> 
> 1.å¿…é¡»å®ç°Writableæ¥å£.
> 2.ååºåˆ—åŒ–æ—¶,éœ€è¦åå°„æœºåˆ¶è°ƒç”¨ç©ºæ„é€ å‡½æ•°,æ‰€ä»¥å¿…é¡»è¦æœ‰ç©ºæ„é€ å‡½æ•°.
``` java
    /**
     * When deserializing, you need to reflect the call to the null parameter constructor.
     * ååºåˆ—åŒ–æ—¶,éœ€è¦åå°„è°ƒç”¨ç©ºå‚æ„é€ å‡½æ•°
     */
    public FlowBean() {
        super();
    }
```
> 3.é‡å†™åºåˆ—åŒ–æ–¹æ³•.
``` java
    /**
     * Serialization method
     * åºåˆ—åŒ–æ–¹æ³•
     *
     * @param out
     * @throws IOException
     */
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeLong(upFlow);
        out.writeLong(downFlow);
        out.writeLong(sumFlow);
    }
```
> 4.é‡å†™ååºåˆ—åŒ–æ–¹æ³•
> 5.æ³¨æ„ååºåˆ—åŒ–çš„é¡ºåºå’Œåºåˆ—åŒ–çš„é¡ºåºå®Œå…¨ä¸€è‡´
``` java
    /**
     * Deserialization method, the deserialization method read order must be consistent with the write order of the write serialization method
     * ååºåˆ—åŒ–æ–¹æ³•,ååºåˆ—åŒ–æ–¹æ³•è¯»é¡ºåºå¿…é¡»å’Œå†™åºåˆ—åŒ–æ–¹æ³•çš„å†™é¡ºåºå¿…é¡»ä¸€è‡´
     *
     * @param in
     * @throws IOException
     */
    @Override
    public void readFields(DataInput in) throws IOException {
        this.upFlow = in.readLong();
        this.downFlow = in.readLong();
        this.sumFlow = in.readLong();
    }
```
> 6.è¦æƒ³æŠŠç»“æœæ˜¾ç¤ºåœ¨æ–‡ä»¶ä¸­ï¼Œéœ€è¦é‡å†™toString()ï¼Œå¯ç”¨â€\tâ€åˆ†å¼€ï¼Œæ–¹ä¾¿åç»­ç”¨.
``` java
    /**
     * Write a to String method to facilitate subsequent printing to text
     * ç¼–å†™toStringæ–¹æ³•,æ–¹ä¾¿åç»­æ‰“å°åˆ°æ–‡æœ¬
     *
     * @return
     */
    @Override
    public String toString() {
        return upFlow + "\t" + downFlow + "\t" + sumFlow;
    }
```
> 7.å¦‚æœéœ€è¦å°†è‡ªå®šä¹‰çš„beanæ”¾åœ¨keyä¸­ä¼ è¾“ï¼Œåˆ™è¿˜éœ€è¦å®ç°comparableæ¥å£ï¼Œå› ä¸ºmapreduceæ¡†ä¸­çš„shuffleè¿‡ç¨‹ä¸€å®šä¼šå¯¹keyè¿›è¡Œæ’åº

#### åºåˆ—åŒ– æ¡ˆä¾‹å®æ“
##### 1.éœ€æ±‚
> ç»Ÿè®¡æ¯ä¸€ä¸ªæ‰‹æœºå·è€—è´¹çš„æ€»ä¸Šè¡Œæµé‡ã€ä¸‹è¡Œæµé‡ã€æ€»æµé‡.
##### 2.è·å–æ•°æ®æº:æ¥è‡ªç½‘ç»œèµ„æº
##### 3.è¾“å…¥æ•°æ®æ ¼å¼:
```
1   130001099990 111.186.104.167 www.baidu.com 28219 21031 200
2   15026889999 180.166.156.78 www.google.com 264 0 200
3   13601029999 212.64.111.89 www.github.com 132 1512 200
4   14512449999 117.135.178.67 www.qq.com 1929 180 200
5   15210039999 211.136.129.80 www.shouhu.com 132 15152 200
6   15510759999 112.65.214.26 www.qingha.com 2008 2779 200
7   15810579999 140.206.76.67 www.alibaba.com 9087 3673 200
8   13900999999 27.115.112.25 www.info.xcar.com.cn 46 177 200
9   13341098674 39.129.1.90 www.yq.aliyun.com 976 7661 200
10  14701159999 218.206.61.16 www.flaticon.com 5432 12 200
11  15116949999 219.159.60.26 www.translate.google.com 3 398 200
12  13261999999 36.111.136.126 www.blog.csdn.net 745 21 200 
13  15910419999 222.74.169.128 www.zhangshengrong.com 3890 496 200
14  18618689999 61.138.127.67 www.cn.bing.com 63 1498 200
15  18810599999 101.124.10.67 www.gitee.com 196 3360 200
16  18901009997 106.39.56.671 www.pai.com 16 289 200
17  13341099905 114.67.225.123 www.importnew.com 203 46 200
18  18221609878 116.196.121.45 www.booking.com 1732 698 200
19  01058484076 192.144.135.12 www.zhipin.com 80 1469 200
20  01082895409 221.176.7.23 www.bing.com 7596 264 200 
21  18674215555 139.219.14.124 www.facebook.com 92 738 200
22  15527194444 211.150.90.01 www.refinery29.com 5493 189 200 
23  31125344449 113.61.165.26 www.thenextweb.com 1892 25 200
24  15542102444 180.218.164.34 www.cinemablend.com 394 29 200
25  18674215555 60.245.45.34 www.oschina.net 4782 968 200
26  18476943333 61.139.47.27 www.tool.cn 3215 14 200
```
##### 4.è¾“å‡ºæ•°æ®æ ¼å¼:
```
    13560436666  1116  954  2070

    æ‰‹æœºå·ç  ä¸Šè¡Œæµé‡ ä¸‹è¡Œæµé‡ æ€»æµé‡
```
##### 5.åˆ†æåŸºæœ¬æ€è·¯:
> Mapé˜¶æ®µ:
> 1.è¯»å–ä¸€è¡Œæ•°æ®,åˆ‡åˆ†å­—æ®µ
> 2.æŠ½å–æ‰‹æœºå·ã€ä¸Šè¡Œæµé‡ã€ä¸‹è¡Œæµé‡
> 3.ä»¥æ‰‹æœºå·ä¸ºkey,beanå¯¹è±¡ä¸ºvalueè¾“å‡º,å³context.write(æ‰‹æœºå·,bean);
> 
> Reduceé˜¶æ®µ:
> 1.ç´¯åŠ ä¸Šè¡Œæµé‡å’Œä¸‹è¡Œæµé‡å¾—åˆ°æ€»æµé‡.
> 2.å®ç°è‡ªå®šä¹‰çš„beanæ¥å°è£…æµé‡ä¿¡æ¯,å¹¶å°†beanä½œä¸ºmapè¾“å‡ºçš„keyæ¥ä¼ è¾“
> 3.MRç¨‹åºåœ¨å¤„ç†æ•°æ®çš„è¿‡ç¨‹ä¸­ä¼šå¯¹æ•°æ®æ’åº(mapè¾“å‡ºçš„kvå¯¹ä¼ è¾“åˆ°reduceä¹‹å‰,ä¼šæ’åº),æ’åºçš„ä¾æ®æ˜¯mapè¾“å‡ºçš„key,æ‰€ä»¥æˆ‘ä»¬å¦‚æœè¦å®ç°è‡ªå·±éœ€è¦çš„æ’åºè§„åˆ™,åˆ™å¯ä»¥è€ƒè™‘å°†æ’åºå› ç´ æ”¾åˆ°keyä¸­,è®©keyå®ç°æ¥å£:WritableComparable,ç„¶åé‡å†™keyçš„compareToæ–¹æ³•
##### 6.ç¼–å†™MapReduceç¨‹åº
> ç¼–å†™æµé‡ç»Ÿè®¡ FlowBean
``` java
package com.geekparkhub.hadoop.flowsum;

import org.apache.hadoop.io.Writable;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 * @author system
 * <p>
 * FlowBean åºåˆ—åŒ–
 * <p>
 */

public class FlowBean implements Writable {

    /**
     * Upstream traffic
     * ä¸Šè¡Œæµé‡
     */
    private long upFlow;

    /**
     * Downstream traffic
     * ä¸‹è¡Œæµé‡
     */
    private long downFlow;

    /**
     * Total flow
     * æ€»æµé‡
     */
    private long sumFlow;

    /**
     * When deserializing, you need to reflect the call to the null parameter constructor.
     * ååºåˆ—åŒ–æ—¶,éœ€è¦åå°„è°ƒç”¨ç©ºå‚æ„é€ å™¨
     */
    public FlowBean() {
        super();
    }

    /**
     * Parametric constructor
     * æœ‰å‚æ„é€ å™¨
     *
     * @param upFlow
     * @param downFlow
     */
    public FlowBean(long upFlow, long downFlow) {
        super();
        upFlow = upFlow;
        downFlow = downFlow;
        sumFlow = upFlow + downFlow;
    }

    /**
     * Serialization method
     * åºåˆ—åŒ–æ–¹æ³•
     *
     * @param out
     * @throws IOException
     */
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeLong(upFlow);
        out.writeLong(downFlow);
        out.writeLong(sumFlow);
    }

    /**
     * Deserialization method, the deserialization method read order must be consistent with the write order of the write serialization method
     * ååºåˆ—åŒ–æ–¹æ³•,ååºåˆ—åŒ–æ–¹æ³•è¯»é¡ºåºå¿…é¡»å’Œå†™åºåˆ—åŒ–æ–¹æ³•çš„å†™é¡ºåºå¿…é¡»ä¸€è‡´
     *
     * @param in
     * @throws IOException
     */
    @Override
    public void readFields(DataInput in) throws IOException {
        upFlow = in.readLong();
        downFlow = in.readLong();
        sumFlow = in.readLong();
    }

    /**
     * Write a to String method to facilitate subsequent printing to text
     * ç¼–å†™toStringæ–¹æ³•,æ–¹ä¾¿åç»­æ‰“å°åˆ°æ–‡æœ¬
     *
     * @return
     */
    @Override
    public String toString() {
        return upFlow + "\t" + downFlow + "\t" + sumFlow;
    }

    /**
     * Get&Set method
     * Get&Setæ–¹æ³•
     *
     * @return
     */
    public long getUpFlow() {
        return upFlow;
    }

    public void setUpFlow(long upFlow) {
        this.upFlow = upFlow;
    }

    public long getDownFlow() {
        return downFlow;
    }

    public void setDownFlow(long downFlow) {
        this.downFlow = downFlow;
    }

    public long getSumFlow() {
        return sumFlow;
    }

    public void setSumFlow(long sumFlow) {
        this.sumFlow = sumFlow;
    }

    public void set(long upFlow2,long downFlow2){
        upFlow = upFlow2;
        downFlow = downFlow2;
        sumFlow = upFlow2 + downFlow2;
    }

}
```
> ç¼–å†™æµé‡ç»Ÿè®¡ FlowCountMapper
``` java
package com.geekparkhub.hadoop.flowsum;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * FlowCountMapper åºåˆ—åŒ–
 * <p>
 */

public class FlowCountMapper extends Mapper<LongWritable, Text, Text, FlowBean> {

    /**
     * Extract k, v
     * æå–k,v
     */
    Text k = new Text();
    FlowBean v = new FlowBean();

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        /**
         * Get the first row of data
         * è·å–ç¬¬ä¸€è¡Œæ•°æ®
         */
        String line = value.toString();

        /**
         * Cutting data
         * åˆ‡å‰²æ•°æ®
         */
        String[] fields = line.split(" ");

        /**
         * Package object
         * å°è£…å¯¹è±¡
         */

        // å°è£…æ‰‹æœºå· | Package phone number
        k.set(fields[1]);

        long upFlow = Long.parseLong(fields[fields.length - 3]);
        long downFlow = Long.parseLong(fields[fields.length - 2]);

        v.setUpFlow(upFlow);
        v.setDownFlow(downFlow);

        /**
         * data input
         * å†™å…¥æ•°æ®
         */
        context.write(k, v);
    }
}
```
> ç¼–å†™æµé‡ç»Ÿè®¡ FlowCountReducer
``` java
package com.geekparkhub.hadoop.flowsum;

import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * FlowCountReducer åºåˆ—åŒ–
 * <p>
 */

public class FlowCountReducer extends Reducer<Text,FlowBean,Text,FlowBean> {

    FlowBean v = new FlowBean();

    @Override
    protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {

        long sum_upFlow = 0;
        long sum_downFlow = 0;

        /**
         * Cumulative summation
         * ç´¯åŠ æ±‚å’Œ
         */
        for (FlowBean flowBean : values){
            sum_upFlow += flowBean.getUpFlow();
            sum_downFlow += flowBean.getDownFlow();
        }
        v.set(sum_upFlow,sum_downFlow);

        /**
         * data input
         * å†™å…¥æ•°æ®
         */
        context.write(key,v);
    }
}
```
> ç¼–å†™æµé‡ç»Ÿè®¡ FlowsumDriver
``` java
package com.geekparkhub.hadoop.flowsum;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.fs.Path;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * FlowsumDriver åºåˆ—åŒ–
 * <p>
 */

public class FlowsumDriver {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        /**
         * Preset data input and output path
         * é¢„è®¾æ•°æ®è¾“å…¥è¾“å‡ºè·¯å¾„
         */
        args = new String[]{"/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/input_flow",
                "/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/output_flow"};

        /**
         * Get configuration information, or job object instance
         * è·å–é…ç½®ä¿¡æ¯,æˆ–è€…jobå¯¹è±¡å®ä¾‹
         */
        Configuration configuration = new Configuration();
        Job job = Job.getInstance(configuration);

        /**
         * Specify the local path where the jar package of the program is located.
         * æŒ‡å®šæœ¬ç¨‹åºçš„jaråŒ…æ‰€åœ¨çš„æœ¬åœ°è·¯å¾„
         */
        job.setJarByClass(FlowsumDriver.class);

        /**
         * Specify the mapper/Reducer business class to be used by this business job
         * æŒ‡å®šæœ¬ä¸šåŠ¡jobè¦ä½¿ç”¨çš„mapper/Reducerä¸šåŠ¡ç±»
         */
        job.setMapperClass(FlowCountMapper.class);
        job.setReducerClass(FlowCountReducer.class);

        /**
         * Specify the kv type of the mapper output data
         * æŒ‡å®šmapperè¾“å‡ºæ•°æ®çš„kvç±»å‹
         */
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FlowBean.class);

        /**
         * Specify the kv type of the final output data
         * æŒ‡å®šæœ€ç»ˆè¾“å‡ºçš„æ•°æ®çš„kvç±»å‹
         */
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FlowBean.class);

        /**
         * Specify the directory where the input input file of the job is located.
         * æŒ‡å®šjobçš„è¾“å…¥åŸå§‹æ–‡ä»¶æ‰€åœ¨ç›®å½•
         */
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        /**
         * Submit the relevant parameters configured in the job, and the jar package where the java class used by the job is located, and submit it to the yarn to run.
         * å°†jobä¸­é…ç½®çš„ç›¸å…³å‚æ•°,ä»¥åŠjobæ‰€ç”¨çš„javaç±»æ‰€åœ¨çš„jaråŒ…,æäº¤ç»™yarnå»è¿è¡Œ
         */
        boolean results = job.waitForCompletion(true);
        System.exit(results ? 0 : 1);
    }
}
```

##### è¾“å…¥è¿è¡Œç»“æœ
```
01058484076	890	1469	2359
01082895409	7596	264	7860
130001099990	28219	21031	49250
13261999999	745	231	976
13341098674	976	7661	8637
13341099905	203	466	669
13601029999	132	1512	1644
13900999999	456	177	633
14512449999	1929	180	2109
14701159999	5432	122	5554
15026889999	264	980	1244
15116949999	743	398	1141
15210039999	132	15152	15284
15510759999	2008	2779	4787
15527194444	5493	189	5682
15542102444	3394	329	3723
15810579999	9087	3673	12760
15910419999	3890	496	4386
18221609878	1732	698	2430
18344215555	3992	738	4730
18476943333	3215	164	3379
18618689999	663	1498	2161
18674215555	4782	968	5750
18810599999	196	3360	3556
18901009997	816	289	1105
31125344449	1892	255	2147
```

### 7.7.2 MapReduce æ¡†æ¶åŸç†
### MapReduce æ¡†æ¶åŸç†æµç¨‹å›¾(ä¸€)
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_026.jpg)

### MapReduce æ¡†æ¶åŸç†æµç¨‹å›¾(äºŒ)
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_027.jpg)

>  æµç¨‹è¯¦è§£ä¸Šé¢çš„æµç¨‹æ˜¯æ•´ä¸ªmapreduceæœ€å…¨å·¥ä½œæµç¨‹,ä½†æ˜¯shuffleè¿‡ç¨‹åªæ˜¯ä»ç¬¬7æ­¥å¼€å§‹ åˆ°ç¬¬16æ­¥ç»“æŸ,å…·ä½“shuffleè¿‡ç¨‹è¯¦è§£,å¦‚ä¸‹:
>  
> 1.maptaskæ”¶é›†æˆ‘ä»¬çš„map()æ–¹æ³•è¾“å‡ºçš„kvå¯¹,æ”¾åˆ°å†…å­˜ç¼“å†²åŒºä¸­.
> 
> 2.ä»å†…å­˜ç¼“å†²åŒºä¸æ–­æº¢å‡ºæœ¬åœ°ç£ç›˜æ–‡ä»¶,å¯èƒ½ä¼šæº¢å‡ºå¤šä¸ªæ–‡ä»¶.
> 
> 3.å¤šä¸ªæº¢å‡ºæ–‡ä»¶ä¼šè¢«åˆå¹¶æˆå¤§çš„æº¢å‡ºæ–‡ä»¶.
> 4.åœ¨æº¢å‡ºè¿‡ç¨‹ä¸­,åŠåˆå¹¶çš„è¿‡ç¨‹ä¸­,éƒ½è¦è°ƒç”¨partitionerè¿›è¡Œåˆ†åŒºå’Œé’ˆå¯¹keyè¿›è¡Œæ’åº.
> 
> 5.reducetaskæ ¹æ®è‡ªå·±çš„åˆ†åŒºå·,å»å„ä¸ªmaptaskæœºå™¨ä¸Šå–ç›¸åº”çš„ç»“æœåˆ†åŒºæ•°æ®.
> 6.reducetaskä¼šå–åˆ°åŒä¸€ä¸ªåˆ†åŒºçš„æ¥è‡ªä¸åŒmaptaskçš„ç»“æœæ–‡ä»¶,reducetaskä¼šå°†è¿™äº›æ–‡ä»¶å†è¿›è¡Œåˆå¹¶(å½’å¹¶æ’åº).
> 
> 7.åˆå¹¶æˆå¤§æ–‡ä»¶å,shuffleçš„è¿‡ç¨‹ä¹Ÿå°±ç»“æŸäº†,åé¢è¿›å…¥reducetaskçš„é€»è¾‘è¿ç®—è¿‡ç¨‹,(ä»æ–‡ä»¶ä¸­å–å‡ºä¸€ä¸ªä¸€ä¸ªçš„é”®å€¼å¯¹group,è°ƒç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„reduce()æ–¹æ³•).
> 
> 8.æ³¨æ„Shuffleä¸­çš„ç¼“å†²åŒºå¤§å°ä¼šå½±å“åˆ°mapreduceç¨‹åºçš„æ‰§è¡Œæ•ˆç‡,åŸåˆ™ä¸Šè¯´,ç¼“å†²åŒºè¶Šå¤§,ç£ç›˜ioçš„æ¬¡æ•°è¶Šå°‘,æ‰§è¡Œé€Ÿåº¦å°±è¶Šå¿«,ç¼“å†²åŒºçš„å¤§å°å¯ä»¥é€šè¿‡å‚æ•°è°ƒæ•´,å‚æ•°:io.sort.mb,é»˜è®¤100M.

### 7.7.3.1 InputFormat æ•°æ®è¾“å…¥
#### åˆ‡ç‰‡ä¸MapTaskå¹¶è¡Œåº¦å†³å®šæœºåˆ¶
##### 1.é—®é¢˜å¼•å‡º
>  MapTaskçš„å¹¶è¡Œåº¦å†³å®šMapé˜¶æ®µçš„ä»»åŠ¡å¤„ç†å¹¶å‘åº¦,è¿›è€Œå½±å“åˆ°æ•´ä¸ªJobçš„å¤„ç†é€Ÿåº¦.
> Q&A 1Gçš„æ•°æ®,å¯åŠ¨8ä¸ªMapTask,å¯ä»¥æé«˜é›†ç¾¤çš„å¹¶å‘å¤„ç†èƒ½åŠ›,é‚£ä¹ˆ1Kçš„æ•°æ®,ä¹Ÿå¯åŠ¨8ä¸ªMapTask,ä¹Ÿä¼šæé«˜é›†ç¾¤æ€§èƒ½å—?
> MapTaskå¹¶è¡Œä»»åŠ¡æ˜¯å¦è¶Šå¤šè¶Šå¥½? å“ªäº›å› ç´ å½±å“åˆ°MapTaskå¹¶è¡Œåº¦?
##### 2.MapTaskå¹¶è¡Œåº¦å†³å®šæœºåˆ¶
> æ•°æ®å—:Blockæ˜¯HDFSç‰©ç†ä¸ŠæŠŠæ•°æ®åˆ†æˆä¸€å—ä¸€å—.
> æ•°æ®åˆ‡ç‰‡:æ•°æ®åˆ‡ç‰‡æ˜¯æŒ‡åœ¨é€»è¾‘ä¸Šå¯¹è¾“å…¥è¿›è¡Œåˆ†ç‰‡,å¹¶ä¸ä¼šåœ¨ç£ç›˜ä¸Šå°†å…¶åˆ‡åˆ†ç‰‡è¿›è¡Œå­˜å‚¨.
##### æ•°æ®åˆ‡ç‰‡ä¸MapTaskå¹¶è¡Œåº¦å†³å®šæœºåˆ¶
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_025.jpg)

#### Jobæäº¤æµç¨‹æºç  å’Œ åˆ‡ç‰‡æºç è¯¦è§£

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_028.jpg)

##### æå– jobæäº¤æµç¨‹ æ ¸å¿ƒæºç 
``` java
waitForCompletion();
    submit();
        // å»ºç«‹è¿æ¥
        connect();
        // åˆ›å»ºæäº¤jobçš„ä»£ç†
        new Cluster(getConfiguration());
            // åˆ¤æ–­æ˜¯æœ¬åœ°yarnè¿˜æ˜¯è¿œç¨‹
            initialize(jobTrackAddr, conf); 
// æäº¤
sjobsubmitter.submitJobInternal(Job.this, cluster);
    // åˆ›å»ºç»™é›†ç¾¤æäº¤æ•°æ®çš„Stagè·¯å¾„
    Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); 
    // è·å–jobid,å¹¶åˆ›å»ºjobè·¯å¾„
    JobID jobId = submitClient.getNewJobID();
    // æ‹·è´jaråŒ…åˆ°é›†ç¾¤
    copyAndConfigureFiles(job, submitJobDir);
        rUploader.uploadFiles(job, jobSubmitDir);
    // è®¡ç®—åˆ‡ç‰‡,ç”Ÿæˆåˆ‡ç‰‡è§„åˆ’æ–‡ä»¶
    writeSplits(job, submitJobDir); 
        maps = writeNewSplits(job, jobSubmitDir); 
            input.getSplits(job); 
    // å‘Stagè·¯å¾„å†™xmlé…ç½®æ–‡ä»¶
    writeConf(conf, submitJobFile);
        conf.writeXml(out);
    // æäº¤job,è¿”å›æäº¤çŠ¶æ€
    status = submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
```

#### FileInputFormat åˆ‡ç‰‡
> FileInputFormatæºç è§£æ(input.getSplits(job))
> æ‰¾åˆ°æ•°æ®å­˜å‚¨çš„ç›®å½•.
> å¼€å§‹éå†å¤„ç†(è§„åˆ’åˆ‡ç‰‡)ä¸‹çš„æ¯ä¸€ä¸ªæ–‡ä»¶éå†ç¬¬ä¸€ä¸ªæ–‡ä»¶ss.txt.
> aï¼‰è·å–æ–‡ä»¶å¤§å°fs.sizeOf(ss.txt).
> bï¼‰è®¡ç®—åˆ‡ç‰‡å¤§å°computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M.
> cï¼‰é»˜è®¤æƒ…å†µä¸‹,åˆ‡ç‰‡å¤§å°=blocksized.
> å¼€å§‹åˆ‡,å½¢æˆç¬¬1ä¸ªåˆ‡ç‰‡:ss.txtâ€”0:128M,ç¬¬2ä¸ªåˆ‡ç‰‡ss.txtâ€”128:256M,ç¬¬3ä¸ªåˆ‡ç‰‡ss.txtâ€”256M:300M,(æ¯æ¬¡åˆ‡ç‰‡æ—¶,éƒ½è¦åˆ¤æ–­åˆ‡å®Œå‰©ä¸‹çš„éƒ¨åˆ†æ˜¯å¦å¤§äºå—çš„1.1å€,ä¸å¤§äº1.1å€å°±åˆ’åˆ†ä¸€å—åˆ‡ç‰‡).
> eï¼‰å°†åˆ‡ç‰‡ä¿¡æ¯å†™åˆ°ä¸€ä¸ªåˆ‡ç‰‡è§„åˆ’æ–‡ä»¶ä¸­.
> fï¼‰æ•´ä¸ªåˆ‡ç‰‡çš„æ ¸å¿ƒè¿‡ç¨‹åœ¨getSplit()æ–¹æ³•ä¸­å®Œæˆ.
> gï¼‰æ•°æ®åˆ‡ç‰‡åªæ˜¯åœ¨é€»è¾‘ä¸Šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œåˆ†ç‰‡,å¹¶ä¸ä¼šå†ç£ç›˜ä¸Šå°†å…¶åˆ‡åˆ†æˆåˆ†ç‰‡è¿›è¡Œå­˜å‚¨,InputSplitåªè®°å½•äº†åˆ†ç‰‡çš„å…ƒæ•°æ®ä¿¡æ¯,æ¯”å¦‚èµ·å§‹ä½ç½®ã€é•¿åº¦ä»¥åŠæ‰€åœ¨çš„èŠ‚ç‚¹åˆ—è¡¨ç­‰.
> hï¼‰æ³¨æ„:blockæ˜¯HDFSç‰©ç†ä¸Šå­˜å‚¨çš„æ•°æ®,åˆ‡ç‰‡æ˜¯å¯¹æ•°æ®é€»è¾‘ä¸Šçš„åˆ’åˆ†.
> æäº¤åˆ‡ç‰‡è§„åˆ’æ–‡ä»¶åˆ°yarnä¸Š,yarnä¸Šçš„MrAppMasterå°±å¯ä»¥æ ¹æ®åˆ‡ç‰‡è§„åˆ’æ–‡ä»¶è®¡ç®—å¼€å¯maptaskä¸ªæ•°.

#### FileInputFormat åˆ‡ç‰‡æœºåˆ¶
##### åˆ‡ç‰‡æœºåˆ¶
> FileInputFormatä¸­é»˜è®¤çš„åˆ‡ç‰‡æœºåˆ¶:
> 1.ç®€å•åœ°æŒ‰ç…§æ–‡ä»¶çš„å†…å®¹é•¿åº¦è¿›è¡Œåˆ‡ç‰‡.
> 2.åˆ‡ç‰‡å¤§å°,é»˜è®¤ç­‰äºBlockå¤§å°.
> 3.åˆ‡ç‰‡æ—¶ä¸è€ƒè™‘æ•°æ®é›†æ•´ä½“,è€Œæ˜¯é€ä¸ªé’ˆå¯¹æ¯ä¸€ä¸ªæ–‡ä»¶å•ç‹¬åˆ‡ç‰‡.
> 4.åœ¨æœ¬åœ°è¿è¡Œæ¨¡å¼ä¸‹Blockå—å¤§å°ä¸º32M,åœ¨é›†ç¾¤è¿è¡Œæ¨¡å¼ä¸‹Blockå—å¤§å°ä¸º128M.
##### æ¡ˆä¾‹åˆ†æ
> æ¯”å¦‚å¾…å¤„ç†æ•°æ®æœ‰ä¸¤ä¸ªæ–‡ä»¶:
> 
> file1.txt 320M
> file2.txt 10M
> 
> ç»è¿‡FileInputFormatçš„åˆ‡ç‰‡æœºåˆ¶è¿ç®—åï¼Œå½¢æˆçš„åˆ‡ç‰‡ä¿¡æ¯å¦‚ä¸‹:
> file1.txt.split1  --  0~128
> file1.txt.split2  --  128~256
> file1.txt.split3  --  256~320
> file2.txt.split1  --  0~10M
> 
> FileInputFormatåˆ‡ç‰‡å¤§å°çš„å‚æ•°é…ç½®
> 
> é€šè¿‡åˆ†ææºç ,åœ¨FileInputFormatä¸­,è®¡ç®—åˆ‡ç‰‡å¤§å°çš„é€»è¾‘:
> 
> åˆ‡ç‰‡ä¸»è¦ç”±è¿™å‡ ä¸ªå€¼æ¥è¿ç®—å†³å®š.
```
Math.max(minSize,Math.min(maxSize,blockSize));
```

```
// é»˜è®¤å€¼ä¸º1
mapreduce.input.fileinputformat.split.minsize=1
```

```
// é»˜è®¤å€¼Long.MAXValue / Longç±»å‹çš„æœ€å¤§å€¼
mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue
```

> å› æ­¤,é»˜è®¤æƒ…å†µä¸‹,åˆ‡ç‰‡å¤§å°=blocksize
> 
> maxsize(åˆ‡ç‰‡æœ€å¤§å€¼):å‚æ•°å¦‚æœè°ƒå¾—æ¯”blocksizeå°,åˆ™ä¼šè®©åˆ‡ç‰‡å˜å°,è€Œä¸”å°±ç­‰äºé…ç½®çš„è¿™ä¸ªå‚æ•°çš„å€¼.
> 
> minsize(åˆ‡ç‰‡æœ€å°å€¼):å‚æ•°è°ƒçš„æ¯”blockSizeå¤§,åˆ™å¯ä»¥è®©åˆ‡ç‰‡å˜å¾—æ¯”blocksizeè¿˜å¤§.
> 
> è·å–åˆ‡ç‰‡ä¿¡æ¯API
```
// æ ¹æ®æ–‡ä»¶ç±»å‹è·å–åˆ‡ç‰‡ä¿¡æ¯
FileSplit inputSplit = (FileSplit) context.getInputSplit();
// è·å–åˆ‡ç‰‡çš„æ–‡ä»¶åç§°
String name = inputSplit.getPath().getName();
```
#### CombineTexInputFormat åˆ‡ç‰‡æœºåˆ¶
> æ¡†æ¶é»˜è®¤çš„TexInputFormatåˆ‡ç‰‡æœºåˆ¶æ˜¯å¯¹ä»»åŠ¡æŒ‰æ–‡ä»¶è§„åˆ’åˆ‡ç‰‡,ä¸ç®¡æ–‡ä»¶å¤šå°,éƒ½ä¼šæ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ‡ç‰‡,éƒ½ä¼šäº¤ç»™ä¸€ä¸ªMapTask,è¿™æ ·å¦‚æœæœ‰å¤§é‡çš„å°æ–‡ä»¶,å°±ä¼šäº§ç”Ÿå¤§é‡çš„MapTask,å¤„ç†æ•ˆç‡éå¸¸ä½.
##### 1.åº”ç”¨åœºæ™¯
> CombineTexInputFormatç”¨äºå°æ–‡ä»¶è¿‡å¤šçš„åœºæ™¯,å®ƒå¯ä»¥å°†å¤šä¸ªå°æ–‡ä»¶ä»é€»è¾‘ä¸Šè§„åˆ’åˆ°ä¸€ä¸ªåˆ‡ç‰‡ä¸­,è¿™æ ·å¤šä¸ªå°æ–‡ä»¶å°±å¯ä»¥äº¤ç»™ä¸€ä¸ªMapTsakå¤„ç†.
##### 2.è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§å€¼è®¾ç½®
```
 // è®¾ç½®ä¸º4MB
 CombineTexInputFormat.setMaxInputSplitSize(job,4194304);
```
> æ³¨æ„:è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§è®¾ç½®æœ€å¥½è¦æ ¹æ®å®é™…çš„å°æ–‡ä»¶å¤§å°æƒ…å†µæ¥è®¾ç½®å…·ä½“å‚æ•°.
##### 3.åˆ‡ç‰‡æœºåˆ¶
> ç”Ÿæˆåˆ‡ç‰‡è¿‡ç¨‹ä¸ºä¸¤éƒ¨åˆ†:è™šæ‹Ÿå­˜å‚¨è¿‡ç¨‹å’Œåˆ‡ç‰‡è¿‡ç¨‹.
> 
###### è™šæ‹Ÿå­˜å‚¨è¿‡ç¨‹:
> å°†è¾“å…¥ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶å¤§å°,ä¾æ¬¡å’Œè®¾ç½®çš„setMaxInputSplitSizeå€¼æ¯”è¾ƒ,å¦‚æœä¸å¤§äºè®¾ç½®çš„æœ€å¤§å€¼,é€»è¾‘ä¸Šåˆ’åˆ†ä¸€ä¸ªå—,å¦‚æœè¾“å…¥æ–‡ä»¶å¤§å°è®¾ç½®çš„æœ€å¤§å€¼ä¸”å¤§äºä¸¤å€,é‚£ä¹ˆä»¥æœ€å¤§å€¼åˆ‡å‰²ä¸€å—,å½“å‰©ä½™æ•°æ®å¤§å°è¶…è¿‡è®¾ç½®çš„æœ€å¤§å€¼ä¸”ä¸å¤§äºæœ€å¤§å€¼2å€,æ­¤æ—¶å°†æ–‡ä»¶å‡åˆ†ä¸º2ä¸ªè™šæ‹Ÿå­˜å‚¨å—(é˜²æ­¢å‡ºç°å¤ªå°åˆ‡ç‰‡).
> 
> ä¾‹å¦‚setMaxInputSplitSizeå€¼ä¸º4M,è¾“å…¥æ–‡ä»¶å¤§å°ä¸º8.02M,åˆ™é€»è¾‘ä¸Šåˆ†æˆä¸€ä¸ª4M,å‰©ä½™çš„å¤§å°ä¸º4.02M,å¦‚æœæŒ‰ç…§4Mé€»è¾‘åˆ’åˆ†,å°±ä¼šå‡ºç°0.02Mçš„å°è™šæ‹Ÿå­˜å‚¨æ–‡ä»¶,æ‰€ä»¥å°†å‰©ä½™çš„4.02Mæ–‡ä»¶åˆ‡åˆ†æˆ(2.01Må’Œ2.01M)ä¸¤ä¸ªæ–‡ä»¶.
> 
###### åˆ‡ç‰‡è¿‡ç¨‹:
> 1.åˆ¤æ–­è™šæ‹Ÿå­˜å‚¨æ‰“æ–‡ä»¶å¤§å°æ˜¯å¦å¤§äºsetMaxInputSplitSizeå€¼,å¤§äºç­‰äºåˆ™å•ç‹¬å½¢æˆä¸€ä¸ªåˆ‡ç‰‡.
> 
> 2.å¦‚æœä¸å¤§äºåˆ™è·Ÿä¸‹ä¸€ä¸ªè™šæ‹Ÿå­˜å‚¨æ–‡ä»¶è¿›è¡Œåˆå¹¶,å¹¶åŒå½¢æˆä¸€ä¸ªåˆ‡ç‰‡.
> 
> 3.æµ‹è¯•ä¾‹å­:æœ‰4ä¸ªå°æ–‡ä»¶å¤§å°åˆ†åˆ«ä¸º,1.7M / 5.1M / 3.4M / 6.8M,è¿™å››ä¸ªå°æ–‡ä»¶,åˆ™è™šæ‹Ÿå­˜å‚¨ä¹‹åå½¢æˆ6ä¸ªæ–‡ä»¶å—,å¤§å°åˆ†åˆ«ä¸ºå¦‚å›¾æ‰€ç¤º.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_029.jpg)

#### CombineTexInputFormat æ¡ˆä¾‹å®æ“
##### 1.éœ€æ±‚
> å°†è¾“å…¥çš„å¤§é‡å°æ–‡ä»¶åˆå¹¶å½¢æˆä¸€ä¸ªåˆ‡ç‰‡ç»Ÿä¸€å¤„ç†.
##### è¾“å…¥æ•°æ®:å‡†å¤‡å››ä¸ªå°æ–‡ä»¶.
##### æœŸæœ›:ä¸€ä¸ªåˆ‡ç‰‡å¤„ç†4ä¸ªæ–‡ä»¶.
##### 2.å®ç°è¿‡ç¨‹
> ä¸åšä»»ä½•å¤„ç†,è¿è¡ŒWordCountç¨‹åº,è§‚å¯Ÿåˆ‡ç‰‡ä¸ªæ•°ä¸º4.

Log printing
``` prolog
2019-03-07 00:25:34,640 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:4
```
> åœ¨WordcountDriverä¸­å¢åŠ å¦‚ä¸‹ä»£ç ,è¿è¡Œç¨‹åº,å¹¶è§‚å¯Ÿè¿è¡Œçš„åˆ‡ç‰‡ä¸ªæ•°ä¸º3.
```
 /**
 * If you do not set the Input Format, it defaults to Text Input Format.class
 * å¦‚æœä¸è®¾ç½®InputFormatï¼Œå®ƒé»˜è®¤ç”¨çš„æ˜¯TextInputFormat.class
 */
 job.setInputFormatClass(CombineTextInputFormat.class);
        
 /**
 * Set the virtual storage slice maximum to 4M
 * è®¾ç½®è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§å€¼ä¸º 4M
 */
 CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);
         
 /**
 * Set the virtual storage slice minimum to 2M
 * è®¾ç½®è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å°å€¼ä¸º 2M
 */
 CombineTextInputFormat.setMinInputSplitSize(job, 2097152);
```
Log printing
``` prolog
2019-03-07 00:40:29,199 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:3
```
> åœ¨WordcountDriverä¸­å¢åŠ å¦‚ä¸‹ä»£ç ,è¿è¡Œç¨‹åº,å¹¶è§‚å¯Ÿè¿è¡Œçš„åˆ‡ç‰‡ä¸ªæ•°ä¸º1.
```
	/**
	* If you do not set the Input Format, it defaults to Text Input Format.class
	* å¦‚æœä¸è®¾ç½®InputFormatï¼Œå®ƒé»˜è®¤ç”¨çš„æ˜¯TextInputFormat.class
	*/
	job.setInputFormatClass(CombineTextInputFormat.class);

	/**
	* Set the virtual storage slice maximum to 20M
	* è®¾ç½®è™šæ‹Ÿå­˜å‚¨åˆ‡ç‰‡æœ€å¤§å€¼ä¸º 20M
	*/
	CombineTextInputFormat.setMaxInputSplitSize(job, 20971520);
```
Log printing
``` prolog
2019-03-07 00:52:11,201 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:1
```

#### FileInputFormat å®ç°ç±»
> Q&A:MapReduceä»»åŠ¡çš„è¾“å…¥æ–‡ä»¶ä¸€èˆ¬æ˜¯å­˜å‚¨åœ¨HDFSé‡Œé¢,è¾“å…¥çš„æ–‡ä»¶æ ¼å¼åŒ…æ‹¬:åŸºäºè¡Œçš„æ—¥å¿—æ–‡ä»¶/äºŒè¿›åˆ¶æ ¼å¼æ–‡ä»¶ç­‰,è¿™äº›æ–‡ä»¶ä¸€èˆ¬ä¼šå¾ˆå¤§,è¾¾åˆ°æ•°åGB,è‡³æ›´å¤§,é‚£ä¹ˆMapReduceæ˜¯å¦‚ä½•è¯»å–è¿™äº›æ•°æ®çš„å‘¢?
> 
> InputFormatå¸¸è§çš„ æ¥å£å®ç°ç±»åŒ…æ‹¬ï¼šTextInputFormat / KeyValueTextInputFormat / NLineInputFormat / CombineTextInputFormatå’Œè‡ªå®šä¹‰InputFormatç­‰.
#### 1.TextInputFormat
> TextInputFormatæ˜¯é»˜è®¤çš„FileInputFormatå®ç°ç±»,æŒ‰è¡Œè¯»å–æ¯æ¡è®°å½•,é”®æ˜¯å­˜å‚¨è¯¥è¡Œåœ¨æ•´ä¸ªæ–‡ä»¶ä¸­çš„èµ·å§‹å­—èŠ‚åç§»é‡,LongWritableç±»å‹,å€¼æ˜¯è¿™è¡Œçš„å†…å®¹,ä¸åŒ…æ‹¬ä»»ä½•è¡Œç»ˆæ­¢ç¬¦(æ¢è¡Œç¬¦,å›è½¦ç¬¦),Textç±»å‹.
> 
> ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹,æ¯”å¦‚ä¸€ä¸ªåˆ†ç‰‡åŒ…å«äº†å¦‚ä¸‹4æ¡æ–‡æœ¬è®°å½•:
```
 Rich learning form
 Intelligent learning engine
 Learning more convenient
 From the real demand for more close to the enterprise
```
> æ¯æ¡è®°å½•è¡¨ç¤ºä¸ºä»¥ä¸‹ é”®/å€¼å¯¹ & Kæ—¢æ˜¯åç§»é‡ Væ•´è¡Œå†…å®¹:
```
 (0,Rich learning form)
 (19,Intelligent learning engine)
 (47,Learning more convenient)
 (72,From the real demand for more close to the enterprise)
```
#### 2.KeyValueTexInputFormat
> æ¯ä¸€è¡Œå‡ä¸ºä¸€æ¡è®°å½•,è¢«åˆ†éš”ç¬¦åˆ†å‰²ä¸ºKey,Value,å¯ä»¥é€šè¿‡åœ¨é©±åŠ¨ç±»ä¸­è®¾ç½®```conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR,"\t");```æ¥è®¾å®šåˆ†éš”ç¬¦,é»˜è®¤åˆ†éš”ç¬¦æ˜¯tab.
> ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹,è¾“å…¥æ˜¯ä¸€ä¸ªåŒ…å«4æ¡è®°å½•çš„åˆ†ç‰‡,å…¶ä¸­â€”â€”>è¡¨ç¤ºä¸€ä¸ª(æ°´å¹³æ–¹å‘çš„)åˆ¶è¡¨ç¬¦:
```
 line1 â€”â€”>Rich learning form
 line2 â€”â€”>Intelligent learning engine
 line3 â€”â€”>Learning more convenient
 line4 â€”â€”>From the real demand for more close to the enterprise
```
> æ¯æ¡è®°å½•è¡¨ç¤ºä¸ºä»¥ä¸‹é”®/å€¼å¯¹:
```
 (line1,Rich learning form)
 (line2,Intelligent learning engine)
 (line3,Learning more convenient)
 (line4,From the real demand for more close to the enterprise)
```
> æ­¤æ—¶çš„é”®æ˜¯æ¯è¡Œæ’åœ¨åˆ¶è¡¨ç¬¦ä¹‹å‰çš„Textåºåˆ—.

#### KeyValueTexInputFormat ä½¿ç”¨æ¡ˆä¾‹
##### 1.éœ€æ±‚
> ç»Ÿè®¡è¾“å…¥æ–‡ä»¶ä¸­ç¬¬ä¸€è¡Œçš„ç¬¬ä¸€ä¸ªå•è¯ç›¸åŒçš„è¡Œæ•°.
##### 2.è¾“å…¥æ•°æ®
```
GeekParkHub
Geek International Park
HackerParkHub
Geek International Park
HackerParkHub
```
##### 3.æœŸæœ›ç»“æœæ•°æ®
```
GeekParkHub 1
Geek International Park 2
HackerParkHub 2
```
##### 4.ä»£ç å®ç°
###### Create KVTextMapper.class
``` java
package com.geekparkhub.hadoop.kv;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * KVTextMapper
 * <p>
 */

public class KVTextMapper extends Mapper<Text, Text, Text, IntWritable> {

    /**
     * Instantiated object
     * å®ä¾‹åŒ–å¯¹è±¡
     */
    IntWritable v = new IntWritable(1);

    @Override
    protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
        /**
         * Write data
         * å†™å‡ºæ•°æ®
         */
        context.write(key, v);
    }
}
```
###### Create KVTextReducer.class
``` java
package com.geekparkhub.hadoop.kv;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * KVTextReducer
 * <p>
 */

public class KVTextReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    IntWritable v = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        /**
         * Cumulative summation
         * ç´¯è®¡æ±‚å’Œ
         */
        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }

        v.set(sum);

        /**
         * Write data
         * å†™å‡ºæ•°æ®
         */
        context.write(key, v);
    }
}
```
###### Create KVTextDriver.class
``` java
package com.geekparkhub.hadoop.kv;

import com.geekparkhub.hadoop.mapreduce.WordcountDriver;
import com.geekparkhub.hadoop.mapreduce.WordcountMapper;
import com.geekparkhub.hadoop.mapreduce.WordcountReducer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * KVTextDriver
 * <p>
 */

public class KVTextDriver {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        args = new String[]{"/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/input_kv",
                "/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/output_kv_001"};

        /**
         * 1. Get the Job object
         * 1. è·å–Jobå¯¹è±¡
         */
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        /**
         * Set the cutting method to KeyValueLineRecordReader
         * è®¾ç½®åˆ‡å‰²æ–¹å¼ä¸º KeyValueLineRecordReader
         */
        conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " ");

        /**
         * 2. Set the jar storage location
         * 2. è®¾ç½®jarå­˜å‚¨ä½ç½®
         */
        job.setJarByClass(KVTextDriver.class);

        /**
         * 3. Associate Map and Reduce classes
         * 3. å…³è”Mapå’ŒReduceç±»
         */
        job.setMapperClass(KVTextMapper.class);
        job.setReducerClass(KVTextReducer.class);

        /**
         * 4. Set the key and value types of the output data in the Mapper stage.
         * 4. è®¾ç½®Mapperé˜¶æ®µè¾“å‡ºæ•°æ®çš„keyä¸valueç±»å‹
         */
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        /**
         * 5. Set the key and value types for the final data output
         * 5. è®¾ç½®æœ€ç»ˆæ•°æ®è¾“å‡ºçš„keyä¸valueç±»å‹
         */
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        /**
         * Set the Format mode to KeyValueTextInputFormat
         * è®¾ç½®Formatæ¨¡å¼ä¸ºKeyValueTextInputFormat
         * If you do not set the Input Format, it defaults to Text Input Format.class
         * å¦‚æœä¸è®¾ç½®InputFormatï¼Œå®ƒé»˜è®¤ç”¨çš„æ˜¯TextInputFormat.class
         */
        job.setInputFormatClass(KeyValueTextInputFormat.class);

        /**
         * 6. Set the input path and output path
         * 6. è®¾ç½®è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
         */
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        /**
         * 7. Submit the Job
         * 7. æäº¤Job
         */
        boolean result = job.waitForCompletion(true);

        /**
         * 8. Log printing
         * 8. æ—¥å¿—æ‰“å°
         */
        System.exit(result ? 0 : 1);
    }
}
```
##### 5.è¿è¡Œç¨‹åº æŸ¥çœ‹part-r-00000ç»“æœ
``` prolog
Geek International Park	2
GeekParkHub	1
HackerParkHub	2
```

#### 3.NLiveInputFormat
> å¦‚æœä½¿ç”¨NLiveInputFormat,ä»£è¡¨æ¯ä¸ªmapè¿›ç¨‹å¤„ç†çš„InputSplitä¸å†æŒ‰Blockå—å»åˆ’åˆ†,è€Œæ˜¯æŒ‰NLiveInputFormatæŒ‡å®šçš„è¡Œæ•°Næ¥åˆ’åˆ†,æ—¢è¾“å…¥æ–‡ä»¶çš„æ€»è¡Œæ•°/N=åˆ‡ç‰‡æ•°,å¦‚æœä¸æ•´é™¤,åˆ‡ç‰‡æ•°=å•†+1
> ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹,ä»ç„¶ä»¥ä¸‹é¢çš„4è¡Œè¾“å…¥ä¸ºä¾‹:
```
 Rich learning form
 Intelligent learning engine
 Learning more convenient
 From the real demand for more close to the enterprise
```
> ä¾‹å¦‚,å¦‚æœNæ˜¯2,åˆ™æ¯ä¸ªè¾“å…¥åˆ†ç‰‡åŒ…æ‹¬ä¸¤è¡Œ,å¼€å¯2ä¸ªMapTask.
```
 (0,Rich learning form)
 (19,Intelligent learning engine)
```
> å¦ä¸€ä¸ªmapperåˆ™æ”¶åˆ°åä¸¤è¡Œ:
```
 (47,Learning more convenient)
 (72,From the real demand for more close to the enterprise)
```
> è¿™é‡Œçš„é”®å’Œå€¼ä¸TextInputFormatç”Ÿæˆçš„ä¸€æ ·.

#### NLiveInputFormat ä½¿ç”¨æ¡ˆä¾‹
##### 1.éœ€æ±‚:
> å¯¹æ¯ä¸ªå•è¯è¿›è¡Œç»Ÿè®¡,æ ¹æ®æ¯ä¸ªè¾“å…¥æ–‡ä»¶çš„è¡Œæ•°æ¥è§„å®šè¾“å‡ºå¤šå°‘ä¸ªåˆ‡ç‰‡,è¦æ±‚æ¯ä¸‰è¡Œæ”¾å…¥ä¸€ä¸ªåˆ‡ç‰‡ä¸­.
##### 2.è¾“å…¥æ•°æ®:
```
GeekParkHub
Geek International Park
GeekParkHub
Geek International Park
GeekParkHub
Geek International Park
GeekParkHub
Geek International Park
GeekParkHub
Geek International Park Geek International Park
Geek International Park
```
##### 3.è¾“å‡ºç»“æœ:
```
INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:4
```
##### 4.ä»£ç å®ç°:
###### Create NLineMapper.class
``` java
package com.geekparkhub.hadoop.nline;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * NLineMapper
 * <p>
 */

public class NLineMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    Text k = new Text();
    IntWritable v = new IntWritable(1);

    /**
     * Rewrite the map() method
     * é‡å†™map()æ–¹æ³•
     *
     * @param key
     * @param value
     * @param context
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

        /**
         * 1. Get the first row of data
         * 1. è·å–ç¬¬ä¸€è¡Œæ•°æ®
         */
        String line = value.toString();

        /**
         * 2. Cutting data
         * 2. åˆ‡å‰²ç©ºæ ¼æ•°æ®
         */
        String[] words = line.split(" ");

        /**
         * 3. Loop through the data
         * 3. å¾ªç¯éå†æ•°æ®
         */
        for (String word : words) {
            K.set(Long.parseLong(word));
            context.write(k, v);
        }
    }
}
```
###### Create NLineReducer.class
``` java
package com.geekparkhub.hadoop.nline;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * NLineReducer
 * <p>
 */

public class NLineReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    IntWritable v = new IntWritable();
    
    /**
     * Rewrite the reduce() method
     * é‡å†™reduce()æ–¹æ³•
     *
     * @param key
     * @param values
     * @param context
     * @throws IOException
     * @throws InterruptedException
     */
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        /**
         * 1. Accumulate summation
         * 1. ç´¯åŠ æ±‚å’Œ
         */
        int sum = 0;
        for (IntWritable value : values) {
            sum += value.get();
        }
        v.set(sum);

        /**
         * 2. Output data
         * 2. è¾“å‡ºæ•°æ®
         */
        context.write(key, v);

    }
}
```
###### Create NLineDriver.class
``` java
package com.geekparkhub.hadoop.nline;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * NLineDriver
 * <p>
 */

public class NLineDriver {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

        args = new String[]{"/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/input_nl",
                "/Volumes/GEEK-SYSTEM/Technical_Framework/Hadoop/projects/mapreduce/src/main/resources/output_nl_001"};

        /**
         * 1. Get the Job object
         * 1. è·å–Jobå¯¹è±¡
         */
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        /**
         * Set Input Split to perform a slice every three times
         * è®¾ç½®InputSplitæ¯ä¸‰è¡Œä¸ºä¸€ä¸ªåˆ‡ç‰‡.
         */
        NLineInputFormat.setNumLinesPerSplit(job,3);

        /**
         * Processing the number of records using the NLine Input Format
         * ä½¿ç”¨NLineInputFormatå¤„ç†è®°å½•æ•°.
         */
        job.setInputFormatClass(NLineInputFormat.class);

        /**
         * 2. Set the jar storage location
         * 2. è®¾ç½®jarå­˜å‚¨ä½ç½®
         */
        job.setJarByClass(NLineDriver.class);

        /**
         * 3. Associate Map and Reduce classes
         * 3. å…³è”Mapå’ŒReduceç±»
         */
        job.setMapperClass(NLineMapper.class);
        job.setReducerClass(NLineReducer.class);

        /**
         * 4. Set the key and value types of the output data in the Mapper stage.
         * 4. è®¾ç½®Mapperé˜¶æ®µè¾“å‡ºæ•°æ®çš„keyä¸valueç±»å‹
         */
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        /**
         * 5. Set the key and value types for the final data output
         * 5. è®¾ç½®æœ€ç»ˆæ•°æ®è¾“å‡ºçš„keyä¸valueç±»å‹
         */
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        /**
         * 6. Set the input path and output path
         * 6. è®¾ç½®è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„
         */
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        /**
         * 7. Submit the Job
         * 7. æäº¤Job
         */
        boolean result = job.waitForCompletion(true);

        /**
         * 8. Log printing
         * 8. æ—¥å¿—æ‰“å°
         */
        System.exit(result ? 0 : 1);
    }
}
```
##### 5.ç»“æœæŸ¥çœ‹:
part-r-00000
```
Geek	12
Hub	5
International	7
Park	12
```
 Log printing
``` prolog
2019-03-07 22:27:28,044 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:4
```

#### è‡ªå®šä¹‰InputFormat
> åœ¨ä¼ä¸šå¼€å‘ä¸­,Hadoopæ¡†æ¶è‡ªå¸¦çš„InputFormatç±»å‹ä¸èƒ½æ»¡è¶³äºæ‰€æœ‰åº”ç”¨åœºæ™¯,éœ€è¦è‡ªå®šä¹‰InputFormatæ¥è§£å†³å®é™…é—®é¢˜.
#### è‡ªå®šä¹‰InputFormatæ­¥éª¤å¦‚ä¸‹:
> 1.è‡ªå®šä¹‰ä¸€ä¸ªç±»ç»§æ‰¿FileInputFormat.
> 2.æ”¹å†™RecordReader,å®ç°ä¸€æ¬¡è¯»å–ä¸€ä¸ªå®Œæ•´æ–‡ä»¶å°è£…ä¸ºKV.
> 3.åœ¨è¾“å‡ºæ—¶ä½¿ç”¨SequenceFileOutPutFormatè¾“å‡ºåˆå¹¶æ–‡ä»¶.
> 
#### è‡ªå®šä¹‰InputFormat æ¡ˆä¾‹å®æ“
##### 1.éœ€æ±‚
> æ— è®ºhdfsè¿˜æ˜¯mapreduce,å¯¹äºå°æ–‡ä»¶éƒ½æœ‰æŸæ•ˆç‡,å®è·µä¸­,åˆéš¾å…é¢ä¸´å¤„ç†å¤§é‡å°æ–‡ä»¶çš„åœºæ™¯,æ­¤æ—¶å°±éœ€è¦æœ‰ç›¸åº”è§£å†³æ–¹æ¡ˆ.
> 
> å°†å¤šä¸ªå°æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶SequenceFile,SequenceFileé‡Œé¢å­˜å‚¨ç€å¤šä¸ªæ–‡ä»¶,å­˜å‚¨çš„å½¢å¼ä¸ºæ–‡ä»¶è·¯å¾„+åç§°ä¸ºkey,æ–‡ä»¶å†…å®¹ä¸ºvalue.
##### 2.è¾“å…¥æ•°æ®
> Create 1.txt / Create 2.txt / Create 3.txt
> 
> æœ€ç»ˆé¢„æœŸæ–‡ä»¶æ ¼å¼ï¼špart-r-00000

##### 3.åˆ†æ
> å°æ–‡ä»¶çš„ä¼˜åŒ–æ— éä»¥ä¸‹å‡ ç§æ–¹å¼:
> åœ¨æ•°æ®é‡‡é›†çš„æ—¶å€™,å°±å°†å°æ–‡ä»¶æˆ–å°æ‰¹æ•°æ®åˆæˆå¤§æ–‡ä»¶å†ä¸Šä¼ HDFS.
> åœ¨ä¸šåŠ¡å¤„ç†ä¹‹å‰,åœ¨HDFSä¸Šä½¿ç”¨mapreduceç¨‹åºå¯¹å°æ–‡ä»¶è¿›è¡Œåˆå¹¶.
> åœ¨mapreduceå¤„ç†æ—¶,å¯é‡‡ç”¨CombineTextInputFormatæé«˜æ•ˆç‡.

##### 4.å…·ä½“å®ç°
> 1.è‡ªå®šä¹‰ä¸€ä¸ªç±»ç»§æ‰¿FileInputFormat
> é‡å†™isSplitable()æ–¹æ³•,è¿”å›falesä¸å¯åˆ‡å‰².
> é‡å†™createRecordReader(),åˆ›å»ºè‡ªå®šä¹‰RecordReaderå¯¹è±¡,å¹¶åˆå§‹åŒ–
> 2.æ”¹å†™RecordReader,å®ç°ä¸€æ¬¡è¯»å–ä¸€ä¸ªå®Œæ•´æ–‡ä»¶å°è£…ä¸ºKV
> é‡‡ç”¨IOæµä¸€æ¬¡è¯»å–ä¸€ä¸ªæ–‡ä»¶è¾“å‡ºåˆ°valueä¸­,å› ä¸ºè®¾ç½®äº†ä¸å¯åˆ‡ç‰‡,æœ€ç»ˆæŠŠæ‰€æœ‰æ–‡ä»¶éƒ½å°è£…åˆ°äº†valueä¸­.
> è·å–æ–‡ä»¶è·¯å¾„ä¿¡æ¯+åç§°,å¹¶è®¾ç½®key
> 3.è®¾ç½®Driver
```
 // è®¾ç½®è¾“å‡ºçš„inputFormat
 job.setInputFormatClass(WholeFileInputformat.class);
 // è®¾ç½®è¾“å‡ºçš„outputFormat
 job.setOutputFormatClass(SequenceFileOutputFormat.class);
```

##### 5.ç¨‹åºå®ç°


## ğŸ”’ å°šæœªè§£é” æ­£åœ¨å­¦ä¹ æ¢ç´¢ä¸­... å°½æƒ…æœŸå¾… Blogæ›´æ–°! ğŸ”’
#### 37.7.3.2 MapReduce å·¥ä½œæµç¨‹

#### 7.7.3.3 Shuffle æœºåˆ¶
##### Shuffle æœºåˆ¶
##### Partition åˆ†åŒº
##### Partitionåˆ†åŒº å®æ“æ¡ˆä¾‹
##### WritableComparable æ’åº
##### WritableComparable æ’åº å®æ“æ¡ˆä¾‹(å…¨æ’åº)
##### WritableComparable æ’åº å®æ“æ¡ˆä¾‹(åŒºå†…æ’åº)
##### Combine åˆå¹¶
##### Combine åˆå¹¶æ¡ˆä¾‹å®æ“

#### 7.7.3.4 Map Task å·¥ä½œæœºåˆ¶
#### 7.7.3.5 Reduce Task å·¥ä½œæœºåˆ¶
#### 7.7.3.6 OutputFromat æ•°æ®è¾“å‡º
#### 7.7.3.7 Join å¤šç§åº”ç”¨
#### 7.7.3.8 è®¡æ•°å™¨åº”ç”¨
#### 7.7.3.9 æ•°æ®æ¸…æ´—(ETL)
#### 7.7.3.10 MapReduce å¼€å‘æ€»ç»“

### 7.7.4 Hadoop æ•°æ®å‹ç¼©
#### æ•°æ®å‹ç¼© æ¦‚è¿°
#### MRæ”¯æŒå‹ç¼©ç¼–ç 

#### å‹ç¼©æ–¹å¼é€‰æ‹©
##### Gzipå‹ç¼©
##### Bzipå‹ç¼©
##### Lzoå‹ç¼©
##### Snappy å‹ç¼©

#### å‹ç¼©ä½ç½®é€‰æ‹©
#### å‹ç¼©å‚æ•°é…ç½®

#### å‹ç¼©å®æ“æ¡ˆä¾‹
##### æ•°æ®æµçš„å‹ç¼©å’Œè§£å‹ç¼©
##### Mapè¾“å‡ºç«¯é‡‡ç”¨å‹ç¼©
##### Reduceè¾“å‡ºç«¯é‡‡ç”¨å‹ç¼©

### 7.7.5 Yarnèµ„æºè°ƒåº¦å™¨ (é¢è¯•é‡ç‚¹)
#### Yarn åŸºæœ¬æ¶æ„
#### Yarn å·¥ä½œæœºåˆ¶
#### ä½œä¸šæäº¤å…¨è¿‡ç¨‹
#### èµ„æºè°ƒåº¦å™¨

### 7.7.6 Hadoop ä¼ä¸šä¼˜åŒ–
#### MapReduce è¿è¡Œç¼“æ…¢çš„åŸå› 
#### MapReduce ä¼˜åŒ–æ–¹æ¡ˆ
##### æ•°æ®è¾“å…¥
##### Mapé˜¶æ®µ
##### Reduceé˜¶æ®µ
##### I/Oä¼ è¾“
##### æ•°æ®å€¾æ–œé—®é¢˜
##### å¸¸ç”¨è°ƒä¼˜å‚æ•°

#### HDFSå°æ–‡ä»¶ä¼˜åŒ–æ–¹æ³•
##### HDFSå°æ–‡ä»¶å¼Šç«¯
##### HDFSå°æ–‡ä»¶è§£å†³æ–¹æ¡ˆ

### 7.7.7 MapReduce æ‰©å±•æ¡ˆä¾‹
#### å€’æ’ç´¢å¼•æ¡ˆä¾‹(å¤šjobä¸²è”)
#### TopNæ¡ˆä¾‹
#### æ‰¾åšå®¢å…±åŒå¥½å‹æ¡ˆä¾‹


### 8. HDFS HAé«˜å¯ç”¨


## 9. å¸¸è§é”™è¯¯(å„ç§å‘)åŠè§£å†³æ–¹æ¡ˆ


## 10. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------