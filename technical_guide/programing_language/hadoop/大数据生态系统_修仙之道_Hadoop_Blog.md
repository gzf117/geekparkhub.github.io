# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Hadoop Blog

@(2019-01-22)[Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Language:Hadoop|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg)|GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub)]

##  ğŸ˜ Hadoop Technology ä¿®ä»™ä¹‹é“ ç‚¼ç²¾åŒ–æ°” ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/hadoop.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ


-------------------

[TOC]



## 1. å¤§æ•°æ® ç®€ä»‹
> å¤§æ•°æ®æ˜¯æŒ‡æ— æ³•åœ¨ä¸€å®šæ—¶é—´å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·å¯¹å…¶å†…å®¹è¿›è¡ŒæŠ“å–ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆã€‚å¤§æ•°æ®æŠ€æœ¯,æ˜¯æŒ‡ä»å„ç§å„æ ·ç±»å‹çš„æ•°æ®ä¸­,å¿«é€Ÿè·å¾—æœ‰ä»·å€¼ä¿¡æ¯çš„èƒ½åŠ›ã€‚é€‚ç”¨äºå¤§æ•°æ®çš„æŠ€æœ¯,åŒ…æ‹¬å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†(MPP)æ®åº“,æ•°æ®æŒ–æ˜ç”µç½‘,åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,åˆ†å¸ƒå¼æ•°æ®åº“,äº‘è®¡ç®—å¹³å°,äº’è”ç½‘,å’Œå¯æ‰©å±•çš„å­˜å‚¨ç³»ç»Ÿ,å¤§æ•°æ®ç”±å·¨å‹æ•°æ®é›†ç»„æˆ,è¿™äº›æ•°æ®é›†å¤§å°å¸¸è¶…å‡ºäººç±»åœ¨å¯æ¥å—æ—¶é—´ä¸‹çš„æ”¶é›†ã€åº‹ç”¨ã€ç®¡ç†å’Œå¤„ç†èƒ½åŠ›,å¤§æ•°æ®çš„å¤§å°ç»å¸¸æ”¹å˜,æˆªè‡³2012å¹´,å•ä¸€æ•°æ®é›†çš„å¤§å°ä»æ•°å¤ªå­—èŠ‚(TB)åå…†äº¿å­—èŠ‚(PB)ç­‰.   â€”â€” [MBAæ™ºåº“ç™¾ç§‘](https://wiki.mbalib.com/wiki/%E5%A4%A7%E6%95%B0%E6%8D%AE)


## 2. å¤§æ•°æ® æ¦‚è®º

### å¤§æ•°æ® æ¦‚å¿µ
> **`å¤§æ•°æ®(BigData)`**æ˜¯æŒ‡**`æ— æ³•åœ¨ä¸€å®šæ—¶é—´èŒƒå›´`**å†…ç”¨å¸¸è§„è½¯ä»¶å·¥å…·è¿›è¡Œæ•æ‰ã€ç®¡ç†å’Œå¤„ç†çš„æ•°æ®é›†åˆ,æ˜¯éœ€è¦æ–°å¤„ç†æ¨¡å¼æ‰èƒ½å…·æœ‰æ›´å¼ºçš„å†³ç­–åŠ›ã€æ´å¯Ÿå‘ç°åŠ›å’Œæµç¨‹ä¼˜åŒ–èƒ½åŠ›çš„æµ·é‡ã€é«˜å¢é•¿ç‡å’Œå¤šæ ·åŒ–çš„ä¿¡æ¯èµ„äº§.
> 
> å¤§æ•°æ®ä¸»è¦è§£å†³:æµ·é‡æ•°æ®çš„**`å­˜å‚¨`**å’Œæµ·é‡æ•°æ®çš„**`åˆ†æè®¡ç®—`**é—®é¢˜.
> 
> æ•°æ®å­˜å‚¨å•ä½: **`bit`** / **`Byte`** / **`KB`** / **`MB`** / **`GB`** / **`TB`**  / **`PB`** / **`EB`** / **`ZB`** / **`YB`** / **`BB`** / **`NB`** / **`DB`**
> 
> **`1 Byte = 8bit`**
> **`1 KB = 1024Byte`**
> **`1 MB = 1024KB`**
> **`1 GB = 1024MB`**
> **`1 TB = 1024GB`**
> **`1 PB = 1024TB`**
> **`1 EB = 1024PB`**
> **`1 ZB = 1024EB`**
> **`1 YB = 1024ZB`**
> **`1 BB = 1024YB`**
> **`1 NB = 1024BB`**
> **`1 DB = 1024NB`**

### å¤§æ•°æ® ç‰¹ç‚¹(4V)
#### 1.Volume (å¤§é‡)
> æˆªæ­¢ç›®å‰,äººç±»ç”Ÿäº§çš„æ‰€æœ‰å°åˆ·ææ–™çš„æ•°æ®é‡æ˜¯200PB(1PB=1024TB),è€Œå†å²ä¸Šå…¨äººç±»æ€»å…±è¯´è¿‡çš„è¯çš„æ•°é‡å¤§çº¦æ˜¯5EB(1EB=1024PB),å½“å‰å…¸å‹ä¸ªäººè®¡ç®—æœºç¡¬ç›˜çš„å®¹é‡ä¸ºTBé‡çº§,è€Œä¸€äº›å¤§ä¼ä¸šçš„æ•°æ®é‡å·²ç»è¿‘EBé‡çº§.
> 
#### 2.Velocity (é«˜é€Ÿ)
> è¿™æ˜¯å¤§æ•°æ®åŒºåˆ†äºä¼ ç»Ÿæ•°æ®æŒ–æ˜çš„æœ€æ˜¾è‘—ç‰¹å¾,æ ¹æ®IDCçš„"æ•°å­—å®‡å®™"æŠ¥å‘Š,é¢„è®¡2020å¹´,å…¨çƒæ•°æ®ä½¿ç”¨é‡å°†è¾¾åˆ°35.2ZB(1 ZB = 1024EB),åœ¨å¦‚æ­¤æµ·é‡çš„æ•°æ®é¢å‰,å¤„ç†æ•°æ®çš„æ•ˆç‡å°±æ˜¯ä¼ä¸šçš„ç”Ÿå‘½.

#### 3.Variety (å¤šæ ·)
> è¿™ç§ç±»å‹çš„å¤šæ ·æ€§ä¹Ÿè®©æ•°æ®è¢«åˆ†ä¸ºç»“æ„åŒ–æ•°æ®å’Œéç»“æ„åŒ–æ•°æ®,ç›¸å¯¹äºä»¥å¾€ä¾¿äºå­˜å‚¨çš„ä»¥æ•°æ®åº“&æ–‡æœ¬ä¸ºä¸»çš„ç»“æ„åŒ–æ•°æ®,éç»“æ„åŒ–æ•°æ®è¶Šæ¥è¶Šå¤š,åŒ…æ‹¬ç½‘ç»œæ—¥å¿—,éŸ³é¢‘,è§†é¢‘,å›¾ç‰‡,åœ°ç†ä½ç½®ä¿¡æ¯ç­‰,è¿™äº›å¤šç±»å‹çš„æ•°æ®å¯¹æ•°æ®çš„å¤„ç†èƒ½åŠ›æå‡ºäº†æ›´é«˜è¦æ±‚.

#### 4.Value (ä½ä»·å€¼å¯†åº¦)
> ä»·å€¼å¯†åº¦çš„é«˜ä½ä¸æ•°æ®æ€»é‡çš„å¤§å°æˆåæ¯”,å¦‚ä½•å¿«é€Ÿå¯¹æœ‰ä»·å€¼æ•°æ®"æçº¯",æˆä¸ºç›®å‰å¤§æ•°æ®èƒŒæ™¯ä¸‹å¾…è§£å†³çš„éš¾é¢˜.

### å¤§æ•°æ® åº”ç”¨åœºæ™¯
> **`ç‰©æµä»“å‚¨`**:å¤§æ•°æ®åˆ†æç³»ç»ŸåŠ©åŠ›å•†å®¶ç²¾ç»†åŒ–è¿è¥,æå‡é”€é‡,èŠ‚çº¦æˆæœ¬.
> 
> **`é›¶å”®`**:åˆ†æç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯,ä¸ºç”¨æˆ·è´­ä¹°å•†å“æä¾›æ–¹ä¾¿,ä»è€Œæå‡å•†å“å”®é‡,ç»å…¸æ•…äº‹æ¡ˆä¾‹ -ã€Šçº¸å°¿å¸ƒ+å•¤é…’ã€‹.
> 
> **`æ—…æ¸¸`**:æ·±åº¦ç»“åˆå¤§æ•°æ®èƒ½åŠ›ä¸æ—…æ¸¸è¡Œä¸šéœ€æ±‚,å…±å»ºæ—…æ¸¸äº§ä¸šæ™ºæ…§ç®¡ç†,æ™ºæ…§æœåŠ¡,æ™ºæ…§è¥é”€çš„æœªæ¥.
> 
> **`å•†å“å¹¿å‘Šæ¨è`**:ä¸ºç”¨æˆ·æ¨èå¯èƒ½å–œæ¬¢çš„å•†å“.
> 
> **`ä¿é™©`**:æµ·é‡æ•°æ®æŒ–æ˜åŠé£é™©é¢„æµ‹,åŠ©åŠ›ä¿é™©è¡Œä¸šç²¾å‡†è¥é”€,æå‡ç²¾ç»†åŒ–å®šä»·èƒ½åŠ›.
> 
> **`é‡‘è`**:å¤šç»´åº¦ä½“ç°ç”¨æˆ·ç‰¹å¾,å¸®åŠ©é‡‘èæœºæ„æ¨èä¼˜è´¨å®¢æˆ·,é˜²èŒƒæ¬ºè¯ˆé£é™©.
> 
> **`æˆ¿äº§`**:å¤§æ•°æ®å…¨é¢åŠ©åŠ›æˆ¿åœ°äº§è¡Œä¸š,æ‰“é€ ç²¾å‡†æŠ•ç­–ä¸è¥é”€,æŒ‘é€‰å‡ºæ›´åˆé€‚çš„åœ°åŸŸ.

### å¤§æ•°æ®éƒ¨é—¨ä¸šåŠ¡æµç¨‹åˆ†æ
``` sequence
äº§å“äººå‘˜->æ•°æ®éƒ¨é—¨:æå‡ºéœ€æ±‚(ç»Ÿè®¡æ€»ç”¨æˆ·æ•°,æ—¥æ´»è·ƒç”¨æˆ·æ•°,å›æµç”¨æˆ·æ•°ç­‰)
æ•°æ®éƒ¨é—¨-->æ•°æ®å¯è§†åŒ–/æŠ¥è¡¨å±•ç¤º/é‚®ä»¶å‘é€/å¤§å±å¹•å±•ç¤ºç­‰:æ­å»ºæ•°æ®å¹³å°,åˆ†ææ•°æ®æŒ‡æ ‡
```
### å¤§æ•°æ®éƒ¨é—¨ç»„ç»‡ç»“æ„(é‡ç‚¹)
**å¤§æ•°æ®éƒ¨é—¨ç»„ç»‡ç»“æ„**
| æ‰€åœ¨ç»„    |  æ‰€åœ¨ç»„å·¥ä½œèŒè´£ |
| :-------- | --------:|
| å¹³å°ç»„  | Hadoop,Flume,Kafka,Hbase,Sparkç­‰æ¡†æ¶å¹³å°æ­å»º,é›†ç¾¤æ€§èƒ½ç›‘æ§,é›†ç¾¤æ€§èƒ½è°ƒä¼˜ |
| æ•°æ®ä»“åº“ç»„  | ETLå·¥ç¨‹å¸ˆ-æ•°æ®æ¸…æ´—,Hiveå·¥ç¨‹å¸ˆ-æ•°æ®åˆ†æ,æ•°æ®ä»“åº“å»ºæ¨¡ |
| æ•°æ®æŒ–æ˜ç»„  | ç®—æ³•å·¥ç¨‹å¸ˆ æ¨èç³»ç»Ÿå·¥ç¨‹å¸ˆ ç”¨æˆ·ç”»åƒå·¥ç¨‹å¸ˆ |
| æ•°æ®æŠ¥è¡¨å¼€å‘ç»„  | JAVAEEå·¥ç¨‹å¸ˆ |

## 3. æ¢è®¨Hadoopæ¡†æ¶ å¤§æ•°æ®ç”Ÿæ€

### Hadoop ç®€ä»‹

> Apache Hadoopæ˜¯ä¸€æ¬¾æ”¯æŒæ•°æ®å¯†é›†å‹åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºå¹¶ä»¥Apache 2.0è®¸å¯åè®®å‘å¸ƒçš„å¼€æºè½¯ä»¶æ¡†æ¶,å®ƒæ”¯æŒåœ¨å•†å“ç¡¬ä»¶æ„å»ºçš„å¤§å‹é›†ç¾¤ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åº,Hadoopæ˜¯æ ¹æ®è°·æ­Œå…¬å¸å‘è¡¨çš„MapReduceå’ŒGoogleæ–‡ä»¶ç³»ç»Ÿçš„è®ºæ–‡è‡ªè¡Œå®ç°è€Œæˆã€‚æ‰€æœ‰çš„Hadoopæ¨¡å—éƒ½æœ‰ä¸€ä¸ªåŸºæœ¬å‡è®¾,å³ç¡¬ä»¶æ•…éšœæ˜¯å¸¸è§æƒ…å†µ,åº”è¯¥ç”±æ¡†æ¶è‡ªåŠ¨å¤„ç†ã€‚
> 
> Hadoopæ¡†æ¶é€æ˜åœ°ä¸ºåº”ç”¨æä¾›å¯é æ€§å’Œæ•°æ®ç§»åŠ¨,å®ƒå®ç°äº†åä¸ºMapReduceçš„ç¼–ç¨‹èŒƒå¼:åº”ç”¨ç¨‹åºè¢«åˆ†å‰²æˆè®¸å¤šå°éƒ¨åˆ†,è€Œæ¯ä¸ªéƒ¨åˆ†éƒ½èƒ½åœ¨é›†ç¾¤ä¸­çš„ä»»æ„èŠ‚ç‚¹ä¸Šè¿è¡Œæˆ–é‡æ–°è¿è¡Œã€‚æ­¤å¤–,Hadoopè¿˜æä¾›äº†åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,ç”¨ä»¥å­˜å‚¨æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹çš„æ•°æ®,è¿™ä¸ºæ•´ä¸ªé›†ç¾¤å¸¦æ¥äº†éå¸¸é«˜çš„å¸¦å®½ã€‚MapReduceå’Œåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿçš„è®¾è®¡,ä½¿å¾—æ•´ä¸ªæ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨å¤„ç†èŠ‚ç‚¹æ•…éšœ,å®ƒä½¿åº”ç”¨ç¨‹åºä¸æˆåƒä¸Šä¸‡çš„ç‹¬ç«‹è®¡ç®—çš„è®¡ç®—æœºå’ŒPBçº§çš„æ•°æ®è¿æ¥èµ·æ¥,ç°åœ¨æ™®éè®¤ä¸ºæ•´ä¸ªApache Hadoop"å¹³å°"åŒ…æ‹¬Hadoopå†…æ ¸ã€MapReduceã€Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ(HDFS)ä»¥åŠä¸€äº›ç›¸å…³é¡¹ç›®,æœ‰Apache Hiveå’ŒApache HBaseç­‰ç­‰.   â€”â€” [ç»´åŸºç™¾ç§‘](https://zh.wikipedia.org/wiki/Apache_Hadoop)

### Hadoop æ˜¯ä»€ä¹ˆ
> Hadoopæ˜¯ç”±ApacheåŸºé‡‘ä¼šæ‰€å¼€å‘çš„åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€æ¶æ„.
> 
> Hadoopä¸»è¦è§£å†³:æµ·é‡**`æ•°æ®çš„å­˜å‚¨`**å’Œæµ·é‡**`æ•°æ®çš„åˆ†æè®¡ç®—`**é—®é¢˜
> 
> å¹¿ä¹‰ä¸Šæ¥è®²,Hadoopé€šå¸¸æ˜¯æŒ‡ä¸€ä¸ªæ›´å¹¿æ³›çš„æ¦‚å¿µ --- Hadoopç”Ÿæ€åœˆ

### Hadoop å‘å±•å†å²
> 1.Hadoopåˆ›å§‹äºº:Doug Cutting
> 
> Luceneæ¡†æ¶æ˜¯Doug Cuttingå¼€åˆ›çš„å¼€æºè½¯ä»¶,ä½¿ç”¨javaç¼–ç¨‹è¯­è¨€å¼€å‘,å®ç°ä¸Googleç±»ä¼¼çš„å…¨æ–‡æœç´¢åŠŸèƒ½,å®ƒæä¾›äº†å…¨æ–‡æ£€ç´¢å¼•æ“çš„æ¶æ„,åŒ…æ‹¬å®Œæ•´çš„æŸ¥è¯¢å¼•æ“å’Œç´¢å¼•å¼•æ“.
> 
> 2.2001å¹´å¹´åº•Luceneæˆä¸ºApacheåŸºé‡‘ä¼šçš„ä¸€ä¸ªå­é¡¹ç›®.
> 
> 3.å¯¹äºæµ·é‡æ•°æ®çš„åœºæ™¯,Luceneé¢å¯¹ä¸GoogleåŒæ ·çš„å›°éš¾,å­˜å‚¨æ•°æ®å›°éš¾,æ£€ç´¢é€Ÿåº¦æ…¢.
> 
> 4.å­¦ä¹ å’Œæ¨¡ä»¿Googleè§£å†³è¿™äº›é—®é¢˜çš„åŠæ³•:(Luceneçš„å‡çº§ç‰ˆ) Nutch.
> 
> 5.å¯ä»¥è¯´Googleæ˜¯Hadoopçš„æ€æƒ³ä¹‹æº(Googleåœ¨å¤§æ•°æ®æ–¹é¢çš„ä¸‰ç¯‡è®ºæ–‡)
> 
> GFS ---> HDFS
> Map-MapReduce ---> MR
> BigTable ---> Hbase
> 
> 6.2003è‡³2004å¹´,Googleå…¬å¼€äº†éƒ¨åˆ†GFSå’ŒMapReduceæ€æƒ³ç»†èŠ‚,ä»¥æ­¤ä¸ºåŸºç¡€Doug Cuttingç­‰å¼€å‘è€…ç”¨äº†2å¹´ä¸šä½™æ—¶é—´å®ç°äº†DFSå’ŒMapReduceæœºåˆ¶,ä½¿Nutchæ€§èƒ½é£™å‡.
> 
> 7.2005å¹´Hadoopä½œä¸ºLuceneçš„å­é¡¹ç›®,Nutchçš„ä¸€éƒ¨åˆ†æ­£å¼å¼•å…¥ApacheåŸºé‡‘ä¼š.
> 
> 8.2006å¹´3æœˆä»½,Map-Reduceå’ŒNDFS(Nutch Distributed File System),åˆ†åˆ«è¢«çº³å…¥ç§°ä¸ºHadoopçš„é¡¹ç›®ä¸­.
> 
> 9.Hadoopåå­—æ¥æºäºDoug Cuttingå­©å­çš„ç©å…·å¤§è±¡.
> 
> 10.Hadoopå°±æ­¤è¯ç”Ÿå¹¶è¿…é€Ÿå‘å±•,æ ‡å¿—ç€å¤§æ•°æ®æ—¶ä»£æ¥ä¸´.

### Hadoop ä¸‰å¤§å‘è¡Œç‰ˆæœ¬
> Hadoop ä¸‰å¤§å‘è¡Œç‰ˆæœ¬ **`Apache`** | **`Cloudera`** | **`Hortonworks`**

#### Apache Hadoop
> Apacheç‰ˆæœ¬æœ€æœ€åŸå§‹(æœ€åŸºç¡€)ç‰ˆæœ¬,å¯¹äºå…¥é—¨å­¦ä¹ æœ€ä½³.
> 
> å®˜ç½‘åœ°å€ : http://hadoop.apache.org/releases.html
> 
> ä¸‹è½½åœ°å€ : https://archive.apache.org/dist/hadoop/common/

#### Cloudera Hadoop
> Clouderaåœ¨å¤§å‹äº’è”ç½‘ä¼ä¸šä¸­åº”ç”¨åœºæ™¯è¾ƒå¤š.
> 
> å®˜ç½‘åœ°å€ : https://www.cloudera.com/downloads/cdh/5-10-0.html
> 
> ä¸‹è½½åœ°å€ : http://archive.cloudera.com/cdh5/cdh/5/
> 
> 2008å¹´æˆç«‹çš„Clouderaæ˜¯æœ€æ—©å°†Hadoopå•†ç”¨å…¬å¸,ä¸ºåˆä½œä¼™ä¼´æä¾›Hadoopçš„å•†ä¸šè§£å†³æ–¹æ¡ˆ,ä¸»è¦æ˜¯åŒ…æ‹¬æ”¯æŒ,å’¨è¯¢æœåŠ¡,åŸ¹è®­.
> 
> 2009å¹´Hadoopåˆ›å§‹äººDoug Cuttingä¹ŸåŠ ç›Ÿäº†Clouderaå…¬å¸,Clouderaäº§å“ä¸»è¦ä¸ºCDH,Cloudera Manager | Cloudera Support.
> 
> CDHæ˜¯Clouderaçš„Hadoopå‘è¡Œç‰ˆ,å®Œå…¨å¼€æº,æ¯”Apache Hadoopåœ¨å…¼å®¹æ€§,å®‰å…¨æ€§,ç¨³å®šæ€§ä¸Šæœ‰æ‰€å¢å¼º.
> 
> Cloudera Manageræ˜¯é›†ç¾¤çš„è½¯ä»¶åˆ†å‘åŠç®¡ç†ç›‘æ§å¹³å°,å¯ä»¥å†å‡ ä¸ªå°æ—¶å†…éƒ¨éƒ¨ç½²å¥½ä¸€ä¸ªHadoopé›†ç¾¤,å¹¶å¯¹é›†ç¾¤èŠ‚ç‚¹åŠæœåŠ¡
è¿›è¡Œå®æ—¶ç›‘æ§,Cloudera Supportå³æ˜¯å¯¹Hadoopçš„æŠ€æœ¯æ”¯æŒ.
> 
> Clouderaçš„æ ‡ä»·ä¸ºæ¯å¹´æ¯ä¸ªèŠ‚ç‚¹4000ç¾å…ƒ,Clouderaå¼€å‘å¹¶è´¡çŒ®äº†å¯å®æ—¶å¤„ç†å¤§æ•°æ®çš„Impalaé¡¹ç›®.

#### Hortonworks Hadoop
> Hortonworksæ–‡æ¡£è¾ƒå¥½.
> 
> å®˜ç½‘åœ°å€ : https://hortonworks.com/products/data-center/hdp/
> 
> ä¸‹è½½åœ°å€ : https://hortonworks.com/downloads/#data-platform
> 
> 2011å¹´æˆç«‹çš„Hortonworksæ˜¯é›…è™ä¸ç¡…è°·é£æŠ•å…¬å¸Benchmark Capitalåˆèµ„ç»„å»º.
> 
> å…¬å¸æˆç«‹ä¹‹åˆå°±å¸çº³äº†å¤§çº¦25åè‡³30åä¸“é—¨ç ”ç©¶Hadoopçš„é›…è™å·¥ç¨‹å¸ˆ,ä¸Šè¿°å·¥ç¨‹å¸ˆå‡åœ¨2005å¹´å¼€å§‹ååŠ©é›…è™å¼€å‘Hadoop,å¹¶è´¡çŒ®äº†80%çš„Hadoopä»£ç .
> 
> é›…è™å·¥ç¨‹å‰¯æ€»è£,é›…è™Hadoopå¼€å‘å›¢é˜Ÿè´Ÿè´£äººEric Baldeschwielerå‡ºä»»Hortonworksçš„é¦–å¸­æ‰§è¡Œå®˜.
> 
> Hortonworksä¸»æ‰“äº§å“æ˜¯Hortonworks Data Platform(HDP),ä¹ŸåŒæ ·æ˜¯100%å¼€æºäº§å“,HDPé™¤å¸¸è§çš„é¡¹ç›®å¤–è¿˜åŒ…æ‹¬Ambari,ä¸€æ¬¾å¼€æºçš„å®‰è£…å’Œç®¡ç†ç³»ç»Ÿ.
> 
> HCatalog ä¸€ä¸ªå…ƒæ•°æ®ç®¡ç†ç³»ç»Ÿ,HCatalogç°å·²é›†æˆåˆ°Facebookå¼€æºçš„Hiveä¸­,Hortonworksçš„Stingerå¼€åˆ›æ€§çš„æå¤§çš„ä¼˜åŒ–äº†Hiveé¡¹ç›®,Hortonworksä¸ºå…¥é—¨æä¾›ä¸€ä¸ªéå¸¸å¥½çš„æ˜“äºä½¿ç”¨çš„æ²™ç›’.
> 
> Hortonworkså¼€å‘äº†å¾ˆå¤šå¢å¼ºç‰¹æ€§å¹¶æäº¤è‡³æ ¸å¿ƒä¸»å¹²,è¿™ä½¿å¾—Apache Hadoopèƒ½å¤Ÿåœ¨åŒ…æ‹¬Window Serverå’ŒWindowns Azureåœ¨å†…çš„Microsoft Windowså¹³å°ä¸Šæœ¬åœ°è¿è¡Œ,å®šä»·ä»¥é›†ç¾¤ä¸ºåŸºç¡€,æ¯10ä¸ªèŠ‚ç‚¹æ¯å¹´ä¸º12500ç¾å…ƒ.

### Hadoop ä¼˜åŠ¿ (4é«˜)
#### 1.é«˜å¯é æ€§
> Hadoopåº•å±‚ç»´æŠ¤å¤šä¸ªæ•°æ®å‰¯æœ¬,æ‰€ä»¥å³ä½¿HaoopæŸä¸ªè®¡ç®—å…ƒç´ æˆ–å­˜å‚¨å‡ºç°æ•…éšœ,ä¹Ÿä¸ä¼šå¯¼è‡´æ•°æ®çš„ä¸¢å¤±.
#### 2.é«˜æ‰©å±•æ€§
> åœ¨é›†ç¾¤é—´åˆ†é…ä»»åŠ¡æ•°æ®,å¯æ–¹ä¾¿çš„æ‰©å±•æ•°ä»¥åƒè®¡çš„èŠ‚ç‚¹.
#### 3.é«˜æ•ˆæ€§
> åœ¨MapReduceçš„æ€æƒ³ä¸‹,Hadoopæ˜¯å¹¶è¡Œå·¥ä½œ,ä»¥åŠ å¿«ä»»åŠ¡å¤„ç†é€Ÿåº¦.
#### 4.é«˜å®¹é”™æ€§
> èƒ½å¤Ÿè‡ªåŠ¨å°†å¤±è´¥çš„ä»»åŠ¡é‡æ–°åˆ†é….



### Hadoop ç»„æˆ(é¢è¯•é‡ç‚¹)
#### Hadoop1.xä¸Hadoop2.x åŒºåˆ«
> Hadoop1.xç»„æˆ : **`MapReduce(è®¡ç®—+èµ„æºè°ƒåº¦)`** | **`HDFS(æ•°æ®å­˜å‚¨)`** | **`Common(è¾…åŠ©å·¥å…·)`**
> 
> Hadoop2.xç»„æˆ : **`MapReduce(è®¡ç®—)`** | **`Yarn(èµ„æºè°ƒåº¦)`** | **`HDFS(æ•°æ®å­˜å‚¨)`** | **`Common(è¾…åŠ©å·¥å…·)`**
> 
> åœ¨Hadoop1.xæ—¶ä»£,Hadoopä¸­çš„MapReduceåŒæ—¶å¤„ç†ä¸šåŠ¡é€»è¾‘è¿ç®—å’Œèµ„æºçš„è°ƒåº¦,æ‰€ä»¥è€¦åˆæ€§è¾ƒå¤§.
> 
> åœ¨Hadoop2.xæ—¶ä»£,å¢åŠ äº†Yarn,Yarnåªè´Ÿè´£èµ„æºçš„è°ƒåº¦,MapReduceåªè´Ÿè´£è¿ç®—.

#### HDFS æ¶æ„æ¦‚è¿°
HDFS (Hadoop Distributed File System) | **`ä¸‰å¤§ç»„ä»¶ nn / dn / 2nn`**
> 1.NameNode(nn) : å­˜å‚¨æ–‡ä»¶çš„å…ƒæ•°æ®,å¦‚æ–‡ä»¶å,æ–‡ä»¶ç›®å½•ç»“æ„,æ–‡ä»¶å±æ€§(ç”Ÿæˆæ—¶é—´,å‰¯æœ¬æ•°,æ–‡ä»¶æƒé™,),ä»¥åŠæ¯ä¸ªæ–‡ä»¶çš„å—åˆ—è¡¨å’Œå—æ‰€åœ¨çš„DataNodeç­‰.
> 
> 2.DataNode(dn) : åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨æ–‡ä»¶å—æ•°æ®,ä»¥åŠå—æ•°æ®çš„æ ¡éªŒå’Œ.
> 
> 3.Secondary NameNode(2nn):ç”¨æ¥ç›‘æ§HDFSçŠ¶æ€çš„è¾…åŠ©åå°ç¨‹åº,æ¯éš”ä¸€æ®µæ—¶é—´è·å–HDFSå…ƒæ•°æ®çš„å¿«ç…§.

#### YARN æ¶æ„æ¦‚è¿°
**`å››å¤§ç»„ä»¶ | RM / NM / AM / Container`**
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_001.jpg)

#### MapReduce æ¶æ„æ¦‚è¿°
> **`ä¸¤å¤§é˜¶æ®µ | Map / Reduce`**
> 
> MapReduceå°†è®¡ç®—è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ:Map å’Œ Reduce
> 
> Mapé˜¶æ®µå¹¶è¡Œå¤„ç†è¾“å…¥æ•°æ® | Reduceé˜¶æ®µå¯¹Mapç»“æœè¿›è¡Œæ±‡æ€»
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_002.jpg)

### å¤§æ•°æ®æŠ€æœ¯ ç”Ÿæ€ä½“ç³»
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_003.jpg)

### æ¨è ç³»ç»Ÿæ¡†æ¶å›¾
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_004.jpg)


## 4. Hadoop è¿è¡Œç¯å¢ƒæ­å»º(å¼€å‘é‡ç‚¹)
### è™šæ‹Ÿæœºç¯å¢ƒ å‡†å¤‡
#### 1.å…‹éš†è™šæ‹Ÿæœº
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_005.jpg)
#### 2.ä¿®æ”¹å…‹éš†è™šæ‹Ÿæœºçš„é™æ€IP
> ä½¿ç”¨rootç”¨æˆ·ç™»å½•
**`vim /etc/udev/rules.d/70-persistent-net.rules`**

æºä»£ç 
``` bash
# This file was automatically generated by the /lib/udev/write_net_rules
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.

# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:a3:d8:a7", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"

# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:67:b3:77", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"
```

å°†NAME="eth1"æ›´æ”¹ä¸ºNAME="eth0",å¹¶å¤åˆ¶00:0c:29:67:b3:77åœ°å€
``` bash
# PCI device 0x8086:0x100f (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:67:b3:77", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

ä¿®æ”¹ç½‘ç»œé…ç½®
ç²˜è´´ä¸Šä¸€æ­¥åœ°å€,ä¿®æ”¹HWADDRå±æ€§
**`vim /etc/sysconfig/network-scripts/ifcfg-eth0`**
```bash
DEVICE=eth0
HWADDR=00:0c:29:67:b3:77
TYPE=Ethernet
UUID=b75136b3-4a81-41b5-9ebd-bfc1831d0df7
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static

IPADDR=192.168.177.131
GATEWAY=192.168.177.2
DNS1=192.168.177.2
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

#### 3.ä¿®æ”¹ä¸»æœºå
`vim /etc/sysconfig/network`
``` bash
NETWORKING=yes
HOSTNAME=corehub-004
```
æ›´æ”¹å®Œæ¯•,`:wq`ä¿å­˜é€€å‡º

#### 4.å…³é—­é˜²ç«å¢™
æš‚æ—¶æ€§å…³é—­é˜²ç«å¢™:`service iptables stop`

#### 5.åˆ›å»ºç”¨æˆ·
`useradd username`

#### 6.é…ç½®ç”¨æˆ·å…·æœ‰rootæƒé™
`vim /etc/sudoers`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_006.jpg)
æ›´æ”¹å®Œæ¯•,`:wq!`ä¿å­˜é€€å‡º

#### 7.åœ¨/optç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶å¤¹
> 1.åˆ›å»ºsoftware,moduleæ–‡ä»¶å¤¹
> software ç”¨äºæ—¥åå­˜å‚¨çš„ç¨‹åºå®‰è£…åŒ…
> module ç”¨äºæ—¥åå­˜å‚¨è§£æåçš„ç¨‹åºjaråŒ…
```
sudo mkdir software
sudo mkdir module
```
``` powershell
[geek-developer@corehub-001 ~]$ cd /opt/
[geek-developer@corehub-001 opt]$ ll
total 8
drwxr-xr-x. 6 root root 4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 root root 4096 Oct  4  2017 rh
[geek-developer@corehub-001 opt]$ sudo mkdir software
[sudo] password for geek-developer: 
[geek-developer@corehub-001 opt]$ sudo mkdir module
[geek-developer@corehub-001 opt]$ ll
total 16
drwxr-xr-x. 6 root root 4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 root root 4096 Jan 24 20:12 module
drwxr-xr-x. 2 root root 4096 Oct  4  2017 rh
drwxr-xr-x. 2 root root 4096 Jan 24 20:11 software
[geek-developer@corehub-001 opt]$ 
```
2.ä¿®æ”¹software,moduleæ–‡ä»¶å¤¹çš„æ‰€æœ‰è€…
``` powershell
[geek-developer@corehub-001 opt]# chown geek-developer:geek-developer software/ module/
[geek-developer@corehub-001 opt]# ll
total 16
drwxr-xr-x. 6 root           root           4096 Jan 17 23:35 devtool
drwxr-xr-x. 2 geek-developer geek-developer 4096 Jan 24 20:12 module
drwxr-xr-x. 2 root           root           4096 Oct  4  2017 rh
drwxr-xr-x. 2 geek-developer geek-developer 4096 Jan 24 20:11 software
[geek-developer@corehub-001 opt]# 
```

### å®‰è£… Hadoop
> åœ¨å®‰è£…Hadoopå‰ææ˜¯éœ€è¦å…ˆå®‰è£…JAVAå¹¶é…ç½®ç¯å¢ƒå˜é‡å³å¯
> 
> Apache Hadoopå®˜æ–¹åœ°å€ : https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/
> 
> é€šè¿‡è¿œç¨‹å·¥å…·,å°†hadoop-2.7.2.tar.gzä¼ è¾“åˆ°/op/tsoftware/ç›®å½•ä¸‹
> 

å°†hadoop-2.7.2.tar.gzè§£å‹/opt/module/ç›®å½•ä¸‹
```
tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module
```
å°†è§£å‹å®Œæˆhadoop-2.7.2é‡å‘½åä¸ºhadoop
``` powershell
[root@corehub-001 software]# cd ..
[root@corehub-001 opt]# cd module/
[root@corehub-001 module]# ll
total 4
drwxr-xr-x. 9 10011 10011 4096 Jan 26  2016 hadoop-2.7.2
[root@corehub-001 module]# mv hadoop-2.7.2 hadoop
[root@corehub-001 module]# ll
total 4
drwxr-xr-x. 9 10011 10011 4096 Jan 26  2016 hadoop
[root@corehub-001 module]# 
```
é…ç½®hadoopç¯å¢ƒå˜é‡
```
[root@corehub-001 module]# cd hadoop/
[root@corehub-001 hadoop]# pwd
/opt/module/hadoop
[root@corehub-001 hadoop]#
```
```
[root@corehub-001 geek-developer]# vim /etc/profile
```
```
##HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```
å®Œæˆç¯å¢ƒå˜é‡,`:wq`ä¿å­˜é€€å‡º
`source /etc/profile` æ›´æ–°é…ç½®æ–‡ä»¶æŒ‡ä»¤
```
[root@corehub-001 geek-developer]# source /etc/profile
[root@corehub-001 geek-developer]# hadoop
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
[root@corehub-001 geek-developer]# 
```


### Hadoop ç›®å½•ç»“æ„
> **`binç›®å½•`** :  HadoopæœåŠ¡è„šæœ¬.
> 
> **`etcç›®å½•`** :  Hadoopçš„é…ç½®æ–‡ä»¶ç›®å½•,å­˜æ”¾Haoopé…ç½®æ–‡ä»¶.
> 
> **`libç›®å½•`** : å­˜æ”¾Hadoopæœ¬åœ°åº“,(å¯¹æ•°æ®è¿›è¡Œå‹ç¼©è§£å‹åŠŸèƒ½).
> 
> **`sbinç›®å½•`** : å­˜æ”¾å¯åŠ¨æˆ–åœæ­¢Hadoopç›¸å…³æœåŠ¡è„šæœ¬.
> 
> **`shareç›®å½•`** : å­˜æ”¾Hadoopä¾èµ–jaråŒ…,æ–‡æ¡£,å®˜æ–¹æ¡ˆä¾‹.


## 5. Hadoop è¿è¡Œæ¨¡å¼
> Hadoopè¿è¡Œæ¨¡å¼åŒ…æ‹¬ : **`æœ¬åœ°è¿è¡Œ`** / **`ä¼ªåˆ†å¸ƒå¼è¿è¡Œ`** / **`å®Œå…¨åˆ†å¸ƒå¼è¿è¡Œ`**
### ğŸ‰ğŸ‰ æœ¬åœ°è¿è¡Œæ¨¡å¼ ğŸ‰ğŸ‰
#### ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’» Grep å®˜æ–¹æ¡ˆä¾‹ ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’»
> å®˜æ–¹æ¡ˆä¾‹åœ°å€ : [Standalone Operation](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SingleCluster.html)
> 
> By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.
> 
> The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.
> ```
> $ mkdir input
> $ cp etc/hadoop/*.xml input
> $ bin/hadoop jar s
> ```

##### 1.å¿«é€Ÿå¼€å§‹,åœ¨hadoopç›®å½•ä¸‹ åˆ›å»ºinputæ–‡ä»¶å¤¹
``` powershell
[geek-developer@corehub-001 ~]$ cd /opt/module/hadoop/
[geek-developer@corehub-001 hadoop]$ ll
total 52
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[geek-developer@corehub-001 hadoop]$ sudo mkdir input
[geek-developer@corehub-001 hadoop]$ ll
total 56
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:23 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[geek-developer@corehub-001 hadoop]$ 
```
##### 2.åœ¨hadoopç›®å½•ä¸­,å°†etcæ–‡ä»¶å¤¹å†… ä»¥.xmlä¸ºåç¼€çš„é…ç½®æ–‡ä»¶æ‹·è´åˆ°inputæ–‡ä»¶å¤¹é‡Œ
```
[geek-developer@corehub-001 hadoop]$ sudo cp etc/hadoop/*.xml input/
[geek-developer@corehub-001 hadoop]$ ls input/
capacity-scheduler.xml  hadoop-policy.xml  httpfs-site.xml  kms-site.xml
core-site.xml           hdfs-site.xml      kms-acls.xml     yarn-site.xml
[geek-developer@corehub-001 hadoop]$ 
```

##### 3.æ‰§è¡Œshareç›®å½•ä¸‹çš„hadoop-mapreduce-examples-2.7.2.jaråŒ…,å¹¶æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºè·¯å¾„,ä»¥ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼å¹¶ç»Ÿè®¡ä¸ªæ•°
`dfs[a-z.]+` ä»¥dfså¼€å¤´,ä»¥aåˆ°zä»»æ„å­—ç¬¦ä»¥.è¿‡æ»¤æ‰ - - å­—ç¬¦
``` powershell
[root@corehub-001 geek-developer]# cd /opt/module/hadoop/
##########æ‰§è¡Œshareç›®å½•ä¸‹çš„hadoop-mapreduce-examples-2.7.2.jaråŒ…,å¹¶æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºè·¯å¾„#############
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input/ output 'dfs[a-z.]+'
19/01/24 22:43:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/24 22:43:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 22:43:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/24 22:43:50 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1034400674_0001_r_000000_0' to file:/opt/module/hadoop/grep-temp-1632689888/_temporary/0/task_local1034400674_0001_r_000000
19/01/24 22:43:50 INFO mapred.LocalJobRunner: reduce > reduce
19/01/24 22:43:50 INFO mapred.Task: Task 'attempt_local1034400674_0001_r_000000_0' done.
19/01/24 22:43:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local1034400674_0001_r_000000_0
19/01/24 22:43:50 INFO mapred.LocalJobRunner: reduce task executor complete.
19/01/24 22:43:50 INFO mapreduce.Job: Job job_local1034400674_0001 running in uber mode : false
19/01/24 22:43:50 INFO mapreduce.Job:  map 100% reduce 100%
19/01/24 22:43:50 INFO mapreduce.Job: Job job_local1034400674_0001 completed successfully
19/01/24 22:43:50 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=2691317
		FILE: Number of bytes written=5002436
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=745
		Map output records=1
		Map output bytes=17
		Map output materialized bytes=67
		Input split bytes=877
		Combine input records=1
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=67
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =8
		Failed Shuffles=0
		Merged Map outputs=8
		GC time elapsed (ms)=147
		Total committed heap usage (bytes)=2574778368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=26007
	File Output Format Counters 
		Bytes Written=123
[root@corehub-001 hadoop]# ll
total 60
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
[root@corehub-001 hadoop]# ll output/
total 4
-rw-r--r--. 1 root root 11 Jan 24 22:43 part-r-00000
-rw-r--r--. 1 root root  0 Jan 24 22:43 _SUCCESS
############cd è¿›å…¥outputç›®å½•ä¸‹############
[root@corehub-001 hadoop]# cd output/
############æœ€åæŸ¥çœ‹ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼å¹¶ç»Ÿè®¡ä¸ªæ•°############
[root@corehub-001 output]# cat part-r-00000 
1	dfsadmin
[root@corehub-001 output]#
```

#### ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’» WordCount å®˜æ–¹æ¡ˆä¾‹ ğŸ‘¨â€ğŸ’»ğŸ‘¨â€ğŸ’»
##### 1.åœ¨hadoopç›®å½•ä¸‹,åˆ›å»ºä¸€ä¸ªwcinputæ–‡ä»¶å¤¹
``` powershell
[root@corehub-001 hadoop]# mkdir wcinput
[root@corehub-001 hadoop]# ll
total 64
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:07 wcinput
[root@corehub-001 hadoop]# 
```
##### 2.åœ¨wcinputæ–‡ä»¶ä¸‹åˆ›å»ºä¸€ä¸ªwc.inputæ–‡ä»¶
```
[root@corehub-001 hadoop]# cd wcinput/
[root@corehub-001 wcinput]# touch wc.input
[root@corehub-001 wcinput]# ll
total 0
-rw-r--r--. 1 root root 0 Jan 24 23:08 wc.input
[root@corehub-001 wcinput]# 
```
##### 3.ç¼–è¾‘wc.inputæ–‡ä»¶å¹¶åœ¨æ–‡ä»¶ä¸­è¾“å…¥å†…å®¹,è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º
```
[root@corehub-001 wcinput]# vim wc.input
```
```
hello-world
hello-world
java
python
php
golang
hadoop yarn
hadoop mapreduce
hive
spark
java
springcloud
springboot
geek
geekpark
geekparkhub
geekparkhub
geek-developer
jeep-711
jeep-711
github
~                                           
~   
```
##### 4.è¿”å›hadoopç›®å½•
```
[root@corehub-001 wcinput]# cd ..
[root@corehub-001 hadoop]# 
```
##### 5.æ‰§è¡Œç¨‹åº
``` powershell
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput
19/01/24 23:20:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/24 23:20:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 23:20:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/24 23:20:50 INFO input.FileInputFormat: Total input paths to process : 1
19/01/24 23:20:50 INFO mapreduce.JobSubmitter: number of splits:1
19/01/24 23:20:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local450202257_0001
19/01/24 23:20:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
19/01/24 23:20:51 INFO mapreduce.Job: Running job: job_local450202257_0001
19/01/24 23:20:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local450202257_0001_r_000000_0' to file:/opt/module/hadoop/wcoutput/_temporary/0/task_local450202257_0001_r_000000
19/01/24 23:20:51 INFO mapred.LocalJobRunner: reduce > reduce
19/01/24 23:20:51 INFO mapred.Task: Task 'attempt_local450202257_0001_r_000000_0' done.
19/01/24 23:20:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local450202257_0001_r_000000_0
19/01/24 23:20:51 INFO mapred.LocalJobRunner: reduce task executor complete.
19/01/24 23:20:52 INFO mapreduce.Job: Job job_local450202257_0001 running in uber mode : false
19/01/24 23:20:52 INFO mapreduce.Job:  map 100% reduce 100%
19/01/24 23:20:52 INFO mapreduce.Job: Job job_local450202257_0001 completed successfully
19/01/24 23:20:52 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=547482
		FILE: Number of bytes written=1105096
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=4
		Map output records=6
		Map output bytes=77
		Map output materialized bytes=77
		Input split bytes=105
		Combine input records=6
		Combine output records=5
		Reduce input groups=5
		Reduce shuffle bytes=77
		Reduce input records=5
		Reduce output records=5
		Spilled Records=10
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=397410304
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=53
	File Output Format Counters 
		Bytes Written=63
[root@corehub-001 hadoop]# 
```
##### 6.æŸ¥çœ‹ç»Ÿè®¡å•è¯çš„ä¸ªæ•°ç»“æœ | ğŸ˜ğŸ˜ æ·±æ·±æ„Ÿå—åˆ°å¤§æ•°æ®çš„é­…åŠ›æ‰€åœ¨ ğŸ˜ğŸ˜
``` powershell
[root@corehub-001 hadoop]# ll
total 68
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:33 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 24 23:34 wcoutput
[root@corehub-001 hadoop]# ll wcoutput/
total 4
-rw-r--r--. 1 root root 184 Jan 24 23:34 part-r-00000
-rw-r--r--. 1 root root   0 Jan 24 23:34 _SUCCESS
[root@corehub-001 hadoop]# cd wcoutput/
[root@corehub-001 wcoutput]# cat part-r-00000 
geek	1
geek-developer	1
geekpark	1
geekparkhub	2
github	1
golang	1
hadoop	2
hello-world	2
hive	1
java	2
jeep-711	2
mapreduce	1
php	1
python	1
spark	1
springboot	1
springcloud	1
yarn	1
[root@corehub-001 wcoutput]# 
```

###  ğŸ‰ğŸ‰ ä¼ªåˆ†å¸ƒå¼ è¿è¡Œæ¨¡å¼ ğŸ‰ğŸ‰
#### å¯åŠ¨HDFSå¹¶è¿è¡ŒMapReduceç¨‹åº
> åˆ†æ : é…ç½®é›†ç¾¤,å¯åŠ¨æµ‹è¯•é›†ç¾¤å¢åˆ æŸ¥,æ‰§è¡ŒWordCountæ¡ˆä¾‹
##### é…ç½®é›†ç¾¤
###### é…ç½® hadoop-env.sh,è·å–jdkå®‰è£…è·¯å¾„
è·å–å¹¶å¤åˆ¶JAVA_HOMEè·¯å¾„
```
[root@corehub-001 hadoop]# echo $JAVA_HOME
/opt/jdk1.8.0_162
```
é…ç½®hadoop-env.sh
```
[root@corehub-001 hadoop]# vim etc/hadoop/hadoop-env.sh
```
``` powershell
# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.

# The java implementation to use.
export JAVA_HOME=/opt/jdk1.8.0_162
```

###### é…ç½® **`core-site.xml`**
> core-site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [core-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/core-default.xml)

```
[root@corehub-001 hadoop]# vim etc/hadoop/core-site.xml
```
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šHDFSä¸­çš„NameNodeåœ°å€ -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://corehub-001:9000</value>
  </property>
<!-- æŒ‡å®šHadoopè¿è¡Œæ—¶äº§ç”Ÿæ–‡ä»¶çš„å­˜å‚¨ç›®å½• -->
   <property>
     <name>hadoop.tmp.dir</name>
     <value>/opt/module/hadoop/data/tmp</value>
   </property>
</configuration>
```
è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º

###### é…ç½® **`hdfs.site.xml`**
> hdfs.site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [hdfs-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)
```
[root@corehub-001 hadoop]# vim etc/hadoop/hdfs-site.xml
```
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šHDFSå‰¯æœ¬æ•°é‡ -->
  <property>
   <name>dfs.replication</name>
     <value>1</value>
  </property>
</configuration>
```
è¾“å…¥å®Œæ¯•æŒ‰esc,è¾“å…¥:wqä¿å­˜é€€å‡º

##### å¯åŠ¨é›†ç¾¤
###### æ ¼å¼åŒ– NameNode
> (ç¬¬ä¸€æ¬¡åˆå§‹åŒ–å¯åŠ¨éœ€è¦æ ¼å¼åŒ–,åªéœ€åœ¨é¦–æ¬¡å¯åŠ¨å‰æ ¼å¼åŒ–)
```
[root@corehub-001 hadoop]# bin/hdfs namenode -format
19/01/25 12:59:38 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = corehub-001/192.168.152.130
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = /opt/module/hadoop/etc/hadoop:/opt/module/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/module/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/module/hadoop/share/hadoop/common/lib/jsch-
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41; compiled by 'jenkins' on 2016-01-26T00:08Z
STARTUP_MSG:   java = 1.8.0_162
************************************************************/
19/01/25 12:59:38 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
19/01/25 12:59:38 INFO namenode.NameNode: createNameNode [-format]
19/01/25 12:59:38 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
19/01/25 12:59:38 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
19/01/25 12:59:38 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
19/01/25 12:59:38 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Jan 25 12:59:38
19/01/25 12:59:39 INFO util.GSet: Computing capacity for map NameNodeRetryCache
19/01/25 12:59:39 INFO util.GSet: VM type       = 64-bit
19/01/25 12:59:39 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
19/01/25 12:59:39 INFO util.GSet: capacity      = 2^15 = 32768 entries
19/01/25 12:59:39 INFO namenode.FSImage: Allocated new BlockPoolId: BP-169105537-192.168.152.130-1548449979185
19/01/25 12:59:39 INFO common.Storage: Storage directory /opt/module/hadoop/data/tmp/dfs/name has been successfully formatted.
19/01/25 12:59:39 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
19/01/25 12:59:39 INFO util.ExitUtil: Exiting with status 0
19/01/25 12:59:39 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at corehub-001/192.168.152.130
************************************************************/
[root@corehub-001 hadoop]# ll
total 72
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 bin
drwxr-xr-x. 3 root  root   4096 Jan 25 12:59 data
drwxr-xr-x. 3 10011 10011  4096 Jan 25  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 25 09:44 input
drwxr-xr-x. 3 10011 10011  4096 Jan 25  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 25  2016 LICENSE.txt
-rw-r--r--. 1 10011 10011   101 Jan 25  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 25 09:45 output
-rw-r--r--. 1 10011 10011  1366 Jan 25  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 25  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 25  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 25 09:47 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 25 09:48 wcoutput
[root@corehub-001 hadoop]# 
```
###### å¯åŠ¨ NameNode
```
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-corehub-001.out
[root@corehub-001 hadoop]# jps
3153 Jps
3022 NameNode
```

###### å¯åŠ¨ DataNode
```
[root@corehub-001 hadoop]# sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-corehub-001.out
[root@corehub-001 hadoop]# jps
3696 DataNode
3858 Jps
3022 NameNode
[root@corehub-001 hadoop]# 
```

##### æŸ¥çœ‹æ˜¯å¦å¯åŠ¨æˆåŠŸ
> å¯é€šè¿‡hadoopæä¾›websiteå›¾å½¢åŒ–ç•Œé¢ æŸ¥çœ‹å¯åŠ¨ç»“æœ
> 
> é€šè¿‡ LinuxHostName:50070ç«¯å£å·å½¢å¼è®¿é—® æˆ– é€šè¿‡ Linux IPaddr:50070ç«¯å£å·è®¿é—®
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_007.jpg)
##### ä»¥hdfsæŒ‡ä»¤ åˆ›å»ºå¤šçº§ç›®å½•
> ç±»ä¼¼äºLinuxç›®å½•æ ‘ç»“æ„ä¸€è‡´,å¯è§ç†Ÿç»ƒæŒæ¡LInuxå‘½ä»¤å°¤ä¸ºé‡è¦
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_008.jpg)

```
[root@corehub-001 hadoop]# bin/hdfs dfs -mkdir -p /user/geekparkhub/input
19/01/25 14:41:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```
```
[root@corehub-001 hadoop]# bin/hdfs dfs -ls -R /
19/01/25 14:44:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user/geekparkhub
drwxr-xr-x   - root supergroup          0 2019-01-25 14:41 /user/geekparkhub/input
[root@corehub-001 hadoop]# 
```
##### åœ¨hadoopç›®å½•ä¸‹,å°†wcinputç›®å½•åŠå…ƒæ•°æ®æ–‡ä»¶ä¸Šä¼ åˆ°/user/geekparkhub/inputç›®å½•ä¸­
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_009.jpg)
```
[root@corehub-001 hadoop]# bin/hdfs dfs -put wcinput/wc.input /user/geekparkhub/input
19/01/25 15:02:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[root@corehub-001 hadoop]#
```

##### æ‰§è¡Œå¹¶è¯»å–HDFSç¨‹åº
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_010.jpg)
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
19/01/25 15:16:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/25 15:16:05 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/25 15:16:05 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/25 15:16:05 INFO input.FileInputFormat: Total input paths to process : 1
19/01/25 15:16:08 INFO mapreduce.Job:  map 100% reduce 100%
19/01/25 15:16:08 INFO mapreduce.Job: Job job_local169102714_0001 completed successfully
19/01/25 15:16:08 INFO mapreduce.Job: Counters: 35
        File System Counters
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=471
        File Output Format Counters 
                Bytes Written=503
[root@corehub-001 hadoop]#
```

#### å¯åŠ¨YARNä¸Šè¿è¡ŒMapReduceç¨‹åº
> åˆ†æ : é…ç½®é›†ç¾¤åœ¨Yarnä¸Šè¿è¡ŒMR,å¯åŠ¨æµ‹è¯•é›†ç¾¤å¢åˆ æŸ¥,åœ¨Yarnä¸Šæ‰§è¡ŒWordcountæ¡ˆä¾‹
##### é…ç½®é›†ç¾¤
###### é…ç½®yarn-env.sh
`vim etc/hadoop/yarn-env.sh`
``` powershell
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# User for YARN daemons
export HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}

# resolve links - $0 may be a softlink
export YARN_CONF_DIR="${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}"

# some Java parameters
export JAVA_HOME=/opt/jdk1.8.0_162
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi

if [ "$JAVA_HOME" = "" ]; then
  echo "Error: JAVA_HOME is not set."
  exit 1
fi

JAVA=$JAVA_HOME/bin/java
JAVA_HEAP_MAX=-Xmx1000m
```
###### é…ç½®yarn-site.xml
> yarn-site.xml å®˜æ–¹æ–‡æ¡£è¯´æ˜ : [yarn-default.xml](http://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)
```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-site.xml
```
``` xml
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
<!-- Site specific YARN configuration properties -->
<!-- Reducerè·å–æ•°æ®æ–¹å¼ -->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
<!-- æŒ‡å®šYarnçš„ResourceManageråœ°å€-->
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>corehub-001</value>
    </property>
</configuration>
```

###### é…ç½®mapred-env.sh
`vim etc/hadoop/mapred-env.sh`
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-env.sh
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

export JAVA_HOME=/opt/jdk1.8.0_162

export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000

export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA

#export HADOOP_JOB_HISTORYSERVER_OPTS=
#export HADOOP_MAPRED_LOG_DIR="" # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.
#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.
#export HADOOP_MAPRED_PID_DIR= # The pid files are stored. /tmp by default.
#export HADOOP_MAPRED_IDENT_STRING= #A string representing this instance of hadoop. $USER by default
#export HADOOP_MAPRED_NICENESS= #The scheduling priority for daemons. Defaults to 0.
```

###### é…ç½®mapred-site.xml
> mapred-site.xml.template é‡å‘½åä¸ºmapred-site.xml
```
[root@corehub-001 hadoop]# mv etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
```
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-site.xml
```
``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<!-- Put site-specific property overrides in this file. -->
<configuration>
<!-- æŒ‡å®šMRè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
</configuration>
```
##### å¯åŠ¨é›†ç¾¤
> å¯åŠ¨å‰å¿…é¡»ä¿è¯NameNodeå’ŒDataNodeå·²ç»å¯åŠ¨
###### å¯åŠ¨ResourceManager
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
39653 ResourceManager
9353 DataNode
9066 NameNode
40171 Jps
[root@corehub-001 hadoop]# 
```
###### å¯åŠ¨NodeManager
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
40880 Jps
40769 NodeManager
39653 ResourceManager
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
###### æ‰§è¡Œwordcountç¨‹åºå¹¶æŸ¥çœ‹è¿è¡Œç»“æœ
> å¯é€šè¿‡hadoopæä¾›websiteå›¾å½¢åŒ–ç•Œé¢ æŸ¥çœ‹å¯åŠ¨ç»“æœ
> 
> é€šè¿‡ LinuxHostName:8088ç«¯å£å·å½¢å¼è®¿é—® æˆ– é€šè¿‡ Linux IPaddr:8088ç«¯å£å·è®¿é—®
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_011.jpg)
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
^H19/01/27 19:37:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 19:37:37 INFO client.RMProxy: Connecting to ResourceManager at corehub-001/192.168.177.130:8032
19/01/27 19:37:38 INFO input.FileInputFormat: Total input paths to process : 1
19/01/27 19:37:38 INFO mapreduce.JobSubmitter: number of splits:1
19/01/27 19:37:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1548588180141_0001
19/01/27 19:37:39 INFO impl.YarnClientImpl: Submitted application application_1548588180141_0001
19/01/27 19:37:39 INFO mapreduce.Job: The url to track the job: http://corehub-001:8088/proxy/application_1548588180141_0001/
19/01/27 19:37:39 INFO mapreduce.Job: Running job: job_1548588180141_0001
19/01/27 19:37:53 INFO mapreduce.Job: Job job_1548588180141_0001 running in uber mode : false
19/01/27 19:37:53 INFO mapreduce.Job:  map 0% reduce 0%
19/01/27 19:38:01 INFO mapreduce.Job:  map 100% reduce 0%
19/01/27 19:38:08 INFO mapreduce.Job:  map 100% reduce 100%
19/01/27 19:38:08 INFO mapreduce.Job: Job job_1548588180141_0001 completed successfully
19/01/27 19:38:08 INFO mapreduce.Job: Counters: 49
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=5298
                Total time spent by all reduces in occupied slots (ms)=4839
                Total time spent by all map tasks (ms)=5298
                Total time spent by all reduce tasks (ms)=4839
                Total vcore-milliseconds taken by all map tasks=5298
                Total vcore-milliseconds taken by all reduce tasks=4839
                Total megabyte-milliseconds taken by all map tasks=5425152
                Total megabyte-milliseconds taken by all reduce tasks=4955136
        Map-Reduce Framework
                Map input records=24
                Map output records=23
                Map output bytes=285
                Map output materialized bytes=262
                Input split bytes=120
                Combine input records=23
                Combine output records=18
                Reduce input groups=18
                Reduce shuffle bytes=262
                Reduce input records=18
                Reduce output records=18
                Spilled Records=36
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=164
                CPU time spent (ms)=1830
                Physical memory (bytes) snapshot=416026624
                Virtual memory (bytes) snapshot=4163923968
                Total committed heap usage (bytes)=275775488
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=196
        File Output Format Counters 
                Bytes Written=184
```


#### é…ç½®å†å²æœåŠ¡å™¨
> ä¸ºäº†æŸ¥çœ‹ç¨‹åºå†å²è¿è¡Œæƒ…å†µ,éœ€è¦é…ç½®ä¸€ä¸‹å†å²æœåŠ¡å™¨
##### é…ç½®mapred-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/mapred-site.xml
```
``` xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<!-- æŒ‡å®šMRè¿è¡Œåœ¨Yarnä¸Š -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
<!-- å†å²æœåŠ¡å™¨åœ°å€ -->
    <property>
       <name>mapreduce.jobhistory.address</name>
       <value>corehub-001:10020</value>
    </property>
<!-- å†å²æœåŠ¡å™¨WEBåœ°å€ -->
    <property>
       <name>mapreduce.jobhistory.webapp.address</name>
       <value>corehub-001:19888</value>
    </property>
</configuration>
```
##### å¯åŠ¨å†å²æœåŠ¡å™¨å¹¶æŸ¥çœ‹æŸ¥çœ‹å†å²æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨ä»¥åŠJobHistoryè¿è¡ŒçŠ¶æ€
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop/logs/mapred-root-historyserver-corehub-001.out
[root@corehub-001 hadoop]# jps
40769 NodeManager
66818 JobHistoryServer
39653 ResourceManager
66948 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_012.jpg)



#### é…ç½®æ—¥å¿—èšé›†
> æ—¥å¿—èšé›†æ¦‚å¿µ:åº”ç”¨è¿è¡Œå®Œæˆä»¥å,å°†ç¨‹åºè¿è¡Œæ—¥å¿—ä¿¡æ¯ä¸Šä¼ åˆ°HDFSç³»ç»Ÿä¸Š.
> æ—¥å¿—èšé›†åŠŸèƒ½ä¼˜åŠ¿:å¯ä»¥æ–¹ä¾¿æŸ¥çœ‹ç¨‹åºè¿è¡Œè¯¦æƒ…,æ–¹ä¾¿å¼€å‘è°ƒè¯•.
> æ³¨æ„:å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½,éœ€è¦**`é‡æ–°å¯åŠ¨NodeManager,ResourceManager,HistoryManageræ­¤ä¸‰é¡¹æœåŠ¡`**.

##### åˆ†åˆ«åœæ­¢HistoryManageræœåŠ¡,NodeManageræœåŠ¡,ResourceManageræœåŠ¡

###### åœæ­¢HistoryManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[root@corehub-001 hadoop]# jps
40769 NodeManager
39653 ResourceManager
94488 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
###### åœæ­¢NodeManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh stop nodemanager
stopping nodemanager
[root@corehub-001 hadoop]# jps
39653 ResourceManager
9353 DataNode
9066 NameNode
96078 Jps
[root@corehub-001 hadoop]# 
```
###### åœæ­¢ResourceManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh stop resourcemanager
stopping resourcemanager
[root@corehub-001 hadoop]# jps
98388 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]# 
```
##### é…ç½®yarn-site.xml
```
[root@corehub-001 hadoop]# vim etc/hadoop/yarn-site.xml
```
```
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
<!-- Site specific YARN configuration properties -->
<!-- Reducerè·å–æ•°æ®æ–¹å¼ -->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
<!-- æŒ‡å®šYarnçš„ResourceManageråœ°å€-->
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>corehub-001</value>
    </property>
<!-- æ—¥å¿—èšé›†åŠŸèƒ½ä½¿ç”¨ -->
    <property>
      <name>yarn.log-aggregation-enable</name>
      <value>true</value>
    </property>
<!-- æ—¥å¿—ä¿ç•™æ—¶é—´è®¾ç½®ä¸º7å¤© æ ¹æ®å¼€å‘æƒ…å†µ,æ—¶é—´å¯è‡ªå®šä¹‰-->
<!-- ä¸€å¤©=3600ç§’ 3600*24*7=604800 -->
    <property>
      <name>yarn.log-aggregation.retain-seconds</name>
      <value>604800</value>
    </property>
</configuration>
```
##### åˆ†åˆ«å¼€å¯HistoryManageræœåŠ¡,NodeManageræœåŠ¡,ResourceManageræœåŠ¡
###### å¼€å¯ResourceManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
113380 ResourceManager
113463 Jps
9353 DataNode
9066 NameNode
[root@corehub-001 hadoop]#
```
###### å¼€å¯NodeManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/yarn-daemon.sh start nodemanager
starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-corehub-001.out
[root@corehub-001 hadoop]# jps
114081 NodeManager
113380 ResourceManager
9353 DataNode
9066 NameNode
114159 Jps
[root@corehub-001 hadoop]# 
```
###### å¼€å¯HistoryManageræœåŠ¡
```
[root@corehub-001 hadoop]# sbin/mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /opt/module/hadoop/logs/mapred-root-historyserver-corehub-001.out
[root@corehub-001 hadoop]# jps
114081 NodeManager
115184 JobHistoryServer
113380 ResourceManager
9353 DataNode
9066 NameNode
115263 Jps
[root@corehub-001 hadoop]# 
```
##### åˆ é™¤HDFSä¸Šå·²ç»å­˜åœ¨çš„è¾“å‡ºç›®å½•
```
[root@corehub-001 hadoop]# bin/hdfs dfs -rm -r /user/geekparkhub/output
19/01/27 22:26:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 22:26:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /user/geekparkhub/output
[root@corehub-001 hadoop]# 
```
##### æ‰§è¡ŒWordCountç¨‹åº
```
[root@corehub-001 hadoop]# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/geekparkhub/input /user/geekparkhub/output
19/01/27 22:32:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/01/27 22:32:30 INFO client.RMProxy: Connecting to ResourceManager at corehub-001/192.168.177.130:8032
19/01/27 22:32:33 INFO input.FileInputFormat: Total input paths to process : 1
19/01/27 22:32:33 INFO mapreduce.JobSubmitter: number of splits:1
19/01/27 22:32:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1548598949012_0001
19/01/27 22:32:35 INFO impl.YarnClientImpl: Submitted application application_1548598949012_0001
19/01/27 22:32:35 INFO mapreduce.Job: The url to track the job: http://corehub-001:8088/proxy/application_1548598949012_0001/
19/01/27 22:32:35 INFO mapreduce.Job: Running job: job_1548598949012_0001
19/01/27 22:33:14 INFO mapreduce.Job: Job job_1548598949012_0001 running in uber mode : false
19/01/27 22:33:14 INFO mapreduce.Job:  map 0% reduce 0%
19/01/27 22:33:23 INFO mapreduce.Job:  map 100% reduce 0%
19/01/27 22:33:32 INFO mapreduce.Job:  map 100% reduce 100%
19/01/27 22:33:33 INFO mapreduce.Job: Job job_1548598949012_0001 completed successfully
19/01/27 22:33:34 INFO mapreduce.Job: Counters: 49
        File System Counters
                FILE: Number of bytes read=262
                FILE: Number of bytes written=235459
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=316
                HDFS: Number of bytes written=184
                HDFS: Number of read operations=6
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters 
                Map-Reduce Framework
                Map input records=24
                Map output records=23
                Map output bytes=285
                Map output materialized bytes=262
                Input split bytes=120
                Combine input records=23
                Combine output records=18
                Reduce input groups=18
                Reduce shuffle bytes=262
                Reduce input records=18
                Reduce output records=18
                Spilled Records=36
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=220
                CPU time spent (ms)=2130
                Physical memory (bytes) snapshot=399134720
                Virtual memory (bytes) snapshot=4166119424
                Total committed heap usage (bytes)=276824064
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=196
        File Output Format Counters 
                Bytes Written=184
```
##### æŸ¥çœ‹æ—¥å¿—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/hadoop/start_013.jpg)

æŸ¥çœ‹æ—¥å¿—æ–¹å¼ ä¹Ÿå¯ä»¥é€šè¿‡è¿›å…¥logæ–‡ä»¶å¤¹è¿›è¡ŒæŸ¥çœ‹
```
[root@corehub-001 hadoop]# ll
total 76
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 bin
drwxr-xr-x. 3 root  root   4096 Jan 27 18:47 data
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 etc
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 include
drwxr-xr-x. 2 root  root   4096 Jan 24 22:28 input
drwxr-xr-x. 3 10011 10011  4096 Jan 26  2016 lib
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 libexec
-rw-r--r--. 1 10011 10011 15429 Jan 26  2016 LICENSE.txt
drwxr-xr-x. 3 root  root   4096 Jan 27 22:23 logs
-rw-r--r--. 1 10011 10011   101 Jan 26  2016 NOTICE.txt
drwxr-xr-x. 2 root  root   4096 Jan 24 22:43 output
-rw-r--r--. 1 10011 10011  1366 Jan 26  2016 README.txt
drwxr-xr-x. 2 10011 10011  4096 Jan 26  2016 sbin
drwxr-xr-x. 4 10011 10011  4096 Jan 26  2016 share
drwxr-xr-x. 2 root  root   4096 Jan 24 23:48 wcinput
drwxr-xr-x. 2 root  root   4096 Jan 24 23:34 wcoutput
[root@corehub-001 hadoop]# ll logs/
total 472
-rw-r--r--. 1 root root  51669 Jan 27 22:36 hadoop-root-datanode-corehub-001.log
-rw-r--r--. 1 root root    715 Jan 27 18:48 hadoop-root-datanode-corehub-001.out
-rw-r--r--. 1 root root  59522 Jan 27 22:36 hadoop-root-namenode-corehub-001.log
-rw-r--r--. 1 root root   4960 Jan 27 18:55 hadoop-root-namenode-corehub-001.out
-rw-r--r--. 1 root root  53574 Jan 27 22:42 mapred-root-historyserver-corehub-001.log
-rw-r--r--. 1 root root   1484 Jan 27 22:24 mapred-root-historyserver-corehub-001.out
-rw-r--r--. 1 root root   1477 Jan 27 20:53 mapred-root-historyserver-corehub-001.out.1
-rw-r--r--. 1 root root      0 Jan 27 18:47 SecurityAuth-root.audit
drwxr-xr-x. 3 root root   4096 Jan 27 22:42 userlogs
-rw-r--r--. 1 root root 126215 Jan 27 22:33 yarn-root-nodemanager-corehub-001.log
-rw-r--r--. 1 root root   1515 Jan 27 22:23 yarn-root-nodemanager-corehub-001.out
-rw-r--r--. 1 root root   1508 Jan 27 19:24 yarn-root-nodemanager-corehub-001.out.1
-rw-r--r--. 1 root root 125846 Jan 27 22:38 yarn-root-resourcemanager-corehub-001.log
-rw-r--r--. 1 root root   1531 Jan 27 22:22 yarn-root-resourcemanager-corehub-001.out
-rw-r--r--. 1 root root   1524 Jan 27 19:23 yarn-root-resourcemanager-corehub-001.out.1
[root@corehub-001 hadoop]# 
```

#### é…ç½®æ–‡ä»¶è¯´æ˜
> Hadoop é…ç½®æ–‡ä»¶åˆ†ä¸ºä¸¤ç±»:é»˜è®¤é…ç½®æ–‡ä»¶å’Œè‡ªå®šä¹‰é…ç½®æ–‡ä»¶,åªæœ‰å¼€å‘è€…æƒ³ä¿®æ”¹æŸä¸€é»˜è®¤é…ç½®æ—¶,æ‰éœ€è¦ä¿®æ”¹è‡ªå®šä¹‰é…ç½®æ–‡ä»¶,æ›´æ”¹ç›¸åº”å±æ€§å€¼.
##### 1.é»˜è®¤é…ç½®æ–‡ä»¶
  

| è¦è·å–çš„é»˜è®¤æ–‡ä»¶ | æ–‡ä»¶å­˜æ”¾åœ¨Hadoopçš„jaråŒ…ä¸­çš„ä½ç½® | å¸¸ç”¨é…ç½®ä¿¡æ¯
| :-------- | --------:| --------:|
| [core-default.xml]    | hadoop-common-2.7.2.jar/core-default.xml | NameNodeå±æ€§å’Œç«¯å£å·,æ•°æ®å­˜å‚¨è·¯å¾„ |
| [hdfs-default.xml]    | hadoop-hdfs-2.7.2.jar/hdfs-default.xml | å‰¯æœ¬æ•° |
| [yarn-default.xml]    | hadoop-yarn-common-2.7.2.jar/yarn-default.xml | ResourceManager&NodeManagerå±æ€§ |
| [mapred-default.xml]    | hadoop-mapred-client-core-2.7.2.jar/mapred-default.xml | åœ¨Yarnè¿è¡Œç¨‹åº,é»˜è®¤æ˜¯åœ¨æœ¬æœºè¿è¡Œ |

##### 2.è‡ªå®šä¹‰é…ç½®æ–‡
> å››ä¸ªé…ç½®æ–‡ä»¶å­˜æ”¾åœ¨**`$HADOOP_HOME/etc/hadoop`**è·¯å¾„ä¸­,å¼€å‘è€…å¯ä»¥æ ¹æ®é¡¹ç›®éœ€æ±‚é‡æ–°è¿›è¡Œä¿®æ”¹é…ç½®
> 
> **`core-site.xml`** | **`hdfs-site.xml`**
> **`yarn-site.xml`** | **`mapred-site.xml`**



## ğŸ”’ å°šæœªè§£é” æ­£åœ¨å­¦ä¹ æ¢ç´¢ä¸­... å°½æƒ…æœŸå¾… Blogæ›´æ–°! ğŸ”’
###  ğŸ‰ğŸ‰ å®Œå…¨åˆ†å¸ƒå¼ è¿è¡Œæ¨¡å¼ (å¼€å‘é‡ç‚¹) ğŸ‰ğŸ‰
#### è™šæ‹Ÿæœºå‡†å¤‡
#### ç¼–å†™é›†ç¾¤åˆ†å‘è„šæœ¬xsync
#### é›†ç¾¤é…ç½®
#### é›†ç¾¤å•ç‚¹å¯åŠ¨
#### SSHæ— å¯†ç é…ç½®
#### ç¾¤èµ·é›†ç¾¤
#### é›†ç¾¤å¯åŠ¨/å…³é—­æ–¹å¼æ€»ç»“
#### é›†ç¾¤æ—¶é—´åŒæ­¥


## 6. Hadoop ç¼–è¯‘æºç 
### å‰æœŸå‡†å¤‡å·¥ä½œ
### jaråŒ…å®‰è£…
### ç¼–è¯‘æºç 


## 7. å¸¸è§é”™è¯¯(å„ç§å‘)åŠè§£å†³æ–¹æ¡ˆ


## 8. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- Emailï¼š<jeep711.home.@gmail.com>â€”â€” <jeep-711@outlook.com> â€”â€” <geekparkhub@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------