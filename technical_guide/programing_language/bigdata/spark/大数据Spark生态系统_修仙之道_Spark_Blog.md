# å¤§æ•°æ®Sparkç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Spark Blog

@(2019-05-15)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Spark|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

## ğŸ˜ Spark Technology ä¿®ä»™ä¹‹é“ é‡‘ä»™é“æœ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/spark.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------


[TOC]


## ğŸ”¥ 1. Spark åŸºç¡€ ğŸ”¥

### 1.1 Spark æ¦‚è¿°
- Sparkæ˜¯ä¸€ç§åŸºäºå†…å­˜å¿«é€Ÿ / é€šç”¨ / å¯æ‰©å±•å¤§æ•°æ®åˆ†æå¼•æ“.
- Sparkåœ¨2009å¹´è¯ç”Ÿäº(UC Berkeley AMP Lab)åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡AMPå®éªŒå®¤,Sparkæ˜¯ä½¿ç”¨å†…å­˜è®¡ç®—çš„å¼€æºå¤§æ•°æ®å¹¶è¡Œè®¡ç®—æ¡†æ¶,å¯ä»¥åº”å¯¹å¤æ‚çš„å¤§æ•°æ®å¤„ç†åœºæ™¯,2013å¹´Sparkæˆä¸ºApacheåŸºé‡‘ä¼šæ——ä¸‹é¡¶çº§é¡¹ç›®.
- Sparkå†…æ ¸æ˜¯ç”±Scalaç¼–ç¨‹è¯­è¨€å¼€å‘,åŒæ—¶ä¹Ÿæä¾›äº†Java/Python/Rè¯­è¨€ç­‰å¼€å‘ç¼–ç¨‹æ¥å£.


#### 1.1.1 Spark æ¨¡å—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_001.jpg)
- 1.`Spark Core` : å®ç°äº†SparkåŸºæœ¬åŠŸèƒ½,åŒ…å«ä»»åŠ¡è°ƒåº¦ / å†…å­˜ç®¡ç† / é”™è¯¯æ¢å¤ / ä¸å­˜å‚¨ç³»ç»Ÿäº¤äº’ç­‰æ¨¡å—,Spark Coreä¸­è¿˜åŒ…å«äº†å¯¹å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed DataSet,ç®€ç§°RDD)APIå®šä¹‰.
- 2.`Spark SQL` : æ˜¯Sparkç”¨æ¥æ“ä½œç»“æ„åŒ–æ•°æ®ç¨‹åºåŒ…,é€šè¿‡Spark SQL,å¯ä»¥ä½¿ç”¨SQLæˆ–è€…Apache Hiveç‰ˆæœ¬çš„SQLæ–¹è¨€(HQL)æ¥æŸ¥è¯¢æ•°æ®,Spark SQLæ”¯æŒå¤šç§æ•°æ®æº,æ¯”å¦‚Hiveè¡¨ã€Parquetä»¥åŠJSONç­‰.
- 3.`Spark Streaming` : æ˜¯Sparkæä¾›å¯¹å®æ—¶æ•°æ®è¿›è¡Œæµå¼è®¡ç®—çš„ç»„ä»¶,æä¾›äº†ç”¨æ¥æ“ä½œæ•°æ®æµçš„API,å¹¶ä¸”ä¸Spark Coreä¸­çš„RDD APIé«˜åº¦å¯¹åº”.
- 4.`Spark MLlib` : æä¾›å¸¸è§çš„æœºå™¨å­¦ä¹ (ML)åŠŸèƒ½ç¨‹åºåº“,åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰,è¿˜æä¾›äº†æ¨¡å‹è¯„ä¼°ã€æ•°æ®å¯¼å…¥ç­‰é¢å¤–æ”¯æŒåŠŸèƒ½.
- 5.`é›†ç¾¤ç®¡ç†å™¨` : Sparkè®¾è®¡ä¸ºå¯ä»¥é«˜æ•ˆåœ°åœ¨ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹åˆ°æ•°åƒä¸ªè®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä¼¸ç¼©è®¡ç®—,ä¸ºäº†å®ç°è¿™æ ·è¦æ±‚,åŒæ—¶è·å¾—æœ€å¤§çµæ´»æ€§,Sparkæ”¯æŒåœ¨å„ç§é›†ç¾¤ç®¡ç†å™¨(Cluster Manager)ä¸Šè¿è¡Œ,åŒ…æ‹¬Hadoop YARNã€ApacheMesos,ä»¥åŠSparkè‡ªå¸¦ç®€æ˜“è°ƒåº¦å™¨,å«ä½œç‹¬ç«‹è°ƒåº¦å™¨.

#### 1.1.2 Spark ç‰¹ç‚¹
- 1.`å¿«é€Ÿ` : ä¸Hadoop MapReduceç›¸æ¯”,SparkåŸºäºå†…å­˜è¿ç®—è¦å¿«100å€ä»¥ä¸Š,åŸºäºç¡¬ç›˜è¿ç®—ä¹Ÿè¦å¿«10å€ä»¥ä¸Š,Sparkå®ç°äº†é«˜æ•ˆDAGæœ‰å‘æ— ç¯å›¾æ‰§è¡Œå¼•æ“,å¯ä»¥é€šè¿‡åŸºäºå†…å­˜æ¥é«˜æ•ˆå¤„ç†æ•°æ®æµ,è®¡ç®—ä¸­é—´ç»“æœæ˜¯å­˜åœ¨äºå†…å­˜ä¸­.
- 2.`æ˜“ç”¨` : Sparkæ”¯æŒJavaã€Pythonå’ŒScalaçš„API,è¿˜æ”¯æŒè¶…è¿‡80ç§é«˜çº§ç®—æ³•,ä½¿å¼€å‘è€…å¯ä»¥å¿«é€Ÿæ„å»ºä¸åŒåº”ç”¨,è€Œä¸”Sparkæ”¯æŒäº¤äº’å¼çš„Pythonå’ŒScalaçš„Shell,å¯ä»¥éå¸¸æ–¹ä¾¿åœ°åœ¨Shellä¸­ä½¿ç”¨Sparké›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜æ–¹æ³•.
- 3.`é€šç”¨æ€§å¼º` : Sparkæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ,Sparkå¯ä»¥ç”¨äºæ‰¹å¤„ç† / äº¤äº’å¼æŸ¥è¯¢(SparkSQL) / å®æ—¶æµå¤„ç†(SparkStreaming) / æœºå™¨å­¦ä¹ (SparkMLlib) / å›¾è®¡ç®—(GraphX),è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨,å‡å°‘äº†å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬.
- 4.`å…¼å®¹æ€§` : Sparkå¯ä»¥éå¸¸æ–¹ä¾¿åœ°ä¸å…¶ä»–çš„å¼€æºäº§å“è¿›è¡Œèåˆ,æ¯”å¦‚Sparkå¯ä»¥ä½¿ç”¨Hadoop YARNå’ŒApacheMesosä½œä¸ºèµ„æºç®¡ç†å’Œè°ƒåº¦å™¨,å¹¶ä¸”å¯ä»¥å¤„ç†æ‰€æœ‰Hadoopæ”¯æŒçš„æ•°æ®,åŒ…æ‹¬HDFSã€HBaseç­‰,è¿™å¯¹äºå·²ç»éƒ¨ç½²Hadoopé›†ç¾¤çš„ç”¨æˆ·ç‰¹åˆ«é‡è¦,å› ä¸ºä¸éœ€è¦åšä»»ä½•æ•°æ®è¿ç§»å°±å¯ä»¥ä½¿ç”¨Sparkå¼ºå¤§å¤„ç†èƒ½åŠ›.

#### 1.1.3 Spark åº”ç”¨åœºæ™¯
- 1.Sparkå…·æœ‰ä¸°å¯Œç»„ä»¶,å¯é€‚ç”¨äºå¤šç§å¤æ‚åº”ç”¨åœºæ™¯,å¦‚SQLæŸ¥è¯¢/æœºå™¨å­¦ä¹ /å›¾å½¢è®¡ç®—/æµå¼è®¡ç®—ç­‰,åŒæ—¶Sparkå¯ä»¥ä¸Hadoopå¾ˆå¥½åœ°é›†æˆåœ¨ä¸€èµ·,ç›®å‰å·²ç»æœ‰éƒ¨åˆ†ä¸»æµå¤§æ•°æ®å‚å•†åœ¨å‘è¡Œç‰ˆHadoopç‰ˆæœ¬ä¸­åŒ…å«Spark/Cloudera/Hortonworks/MapReduceç­‰.
- 2.Sparkå¾—åˆ°äº†ä¼—å¤šå¤§æ•°æ®å…¬å¸çš„æ”¯æŒ,è¿™äº›å…¬å¸åŒ…æ‹¬Hortonworksã€IBMã€Intelã€Clouderaã€MapRã€Pivotalã€ç™¾åº¦ã€é˜¿é‡Œã€è…¾è®¯ã€äº¬ä¸œã€æºç¨‹ã€ä¼˜é…·åœŸè±†,å½“å‰ç™¾åº¦çš„Sparkå·²åº”ç”¨äºå¤§æœç´¢ã€ç›´è¾¾å·ã€ç™¾åº¦å¤§æ•°æ®ç­‰ä¸šåŠ¡,é˜¿é‡Œåˆ©ç”¨GraphXæ„å»ºäº†å¤§è§„æ¨¡å›¾è®¡ç®—å’Œå›¾æŒ–æ˜ç³»ç»Ÿ,å®ç°äº†å¾ˆå¤šç”Ÿäº§ç³»ç»Ÿçš„æ¨èç®—æ³•,è…¾è®¯Sparké›†ç¾¤è¾¾åˆ°8000å°è§„æ¨¡,æ˜¯å½“å‰å·²çŸ¥ä¸–ç•Œä¸Šæœ€å¤§çš„Sparké›†ç¾¤.


### 1.2 Spark éƒ¨ç½²
- Sparkå®˜æ–¹åœ°å€ : [spark.apache.org](http://spark.apache.org)
- Sparkå®˜æ–¹ä¸‹è½½ : [spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
- Sparkå®˜æ–¹æ–‡æ¡£ : [spark.apache.org/docs/2.1.1/](https://spark.apache.org/docs/2.1.1/)

è§£å‹`spark-2.1.1-bin-hadoop2.7.tgz`
```
[root@systemhub511 software]# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/
```
é‡å‘½å`spark-2.1.1-bin-hadoop2.7`
```
[root@systemhub511 module]# mv spark-2.1.1-bin-hadoop2.7/ spark
```

### 1.3 Spark è¿è¡Œæ¨¡å¼
#### ğŸ’¥ 1.3.1 Loacl Mode ğŸ’¥
##### 1.3.1.1 Loacl Mode æ¦‚è¿°
- Localæ¨¡å¼å°±æ˜¯è¿è¡Œåœ¨å•å°æœ¬åœ°è®¡ç®—æœºæ¨¡å¼,é€šå¸¸å°±æ˜¯ç”¨äºåœ¨æœ¬åœ°ä¸Šç»ƒæ‰‹æˆ–æµ‹è¯•,å®ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹é›†ä¸­æ–¹å¼è®¾ç½®Master.
- 1.`local` : æ‰€æœ‰è®¡ç®—éƒ½è¿è¡Œåœ¨ä¸€ä¸ªçº¿ç¨‹å½“ä¸­,æ²¡æœ‰ä»»ä½•å¹¶è¡Œè®¡ç®—,é€šå¸¸åœ¨æœ¬æœºæ‰§è¡Œæµ‹è¯•ä»£ç å°±ç”¨è¿™ç§æ¨¡å¼.
- 2.`local[K]` : æŒ‡å®šä½¿ç”¨å¤šå°‘ä¸ªçº¿ç¨‹æ¥è¿è¡Œè®¡ç®—,æ¯”å¦‚`local[4]`å°±æ˜¯è¿è¡Œ4ä¸ªWorkerçº¿ç¨‹,é€šå¸¸Cpuæœ‰å‡ ä¸ªCore,å°±æŒ‡å®šå‡ ä¸ªçº¿ç¨‹,æœ€å¤§åŒ–åˆ©ç”¨Cpuè®¡ç®—èƒ½åŠ›.
- 3.`local[*]` : è¿™ç§æ¨¡å¼ç›´æ¥æŒ‰ç…§Cpuæœ€å¤šCoresæ¥è®¾ç½®çº¿ç¨‹æ•°é‡.

##### 1.3.1.2 (æ±‚Ï€) & (WordCount) & (æœ¬åœ°è°ƒè¯•) å®˜æ–¹æ¡ˆä¾‹
- 1.åŸºæœ¬è¯­æ³•
```
bin/spark-submit \
--class <main-class>
--master <master-url> \
--deploy-mode <deploy-mode> \
--conf <key>=<value> \
... # other options
<application-jar> \
[application-arguments]
```
- 2.å‚æ•°è¯´æ˜
- `--master`: æŒ‡å®šMasteråœ°å€,é»˜è®¤ä¸ºLocal.
- `--class`: åº”ç”¨ä¸»å¯åŠ¨ç±»(å¦‚org.apache.spark.examples.SparkPi).
- `--deploy-mode` : æ˜¯å¦å‘å¸ƒé©±åŠ¨åˆ°workerèŠ‚ç‚¹(cluster)æˆ–è€…ä½œä¸ºä¸€ä¸ªæœ¬åœ°å®¢æˆ·ç«¯(client)(default: client)*
- `--conf` : ä»»æ„Sparké…ç½®å±æ€§,æ ¼å¼`key=value`,å¦‚æœå€¼åŒ…å«ç©ºæ ¼,å¯ä»¥åŠ å¼•å·`"key=value"`
- `application-jar` : æ‰“åŒ…å¥½åº”ç”¨jar,åŒ…å«ä¾èµ–,URLåœ¨é›†ç¾¤ä¸­å…¨å±€å¯è§,æ¯”å¦‚`hdfs://`å…±äº«å­˜å‚¨ç³»ç»Ÿ,å¦‚æœæ˜¯`file://path`,é‚£ä¹ˆæ‰€æœ‰èŠ‚ç‚¹çš„pathéƒ½åŒ…å«åŒæ ·çš„jaråŒ….
- `application-arguments` : ä¼ ç»™main()æ–¹æ³•çš„å‚æ•°.
- `--executor-memory 1G` : æŒ‡å®šæ¯ä¸ªexecutorå¯ç”¨å†…å­˜ä¸º1G
- `--total-executor-cores 2` : æŒ‡å®šæ¯ä¸ªexecutorä½¿ç”¨cpuæ ¸æ•°ä¸º2ä¸ª

- 3.æ±‚Ï€ç¨‹åº
- 3.1 æ±‚Ï€æ‰§è¡Œè¯­å¥
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 3.2 å¼€å§‹æ‰§è¡Œä»»åŠ¡
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
```
- 3.3 æŸ¥çœ‹æ‰§è¡Œç»“æœ | è¯¥ç®—æ³•æ˜¯åˆ©ç”¨`è’™ç‰¹Â·å¡ç½—ç®—æ³•`æ±‚Ï€
```
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.059446 s
Pi is roughly 3.1411463141146316
```
- 3.4 å¯åŠ¨`spark-shell`
```
[root@systemhub511 spark]# bin/spark-shell
Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = local[*], app id = local-1558677071165).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
- 3.5 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:4040`

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_002.jpg)

- 4.è¿è¡ŒWordCountç¨‹åº
- 4.1 åœ¨sparkæ ¹ç›®å½•åˆ›å»ºwordcountç›®å½•
```
[root@systemhub511 spark]# mkdir -p input/wordcount
```
- 4.2 åœ¨wordcountç›®å½•åˆ›å»ºæ•°æ®æ–‡ä»¶ | vim `wordcount_001.txt`
```
[root@systemhub511 spark]# cd input/wordcount/
[root@systemhub511 wordcount]# vim wordcount_001.txt
```
```
hadoop spark hive
hadoop spark hadoop
hbase flume hive
scala java oozie
```
- 4.3 æ‰§è¡ŒWordCountå¹¶æŸ¥çœ‹æ‰“å°ç»“æœ
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (spark,2), (hive,2), (hadoop,3), (oozie,1), (flume,1), (java,1), (hbase,1))

scala> 
```
- 4.4 å°†WordCountæ‰§è¡Œç»“æœè¾“å‡ºè‡³æœ¬åœ°æ–‡ä»¶
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("./output/wordcount/")
```
- 4.5 æŸ¥çœ‹æ–‡ä»¶ç»“æœ
```
[root@systemhub511 spark]# cd output/wordcount/
[root@systemhub511 wordcount]# ll
total 4
-rw-r--r--. 1 root root 79 May 24 14:48 part-00000
-rw-r--r--. 1 root root  0 May 24 14:48 _SUCCESS
[root@systemhub511 wordcount]# cat part-00000 
(scala,1)
(spark,2)
(hive,2)
(hadoop,3)
(oozie,1)
(flume,1)
(java,1)
(hbase,1)
[root@systemhub511 wordcount]# 
```

##### 1.3.1.3 æäº¤æµç¨‹
- æäº¤ä»»åŠ¡åˆ†æ | Sparké€šç”¨è¿è¡Œç®€æ˜“æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_003.jpg)
- æäº¤ä»»åŠ¡è§’è‰² : Driver (é©±åŠ¨å™¨) & Executor (æ‰§è¡Œå™¨)
- `1. Driver (é©±åŠ¨å™¨)`
- Sparké©±åŠ¨å™¨æ˜¯æ‰§è¡Œå¼€å‘ç¨‹åºä¸­mainæ–¹æ³•è¿›ç¨‹,å®ƒè´Ÿè´£å¼€å‘äººå‘˜ç¼–å†™ç”¨æ¥åˆ›å»ºSparkContext / åˆ›å»ºRDD,ä»¥åŠè¿›è¡ŒRDDè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œä»£ç çš„æ‰§è¡Œ,å¦‚æœä½¿ç”¨spark shell,é‚£ä¹ˆå½“å¯åŠ¨Spark shellçš„æ—¶å€™,ç³»ç»Ÿåå°è‡ªå¯ä¸€ä¸ªSparké©±åŠ¨å™¨ç¨‹åº,å°±æ˜¯åœ¨Spark shellä¸­é¢„åŠ è½½ä¸€ä¸ªå«ä½œscçš„SparkContextå¯¹è±¡,å¦‚æœé©±åŠ¨å™¨ç¨‹åºç»ˆæ­¢,é‚£ä¹ˆSparkåº”ç”¨ä¹Ÿå°±ç»“æŸäº†.
- 1.1 Driverä¸»è¦è´Ÿè´£ : 1.å°†å¼€å‘è€…ç¨‹åºè½¬ä¸ºä»»åŠ¡. `->` 2.è·Ÿè¸ªExecutorè¿è¡ŒçŠ¶å†µ. `->` 3.ä¸ºæ‰§è¡Œå™¨èŠ‚ç‚¹è°ƒåº¦ä»»åŠ¡. `->` 4.WebUIå±•ç¤ºåº”ç”¨è¿è¡ŒçŠ¶å†µ.
- `2. Executor (æ‰§è¡Œå™¨)`
- Spark Executoræ˜¯ä¸€ä¸ªå·¥ä½œè¿›ç¨‹,è´Ÿè´£åœ¨Sparkä½œä¸šä¸­è¿è¡Œä»»åŠ¡,ä»»åŠ¡é—´ç›¸äº’ç‹¬ç«‹,Sparkåº”ç”¨å¯åŠ¨æ—¶,ExecutorèŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨,å¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ªSparkåº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨,å¦‚æœæœ‰ExecutorèŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒ,Sparkåº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œ,ä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»–ExecutorèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œ.
- 2.2 Executorä¸»è¦è´Ÿè´£ : 1.è´Ÿè´£è¿è¡Œç»„æˆSparkåº”ç”¨ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™é©±åŠ¨å™¨è¿›ç¨‹. `->` 2.é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨(Block Manager)ä¸ºå¼€å‘è€…ç¨‹åºä¸­è¦æ±‚ç¼“å­˜RDDæä¾›å†…å­˜å¼å­˜å‚¨,RDDæ˜¯ç›´æ¥ç¼“å­˜åœ¨Executorè¿›ç¨‹å†…,å› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—.

##### 1.3.1.4 æ•°æ®æµç¨‹

| å‚æ•°åˆ—è¡¨      |     å‚æ•°æè¿° |
| :--------: | :--------:|
| `textFile("input")`    |   è¯»å–æœ¬åœ°æ–‡ä»¶inputæ–‡ä»¶å¤¹æ•°æ® |
| `flatMap(_.split(" "))` | å‹å¹³æ“ä½œ,æŒ‰ç…§ç©ºæ ¼åˆ†å‰²ç¬¦å°†ä¸€è¡Œæ•°æ®æ˜ å°„æˆä¸€ä¸ªä¸ªå•è¯ |
| `map((_,1))` | å¯¹æ¯ä¸€ä¸ªå…ƒç´ æ“ä½œ,å°†å•è¯æ˜ å°„ä¸ºå…ƒç»„ |
| `reduceByKey(_+_)` | æŒ‰ç…§keyå°†å€¼è¿›è¡Œèšåˆç›¸åŠ  |
| `collect` | å°†æ•°æ®æ”¶é›†åˆ°Driverç«¯å±•ç¤º |

- WordCount ç¨‹åºåˆ†æ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_004.jpg)


#### ğŸ’¥ 1.3.2 Standalone Mode ğŸ’¥
##### 1.3.2.1 Standalone Mode æ¦‚è¿°
- ç”±`Master`+`Slave`æ„å»ºè€Œæˆçš„Sparké›†ç¾¤,Sparkè¿è¡Œåœ¨é›†ç¾¤ä¸­.
- Standaloneè¿è¡Œæ¨¡å¼
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_005.jpg)

##### 1.3.2.2 StandaloneMode QuickStart
- 1.åœ¨sparkæ ¹ç›®å½•ä¸‹è¿›å…¥confç›®å½•
```
[root@systemhub511 spark]# cd conf/
```
- 2.ä¿®æ”¹é…ç½®æ–‡ä»¶åç§° | `slaves` & `spark-env.sh`
```
[root@systemhub511 conf]# mv slaves.template slaves
[root@systemhub511 conf]# mv spark-env.sh.template spark-env.sh
```
- 3.ä¿®æ”¹slaveæ–‡ä»¶,æ·»åŠ workèŠ‚ç‚¹ | vim `slaves`
```
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
systemhub511
systemhub611
systemhub711
```
- 4.ä¿®æ”¹spark-env.shæ–‡ä»¶ | vim `spark-env.sh`
```
# Options for the daemons used in the standalone deploy mode
SPARK_MASTER_HOST=systemhub511
SPARK_MASTER_PORT=7077
```

- 5.å°†sparkåˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 6.å¯åŠ¨sparké›†ç¾¤ | `sbin/start-all.sh`
```
[root@systemhub511 spark]# sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-systemhub511.out
systemhub711: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub711.out
systemhub611: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub611.out
systemhub511: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub511.out
[root@systemhub511 spark]# 
```
- 7.æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
``` powershell
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
30651 org.apache.spark.deploy.worker.Worker
30443 org.apache.spark.deploy.master.Master
813 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
10369 org.apache.spark.deploy.worker.Worker
11777 sun.tools.jps.Jps
================        root@systemhub711 All Processes         ===========
8960 org.apache.spark.deploy.worker.Worker
10364 sun.tools.jps.Jps
[root@systemhub511 spark]# 
```

- 8.(æ±‚Ï€)å®˜æ–¹æ¡ˆä¾‹
- 8.1 æ‰§è¡Œè¯­å¥ | æŒ‡å®š spark master
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://systemhub511:7077 \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 8.2 æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœ
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://systemhub511:7077 \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 6.478381 s
Pi is roughly 3.1405883140588315
```

- 8.3 å¯åŠ¨`sparkshell`,å¹¶æ‰§è¡ŒWordCountç¨‹åºæŸ¥çœ‹ç»“æœ
- å‚æ•°ï¼š`--master spark://systemhub511:7077` æŒ‡å®šè¦è¿æ¥é›†ç¾¤master
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077

Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = spark://systemhub511:7077, app id = app-20190524174512-0001).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (hive,2), (oozie,1), (java,1), (spark,2), (hadoop,3), (flume,1), (hbase,1))

scala> 
```

- 8.4 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:8088`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_006.jpg)

- 8.5 é…ç½®å†å²æœåŠ¡å™¨(JobHistoryServer)
- é‡å‘½å`spark-default.conf.template`
```
[root@systemhub511 conf]# mv spark-defaults.conf.template spark-defaults.conf
```
- 8.5.1 é…ç½®`spark-default.conf` | vim `spark-default.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
```
- 8.5.2 é…ç½®spark-env.sh | vim `spark-env.sh`
```
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- å‚æ•°æè¿° : 
```
spark.eventLog.dirï¼šApplicationåœ¨è¿è¡Œè¿‡ç¨‹ä¸­æ‰€æœ‰ä¿¡æ¯å‡è®°å½•åœ¨è¯¥å±æ€§æŒ‡å®šçš„è·¯å¾„ä¸‹.

spark.history.ui.port=18080 WEBUIè®¿é—®ç«¯å£å·ä¸º18080

spark.history.fs.logDirectory=hdfs://systemhub511:9000/directory é…ç½®äº†è¯¥å±æ€§å,åœ¨start-history-server.shæ—¶å°±æ— éœ€å†æ˜¾ç¤ºæŒ‡å®šè·¯å¾„,Spark History Serveråªå±•ç¤ºè¯¥æŒ‡å®šè·¯å¾„ä¸‹ä¿¡æ¯.

spark.history.retainedApplications=30 æŒ‡å®šä¿å­˜Applicationå†å²è®°å½•ä¸ªæ•°,å¦‚æœè¶…è¿‡è¿™ä¸ªå€¼,æ—§åº”ç”¨ç¨‹åºä¿¡æ¯å°†è¢«åˆ é™¤,è¿™ä¸ªæ˜¯å†…å­˜ä¸­åº”ç”¨æ•°,è€Œä¸æ˜¯é¡µé¢ä¸Šæ˜¾ç¤ºåº”ç”¨æ•°.
```
- 8.5.3 åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 8.5.4 å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 hadoop]# sbin/start-dfs.sh
```
- 8.5.5 æ‰‹åŠ¨åˆ›å»ºHDFS /directoryç›®å½•
```
[root@systemhub511 spark]# hadoop fs -mkdir /directory
``` 
- 8.5.6 å¯åŠ¨Sparké›†ç¾¤
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 8.5.6 å¯åŠ¨Sparkå†å²æœåŠ¡
```
[root@systemhub511 spark]# sbin/start-history-server.sh
```
- 8.5.7 å¯åŠ¨`sparkshell`
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077
sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```
- 8.5.8 æŸ¥çœ‹å†å²æœåŠ¡ | `http://hostname:18080`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_007.jpg)

#### ğŸ’¥ 1.3.3 Yarn Mode ğŸ’¥
##### 1.3.3.1 Yarn Mode æ¦‚è¿°





#### ğŸ’¥ 1.3.4 Mesos Mode ğŸ’¥
##### 1.3.4.1 Mesos Mode æ¦‚è¿°



### ğŸ”¥ 1.3 Spark Core ğŸ”¥
#### 1.3.1 RDD æ¦‚è¿°
#### 1.3.2 RDD ç¼–ç¨‹
#### 1.3.3 RDD æŒä¹…åŒ–
#### 1.3.4 RDD ä¾èµ–å…³ç³»
#### 1.3.5 é”®å€¼å¯¹æ“ä½œ
#### 1.3.6 æ•°æ®è¯»å–ä¿å­˜
#### 1.3.7 Spark è¿›é˜¶
#### 1.3.8 Spark Core å®ä¾‹


### ğŸ”¥ 1.4 Spark SQL ğŸ”¥
#### 1.4.1 Spark SQL æ¦‚è¿°
#### 1.4.2 Spark SQL æŸ¥è¯¢
#### 1.4.3 DataFrame
#### 1.4.4 DataSet
#### 1.4.5 èšåˆå‡½æ•°
#### 1.4.6 Spark SQL æ•°æ®æº
#### 1.4.7 OLAP Server
#### 1.4.8 Spark SQL å®ä¾‹


### ğŸ”¥ 1.5 Spark Streaming ğŸ”¥
#### 1.5.1 Spark Streaming æ¦‚è¿°
#### 1.5.2 Spark Streaming Program
#### 1.5.3 DataStream æ¦‚è¿°
#### 1.5.4 DataStream è¾“å…¥
#### 1.5.5 DataStream è½¬æ¢
#### 1.5.6 DataStream è¾“å‡º
#### 1.5.7 7*24hourè¿è¡Œ
#### 1.5.8 Spark Streaming å®ä¾‹



## ğŸ”¥ 2. Spark é«˜é˜¶ ğŸ”¥
### 2.1 å†…æ ¸æœºåˆ¶
### 2.1 æ€§èƒ½è°ƒä¼˜







## 3. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------