# å¤§æ•°æ®Sparkç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Spark Blog

@(2019-05-15)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Spark|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

## ğŸ˜ Spark Technology ä¿®ä»™ä¹‹é“ é‡‘ä»™é“æœ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/spark.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------


[TOC]


## ğŸ”¥ 1. Spark åŸºç¡€ ğŸ”¥

### 1.1 Spark æ¦‚è¿°
- Sparkæ˜¯ä¸€ç§åŸºäºå†…å­˜å¿«é€Ÿ / é€šç”¨ / å¯æ‰©å±•å¤§æ•°æ®åˆ†æå¼•æ“.
- Sparkåœ¨2009å¹´è¯ç”Ÿäº(UC Berkeley AMP Lab)åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡AMPå®éªŒå®¤,Sparkæ˜¯ä½¿ç”¨å†…å­˜è®¡ç®—çš„å¼€æºå¤§æ•°æ®å¹¶è¡Œè®¡ç®—æ¡†æ¶,å¯ä»¥åº”å¯¹å¤æ‚çš„å¤§æ•°æ®å¤„ç†åœºæ™¯,2013å¹´Sparkæˆä¸ºApacheåŸºé‡‘ä¼šæ——ä¸‹é¡¶çº§é¡¹ç›®.
- Sparkå†…æ ¸æ˜¯ç”±Scalaç¼–ç¨‹è¯­è¨€å¼€å‘,åŒæ—¶ä¹Ÿæä¾›äº†Java/Python/Rè¯­è¨€ç­‰å¼€å‘ç¼–ç¨‹æ¥å£.


#### 1.1.1 Spark æ¨¡å—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_001.jpg)
- 1.`Spark Core` : å®ç°äº†SparkåŸºæœ¬åŠŸèƒ½,åŒ…å«ä»»åŠ¡è°ƒåº¦ / å†…å­˜ç®¡ç† / é”™è¯¯æ¢å¤ / ä¸å­˜å‚¨ç³»ç»Ÿäº¤äº’ç­‰æ¨¡å—,Spark Coreä¸­è¿˜åŒ…å«äº†å¯¹å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed DataSet,ç®€ç§°RDD)APIå®šä¹‰.
- 2.`Spark SQL` : æ˜¯Sparkç”¨æ¥æ“ä½œç»“æ„åŒ–æ•°æ®ç¨‹åºåŒ…,é€šè¿‡Spark SQL,å¯ä»¥ä½¿ç”¨SQLæˆ–è€…Apache Hiveç‰ˆæœ¬çš„SQLæ–¹è¨€(HQL)æ¥æŸ¥è¯¢æ•°æ®,Spark SQLæ”¯æŒå¤šç§æ•°æ®æº,æ¯”å¦‚Hiveè¡¨ã€Parquetä»¥åŠJSONç­‰.
- 3.`Spark Streaming` : æ˜¯Sparkæä¾›å¯¹å®æ—¶æ•°æ®è¿›è¡Œæµå¼è®¡ç®—çš„ç»„ä»¶,æä¾›äº†ç”¨æ¥æ“ä½œæ•°æ®æµçš„API,å¹¶ä¸”ä¸Spark Coreä¸­çš„RDD APIé«˜åº¦å¯¹åº”.
- 4.`Spark MLlib` : æä¾›å¸¸è§çš„æœºå™¨å­¦ä¹ (ML)åŠŸèƒ½ç¨‹åºåº“,åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰,è¿˜æä¾›äº†æ¨¡å‹è¯„ä¼°ã€æ•°æ®å¯¼å…¥ç­‰é¢å¤–æ”¯æŒåŠŸèƒ½.
- 5.`é›†ç¾¤ç®¡ç†å™¨` : Sparkè®¾è®¡ä¸ºå¯ä»¥é«˜æ•ˆåœ°åœ¨ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹åˆ°æ•°åƒä¸ªè®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä¼¸ç¼©è®¡ç®—,ä¸ºäº†å®ç°è¿™æ ·è¦æ±‚,åŒæ—¶è·å¾—æœ€å¤§çµæ´»æ€§,Sparkæ”¯æŒåœ¨å„ç§é›†ç¾¤ç®¡ç†å™¨(Cluster Manager)ä¸Šè¿è¡Œ,åŒ…æ‹¬Hadoop YARNã€ApacheMesos,ä»¥åŠSparkè‡ªå¸¦ç®€æ˜“è°ƒåº¦å™¨,å«ä½œç‹¬ç«‹è°ƒåº¦å™¨.

#### 1.1.2 Spark ç‰¹ç‚¹
- 1.`å¿«é€Ÿ` : ä¸Hadoop MapReduceç›¸æ¯”,SparkåŸºäºå†…å­˜è¿ç®—è¦å¿«100å€ä»¥ä¸Š,åŸºäºç¡¬ç›˜è¿ç®—ä¹Ÿè¦å¿«10å€ä»¥ä¸Š,Sparkå®ç°äº†é«˜æ•ˆDAGæœ‰å‘æ— ç¯å›¾æ‰§è¡Œå¼•æ“,å¯ä»¥é€šè¿‡åŸºäºå†…å­˜æ¥é«˜æ•ˆå¤„ç†æ•°æ®æµ,è®¡ç®—ä¸­é—´ç»“æœæ˜¯å­˜åœ¨äºå†…å­˜ä¸­.
- 2.`æ˜“ç”¨` : Sparkæ”¯æŒJavaã€Pythonå’ŒScalaçš„API,è¿˜æ”¯æŒè¶…è¿‡80ç§é«˜çº§ç®—æ³•,ä½¿å¼€å‘è€…å¯ä»¥å¿«é€Ÿæ„å»ºä¸åŒåº”ç”¨,è€Œä¸”Sparkæ”¯æŒäº¤äº’å¼çš„Pythonå’ŒScalaçš„Shell,å¯ä»¥éå¸¸æ–¹ä¾¿åœ°åœ¨Shellä¸­ä½¿ç”¨Sparké›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜æ–¹æ³•.
- 3.`é€šç”¨æ€§å¼º` : Sparkæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ,Sparkå¯ä»¥ç”¨äºæ‰¹å¤„ç† / äº¤äº’å¼æŸ¥è¯¢(SparkSQL) / å®æ—¶æµå¤„ç†(SparkStreaming) / æœºå™¨å­¦ä¹ (SparkMLlib) / å›¾è®¡ç®—(GraphX),è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨,å‡å°‘äº†å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬.
- 4.`å…¼å®¹æ€§` : Sparkå¯ä»¥éå¸¸æ–¹ä¾¿åœ°ä¸å…¶ä»–çš„å¼€æºäº§å“è¿›è¡Œèåˆ,æ¯”å¦‚Sparkå¯ä»¥ä½¿ç”¨Hadoop YARNå’ŒApacheMesosä½œä¸ºèµ„æºç®¡ç†å’Œè°ƒåº¦å™¨,å¹¶ä¸”å¯ä»¥å¤„ç†æ‰€æœ‰Hadoopæ”¯æŒçš„æ•°æ®,åŒ…æ‹¬HDFSã€HBaseç­‰,è¿™å¯¹äºå·²ç»éƒ¨ç½²Hadoopé›†ç¾¤çš„ç”¨æˆ·ç‰¹åˆ«é‡è¦,å› ä¸ºä¸éœ€è¦åšä»»ä½•æ•°æ®è¿ç§»å°±å¯ä»¥ä½¿ç”¨Sparkå¼ºå¤§å¤„ç†èƒ½åŠ›.

#### 1.1.3 Spark åº”ç”¨åœºæ™¯
- 1.Sparkå…·æœ‰ä¸°å¯Œç»„ä»¶,å¯é€‚ç”¨äºå¤šç§å¤æ‚åº”ç”¨åœºæ™¯,å¦‚SQLæŸ¥è¯¢/æœºå™¨å­¦ä¹ /å›¾å½¢è®¡ç®—/æµå¼è®¡ç®—ç­‰,åŒæ—¶Sparkå¯ä»¥ä¸Hadoopå¾ˆå¥½åœ°é›†æˆåœ¨ä¸€èµ·,ç›®å‰å·²ç»æœ‰éƒ¨åˆ†ä¸»æµå¤§æ•°æ®å‚å•†åœ¨å‘è¡Œç‰ˆHadoopç‰ˆæœ¬ä¸­åŒ…å«Spark/Cloudera/Hortonworks/MapReduceç­‰.
- 2.Sparkå¾—åˆ°äº†ä¼—å¤šå¤§æ•°æ®å…¬å¸çš„æ”¯æŒ,è¿™äº›å…¬å¸åŒ…æ‹¬Hortonworksã€IBMã€Intelã€Clouderaã€MapRã€Pivotalã€ç™¾åº¦ã€é˜¿é‡Œã€è…¾è®¯ã€äº¬ä¸œã€æºç¨‹ã€ä¼˜é…·åœŸè±†,å½“å‰ç™¾åº¦çš„Sparkå·²åº”ç”¨äºå¤§æœç´¢ã€ç›´è¾¾å·ã€ç™¾åº¦å¤§æ•°æ®ç­‰ä¸šåŠ¡,é˜¿é‡Œåˆ©ç”¨GraphXæ„å»ºäº†å¤§è§„æ¨¡å›¾è®¡ç®—å’Œå›¾æŒ–æ˜ç³»ç»Ÿ,å®ç°äº†å¾ˆå¤šç”Ÿäº§ç³»ç»Ÿçš„æ¨èç®—æ³•,è…¾è®¯Sparké›†ç¾¤è¾¾åˆ°8000å°è§„æ¨¡,æ˜¯å½“å‰å·²çŸ¥ä¸–ç•Œä¸Šæœ€å¤§çš„Sparké›†ç¾¤.


### 1.2 Spark éƒ¨ç½²
- Sparkå®˜æ–¹åœ°å€ : [spark.apache.org](http://spark.apache.org)
- Sparkå®˜æ–¹ä¸‹è½½ : [spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
- Sparkå®˜æ–¹æ–‡æ¡£ : [spark.apache.org/docs/2.1.1/](https://spark.apache.org/docs/2.1.1/)

è§£å‹`spark-2.1.1-bin-hadoop2.7.tgz`
```
[root@systemhub511 software]# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/
```
é‡å‘½å`spark-2.1.1-bin-hadoop2.7`
```
[root@systemhub511 module]# mv spark-2.1.1-bin-hadoop2.7/ spark
```

### 1.3 Spark è¿è¡Œæ¨¡å¼
#### ğŸ’¥ 1.3.1 Loacl Mode ğŸ’¥
##### 1.3.1.1 Loacl Mode æ¦‚è¿°
- Localæ¨¡å¼å°±æ˜¯è¿è¡Œåœ¨å•å°æœ¬åœ°è®¡ç®—æœºæ¨¡å¼,é€šå¸¸å°±æ˜¯ç”¨äºåœ¨æœ¬åœ°ä¸Šç»ƒæ‰‹æˆ–æµ‹è¯•,å®ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹é›†ä¸­æ–¹å¼è®¾ç½®Master.
- 1.`local` : æ‰€æœ‰è®¡ç®—éƒ½è¿è¡Œåœ¨ä¸€ä¸ªçº¿ç¨‹å½“ä¸­,æ²¡æœ‰ä»»ä½•å¹¶è¡Œè®¡ç®—,é€šå¸¸åœ¨æœ¬æœºæ‰§è¡Œæµ‹è¯•ä»£ç å°±ç”¨è¿™ç§æ¨¡å¼.
- 2.`local[K]` : æŒ‡å®šä½¿ç”¨å¤šå°‘ä¸ªçº¿ç¨‹æ¥è¿è¡Œè®¡ç®—,æ¯”å¦‚`local[4]`å°±æ˜¯è¿è¡Œ4ä¸ªWorkerçº¿ç¨‹,é€šå¸¸Cpuæœ‰å‡ ä¸ªCore,å°±æŒ‡å®šå‡ ä¸ªçº¿ç¨‹,æœ€å¤§åŒ–åˆ©ç”¨Cpuè®¡ç®—èƒ½åŠ›.
- 3.`local[*]` : è¿™ç§æ¨¡å¼ç›´æ¥æŒ‰ç…§Cpuæœ€å¤šCoresæ¥è®¾ç½®çº¿ç¨‹æ•°é‡.

##### 1.3.1.2 (æ±‚Ï€) & (WordCount) & (æœ¬åœ°è°ƒè¯•) å®˜æ–¹æ¡ˆä¾‹
- 1.åŸºæœ¬è¯­æ³•
```
bin/spark-submit \
--class <main-class>
--master <master-url> \
--deploy-mode <deploy-mode> \
--conf <key>=<value> \
... # other options
<application-jar> \
[application-arguments]
```
- 2.å‚æ•°è¯´æ˜
- `--master`: æŒ‡å®šMasteråœ°å€,é»˜è®¤ä¸ºLocal.
- `--class`: åº”ç”¨ä¸»å¯åŠ¨ç±»(å¦‚org.apache.spark.examples.SparkPi).
- `--deploy-mode` : æ˜¯å¦å‘å¸ƒé©±åŠ¨åˆ°workerèŠ‚ç‚¹(cluster)æˆ–è€…ä½œä¸ºä¸€ä¸ªæœ¬åœ°å®¢æˆ·ç«¯(client)(default: client)*
- `--conf` : ä»»æ„Sparké…ç½®å±æ€§,æ ¼å¼`key=value`,å¦‚æœå€¼åŒ…å«ç©ºæ ¼,å¯ä»¥åŠ å¼•å·`"key=value"`
- `application-jar` : æ‰“åŒ…å¥½åº”ç”¨jar,åŒ…å«ä¾èµ–,URLåœ¨é›†ç¾¤ä¸­å…¨å±€å¯è§,æ¯”å¦‚`hdfs://`å…±äº«å­˜å‚¨ç³»ç»Ÿ,å¦‚æœæ˜¯`file://path`,é‚£ä¹ˆæ‰€æœ‰èŠ‚ç‚¹çš„pathéƒ½åŒ…å«åŒæ ·çš„jaråŒ….
- `application-arguments` : ä¼ ç»™main()æ–¹æ³•çš„å‚æ•°.
- `--executor-memory 1G` : æŒ‡å®šæ¯ä¸ªexecutorå¯ç”¨å†…å­˜ä¸º1G
- `--total-executor-cores 2` : æŒ‡å®šæ¯ä¸ªexecutorä½¿ç”¨cpuæ ¸æ•°ä¸º2ä¸ª

- 3.æ±‚Ï€ç¨‹åº
- 3.1 æ±‚Ï€æ‰§è¡Œè¯­å¥
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 3.2 å¼€å§‹æ‰§è¡Œä»»åŠ¡
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
```
- 3.3 æŸ¥çœ‹æ‰§è¡Œç»“æœ | è¯¥ç®—æ³•æ˜¯åˆ©ç”¨`è’™ç‰¹Â·å¡ç½—ç®—æ³•`æ±‚Ï€
```
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.059446 s
Pi is roughly 3.1411463141146316
```
- 3.4 å¯åŠ¨`spark-shell`
```
[root@systemhub511 spark]# bin/spark-shell
Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = local[*], app id = local-1558677071165).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
- 3.5 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:4040`

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_002.jpg)

- 4.è¿è¡ŒWordCountç¨‹åº
- 4.1 åœ¨sparkæ ¹ç›®å½•åˆ›å»ºwordcountç›®å½•
```
[root@systemhub511 spark]# mkdir -p input/wordcount
```
- 4.2 åœ¨wordcountç›®å½•åˆ›å»ºæ•°æ®æ–‡ä»¶ | vim `wordcount_001.txt`
```
[root@systemhub511 spark]# cd input/wordcount/
[root@systemhub511 wordcount]# vim wordcount_001.txt
```
```
hadoop spark hive
hadoop spark hadoop
hbase flume hive
scala java oozie
```
- 4.3 æ‰§è¡ŒWordCountå¹¶æŸ¥çœ‹æ‰“å°ç»“æœ
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (spark,2), (hive,2), (hadoop,3), (oozie,1), (flume,1), (java,1), (hbase,1))

scala> 
```
- 4.4 å°†WordCountæ‰§è¡Œç»“æœè¾“å‡ºè‡³æœ¬åœ°æ–‡ä»¶
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("./output/wordcount/")
```
- 4.5 æŸ¥çœ‹æ–‡ä»¶ç»“æœ
```
[root@systemhub511 spark]# cd output/wordcount/
[root@systemhub511 wordcount]# ll
total 4
-rw-r--r--. 1 root root 79 May 24 14:48 part-00000
-rw-r--r--. 1 root root  0 May 24 14:48 _SUCCESS
[root@systemhub511 wordcount]# cat part-00000 
(scala,1)
(spark,2)
(hive,2)
(hadoop,3)
(oozie,1)
(flume,1)
(java,1)
(hbase,1)
[root@systemhub511 wordcount]# 
```

##### 1.3.1.3 æäº¤æµç¨‹
- æäº¤ä»»åŠ¡åˆ†æ | Sparké€šç”¨è¿è¡Œç®€æ˜“æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_003.jpg)
- æäº¤ä»»åŠ¡è§’è‰² : Driver (é©±åŠ¨å™¨) & Executor (æ‰§è¡Œå™¨)
- `1. Driver (é©±åŠ¨å™¨)`
- Sparké©±åŠ¨å™¨æ˜¯æ‰§è¡Œå¼€å‘ç¨‹åºä¸­mainæ–¹æ³•è¿›ç¨‹,å®ƒè´Ÿè´£å¼€å‘äººå‘˜ç¼–å†™ç”¨æ¥åˆ›å»ºSparkContext / åˆ›å»ºRDD,ä»¥åŠè¿›è¡ŒRDDè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œä»£ç çš„æ‰§è¡Œ,å¦‚æœä½¿ç”¨spark shell,é‚£ä¹ˆå½“å¯åŠ¨Spark shellçš„æ—¶å€™,ç³»ç»Ÿåå°è‡ªå¯ä¸€ä¸ªSparké©±åŠ¨å™¨ç¨‹åº,å°±æ˜¯åœ¨Spark shellä¸­é¢„åŠ è½½ä¸€ä¸ªå«ä½œscçš„SparkContextå¯¹è±¡,å¦‚æœé©±åŠ¨å™¨ç¨‹åºç»ˆæ­¢,é‚£ä¹ˆSparkåº”ç”¨ä¹Ÿå°±ç»“æŸäº†.
- 1.1 Driverä¸»è¦è´Ÿè´£ : 1.å°†å¼€å‘è€…ç¨‹åºè½¬ä¸ºä»»åŠ¡. `->` 2.è·Ÿè¸ªExecutorè¿è¡ŒçŠ¶å†µ. `->` 3.ä¸ºæ‰§è¡Œå™¨èŠ‚ç‚¹è°ƒåº¦ä»»åŠ¡. `->` 4.WebUIå±•ç¤ºåº”ç”¨è¿è¡ŒçŠ¶å†µ.
- `2. Executor (æ‰§è¡Œå™¨)`
- Spark Executoræ˜¯ä¸€ä¸ªå·¥ä½œè¿›ç¨‹,è´Ÿè´£åœ¨Sparkä½œä¸šä¸­è¿è¡Œä»»åŠ¡,ä»»åŠ¡é—´ç›¸äº’ç‹¬ç«‹,Sparkåº”ç”¨å¯åŠ¨æ—¶,ExecutorèŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨,å¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ªSparkåº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨,å¦‚æœæœ‰ExecutorèŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒ,Sparkåº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œ,ä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»–ExecutorèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œ.
- 2.2 Executorä¸»è¦è´Ÿè´£ : 1.è´Ÿè´£è¿è¡Œç»„æˆSparkåº”ç”¨ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™é©±åŠ¨å™¨è¿›ç¨‹. `->` 2.é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨(Block Manager)ä¸ºå¼€å‘è€…ç¨‹åºä¸­è¦æ±‚ç¼“å­˜RDDæä¾›å†…å­˜å¼å­˜å‚¨,RDDæ˜¯ç›´æ¥ç¼“å­˜åœ¨Executorè¿›ç¨‹å†…,å› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—.

##### 1.3.1.4 æ•°æ®æµç¨‹

| å‚æ•°åˆ—è¡¨      |     å‚æ•°æè¿° |
| :--------: | :--------:|
| `textFile("input")`    |   è¯»å–æœ¬åœ°æ–‡ä»¶inputæ–‡ä»¶å¤¹æ•°æ® |
| `flatMap(_.split(" "))` | å‹å¹³æ“ä½œ,æŒ‰ç…§ç©ºæ ¼åˆ†å‰²ç¬¦å°†ä¸€è¡Œæ•°æ®æ˜ å°„æˆä¸€ä¸ªä¸ªå•è¯ |
| `map((_,1))` | å¯¹æ¯ä¸€ä¸ªå…ƒç´ æ“ä½œ,å°†å•è¯æ˜ å°„ä¸ºå…ƒç»„ |
| `reduceByKey(_+_)` | æŒ‰ç…§keyå°†å€¼è¿›è¡Œèšåˆç›¸åŠ  |
| `collect` | å°†æ•°æ®æ”¶é›†åˆ°Driverç«¯å±•ç¤º |

- WordCount ç¨‹åºåˆ†æ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_004.jpg)


#### ğŸ’¥ 1.3.2 Standalone Mode ğŸ’¥
##### 1.3.2.1 Standalone Mode æ¦‚è¿°
- ç”±`Master`+`Slave`æ„å»ºè€Œæˆçš„Sparké›†ç¾¤,Sparkè¿è¡Œåœ¨é›†ç¾¤ä¸­.
- Standaloneè¿è¡Œæ¨¡å¼
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_005.jpg)

##### 1.3.2.2 StandaloneMode QuickStart
- 1.åœ¨sparkæ ¹ç›®å½•ä¸‹è¿›å…¥confç›®å½•
```
[root@systemhub511 spark]# cd conf/
```
- 2.ä¿®æ”¹é…ç½®æ–‡ä»¶åç§° | `slaves` & `spark-env.sh`
```
[root@systemhub511 conf]# mv slaves.template slaves
[root@systemhub511 conf]# mv spark-env.sh.template spark-env.sh
```
- 3.ä¿®æ”¹slaveæ–‡ä»¶,æ·»åŠ workèŠ‚ç‚¹ | vim `slaves`
```
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
systemhub511
systemhub611
systemhub711
```
- 4.ä¿®æ”¹spark-env.shæ–‡ä»¶ | vim `spark-env.sh`
```
# Options for the daemons used in the standalone deploy mode
SPARK_MASTER_HOST=systemhub511
SPARK_MASTER_PORT=7077
```

- 5.å°†sparkåˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 6.å¯åŠ¨sparké›†ç¾¤ | `sbin/start-all.sh`
```
[root@systemhub511 spark]# sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-systemhub511.out
systemhub711: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub711.out
systemhub611: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub611.out
systemhub511: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub511.out
[root@systemhub511 spark]# 
```
- 7.æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
``` powershell
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
30651 org.apache.spark.deploy.worker.Worker
30443 org.apache.spark.deploy.master.Master
813 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
10369 org.apache.spark.deploy.worker.Worker
11777 sun.tools.jps.Jps
================        root@systemhub711 All Processes         ===========
8960 org.apache.spark.deploy.worker.Worker
10364 sun.tools.jps.Jps
[root@systemhub511 spark]# 
```

- 8.(æ±‚Ï€)å®˜æ–¹æ¡ˆä¾‹
- 8.1 æ‰§è¡Œè¯­å¥ | æŒ‡å®š spark master
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://systemhub511:7077 \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 8.2 æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœ
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://systemhub511:7077 \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 6.478381 s
Pi is roughly 3.1405883140588315
```

- 8.3 å¯åŠ¨`sparkshell`,å¹¶æ‰§è¡ŒWordCountç¨‹åºæŸ¥çœ‹ç»“æœ
- å‚æ•°ï¼š`--master spark://systemhub511:7077` æŒ‡å®šè¦è¿æ¥é›†ç¾¤master
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077

Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = spark://systemhub511:7077, app id = app-20190524174512-0001).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (hive,2), (oozie,1), (java,1), (spark,2), (hadoop,3), (flume,1), (hbase,1))

scala> 
```

- 8.4 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:8088`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_006.jpg)

- 8.5 é…ç½®å†å²æœåŠ¡å™¨(JobHistoryServer)
- é‡å‘½å`spark-default.conf.template`
```
[root@systemhub511 conf]# mv spark-defaults.conf.template spark-defaults.conf
```
- 8.5.1 é…ç½®`spark-default.conf` | vim `spark-default.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
```
- 8.5.2 é…ç½®spark-env.sh | vim `spark-env.sh`
```
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- å‚æ•°æè¿° : 
```
spark.eventLog.dirï¼šApplicationåœ¨è¿è¡Œè¿‡ç¨‹ä¸­æ‰€æœ‰ä¿¡æ¯å‡è®°å½•åœ¨è¯¥å±æ€§æŒ‡å®šçš„è·¯å¾„ä¸‹.

spark.history.ui.port=18080 WEBUIè®¿é—®ç«¯å£å·ä¸º18080

spark.history.fs.logDirectory=hdfs://systemhub511:9000/directory é…ç½®äº†è¯¥å±æ€§å,åœ¨start-history-server.shæ—¶å°±æ— éœ€å†æ˜¾ç¤ºæŒ‡å®šè·¯å¾„,Spark History Serveråªå±•ç¤ºè¯¥æŒ‡å®šè·¯å¾„ä¸‹ä¿¡æ¯.

spark.history.retainedApplications=30 æŒ‡å®šä¿å­˜Applicationå†å²è®°å½•ä¸ªæ•°,å¦‚æœè¶…è¿‡è¿™ä¸ªå€¼,æ—§åº”ç”¨ç¨‹åºä¿¡æ¯å°†è¢«åˆ é™¤,è¿™ä¸ªæ˜¯å†…å­˜ä¸­åº”ç”¨æ•°,è€Œä¸æ˜¯é¡µé¢ä¸Šæ˜¾ç¤ºåº”ç”¨æ•°.
```
- 8.5.3 åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 8.5.4 å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 hadoop]# sbin/start-dfs.sh
```
- 8.5.5 æ‰‹åŠ¨åˆ›å»ºHDFS /directoryç›®å½•
```
[root@systemhub511 spark]# hadoop fs -mkdir /directory
``` 
- 8.5.6 å¯åŠ¨Sparké›†ç¾¤
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 8.5.6 å¯åŠ¨Sparkå†å²æœåŠ¡
```
[root@systemhub511 spark]# sbin/start-history-server.sh
```
- 8.5.7 å¯åŠ¨`sparkshell`
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077
sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```
- 8.5.8 æŸ¥çœ‹å†å²æœåŠ¡ | `http://hostname:18080`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_007.jpg)

##### 1.3.2.3 Spark HA é«˜å¯ç”¨
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_008.jpg)

- 1.åœæ­¢é›†ç¾¤æ‰€æœ‰æœåŠ¡
- 2.é…ç½®spark-env.sh | vim `spark-env.sh`
```
# SPARK_MASTER_HOST=systemhub511
# SPARK_MASTER_PORT=7077
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=systemhub511,systemhub611,systemhub711 -Dspark.deploy.zookeeper.dir=/spark"
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- 3.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 4.å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 spark]# /opt/module/hadoop/sbin/start-dfs.sh
```
- 5.å¯åŠ¨Zookeeperé›†ç¾¤
```
[root@systemhub511 spark]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub611 ~]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub711 ~]# /opt/module/zookeeper/bin/zkServer.sh start
```
- 6.åœ¨systemhub511å¯åŠ¨å…¨éƒ¨æœåŠ¡èŠ‚ç‚¹
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 7.åœ¨systemhub611å•ç‹¬å¯åŠ¨masterå¤‡ä»½èŠ‚ç‚¹
```
[root@systemhub611 ~]# /opt/module/spark/sbin/start-master.sh
```
- 8.è®¿é—®SparkHAé›†ç¾¤
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077,systemhub611:7077
```
`http://systemhub511:8080` | systemhub511èŠ‚ç‚¹çŠ¶æ€ä¸º`ALIVE`
`http://systemhub611:8080` | systemhub611èŠ‚ç‚¹çŠ¶æ€ä¸º`STANDBY`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_009.jpg)

- 9.æ•…éšœè½¬ç§»æµ‹è¯•
- æ‰‹åŠ¨æ€æ­»systemhub511æœåŠ¡å™¨Masterè¿›ç¨‹,å¹¶æŸ¥çœ‹systemhub511æ˜¯å¦å°†ä»»åŠ¡è½¬ç§»ç»™systemhub611å¤‡ä»½èŠ‚ç‚¹ä½œä¸ºä¸»èŠ‚ç‚¹.
- 9.1 æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
```
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
32242 org.apache.hadoop.hdfs.server.namenode.NameNode
11206 org.apache.spark.deploy.master.Master
11368 org.apache.spark.deploy.worker.Worker
9705 org.apache.zookeeper.server.quorum.QuorumPeerMain
32444 org.apache.hadoop.hdfs.server.datanode.DataNode
5228 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
9157 org.apache.spark.deploy.master.Master
8901 org.apache.spark.deploy.worker.Worker
2822 sun.tools.jps.Jps
30214 org.apache.hadoop.hdfs.server.datanode.DataNode
7495 org.apache.zookeeper.server.quorum.QuorumPeerMain
================        root@systemhub711 All Processes         ===========
5312 org.apache.spark.deploy.worker.Worker
31568 sun.tools.jps.Jps
26869 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
26647 org.apache.hadoop.hdfs.server.datanode.DataNode
4014 org.apache.zookeeper.server.quorum.QuorumPeerMain
[root@systemhub511 spark]# 
```

- 9.2 Kill systemhub511 Masterä¸»èŠ‚ç‚¹
```
[root@systemhub511 spark]# kill -9 11206
```
- 9.3 systemhub511èŠ‚ç‚¹å·²å®•æœº | systemhub611å¤‡ä»½èŠ‚ç‚¹çŠ¶æ€å·²è½¬åŒ–ä¸ºALIVEä¸»èŠ‚ç‚¹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_010.jpg)


#### ğŸ’¥ 1.3.3 Yarn Mode ğŸ’¥
##### 1.3.3.1 Yarn Mode æ¦‚è¿°
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_011.jpg)
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Yarn,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤.
- ä¸¤ç§æ¨¡å¼`yarn-client`å’Œ`yarn-cluster`,ä¸»è¦åŒºåˆ«åœ¨äº : Driverç¨‹åºè¿è¡ŒèŠ‚ç‚¹
- `yarn-client` : Driverç¨‹åºè¿è¡Œåœ¨å®¢æˆ·ç«¯,é€‚ç”¨äºäº¤äº’è°ƒè¯•,ç«‹å³çœ‹åˆ°appè¾“å‡º.
- `yarn-cluster` : Driverç¨‹åºè¿è¡Œåœ¨ç”±RM(ResourceManager)å¯åŠ¨AP(APPMaster)é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ.

##### 1.3.3.2 YarnMode QuickStart

- 1.é…ç½®spark-env.sh | vim `spark-env.sh`
```
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```
- vim `spark-defaults.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
spark.yarn.historyServer.address=systemhub511:18080
spark.history.ui.port=18080
```
- vim `yarn-site.xml`
``` xml
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„ç‰©ç†å†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true -->
<property>
  <name>yarn.nodemanager.pmem-check-enabled</name>
  <value>false</value>
</property>

<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true-->
<property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
</property>
```

- 2.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```

- 3.æäº¤ä»»åŠ¡åˆ°Yarnæ‰§è¡Œ
```
bin/spark-submit \ 
--class org.apache.spark.examples.SparkPi \ 
--master yarn \ 
--deploy-mode client \ 
./examples/jars/spark-examples_2.11-2.1.1.jar\ 
100
```

#### ğŸ’¥ 1.3.4 Mesos Mode ğŸ’¥
##### 1.3.4.1 Mesos Mode æ¦‚è¿°
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Mesos,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤,å›½å†…åº”ç”¨æ¯”è¾ƒå°‘,æ›´å¤šæ˜¯è¿ç”¨yarnè°ƒåº¦.


#### ğŸ’¥ 1.3.5 è¿è¡Œæ¨¡å¼å¯¹æ¯” ğŸ’¥

| æ¨¡å¼      |     é›†ç¾¤æ•°é‡ |   é›†ç¾¤è¿›ç¨‹   |   æ‰€å±è€…   |
| :--------: | :--------:| :------: | :------: |
| Loacl Mode    |   1 |  æ—   |  Spark  |
| Standalone Mode    |   3 |  Master & Worker  |  Spark  |
| Yarn Mode    |   1 |  Yarn & HDFS  |  Hadoop  |

#### ğŸ’¥ 1.3.6 WordCount å®ä¾‹ ğŸ’¥
- Spark Shellä»…åœ¨æµ‹è¯•å’ŒéªŒè¯ç¨‹åºæ—¶ä½¿ç”¨çš„è¾ƒå¤š,åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šå¸¸ä¼šåœ¨IDEä¸­ç¼–åˆ¶ç¨‹åº,ç„¶åæ‰“æˆjaråŒ…æäº¤åˆ°é›†ç¾¤,æœ€å¸¸ç”¨æ˜¯åˆ›å»ºMavenå·¥ç¨‹,åˆ©ç”¨Mavenæ¥ç®¡ç†jaråŒ…ä¾èµ–.
- 1.JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
- 2.çˆ¶å·¥ç¨‹é…ç½®ä¿¡æ¯ | pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub.core.spark</groupId>
    <artifactId>spark_server</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>

    <modules>
        <module>spark-common</module>
    </modules>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
    </dependencies>

</project>
```
- 3.åˆ›å»ºå­æ¨¡å— spark-common | å­æ¨¡å—é…ç½®ä¿¡æ¯ pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>spark_server</artifactId>
        <groupId>com.geekparkhub.core.spark</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>spark-common</artifactId>

    <build>
        <finalName>WordCount</finalName>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

</project>
```

- 4.åœ¨`spark-common`å­æ¨¡å—ä¸­åˆ›å»ºscalaæºç ç›®å½• | Create `WordCount.scala`
``` scala
package com.geekparkhub.core.spark.application.wordcount

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * WordCountApplication
  * <p>
  */

object WordCount {
  def main(args: Array[String]): Unit = {

    /**
      * Create SparkConf
      * åˆ›å»º SparkConf
      */
    val sparkConf = new SparkConf().setMaster(args(0)).setAppName("WordCountApplication")

    /**
      * Create SparkContext
      * åˆ›å»º SparkContext
      */
    val sc = new SparkContext()

    /**
      * Read file
      * è¯»å–æ–‡ä»¶
      */
    val line: RDD[String] = sc.textFile(args(1))

    /**
      * To flatten
      * å‹å¹³
      */
    val word: RDD[String] = line.flatMap(_.split(" "))

    /**
      * Word conversion dual group
      * å•è¯è½¬æ¢äºŒå…ƒç»„
      */
    val wordAndOne: RDD[(String, Int)] = word.map((_, 1))

    /**
      * Count the total number of words
      * ç»Ÿè®¡å•è¯æ€»æ•°
      */
    val wordCount: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)

    /**
      * Write out the file
      * å†™å‡ºæ–‡ä»¶
      */
    wordCount.saveAsTextFile(args(2))

    /**
      * Close resource
      * å…³é—­èµ„æº
      */
    sc.stop()
  }
}
```
- 5.å°†`spark-common`å­æ¨¡å—æ‰“è‡³æˆjaråŒ…ä¸Šä¼ è‡³systemhub511æœåŠ¡å™¨
- 6.å¯åŠ¨HDFS | åœ¨HDFSåˆ›å»ºå¤šçº§ç›®å½•
```
[root@systemhub511 ~]# hadoop fs -mkdir -p /core_flow/spark/input/wordcount
```

- 7.å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ è‡³HDFSç›®å½•
```
hadoop fs -put /opt/module/spark/input/wordcount/wordcount_001.txt /core_flow/spark/input/wordcount
```
- 8.Yarnæ‰§è¡Œæäº¤ä»»åŠ¡è‡³
```
bin/spark-submit \
--class com.geekparkhub.core.spark.application.wordcount.WordCount \
--master yarn \
./lib_jar/WordCount.jar yarn \
/core_flow/spark/input/wordcount/wordcount_001.txt \
/core_flow/spark/output/wordcount
```
- 9.æŸ¥çœ‹ä»»åŠ¡æ±‡æ€»ç»“æœ
- 9.1 `hadoop fs -ls -R`
```
[root@systemhub511 spark]# hadoop fs -ls -R /core_flow/spark/output/wordcount/
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/_SUCCESS
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00000
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00001
[root@systemhub511 spark]# 
```
- 9.2 part-00000
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00000
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(scala,1)
(hive,2)
(oozie,1)
(java,1)
[root@systemhub511 spark]# 
```
- 9.3 part-00001
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00001
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(spark,2)
(hadoop,3)
(flume,1)
(hbase,1)
[root@systemhub511 spark]# 
```


### ğŸ”¥ 1.3 Spark Core ğŸ”¥
#### 1.3.1 RDD æ¦‚è¿°
##### 1.3.1.1 ä»€ä¹ˆæ˜¯RDD
> `RDD` (`Resilient Distributed Dataset`)`å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†`æ˜¯Sparkä¸­æœ€åŸºæœ¬æ•°æ®æŠ½è±¡,ä»£ç ä¸­æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»,å®ƒä»£è¡¨ä¸€ä¸ªå¼¹æ€§/ä¸å¯å˜/å¯åˆ†åŒº/é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆ.

##### 1.3.1.2 RDD å±æ€§
```
 * Internally, each RDD is characterized by five main properties:
 *
 *  - 1. A list of partitions
 *  - 2. A function for computing each split
 *  - 3. A list of dependencies on other RDDs
 *  - 4. Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - 5. Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
```
> 1.ä¸€ç»„åˆ†åŒº(Partition),å³æ•°æ®é›†åŸºæœ¬ç»„æˆå•ä½;
> 2.ä¸€ä¸ªè®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å‡½æ•°;
> 3.RDDä¹‹é—´ä¾èµ–å…³ç³»;
> 4.ä¸€ä¸ªPartitioner,å³RDDåˆ†ç‰‡å‡½æ•°;
> 5.ä¸€ä¸ªåˆ—è¡¨,å­˜å‚¨å­˜å–æ¯ä¸ªPartitionçš„ä¼˜å…ˆä½ç½®(preferred location)

##### 1.3.1.3 RDD ç‰¹ç‚¹
> RDDè¡¨ç¤ºåªè¯»åˆ†åŒºæ•°æ®é›†,å¯¹RDDè¿›è¡Œæ”¹åŠ¨,åªèƒ½é€šè¿‡RDDè½¬æ¢æ“ä½œ,ç”±ä¸€ä¸ªRDDå¾—åˆ°ä¸€ä¸ªæ–°çš„RDD,æ–°çš„RDDåŒ…å«äº†ä»å…¶ä»–RDDè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´å­˜åœ¨ä¾èµ–,RDDæ‰§è¡Œæ˜¯æŒ‰ç…§è¡€ç¼˜å…³ç³»å»¶æ—¶è®¡ç®—,å¦‚æœè¡€ç¼˜å…³ç³»è¾ƒé•¿,å¯ä»¥é€šè¿‡æŒä¹…åŒ–RDDæ¥åˆ‡æ–­è¡€ç¼˜å…³ç³».

###### 1.3.1.3.1 å¼¹æ€§
- å­˜å‚¨å¼¹æ€§ : å†…å­˜ä¸ç£ç›˜çš„è‡ªåŠ¨åˆ‡æ¢.
- å®¹é”™å¼¹æ€§ : æ•°æ®ä¸¢å¤±å¯ä»¥è‡ªåŠ¨æ¢å¤.
- è®¡ç®—å¼¹æ€§ : è®¡ç®—å‡ºé”™é‡è¯•æœºåˆ¶.
- åˆ†ç‰‡å¼¹æ€§ : å¯æ ¹æ®éœ€è¦é‡æ–°åˆ†ç‰‡.


###### 1.3.1.3.2 åˆ†åŒº
> RDDé€»è¾‘ä¸Šæ˜¯åˆ†åŒºçš„,æ¯ä¸ªåˆ†åŒºæ•°æ®æ˜¯æŠ½è±¡å­˜åœ¨çš„,è®¡ç®—æ—¶ä¼šé€šè¿‡ä¸€ä¸ªcomputeå‡½æ•°å¾—åˆ°æ¯ä¸ªåˆ†åŒºæ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å·²æœ‰æ–‡ä»¶ç³»ç»Ÿæ„å»º,åˆ™computeå‡½æ•°æ˜¯è¯»å–æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿä¸­æ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å…¶ä»–RDDè½¬æ¢è€Œæ¥,åˆ™computeå‡½æ•°æ˜¯æ‰§è¡Œè½¬æ¢é€»è¾‘å°†å…¶ä»–RDDæ•°æ®è¿›è¡Œè½¬æ¢.

###### 1.3.1.3.3 åªè¯»
> RDDæ˜¯åªè¯»çš„,è¦æƒ³æ”¹å˜RDDä¸­æ•°æ®,åªèƒ½åœ¨ç°æœ‰RDDåŸºç¡€ä¸Šåˆ›å»ºæ–°çš„RDD.
> 
> ç”±ä¸€ä¸ªRDDè½¬æ¢åˆ°å¦ä¸€ä¸ªRDD,å¯ä»¥é€šè¿‡ä¸°å¯Œçš„æ“ä½œç®—å­å®ç°,ä¸å†åƒMapReduceé‚£æ ·åªèƒ½å†™mapå’Œreduce.
> 
> RDDæ“ä½œç®—å­åŒ…æ‹¬ä¸¤ç±»,ä¸€ç±»æ˜¯`transformations`,å®ƒæ˜¯ç”¨æ¥å°†RDDè¿›è¡Œè½¬åŒ–,æ„å»ºRDDçš„è¡€ç¼˜å…³ç³»,å¦ä¸€ç±»æ˜¯`actions`,å®ƒæ˜¯ç”¨æ¥è§¦å‘RDDè®¡ç®—å¾—åˆ°RDDç›¸å…³è®¡ç®—ç»“æœæˆ–è€…å°†RDDä¿å­˜æ–‡ä»¶ç³»ç»Ÿä¸­.

###### 1.3.1.3.4 ä¾èµ–
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_012.jpg)

> å¦‚å›¾æ‰€ç¤º,RDDsé€šè¿‡æ“ä½œç®—å­è¿›è¡Œè½¬æ¢,è½¬æ¢å¾—åˆ°æ–°RDDåŒ…å«äº†ä»å…¶ä»–RDDsè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´ç»´æŠ¤ç€è¿™ç§è¡€ç¼˜å…³ç³»,ä¹Ÿç§°ä¹‹ä¸ºä¾èµ–.
> 
> ä¾èµ–åŒ…æ‹¬ä¸¤ç§,ä¸€ç§æ˜¯çª„ä¾èµ–,RDDsä¹‹é—´åˆ†åŒºæ˜¯ä¸€ä¸€å¯¹åº”,å¦ä¸€ç§æ˜¯å®½ä¾èµ–,ä¸‹æ¸¸RDDçš„æ¯ä¸ªåˆ†åŒºä¸ä¸Šæ¸¸RDD(ä¹Ÿç§°ä¹‹ä¸ºçˆ¶RDD)çš„æ¯ä¸ªåˆ†åŒºéƒ½æœ‰å…³,æ˜¯å¤šå¯¹å¤šå…³ç³».

###### 1.3.1.3.5 ç¼“å­˜
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_013.jpg)

> å¦‚æœåœ¨åº”ç”¨ç¨‹åºä¸­å¤šæ¬¡ä½¿ç”¨åŒä¸€ä¸ªRDDæ—¶,å¯ä»¥å°†è¯¥RDDç¼“å­˜èµ·æ¥,è¯¥RDDåªæœ‰åœ¨ç¬¬ä¸€æ¬¡è®¡ç®—æ—¶ä¼šæ ¹æ®è¡€ç¼˜å…³ç³»å¾—åˆ°åˆ†åŒºæ•°æ®,åœ¨åç»­å…¶ä»–åœ°æ–¹ç”¨åˆ°è¯¥RDDæ—¶,ä¼šç›´æ¥ä»ç¼“å­˜å¤„å–è€Œä¸ç”¨å†æ ¹æ®è¡€ç¼˜å…³ç³»è®¡ç®—,è¿™æ ·å°±åŠ é€ŸåæœŸçš„é‡ç”¨.
> 
> å¦‚å›¾æ‰€ç¤º,RDD-1ç»è¿‡ä¸€ç³»åˆ—è½¬æ¢åå¾—åˆ°RDD-nå¹¶ä¿å­˜åˆ°HDFS,RDD-1åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ä¼šæœ‰ä¸ªä¸­é—´ç»“æœ,å¦‚æœå°†å…¶ç¼“å­˜åˆ°å†…å­˜,é‚£ä¹ˆåœ¨éšåRDD-1è½¬æ¢åˆ°RDD-mè¿™ä¸€è¿‡ç¨‹ä¸­,å°±ä¸ä¼šè®¡ç®—å…¶ä¹‹å‰çš„RDD-0.

###### 1.3.1.3.6 CheckPoint
> è™½ç„¶RDDè¡€ç¼˜å…³ç³»å¤©ç„¶åœ°å¯ä»¥å®ç°å®¹é”™,å½“RDDæŸä¸ªåˆ†åŒºæ•°æ®å¤±è´¥æˆ–ä¸¢å¤±,å¯ä»¥é€šè¿‡è¡€ç¼˜å…³ç³»é‡å»º,ä½†æ˜¯å¯¹äºé•¿æ—¶é—´è¿­ä»£å‹åº”ç”¨æ¥è¯´éšç€è¿­ä»£è¿›è¡Œ,RDDsä¹‹é—´è¡€ç¼˜å…³ç³»ä¼šè¶Šæ¥è¶Šé•¿,ä¸€æ—¦åœ¨åç»­è¿­ä»£è¿‡ç¨‹ä¸­å‡ºé”™,åˆ™éœ€è¦é€šè¿‡éå¸¸é•¿çš„è¡€ç¼˜å…³ç³»å»é‡å»º,åŠ¿å¿…å½±å“æ€§èƒ½.
> 
> ä¸ºæ­¤,RDDæ”¯æŒcheckpointå°†æ•°æ®ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ä¸­,è¿™æ ·å°±å¯ä»¥åˆ‡æ–­ä¹‹å‰è¡€ç¼˜å…³ç³»,å› ä¸ºcheckpointåçš„RDDä¸éœ€è¦çŸ¥é“å®ƒçš„çˆ¶RDDs,å®ƒå¯ä»¥ä»checkpointå¤„æ‹¿åˆ°æ•°æ®.


#### 1.3.2 RDD ç¼–ç¨‹
##### 1.3.2.1 ç¼–ç¨‹æ¨¡å‹
> åœ¨Sparkä¸­,RDDè¢«è¡¨ç¤ºä¸ºå¯¹è±¡,é€šè¿‡å¯¹è±¡æ–¹æ³•è°ƒç”¨RDDè¿›è¡Œè½¬æ¢,ç»è¿‡ä¸€ç³»åˆ—çš„`transformations`å®šä¹‰RDDä¹‹å,å°±å¯ä»¥è°ƒç”¨`actions`è§¦å‘RDDè®¡ç®—,`action`å¯ä»¥æ˜¯å‘åº”ç”¨ç¨‹åºè¿”å›ç»“æœ(count,collectç­‰),æˆ–è€…æ˜¯å‘å­˜å‚¨ç³»ç»Ÿä¿å­˜æ•°æ®(saveAsTextFileç­‰).
> åœ¨Sparkä¸­,åªæœ‰é‡åˆ°`action`æ‰ä¼šæ‰§è¡ŒRDDè®¡ç®—(å³å»¶è¿Ÿè®¡ç®—),è¿™æ ·åœ¨è¿è¡Œæ—¶å¯ä»¥é€šè¿‡ç®¡é“æ–¹å¼ä¼ è¾“å¤šä¸ªè½¬æ¢.
> ä½¿ç”¨Sparkå¼€å‘è€…éœ€è¦ç¼–å†™ä¸€ä¸ªDriverç¨‹åº,å®ƒè¢«æäº¤åˆ°é›†ç¾¤ä»¥è°ƒåº¦è¿è¡ŒWorker,Driverä¸­å®šä¹‰äº†ä¸€ä¸ªæˆ–å¤šä¸ªRDD.å¹¶è°ƒç”¨RDDä¸Šçš„action.Workeråˆ™æ‰§è¡ŒRDDåˆ†åŒºè®¡ç®—ä»»åŠ¡.
##### 1.3.2.2 RDD åˆ›å»º
- Sparkåˆ›å»ºRDDåˆ›å»ºæ–¹å¼å¯ä»¥åˆ†ä¸ºä¸‰ç§:
- 1.ä»é›†åˆä¸­åˆ›å»ºRDD
- 2.ä»å¤–éƒ¨å­˜å‚¨åˆ›å»ºRDD
- 3.ä»å…¶ä»–RDDåˆ›å»º
###### 1.3.2.1 é›†åˆåˆ›å»ºRDD
- ä»é›†åˆä¸­åˆ›å»ºRDD,Sparkä¸»è¦æä¾›äº†ä¸¤ç§å‡½æ•° : `parallelize`å’Œ`makeRDD`
- 1.ä½¿ç”¨`parallelize()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- 2.ä½¿ç”¨`makeRDD()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val makerdd = sc.makeRDD(Array(511,611,711))
makerdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:24
scala> makerdd.collect
res1: Array[Int] = Array(511, 611, 711)
scala> 
```
###### 1.3.2.2 å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿæ•°æ®é›†åˆ›å»ºRDD
- é™¤äº†åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ,è¿˜æœ‰æ‰€æœ‰Hadoopæ”¯æŒæ•°æ®é›†,æ¯”å¦‚HDFS/Cassandra/HBaseç­‰.
- è¯¦è§ 1.3.4 æ•°æ®è¯»å–ä¿å­˜
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt")
res2: org.apache.spark.rdd.RDD[String] = /opt/module/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[3] at textFile at <console>:25
scala> 
```

###### 1.3.2.3 ä»å…¶ä»–åˆ›å»ºRDD
- è¯¦è§1.3.2.3 RDD è½¬æ¢


##### 1.3.2.3 RDD è½¬æ¢
- RDDæ•´ä½“åˆ†ä¸º`Value`ç±»å‹å’Œ`Key-Value`ç±»å‹

##### 1.3.2.3.1 Value ç±»å‹
###### 1.3.2.3.1.1 `map(func)` æ¡ˆä¾‹
- ä½œç”¨ : è¿”å›ä¸€ä¸ªæ–°RDD,è¯¥RDDç”±æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ç»è¿‡funcå‡½æ•°è½¬æ¢åç»„æˆ.
- åˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- æ‰“å°RDDæœ€ç»ˆç»“æœ
```
scala> rdd.map((_,1)).collect
res4: Array[(Int, Int)] = Array((511,1), (611,1), (711,1))
scala> 
```
- å°†æ‰€æœ‰å…ƒç´ RDD*2,æœ€ç»ˆç»“æœ
```
scala> rdd.map((_*2)).collect
res5: Array[Int] = Array(1022, 1222, 1422)
scala> 
```
###### 1.3.2.3.1.2 `mapPartitions(func)` æ¡ˆä¾‹
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†ç‹¬ç«‹åœ°åœ¨RDDæ¯ä¸€ä¸ªåˆ†ç‰‡ä¸Šè¿è¡Œ,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcå‡½æ•°ç±»å‹å¿…é¡»æ˜¯Iterator[T] => Iterator[U]
- å‡è®¾æœ‰Nä¸ªå…ƒç´ ,æœ‰Mä¸ªåˆ†åŒº,é‚£ä¹ˆmapå‡½æ•°å°†è¢«è°ƒç”¨Næ¬¡,è€ŒmapPartitionsè¢«è°ƒç”¨Mæ¬¡,ä¸€ä¸ªå‡½æ•°ä¸€æ¬¡å¤„ç†æ‰€æœ‰åˆ†åŒº.
```
scala> rdd.mapPartitions(_.map(_*2)).collect
res11: Array[Int] = Array(1022, 1222, 1422)
scala> 
```

###### 1.3.2.3.1.3 `mapPartitionsWithIndex(func)` æ¡ˆä¾‹
- ä½œç”¨ : ç±»ä¼¼äºmapPartitions,ä½†funcå¸¦æœ‰ä¸€ä¸ªæ•´æ•°å‚æ•°è¡¨ç¤ºåˆ†ç‰‡ç´¢å¼•å€¼,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcçš„å‡½æ•°ç±»å‹å¿…é¡»æ˜¯(Int, Interator[T]) => Iterator[U];
```
scala> rdd.mapPartitionsWithIndex((index,items)=>(items.map((index,_)))).collect
res13: Array[(Int, Int)] = Array((1,511), (2,611), (3,711))
scala> 
```

###### 1.3.2.3.1.4 `flatMap(func)` æ¡ˆä¾‹
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ å¯ä»¥è¢«æ˜ å°„ä¸º0æˆ–å¤šä¸ªè¾“å‡ºå…ƒç´ (æ‰€ä»¥funcåº”è¯¥è¿”å›ä¸€ä¸ªåºåˆ—,è€Œä¸æ˜¯å•ä¸€å…ƒç´ )
```
scala> val text = sc.textFile("/core_flow/spark/input/wordcount/wordcount_001.txt")
text: org.apache.spark.rdd.RDD[String] = /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[15] at textFile at <console>:24
scala> text.flatMap(_.split(" ")).collect
res16: Array[String] = Array(hadoop, spark, hive, hadoop, spark, hadoop, hbase, flume, hive, scala, java, oozie)
scala> 
```

###### 1.3.2.3.1.5 `map()`ä¸`mapPartition()`åŒºåˆ«
- 1.map() : æ¯æ¬¡å¤„ç†ä¸€æ¡æ•°æ®
- 2.mapPartition() : æ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®,è¿™ä¸ªåˆ†åŒºçš„æ•°æ®å¤„ç†å®Œå,åŸRDDä¸­åˆ†åŒºçš„æ•°æ®æ‰èƒ½é‡Šæ”¾,å¯èƒ½å¯¼è‡´OOM.
- 3.å¼€å‘æŒ‡å¯¼ : å½“å†…å­˜ç©ºé—´è¾ƒå¤§çš„æ—¶å€™å»ºè®®ä½¿ç”¨mapPartition(),ä»¥æé«˜å¤„ç†æ•ˆç‡.

###### 1.3.2.3.1.6 `glom` æ¡ˆä¾‹
- ä½œç”¨ : å°†æ¯ä¸€ä¸ªåˆ†åŒºå½¢æˆä¸€ä¸ªæ•°ç»„,å½¢æˆæ–°çš„RDDç±»å‹æ—¶RDD[Array[T]]
```
scala> rdd.glom.collect
res17: Array[Array[Int]] = Array(Array(), Array(511), Array(611), Array(711))   
scala> 
```

###### 1.3.2.3.1.7 `groupBy(func)` æ¡ˆä¾‹
- ä½œç”¨ : åˆ†ç»„æŒ‰ç…§ä¼ å…¥å‡½æ•°çš„è¿”å›å€¼è¿›è¡Œåˆ†ç»„,å°†ç›¸åŒçš„keyå¯¹åº”çš„å€¼æ”¾å…¥ä¸€ä¸ªè¿­ä»£å™¨.
```
scala> rdd.groupBy(_ % 2).collect
res18: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(611, 711, 511)))    
scala> 
```

###### 1.3.2.3.1.8 `filter(func)` æ¡ˆä¾‹
- ä½œç”¨ : è¿‡æ»¤è¿”å›ä¸€ä¸ªæ–°çš„RDD,è¯¥RDDç”±ç»è¿‡funcå‡½æ•°è®¡ç®—åè¿”å›å€¼ä¸ºtrueçš„è¾“å…¥å…ƒç´ ç»„æˆ.
```
scala> rdd.filter(_%3==0).collect
res20: Array[Int] = Array(711)
scala> 
```

###### 1.3.2.3.1.9 `sample(withReplacement,fraction,seed)` æ¡ˆä¾‹
- ä½œç”¨ : ä»¥æŒ‡å®šéšæœºç§å­éšæœºæŠ½æ ·å‡ºæ•°é‡ä¸ºfractionçš„æ•°æ®,withReplacementè¡¨ç¤ºæ˜¯æŠ½å‡ºçš„æ•°æ®æ˜¯å¦æ”¾å›,trueä¸ºæœ‰æ”¾å›çš„æŠ½æ ·,falseä¸ºæ— æ”¾å›çš„æŠ½æ ·,seedç”¨äºæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­.
```
scala> val rdd = sc.parallelize(1 to 100)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at parallelize at <console>:24
scala> rdd.sample(false,0.1,3).collect
res22: Array[Int] = Array(1, 33, 37, 50, 59, 69, 75, 78, 85, 98) 
scala> 
```

###### 1.3.2.3.1.10 `distinct([numTasks]))` æ¡ˆä¾‹
- ä½œç”¨ : å¯¹æºRDDè¿›è¡Œå»é‡åè¿”å›ä¸€ä¸ªæ–°çš„RDD,é»˜è®¤æƒ…å†µä¸‹,åªæœ‰8ä¸ªå¹¶è¡Œä»»åŠ¡æ¥æ“ä½œ,ä½†æ˜¯å¯ä»¥ä¼ å…¥ä¸€ä¸ªå¯é€‰çš„numTaskså‚æ•°æ”¹å˜å®ƒ.
- ä½¿ç”¨distinct()å¯¹å…¶å»é‡æ“ä½œ.
```
scala> rdd.distinct(4).collect
res23: Array[Int] = Array(84, 100, 96, 52, 56, 4, 76, 16, 28, 80, 48, 32, 36, 24, 64, 92, 40, 72, 8, 12, 20, 60, 44, 88, 68, 13, 41, 61, 81, 21, 77, 53, 97, 25, 29, 65, 73, 57, 93, 33, 37, 45, 1, 89, 17, 69, 9, 85, 49, 5, 34, 82, 66, 22, 54, 98, 46, 30, 14, 50, 62, 42, 74, 90, 6, 70, 18, 38, 86, 58, 78, 26, 94, 10, 2, 19, 39, 15, 47, 71, 55, 95, 79, 59, 11, 35, 27, 75, 51, 23, 63, 83, 67, 3, 7, 91, 31, 87, 43, 99)
scala> 
```
###### 1.3.2.3.1.11 `coalesce(numPartitions)` æ¡ˆä¾‹
- ä½œç”¨ : ç¼©å‡åˆ†åŒºæ•°,ç”¨äºå¤§æ•°æ®é›†è¿‡æ»¤å,æé«˜å°æ•°æ®é›†çš„æ‰§è¡Œæ•ˆç‡.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶ç¼©å‡åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at <console>:24

scala> rdd.partitions.size
res24: Int = 4

scala> val coalesceRDD = rdd.coalesce(3)
coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[28] at coalesce at <console>:26

scala> coalesceRDD.partitions.size
res25: Int = 3
scala> 
```
###### 1.3.2.3.1.12 `repartition(numPartitions)` æ¡ˆä¾‹
- ä½œç”¨ : æ ¹æ®åˆ†åŒºæ•°,é‡æ–°é€šè¿‡ç½‘ç»œéšæœºæ´—ç‰Œæ‰€æœ‰æ•°æ®.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶é‡æ–°åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:24

scala> rdd.partitions.size
res26: Int = 4

scala> val rerdd = rdd.repartition(2)
rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at repartition at <console>:26

scala> rerdd.partitions.size
res27: Int = 2
scala> 
```

###### 1.3.2.3.1.13 `coalesce`ä¸`repartition`åŒºåˆ«
> 1.`coalesce`é‡æ–°åˆ†åŒº,å¯ä»¥é€‰æ‹©æ˜¯å¦è¿›è¡Œshuffleè¿‡ç¨‹,ç”±å‚æ•°`shuffle: Boolean = false/true`å†³å®š.
> 
> 2.`repartition`å®é™…ä¸Šæ˜¯è°ƒç”¨coalesce,è¿›è¡Œshuffleè¿‡ç¨‹,æºç æ¼”ç¤º:
``` scala
def repartition(numpartitions: int)(implicit ord: ordering[t] = null): rdd[t] = withscope {
coalesce(numpartitions, shuffle = true)
}
```
###### 1.3.2.3.1.14 `sortBy(func,[ascending],[numTasks])` æ¡ˆä¾‹
- ä½œç”¨ : ä½¿ç”¨funcå…ˆå¯¹æ•°æ®è¿›è¡Œå¤„ç†,æŒ‰ç…§å¤„ç†åçš„æ•°æ®æ¯”è¾ƒç»“æœæ’åº,é»˜è®¤ä¸ºæ­£åº.
- åˆ›å»ºRDD,æŒ‰ç…§ä¸åŒè§„åˆ™è¿›è¡Œæ’åº | æŒ‰ç…§è‡ªèº«å¤§å°æ’åº / æŒ‰ç…§ä¸3ä½™æ•°å¤§å°æ’åº / æŒ‰ç…§å€’åºæ’åº
```
scala>  val rdd = sc.parallelize(List(2,1,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at <console>:24

scala> rdd.sortBy(x => x).collect()
res29: Array[Int] = Array(1, 2, 3, 4)

scala> rdd.sortBy(x => x%3).collect()
res30: Array[Int] = Array(3, 1, 4, 2)

scala> rdd.sortBy(x => x,false).collect()
res31: Array[Int] = Array(4, 3, 2, 1)

scala> 
```

###### 1.3.2.3.1.15 `pipe(command,[envVars])` æ¡ˆä¾‹
- ä½œç”¨ : ç®¡é“é’ˆå¯¹æ¯ä¸ªåˆ†åŒº,éƒ½æ‰§è¡Œä¸€ä¸ªshellè„šæœ¬,è¿”å›è¾“å‡ºRDD.
- åˆ›å»ºè„šæœ¬,ä½¿ç”¨ç®¡é“å°†è„šæœ¬ä½œç”¨äºRDDä¸Š
```
[root@systemhub511 ~]# vim /opt/module/spark/input/pipe.sh
[root@systemhub511 ~]# chmod 777 /opt/module/spark/input/pipe.sh
```
- vim `pipe.sh`
``` powershell
#!/bin/
shecho"Start"
while read LINE;do
	echo ">>>" ${LINE}
done
```

##### 1.3.2.3.2 åŒValueç±»å‹äº¤äº’

##### 1.3.2.3.3 Key-Value ç±»å‹


##### 1.3.2.4 Action



#### 1.3.3 Key-Value RDD æ•°æ®åˆ†åŒº
#### 1.3.4 æ•°æ®è¯»å–ä¿å­˜
#### 1.3.5 RDD ç¼–ç¨‹è¿›é˜¶






### ğŸ”¥ 1.4 Spark SQL ğŸ”¥
#### 1.4.1 Spark SQL æ¦‚è¿°
#### 1.4.2 Spark SQL æŸ¥è¯¢
#### 1.4.3 DataFrame
#### 1.4.4 DataSet
#### 1.4.5 èšåˆå‡½æ•°
#### 1.4.6 Spark SQL æ•°æ®æº
#### 1.4.7 OLAP Server
#### 1.4.8 Spark SQL å®ä¾‹


### ğŸ”¥ 1.5 Spark Streaming ğŸ”¥
#### 1.5.1 Spark Streaming æ¦‚è¿°
#### 1.5.2 Spark Streaming Program
#### 1.5.3 DataStream æ¦‚è¿°
#### 1.5.4 DataStream è¾“å…¥
#### 1.5.5 DataStream è½¬æ¢
#### 1.5.6 DataStream è¾“å‡º
#### 1.5.7 7*24hourè¿è¡Œ
#### 1.5.8 Spark Streaming å®ä¾‹



## ğŸ”¥ 2. Spark é«˜é˜¶ ğŸ”¥
### 2.1 å†…æ ¸æœºåˆ¶
### 2.1 æ€§èƒ½è°ƒä¼˜







## 3. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------