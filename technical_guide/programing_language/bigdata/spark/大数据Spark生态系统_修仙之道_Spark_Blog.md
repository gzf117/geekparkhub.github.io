,# å¤§æ•°æ®Sparkç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Spark Blog

@(2019-05-15)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Spark|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

## ğŸ˜ Spark Technology ä¿®ä»™ä¹‹é“ é‡‘ä»™é“æœ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/spark.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------


[TOC]


## ğŸ”¥ 1. Spark åŸºç¡€ ğŸ”¥

### 1.1 Spark æ¦‚è¿°
- Sparkæ˜¯ä¸€ç§åŸºäºå†…å­˜å¿«é€Ÿ / é€šç”¨ / å¯æ‰©å±•å¤§æ•°æ®åˆ†æå¼•æ“.
- Sparkåœ¨2009å¹´è¯ç”Ÿäº(UC Berkeley AMP Lab)åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡AMPå®éªŒå®¤,Sparkæ˜¯ä½¿ç”¨å†…å­˜è®¡ç®—çš„å¼€æºå¤§æ•°æ®å¹¶è¡Œè®¡ç®—æ¡†æ¶,å¯ä»¥åº”å¯¹å¤æ‚çš„å¤§æ•°æ®å¤„ç†åœºæ™¯,2013å¹´Sparkæˆä¸ºApacheåŸºé‡‘ä¼šæ——ä¸‹é¡¶çº§é¡¹ç›®.
- Sparkå†…æ ¸æ˜¯ç”±Scalaç¼–ç¨‹è¯­è¨€å¼€å‘,åŒæ—¶ä¹Ÿæä¾›äº†Java/Python/Rè¯­è¨€ç­‰å¼€å‘ç¼–ç¨‹æ¥å£.


#### 1.1.1 Spark æ¨¡å—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_001.jpg)
- 1.`Spark Core` : å®ç°äº†SparkåŸºæœ¬åŠŸèƒ½,åŒ…å«ä»»åŠ¡è°ƒåº¦ / å†…å­˜ç®¡ç† / é”™è¯¯æ¢å¤ / ä¸å­˜å‚¨ç³»ç»Ÿäº¤äº’ç­‰æ¨¡å—,Spark Coreä¸­è¿˜åŒ…å«äº†å¯¹å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed DataSet,ç®€ç§°RDD)APIå®šä¹‰.
- 2.`Spark SQL` : æ˜¯Sparkç”¨æ¥æ“ä½œç»“æ„åŒ–æ•°æ®ç¨‹åºåŒ…,é€šè¿‡Spark SQL,å¯ä»¥ä½¿ç”¨SQLæˆ–è€…Apache Hiveç‰ˆæœ¬çš„SQLæ–¹è¨€(HQL)æ¥æŸ¥è¯¢æ•°æ®,Spark SQLæ”¯æŒå¤šç§æ•°æ®æº,æ¯”å¦‚Hiveè¡¨ã€Parquetä»¥åŠJSONç­‰.
- 3.`Spark Streaming` : æ˜¯Sparkæä¾›å¯¹å®æ—¶æ•°æ®è¿›è¡Œæµå¼è®¡ç®—çš„ç»„ä»¶,æä¾›äº†ç”¨æ¥æ“ä½œæ•°æ®æµçš„API,å¹¶ä¸”ä¸Spark Coreä¸­çš„RDD APIé«˜åº¦å¯¹åº”.
- 4.`Spark MLlib` : æä¾›å¸¸è§çš„æœºå™¨å­¦ä¹ (ML)åŠŸèƒ½ç¨‹åºåº“,åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰,è¿˜æä¾›äº†æ¨¡å‹è¯„ä¼°ã€æ•°æ®å¯¼å…¥ç­‰é¢å¤–æ”¯æŒåŠŸèƒ½.
- 5.`é›†ç¾¤ç®¡ç†å™¨` : Sparkè®¾è®¡ä¸ºå¯ä»¥é«˜æ•ˆåœ°åœ¨ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹åˆ°æ•°åƒä¸ªè®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä¼¸ç¼©è®¡ç®—,ä¸ºäº†å®ç°è¿™æ ·è¦æ±‚,åŒæ—¶è·å¾—æœ€å¤§çµæ´»æ€§,Sparkæ”¯æŒåœ¨å„ç§é›†ç¾¤ç®¡ç†å™¨(Cluster Manager)ä¸Šè¿è¡Œ,åŒ…æ‹¬Hadoop YARNã€ApacheMesos,ä»¥åŠSparkè‡ªå¸¦ç®€æ˜“è°ƒåº¦å™¨,å«ä½œç‹¬ç«‹è°ƒåº¦å™¨.

#### 1.1.2 Spark ç‰¹ç‚¹
- 1.`å¿«é€Ÿ` : ä¸Hadoop MapReduceç›¸æ¯”,SparkåŸºäºå†…å­˜è¿ç®—è¦å¿«100å€ä»¥ä¸Š,åŸºäºç¡¬ç›˜è¿ç®—ä¹Ÿè¦å¿«10å€ä»¥ä¸Š,Sparkå®ç°äº†é«˜æ•ˆDAGæœ‰å‘æ— ç¯å›¾æ‰§è¡Œå¼•æ“,å¯ä»¥é€šè¿‡åŸºäºå†…å­˜æ¥é«˜æ•ˆå¤„ç†æ•°æ®æµ,è®¡ç®—ä¸­é—´ç»“æœæ˜¯å­˜åœ¨äºå†…å­˜ä¸­.
- 2.`æ˜“ç”¨` : Sparkæ”¯æŒJavaã€Pythonå’ŒScalaçš„API,è¿˜æ”¯æŒè¶…è¿‡80ç§é«˜çº§ç®—æ³•,ä½¿å¼€å‘è€…å¯ä»¥å¿«é€Ÿæ„å»ºä¸åŒåº”ç”¨,è€Œä¸”Sparkæ”¯æŒäº¤äº’å¼çš„Pythonå’ŒScalaçš„Shell,å¯ä»¥éå¸¸æ–¹ä¾¿åœ°åœ¨Shellä¸­ä½¿ç”¨Sparké›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜æ–¹æ³•.
- 3.`é€šç”¨æ€§å¼º` : Sparkæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ,Sparkå¯ä»¥ç”¨äºæ‰¹å¤„ç† / äº¤äº’å¼æŸ¥è¯¢(SparkSQL) / å®æ—¶æµå¤„ç†(SparkStreaming) / æœºå™¨å­¦ä¹ (SparkMLlib) / å›¾è®¡ç®—(GraphX),è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨,å‡å°‘äº†å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬.
- 4.`å…¼å®¹æ€§` : Sparkå¯ä»¥éå¸¸æ–¹ä¾¿åœ°ä¸å…¶ä»–çš„å¼€æºäº§å“è¿›è¡Œèåˆ,æ¯”å¦‚Sparkå¯ä»¥ä½¿ç”¨Hadoop YARNå’ŒApacheMesosä½œä¸ºèµ„æºç®¡ç†å’Œè°ƒåº¦å™¨,å¹¶ä¸”å¯ä»¥å¤„ç†æ‰€æœ‰Hadoopæ”¯æŒçš„æ•°æ®,åŒ…æ‹¬HDFSã€HBaseç­‰,è¿™å¯¹äºå·²ç»éƒ¨ç½²Hadoopé›†ç¾¤çš„ç”¨æˆ·ç‰¹åˆ«é‡è¦,å› ä¸ºä¸éœ€è¦åšä»»ä½•æ•°æ®è¿ç§»å°±å¯ä»¥ä½¿ç”¨Sparkå¼ºå¤§å¤„ç†èƒ½åŠ›.

#### 1.1.3 Spark åº”ç”¨åœºæ™¯
- 1.Sparkå…·æœ‰ä¸°å¯Œç»„ä»¶,å¯é€‚ç”¨äºå¤šç§å¤æ‚åº”ç”¨åœºæ™¯,å¦‚SQLæŸ¥è¯¢/æœºå™¨å­¦ä¹ /å›¾å½¢è®¡ç®—/æµå¼è®¡ç®—ç­‰,åŒæ—¶Sparkå¯ä»¥ä¸Hadoopå¾ˆå¥½åœ°é›†æˆåœ¨ä¸€èµ·,ç›®å‰å·²ç»æœ‰éƒ¨åˆ†ä¸»æµå¤§æ•°æ®å‚å•†åœ¨å‘è¡Œç‰ˆHadoopç‰ˆæœ¬ä¸­åŒ…å«Spark/Cloudera/Hortonworks/MapReduceç­‰.
- 2.Sparkå¾—åˆ°äº†ä¼—å¤šå¤§æ•°æ®å…¬å¸çš„æ”¯æŒ,è¿™äº›å…¬å¸åŒ…æ‹¬Hortonworksã€IBMã€Intelã€Clouderaã€MapRã€Pivotalã€ç™¾åº¦ã€é˜¿é‡Œã€è…¾è®¯ã€äº¬ä¸œã€æºç¨‹ã€ä¼˜é…·åœŸè±†,å½“å‰ç™¾åº¦çš„Sparkå·²åº”ç”¨äºå¤§æœç´¢ã€ç›´è¾¾å·ã€ç™¾åº¦å¤§æ•°æ®ç­‰ä¸šåŠ¡,é˜¿é‡Œåˆ©ç”¨GraphXæ„å»ºäº†å¤§è§„æ¨¡å›¾è®¡ç®—å’Œå›¾æŒ–æ˜ç³»ç»Ÿ,å®ç°äº†å¾ˆå¤šç”Ÿäº§ç³»ç»Ÿçš„æ¨èç®—æ³•,è…¾è®¯Sparké›†ç¾¤è¾¾åˆ°8000å°è§„æ¨¡,æ˜¯å½“å‰å·²çŸ¥ä¸–ç•Œä¸Šæœ€å¤§çš„Sparké›†ç¾¤.


### 1.2 Spark éƒ¨ç½²
- Sparkå®˜æ–¹åœ°å€ : [spark.apache.org](http://spark.apache.org)
- Sparkå®˜æ–¹ä¸‹è½½ : [spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
- Sparkå®˜æ–¹æ–‡æ¡£ : [spark.apache.org/docs/2.1.1/](https://spark.apache.org/docs/2.1.1/)

è§£å‹`spark-2.1.1-bin-hadoop2.7.tgz`
```
[root@systemhub511 software]# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/
```
é‡å‘½å`spark-2.1.1-bin-hadoop2.7`
```
[root@systemhub511 module]# mv spark-2.1.1-bin-hadoop2.7/ spark
```

### 1.3 Spark è¿è¡Œæ¨¡å¼
#### ğŸ’¥ 1.3.1 Loacl Mode ğŸ’¥
##### 1.3.1.1 Loacl Mode æ¦‚è¿°
- Localæ¨¡å¼å°±æ˜¯è¿è¡Œåœ¨å•å°æœ¬åœ°è®¡ç®—æœºæ¨¡å¼,é€šå¸¸å°±æ˜¯ç”¨äºåœ¨æœ¬åœ°ä¸Šç»ƒæ‰‹æˆ–æµ‹è¯•,å®ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹é›†ä¸­æ–¹å¼è®¾ç½®Master.
- 1.`local` : æ‰€æœ‰è®¡ç®—éƒ½è¿è¡Œåœ¨ä¸€ä¸ªçº¿ç¨‹å½“ä¸­,æ²¡æœ‰ä»»ä½•å¹¶è¡Œè®¡ç®—,é€šå¸¸åœ¨æœ¬æœºæ‰§è¡Œæµ‹è¯•ä»£ç å°±ç”¨è¿™ç§æ¨¡å¼.
- 2.`local[K]` : æŒ‡å®šä½¿ç”¨å¤šå°‘ä¸ªçº¿ç¨‹æ¥è¿è¡Œè®¡ç®—,æ¯”å¦‚`local[4]`å°±æ˜¯è¿è¡Œ4ä¸ªWorkerçº¿ç¨‹,é€šå¸¸Cpuæœ‰å‡ ä¸ªCore,å°±æŒ‡å®šå‡ ä¸ªçº¿ç¨‹,æœ€å¤§åŒ–åˆ©ç”¨Cpuè®¡ç®—èƒ½åŠ›.
- 3.`local[*]` : è¿™ç§æ¨¡å¼ç›´æ¥æŒ‰ç…§Cpuæœ€å¤šCoresæ¥è®¾ç½®çº¿ç¨‹æ•°é‡.

##### 1.3.1.2 (æ±‚Ï€) & (WordCount) & (æœ¬åœ°è°ƒè¯•) å®˜æ–¹æ¡ˆä¾‹
- 1.åŸºæœ¬è¯­æ³•
```
bin/spark-submit \
--class <main-class>
--master <master-url> \
--deploy-mode <deploy-mode> \
--conf <key>=<value> \
... # other options
<application-jar> \
[application-arguments]
```
- 2.å‚æ•°è¯´æ˜
- `--master`: æŒ‡å®šMasteråœ°å€,é»˜è®¤ä¸ºLocal.
- `--class`: åº”ç”¨ä¸»å¯åŠ¨ç±»(å¦‚org.apache.spark.examples.SparkPi).
- `--deploy-mode` : æ˜¯å¦å‘å¸ƒé©±åŠ¨åˆ°workerèŠ‚ç‚¹(cluster)æˆ–è€…ä½œä¸ºä¸€ä¸ªæœ¬åœ°å®¢æˆ·ç«¯(client)(default: client)*
- `--conf` : ä»»æ„Sparké…ç½®å±æ€§,æ ¼å¼`key=value`,å¦‚æœå€¼åŒ…å«ç©ºæ ¼,å¯ä»¥åŠ å¼•å·`"key=value"`
- `application-jar` : æ‰“åŒ…å¥½åº”ç”¨jar,åŒ…å«ä¾èµ–,URLåœ¨é›†ç¾¤ä¸­å…¨å±€å¯è§,æ¯”å¦‚`hdfs://`å…±äº«å­˜å‚¨ç³»ç»Ÿ,å¦‚æœæ˜¯`file://path`,é‚£ä¹ˆæ‰€æœ‰èŠ‚ç‚¹çš„pathéƒ½åŒ…å«åŒæ ·çš„jaråŒ….
- `application-arguments` : ä¼ ç»™main()æ–¹æ³•çš„å‚æ•°.
- `--executor-memory 1G` : æŒ‡å®šæ¯ä¸ªexecutorå¯ç”¨å†…å­˜ä¸º1G
- `--total-executor-cores 2` : æŒ‡å®šæ¯ä¸ªexecutorä½¿ç”¨cpuæ ¸æ•°ä¸º2ä¸ª

- 3.æ±‚Ï€ç¨‹åº
- 3.1 æ±‚Ï€æ‰§è¡Œè¯­å¥
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 3.2 å¼€å§‹æ‰§è¡Œä»»åŠ¡
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
```
- 3.3 æŸ¥çœ‹æ‰§è¡Œç»“æœ | è¯¥ç®—æ³•æ˜¯åˆ©ç”¨`è’™ç‰¹Â·å¡ç½—ç®—æ³•`æ±‚Ï€
```
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.059446 s
Pi is roughly 3.1411463141146316
```
- 3.4 å¯åŠ¨`spark-shell`
```
[root@systemhub511 spark]# bin/spark-shell
Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = local[*], app id = local-1558677071165).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
- 3.5 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:4040`

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_002.jpg)

- 4.è¿è¡ŒWordCountç¨‹åº
- 4.1 åœ¨sparkæ ¹ç›®å½•åˆ›å»ºwordcountç›®å½•
```
[root@systemhub511 spark]# mkdir -p input/wordcount
```
- 4.2 åœ¨wordcountç›®å½•åˆ›å»ºæ•°æ®æ–‡ä»¶ | vim `wordcount_001.txt`
```
[root@systemhub511 spark]# cd input/wordcount/
[root@systemhub511 wordcount]# vim wordcount_001.txt
```
```
hadoop spark hive
hadoop spark hadoop
hbase flume hive
scala java oozie
```
- 4.3 æ‰§è¡ŒWordCountå¹¶æŸ¥çœ‹æ‰“å°ç»“æœ
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (spark,2), (hive,2), (hadoop,3), (oozie,1), (flume,1), (java,1), (hbase,1))

scala> 
```
- 4.4 å°†WordCountæ‰§è¡Œç»“æœè¾“å‡ºè‡³æœ¬åœ°æ–‡ä»¶
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("./output/wordcount/")
```
- 4.5 æŸ¥çœ‹æ–‡ä»¶ç»“æœ
```
[root@systemhub511 spark]# cd output/wordcount/
[root@systemhub511 wordcount]# ll
total 4
-rw-r--r--. 1 root root 79 May 24 14:48 part-00000
-rw-r--r--. 1 root root  0 May 24 14:48 _SUCCESS
[root@systemhub511 wordcount]# cat part-00000 
(scala,1)
(spark,2)
(hive,2)
(hadoop,3)
(oozie,1)
(flume,1)
(java,1)
(hbase,1)
[root@systemhub511 wordcount]# 
```

##### 1.3.1.3 æäº¤æµç¨‹
- æäº¤ä»»åŠ¡åˆ†æ | Sparké€šç”¨è¿è¡Œç®€æ˜“æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_003.jpg)
- æäº¤ä»»åŠ¡è§’è‰² : Driver (é©±åŠ¨å™¨) & Executor (æ‰§è¡Œå™¨)
- `1. Driver (é©±åŠ¨å™¨)`
- Sparké©±åŠ¨å™¨æ˜¯æ‰§è¡Œå¼€å‘ç¨‹åºä¸­mainæ–¹æ³•è¿›ç¨‹,å®ƒè´Ÿè´£å¼€å‘äººå‘˜ç¼–å†™ç”¨æ¥åˆ›å»ºSparkContext / åˆ›å»ºRDD,ä»¥åŠè¿›è¡ŒRDDè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œä»£ç çš„æ‰§è¡Œ,å¦‚æœä½¿ç”¨spark shell,é‚£ä¹ˆå½“å¯åŠ¨Spark shellçš„æ—¶å€™,ç³»ç»Ÿåå°è‡ªå¯ä¸€ä¸ªSparké©±åŠ¨å™¨ç¨‹åº,å°±æ˜¯åœ¨Spark shellä¸­é¢„åŠ è½½ä¸€ä¸ªå«ä½œscçš„SparkContextå¯¹è±¡,å¦‚æœé©±åŠ¨å™¨ç¨‹åºç»ˆæ­¢,é‚£ä¹ˆSparkåº”ç”¨ä¹Ÿå°±ç»“æŸäº†.
- 1.1 Driverä¸»è¦è´Ÿè´£ : 1.å°†å¼€å‘è€…ç¨‹åºè½¬ä¸ºä»»åŠ¡. `->` 2.è·Ÿè¸ªExecutorè¿è¡ŒçŠ¶å†µ. `->` 3.ä¸ºæ‰§è¡Œå™¨èŠ‚ç‚¹è°ƒåº¦ä»»åŠ¡. `->` 4.WebUIå±•ç¤ºåº”ç”¨è¿è¡ŒçŠ¶å†µ.
- `2. Executor (æ‰§è¡Œå™¨)`
- Spark Executoræ˜¯ä¸€ä¸ªå·¥ä½œè¿›ç¨‹,è´Ÿè´£åœ¨Sparkä½œä¸šä¸­è¿è¡Œä»»åŠ¡,ä»»åŠ¡é—´ç›¸äº’ç‹¬ç«‹,Sparkåº”ç”¨å¯åŠ¨æ—¶,ExecutorèŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨,å¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ªSparkåº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨,å¦‚æœæœ‰ExecutorèŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒ,Sparkåº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œ,ä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»–ExecutorèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œ.
- 2.2 Executorä¸»è¦è´Ÿè´£ : 1.è´Ÿè´£è¿è¡Œç»„æˆSparkåº”ç”¨ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™é©±åŠ¨å™¨è¿›ç¨‹. `->` 2.é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨(Block Manager)ä¸ºå¼€å‘è€…ç¨‹åºä¸­è¦æ±‚ç¼“å­˜RDDæä¾›å†…å­˜å¼å­˜å‚¨,RDDæ˜¯ç›´æ¥ç¼“å­˜åœ¨Executorè¿›ç¨‹å†…,å› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—.

##### 1.3.1.4 æ•°æ®æµç¨‹

| å‚æ•°åˆ—è¡¨      |     å‚æ•°æè¿° |
| :--------: | :--------:|
| `textFile("input")`    |   è¯»å–æœ¬åœ°æ–‡ä»¶inputæ–‡ä»¶å¤¹æ•°æ® |
| `flatMap(_.split(" "))` | å‹å¹³æ“ä½œ,æŒ‰ç…§ç©ºæ ¼åˆ†å‰²ç¬¦å°†ä¸€è¡Œæ•°æ®æ˜ å°„æˆä¸€ä¸ªä¸ªå•è¯ |
| `map((_,1))` | å¯¹æ¯ä¸€ä¸ªå…ƒç´ æ“ä½œ,å°†å•è¯æ˜ å°„ä¸ºå…ƒç»„ |
| `reduceByKey(_+_)` | æŒ‰ç…§keyå°†å€¼è¿›è¡Œèšåˆç›¸åŠ  |
| `collect` | å°†æ•°æ®æ”¶é›†åˆ°Driverç«¯å±•ç¤º |

- WordCount ç¨‹åºåˆ†æ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_004.jpg)


#### ğŸ’¥ 1.3.2 Standalone Mode ğŸ’¥
##### 1.3.2.1 Standalone Mode æ¦‚è¿°
- ç”±`Master`+`Slave`æ„å»ºè€Œæˆçš„Sparké›†ç¾¤,Sparkè¿è¡Œåœ¨é›†ç¾¤ä¸­.
- Standaloneè¿è¡Œæ¨¡å¼
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_005.jpg)

##### 1.3.2.2 StandaloneMode QuickStart
- 1.åœ¨sparkæ ¹ç›®å½•ä¸‹è¿›å…¥confç›®å½•
```
[root@systemhub511 spark]# cd conf/
```
- 2.ä¿®æ”¹é…ç½®æ–‡ä»¶åç§° | `slaves` & `spark-env.sh`
```
[root@systemhub511 conf]# mv slaves.template slaves
[root@systemhub511 conf]# mv spark-env.sh.template spark-env.sh
```
- 3.ä¿®æ”¹slaveæ–‡ä»¶,æ·»åŠ workèŠ‚ç‚¹ | vim `slaves`
```
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
systemhub511
systemhub611
systemhub711
```
- 4.ä¿®æ”¹spark-env.shæ–‡ä»¶ | vim `spark-env.sh`
```
# Options for the daemons used in the standalone deploy mode
SPARK_MASTER_HOST=systemhub511
SPARK_MASTER_PORT=7077
```

- 5.å°†sparkåˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 6.å¯åŠ¨sparké›†ç¾¤ | `sbin/start-all.sh`
```
[root@systemhub511 spark]# sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-systemhub511.out
systemhub711: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub711.out
systemhub611: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub611.out
systemhub511: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub511.out
[root@systemhub511 spark]# 
```
- 7.æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
``` powershell
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
30651 org.apache.spark.deploy.worker.Worker
30443 org.apache.spark.deploy.master.Master
813 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
10369 org.apache.spark.deploy.worker.Worker
11777 sun.tools.jps.Jps
================        root@systemhub711 All Processes         ===========
8960 org.apache.spark.deploy.worker.Worker
10364 sun.tools.jps.Jps
[root@systemhub511 spark]# 
```

- 8.(æ±‚Ï€)å®˜æ–¹æ¡ˆä¾‹
- 8.1 æ‰§è¡Œè¯­å¥ | æŒ‡å®š spark master
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://systemhub511:7077 \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 8.2 æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœ
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://systemhub511:7077 \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 6.478381 s
Pi is roughly 3.1405883140588315
```

- 8.3 å¯åŠ¨`sparkshell`,å¹¶æ‰§è¡ŒWordCountç¨‹åºæŸ¥çœ‹ç»“æœ
- å‚æ•°ï¼š`--master spark://systemhub511:7077` æŒ‡å®šè¦è¿æ¥é›†ç¾¤master
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077

Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = spark://systemhub511:7077, app id = app-20190524174512-0001).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (hive,2), (oozie,1), (java,1), (spark,2), (hadoop,3), (flume,1), (hbase,1))

scala> 
```

- 8.4 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:8088`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_006.jpg)

- 8.5 é…ç½®å†å²æœåŠ¡å™¨(JobHistoryServer)
- é‡å‘½å`spark-default.conf.template`
```
[root@systemhub511 conf]# mv spark-defaults.conf.template spark-defaults.conf
```
- 8.5.1 é…ç½®`spark-default.conf` | vim `spark-default.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
```
- 8.5.2 é…ç½®spark-env.sh | vim `spark-env.sh`
```
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- å‚æ•°æè¿° : 
```
spark.eventLog.dirï¼šApplicationåœ¨è¿è¡Œè¿‡ç¨‹ä¸­æ‰€æœ‰ä¿¡æ¯å‡è®°å½•åœ¨è¯¥å±æ€§æŒ‡å®šçš„è·¯å¾„ä¸‹.

spark.history.ui.port=18080 WEBUIè®¿é—®ç«¯å£å·ä¸º18080

spark.history.fs.logDirectory=hdfs://systemhub511:9000/directory é…ç½®äº†è¯¥å±æ€§å,åœ¨start-history-server.shæ—¶å°±æ— éœ€å†æ˜¾ç¤ºæŒ‡å®šè·¯å¾„,Spark History Serveråªå±•ç¤ºè¯¥æŒ‡å®šè·¯å¾„ä¸‹ä¿¡æ¯.

spark.history.retainedApplications=30 æŒ‡å®šä¿å­˜Applicationå†å²è®°å½•ä¸ªæ•°,å¦‚æœè¶…è¿‡è¿™ä¸ªå€¼,æ—§åº”ç”¨ç¨‹åºä¿¡æ¯å°†è¢«åˆ é™¤,è¿™ä¸ªæ˜¯å†…å­˜ä¸­åº”ç”¨æ•°,è€Œä¸æ˜¯é¡µé¢ä¸Šæ˜¾ç¤ºåº”ç”¨æ•°.
```
- 8.5.3 åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 8.5.4 å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 hadoop]# sbin/start-dfs.sh
```
- 8.5.5 æ‰‹åŠ¨åˆ›å»ºHDFS /directoryç›®å½•
```
[root@systemhub511 spark]# hadoop fs -mkdir /directory
``` 
- 8.5.6 å¯åŠ¨Sparké›†ç¾¤
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 8.5.6 å¯åŠ¨Sparkå†å²æœåŠ¡
```
[root@systemhub511 spark]# sbin/start-history-server.sh
```
- 8.5.7 å¯åŠ¨`sparkshell`
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077
sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```
- 8.5.8 æŸ¥çœ‹å†å²æœåŠ¡ | `http://hostname:18080`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_007.jpg)

##### 1.3.2.3 Spark HA é«˜å¯ç”¨
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_008.jpg)

- 1.åœæ­¢é›†ç¾¤æ‰€æœ‰æœåŠ¡
- 2.é…ç½®spark-env.sh | vim `spark-env.sh`
```
# SPARK_MASTER_HOST=systemhub511
# SPARK_MASTER_PORT=7077
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=systemhub511,systemhub611,systemhub711 -Dspark.deploy.zookeeper.dir=/spark"
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- 3.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 4.å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 spark]# /opt/module/hadoop/sbin/start-dfs.sh
```
- 5.å¯åŠ¨Zookeeperé›†ç¾¤
```
[root@systemhub511 spark]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub611 ~]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub711 ~]# /opt/module/zookeeper/bin/zkServer.sh start
```
- 6.åœ¨systemhub511å¯åŠ¨å…¨éƒ¨æœåŠ¡èŠ‚ç‚¹
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 7.åœ¨systemhub611å•ç‹¬å¯åŠ¨masterå¤‡ä»½èŠ‚ç‚¹
```
[root@systemhub611 ~]# /opt/module/spark/sbin/start-master.sh
```
- 8.è®¿é—®SparkHAé›†ç¾¤
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077,systemhub611:7077
```
`http://systemhub511:8080` | systemhub511èŠ‚ç‚¹çŠ¶æ€ä¸º`ALIVE`
`http://systemhub611:8080` | systemhub611èŠ‚ç‚¹çŠ¶æ€ä¸º`STANDBY`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_009.jpg)

- 9.æ•…éšœè½¬ç§»æµ‹è¯•
- æ‰‹åŠ¨æ€æ­»systemhub511æœåŠ¡å™¨Masterè¿›ç¨‹,å¹¶æŸ¥çœ‹systemhub511æ˜¯å¦å°†ä»»åŠ¡è½¬ç§»ç»™systemhub611å¤‡ä»½èŠ‚ç‚¹ä½œä¸ºä¸»èŠ‚ç‚¹.
- 9.1 æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
```
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
32242 org.apache.hadoop.hdfs.server.namenode.NameNode
11206 org.apache.spark.deploy.master.Master
11368 org.apache.spark.deploy.worker.Worker
9705 org.apache.zookeeper.server.quorum.QuorumPeerMain
32444 org.apache.hadoop.hdfs.server.datanode.DataNode
5228 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
9157 org.apache.spark.deploy.master.Master
8901 org.apache.spark.deploy.worker.Worker
2822 sun.tools.jps.Jps
30214 org.apache.hadoop.hdfs.server.datanode.DataNode
7495 org.apache.zookeeper.server.quorum.QuorumPeerMain
================        root@systemhub711 All Processes         ===========
5312 org.apache.spark.deploy.worker.Worker
31568 sun.tools.jps.Jps
26869 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
26647 org.apache.hadoop.hdfs.server.datanode.DataNode
4014 org.apache.zookeeper.server.quorum.QuorumPeerMain
[root@systemhub511 spark]# 
```

- 9.2 Kill systemhub511 Masterä¸»èŠ‚ç‚¹
```
[root@systemhub511 spark]# kill -9 11206
```
- 9.3 systemhub511èŠ‚ç‚¹å·²å®•æœº | systemhub611å¤‡ä»½èŠ‚ç‚¹çŠ¶æ€å·²è½¬åŒ–ä¸ºALIVEä¸»èŠ‚ç‚¹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_010.jpg)


#### ğŸ’¥ 1.3.3 Yarn Mode ğŸ’¥
##### 1.3.3.1 Yarn Mode æ¦‚è¿°
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_011.jpg)
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Yarn,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤.
- ä¸¤ç§æ¨¡å¼`yarn-client`å’Œ`yarn-cluster`,ä¸»è¦åŒºåˆ«åœ¨äº : Driverç¨‹åºè¿è¡ŒèŠ‚ç‚¹
- `yarn-client` : Driverç¨‹åºè¿è¡Œåœ¨å®¢æˆ·ç«¯,é€‚ç”¨äºäº¤äº’è°ƒè¯•,ç«‹å³çœ‹åˆ°appè¾“å‡º.
- `yarn-cluster` : Driverç¨‹åºè¿è¡Œåœ¨ç”±RM(ResourceManager)å¯åŠ¨AP(APPMaster)é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ.

##### 1.3.3.2 YarnMode QuickStart

- 1.é…ç½®spark-env.sh | vim `spark-env.sh`
```
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```
- vim `spark-defaults.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
spark.yarn.historyServer.address=systemhub511:18080
spark.history.ui.port=18080
```
- vim `yarn-site.xml`
``` xml
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„ç‰©ç†å†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true -->
<property>
  <name>yarn.nodemanager.pmem-check-enabled</name>
  <value>false</value>
</property>

<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true-->
<property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
</property>
```

- 2.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```

- 3.æäº¤ä»»åŠ¡åˆ°Yarnæ‰§è¡Œ
```
bin/spark-submit \ 
--class org.apache.spark.examples.SparkPi \ 
--master yarn \ 
--deploy-mode client \ 
./examples/jars/spark-examples_2.11-2.1.1.jar\ 
100
```

#### ğŸ’¥ 1.3.4 Mesos Mode ğŸ’¥
##### 1.3.4.1 Mesos Mode æ¦‚è¿°
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Mesos,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤,å›½å†…åº”ç”¨æ¯”è¾ƒå°‘,æ›´å¤šæ˜¯è¿ç”¨yarnè°ƒåº¦.


#### ğŸ’¥ 1.3.5 è¿è¡Œæ¨¡å¼å¯¹æ¯” ğŸ’¥

| æ¨¡å¼      |     é›†ç¾¤æ•°é‡ |   é›†ç¾¤è¿›ç¨‹   |   æ‰€å±è€…   |
| :--------: | :--------:| :------: | :------: |
| Loacl Mode    |   1 |  æ—   |  Spark  |
| Standalone Mode    |   3 |  Master & Worker  |  Spark  |
| Yarn Mode    |   1 |  Yarn & HDFS  |  Hadoop  |

#### ğŸ’¥ 1.3.6 WordCount å®ä¾‹ ğŸ’¥
- Spark Shellä»…åœ¨æµ‹è¯•å’ŒéªŒè¯ç¨‹åºæ—¶ä½¿ç”¨çš„è¾ƒå¤š,åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šå¸¸ä¼šåœ¨IDEä¸­ç¼–åˆ¶ç¨‹åº,ç„¶åæ‰“æˆjaråŒ…æäº¤åˆ°é›†ç¾¤,æœ€å¸¸ç”¨æ˜¯åˆ›å»ºMavenå·¥ç¨‹,åˆ©ç”¨Mavenæ¥ç®¡ç†jaråŒ…ä¾èµ–.
- 1.JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
- 2.çˆ¶å·¥ç¨‹é…ç½®ä¿¡æ¯ | pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub.core.spark</groupId>
    <artifactId>spark_server</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>

    <modules>
        <module>spark-common</module>
    </modules>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
    </dependencies>

</project>
```
- 3.åˆ›å»ºå­æ¨¡å— spark-common | å­æ¨¡å—é…ç½®ä¿¡æ¯ pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>spark_server</artifactId>
        <groupId>com.geekparkhub.core.spark</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>spark-common</artifactId>

    <build>
        <finalName>WordCount</finalName>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

</project>
```

- 4.åœ¨`spark-common`å­æ¨¡å—ä¸­åˆ›å»ºscalaæºç ç›®å½• | Create `WordCount.scala`
``` scala
package com.geekparkhub.core.spark.application.wordcount

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * WordCountApplication
  * <p>
  */

object WordCount {
  def main(args: Array[String]): Unit = {

    /**
      * Create SparkConf
      * åˆ›å»º SparkConf
      */
    val sparkConf = new SparkConf().setMaster(args(0)).setAppName("WordCountApplication")

    /**
      * Create SparkContext
      * åˆ›å»º SparkContext
      */
    val sc = new SparkContext()

    /**
      * Read file
      * è¯»å–æ–‡ä»¶
      */
    val line: RDD[String] = sc.textFile(args(1))

    /**
      * To flatten
      * å‹å¹³
      */
    val word: RDD[String] = line.flatMap(_.split(" "))

    /**
      * Word conversion dual group
      * å•è¯è½¬æ¢äºŒå…ƒç»„
      */
    val wordAndOne: RDD[(String, Int)] = word.map((_, 1))

    /**
      * Count the total number of words
      * ç»Ÿè®¡å•è¯æ€»æ•°
      */
    val wordCount: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)

    /**
      * Write out the file
      * å†™å‡ºæ–‡ä»¶
      */
    wordCount.saveAsTextFile(args(2))

    /**
      * Close resource
      * å…³é—­èµ„æº
      */
    sc.stop()
  }
}
```
- 5.å°†`spark-common`å­æ¨¡å—æ‰“è‡³æˆjaråŒ…ä¸Šä¼ è‡³systemhub511æœåŠ¡å™¨
- 6.å¯åŠ¨HDFS | åœ¨HDFSåˆ›å»ºå¤šçº§ç›®å½•
```
[root@systemhub511 ~]# hadoop fs -mkdir -p /core_flow/spark/input/wordcount
```

- 7.å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ è‡³HDFSç›®å½•
```
hadoop fs -put /opt/module/spark/input/wordcount/wordcount_001.txt /core_flow/spark/input/wordcount
```
- 8.Yarnæ‰§è¡Œæäº¤ä»»åŠ¡è‡³
```
bin/spark-submit \
--class com.geekparkhub.core.spark.application.wordcount.WordCount \
--master yarn \
./lib_jar/WordCount.jar yarn \
/core_flow/spark/input/wordcount/wordcount_001.txt \
/core_flow/spark/output/wordcount
```
- 9.æŸ¥çœ‹ä»»åŠ¡æ±‡æ€»ç»“æœ
- 9.1 `hadoop fs -ls -R`
```
[root@systemhub511 spark]# hadoop fs -ls -R /core_flow/spark/output/wordcount/
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/_SUCCESS
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00000
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00001
[root@systemhub511 spark]# 
```
- 9.2 part-00000
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00000
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(scala,1)
(hive,2)
(oozie,1)
(java,1)
[root@systemhub511 spark]# 
```
- 9.3 part-00001
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00001
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(spark,2)
(hadoop,3)
(flume,1)
(hbase,1)
[root@systemhub511 spark]# 
```


### ğŸ”¥ 1.3 Spark Core ğŸ”¥
#### 1.3.1 RDD æ¦‚è¿°
##### 1.3.1.1 ä»€ä¹ˆæ˜¯RDD
> `RDD` (`Resilient Distributed Dataset`)`å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†`æ˜¯Sparkä¸­æœ€åŸºæœ¬æ•°æ®æŠ½è±¡,ä»£ç ä¸­æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»,å®ƒä»£è¡¨ä¸€ä¸ªå¼¹æ€§/ä¸å¯å˜/å¯åˆ†åŒº/é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆ.

##### 1.3.1.2 RDD å±æ€§
```
 * Internally, each RDD is characterized by five main properties:
 *
 *  - 1. A list of partitions
 *  - 2. A function for computing each split
 *  - 3. A list of dependencies on other RDDs
 *  - 4. Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - 5. Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
```
> 1.ä¸€ç»„åˆ†åŒº(Partition),å³æ•°æ®é›†åŸºæœ¬ç»„æˆå•ä½;
> 2.ä¸€ä¸ªè®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å‡½æ•°;
> 3.RDDä¹‹é—´ä¾èµ–å…³ç³»;
> 4.ä¸€ä¸ªPartitioner,å³RDDåˆ†ç‰‡å‡½æ•°;
> 5.ä¸€ä¸ªåˆ—è¡¨,å­˜å‚¨å­˜å–æ¯ä¸ªPartitionçš„ä¼˜å…ˆä½ç½®(preferred location)

##### 1.3.1.3 RDD ç‰¹ç‚¹
> RDDè¡¨ç¤ºåªè¯»åˆ†åŒºæ•°æ®é›†,å¯¹RDDè¿›è¡Œæ”¹åŠ¨,åªèƒ½é€šè¿‡RDDè½¬æ¢æ“ä½œ,ç”±ä¸€ä¸ªRDDå¾—åˆ°ä¸€ä¸ªæ–°çš„RDD,æ–°çš„RDDåŒ…å«äº†ä»å…¶ä»–RDDè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´å­˜åœ¨ä¾èµ–,RDDæ‰§è¡Œæ˜¯æŒ‰ç…§è¡€ç¼˜å…³ç³»å»¶æ—¶è®¡ç®—,å¦‚æœè¡€ç¼˜å…³ç³»è¾ƒé•¿,å¯ä»¥é€šè¿‡æŒä¹…åŒ–RDDæ¥åˆ‡æ–­è¡€ç¼˜å…³ç³».

###### 1.3.1.3.1 å¼¹æ€§
- å­˜å‚¨å¼¹æ€§ : å†…å­˜ä¸ç£ç›˜çš„è‡ªåŠ¨åˆ‡æ¢.
- å®¹é”™å¼¹æ€§ : æ•°æ®ä¸¢å¤±å¯ä»¥è‡ªåŠ¨æ¢å¤.
- è®¡ç®—å¼¹æ€§ : è®¡ç®—å‡ºé”™é‡è¯•æœºåˆ¶.
- åˆ†ç‰‡å¼¹æ€§ : å¯æ ¹æ®éœ€è¦é‡æ–°åˆ†ç‰‡.


###### 1.3.1.3.2 åˆ†åŒº
> RDDé€»è¾‘ä¸Šæ˜¯åˆ†åŒºçš„,æ¯ä¸ªåˆ†åŒºæ•°æ®æ˜¯æŠ½è±¡å­˜åœ¨çš„,è®¡ç®—æ—¶ä¼šé€šè¿‡ä¸€ä¸ªcomputeå‡½æ•°å¾—åˆ°æ¯ä¸ªåˆ†åŒºæ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å·²æœ‰æ–‡ä»¶ç³»ç»Ÿæ„å»º,åˆ™computeå‡½æ•°æ˜¯è¯»å–æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿä¸­æ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å…¶ä»–RDDè½¬æ¢è€Œæ¥,åˆ™computeå‡½æ•°æ˜¯æ‰§è¡Œè½¬æ¢é€»è¾‘å°†å…¶ä»–RDDæ•°æ®è¿›è¡Œè½¬æ¢.

###### 1.3.1.3.3 åªè¯»
> RDDæ˜¯åªè¯»çš„,è¦æƒ³æ”¹å˜RDDä¸­æ•°æ®,åªèƒ½åœ¨ç°æœ‰RDDåŸºç¡€ä¸Šåˆ›å»ºæ–°çš„RDD.
> 
> ç”±ä¸€ä¸ªRDDè½¬æ¢åˆ°å¦ä¸€ä¸ªRDD,å¯ä»¥é€šè¿‡ä¸°å¯Œçš„æ“ä½œç®—å­å®ç°,ä¸å†åƒMapReduceé‚£æ ·åªèƒ½å†™mapå’Œreduce.
> 
> RDDæ“ä½œç®—å­åŒ…æ‹¬ä¸¤ç±»,ä¸€ç±»æ˜¯`transformations`,å®ƒæ˜¯ç”¨æ¥å°†RDDè¿›è¡Œè½¬åŒ–,æ„å»ºRDDçš„è¡€ç¼˜å…³ç³»,å¦ä¸€ç±»æ˜¯`actions`,å®ƒæ˜¯ç”¨æ¥è§¦å‘RDDè®¡ç®—å¾—åˆ°RDDç›¸å…³è®¡ç®—ç»“æœæˆ–è€…å°†RDDä¿å­˜æ–‡ä»¶ç³»ç»Ÿä¸­.

###### 1.3.1.3.4 ä¾èµ–
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_012.jpg)

> å¦‚å›¾æ‰€ç¤º,RDDsé€šè¿‡æ“ä½œç®—å­è¿›è¡Œè½¬æ¢,è½¬æ¢å¾—åˆ°æ–°RDDåŒ…å«äº†ä»å…¶ä»–RDDsè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´ç»´æŠ¤ç€è¿™ç§è¡€ç¼˜å…³ç³»,ä¹Ÿç§°ä¹‹ä¸ºä¾èµ–.
> 
> ä¾èµ–åŒ…æ‹¬ä¸¤ç§,ä¸€ç§æ˜¯çª„ä¾èµ–,RDDsä¹‹é—´åˆ†åŒºæ˜¯ä¸€ä¸€å¯¹åº”,å¦ä¸€ç§æ˜¯å®½ä¾èµ–,ä¸‹æ¸¸RDDçš„æ¯ä¸ªåˆ†åŒºä¸ä¸Šæ¸¸RDD(ä¹Ÿç§°ä¹‹ä¸ºçˆ¶RDD)çš„æ¯ä¸ªåˆ†åŒºéƒ½æœ‰å…³,æ˜¯å¤šå¯¹å¤šå…³ç³».

###### 1.3.1.3.5 ç¼“å­˜
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_013.jpg)

> å¦‚æœåœ¨åº”ç”¨ç¨‹åºä¸­å¤šæ¬¡ä½¿ç”¨åŒä¸€ä¸ªRDDæ—¶,å¯ä»¥å°†è¯¥RDDç¼“å­˜èµ·æ¥,è¯¥RDDåªæœ‰åœ¨ç¬¬ä¸€æ¬¡è®¡ç®—æ—¶ä¼šæ ¹æ®è¡€ç¼˜å…³ç³»å¾—åˆ°åˆ†åŒºæ•°æ®,åœ¨åç»­å…¶ä»–åœ°æ–¹ç”¨åˆ°è¯¥RDDæ—¶,ä¼šç›´æ¥ä»ç¼“å­˜å¤„å–è€Œä¸ç”¨å†æ ¹æ®è¡€ç¼˜å…³ç³»è®¡ç®—,è¿™æ ·å°±åŠ é€ŸåæœŸçš„é‡ç”¨.
> 
> å¦‚å›¾æ‰€ç¤º,RDD-1ç»è¿‡ä¸€ç³»åˆ—è½¬æ¢åå¾—åˆ°RDD-nå¹¶ä¿å­˜åˆ°HDFS,RDD-1åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ä¼šæœ‰ä¸ªä¸­é—´ç»“æœ,å¦‚æœå°†å…¶ç¼“å­˜åˆ°å†…å­˜,é‚£ä¹ˆåœ¨éšåRDD-1è½¬æ¢åˆ°RDD-mè¿™ä¸€è¿‡ç¨‹ä¸­,å°±ä¸ä¼šè®¡ç®—å…¶ä¹‹å‰çš„RDD-0.

###### 1.3.1.3.6 CheckPoint
> è™½ç„¶RDDè¡€ç¼˜å…³ç³»å¤©ç„¶åœ°å¯ä»¥å®ç°å®¹é”™,å½“RDDæŸä¸ªåˆ†åŒºæ•°æ®å¤±è´¥æˆ–ä¸¢å¤±,å¯ä»¥é€šè¿‡è¡€ç¼˜å…³ç³»é‡å»º,ä½†æ˜¯å¯¹äºé•¿æ—¶é—´è¿­ä»£å‹åº”ç”¨æ¥è¯´éšç€è¿­ä»£è¿›è¡Œ,RDDsä¹‹é—´è¡€ç¼˜å…³ç³»ä¼šè¶Šæ¥è¶Šé•¿,ä¸€æ—¦åœ¨åç»­è¿­ä»£è¿‡ç¨‹ä¸­å‡ºé”™,åˆ™éœ€è¦é€šè¿‡éå¸¸é•¿çš„è¡€ç¼˜å…³ç³»å»é‡å»º,åŠ¿å¿…å½±å“æ€§èƒ½.
> 
> ä¸ºæ­¤,RDDæ”¯æŒcheckpointå°†æ•°æ®ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ä¸­,è¿™æ ·å°±å¯ä»¥åˆ‡æ–­ä¹‹å‰è¡€ç¼˜å…³ç³»,å› ä¸ºcheckpointåçš„RDDä¸éœ€è¦çŸ¥é“å®ƒçš„çˆ¶RDDs,å®ƒå¯ä»¥ä»checkpointå¤„æ‹¿åˆ°æ•°æ®.


#### 1.3.2 RDD ç¼–ç¨‹
##### 1.3.2.1 ç¼–ç¨‹æ¨¡å‹
> åœ¨Sparkä¸­,RDDè¢«è¡¨ç¤ºä¸ºå¯¹è±¡,é€šè¿‡å¯¹è±¡æ–¹æ³•è°ƒç”¨RDDè¿›è¡Œè½¬æ¢,ç»è¿‡ä¸€ç³»åˆ—çš„`transformations`å®šä¹‰RDDä¹‹å,å°±å¯ä»¥è°ƒç”¨`actions`è§¦å‘RDDè®¡ç®—,`action`å¯ä»¥æ˜¯å‘åº”ç”¨ç¨‹åºè¿”å›ç»“æœ(count,collectç­‰),æˆ–è€…æ˜¯å‘å­˜å‚¨ç³»ç»Ÿä¿å­˜æ•°æ®(saveAsTextFileç­‰).
> åœ¨Sparkä¸­,åªæœ‰é‡åˆ°`action`æ‰ä¼šæ‰§è¡ŒRDDè®¡ç®—(å³å»¶è¿Ÿè®¡ç®—),è¿™æ ·åœ¨è¿è¡Œæ—¶å¯ä»¥é€šè¿‡ç®¡é“æ–¹å¼ä¼ è¾“å¤šä¸ªè½¬æ¢.
> ä½¿ç”¨Sparkå¼€å‘è€…éœ€è¦ç¼–å†™ä¸€ä¸ªDriverç¨‹åº,å®ƒè¢«æäº¤åˆ°é›†ç¾¤ä»¥è°ƒåº¦è¿è¡ŒWorker,Driverä¸­å®šä¹‰äº†ä¸€ä¸ªæˆ–å¤šä¸ªRDD.å¹¶è°ƒç”¨RDDä¸Šçš„action.Workeråˆ™æ‰§è¡ŒRDDåˆ†åŒºè®¡ç®—ä»»åŠ¡.
##### 1.3.2.2 RDD åˆ›å»º
- Sparkåˆ›å»ºRDDåˆ›å»ºæ–¹å¼å¯ä»¥åˆ†ä¸ºä¸‰ç§:
- 1.ä»é›†åˆä¸­åˆ›å»ºRDD
- 2.ä»å¤–éƒ¨å­˜å‚¨åˆ›å»ºRDD
- 3.ä»å…¶ä»–RDDåˆ›å»º
###### 1.3.2.1 é›†åˆåˆ›å»ºRDD
- ä»é›†åˆä¸­åˆ›å»ºRDD,Sparkä¸»è¦æä¾›äº†ä¸¤ç§å‡½æ•° : `parallelize`å’Œ`makeRDD`
- 1.ä½¿ç”¨`parallelize()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- 2.ä½¿ç”¨`makeRDD()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val makerdd = sc.makeRDD(Array(511,611,711))
makerdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:24
scala> makerdd.collect
res1: Array[Int] = Array(511, 611, 711)
scala> 
```
###### 1.3.2.2 å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿæ•°æ®é›†åˆ›å»ºRDD
- é™¤äº†åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ,è¿˜æœ‰æ‰€æœ‰Hadoopæ”¯æŒæ•°æ®é›†,æ¯”å¦‚HDFS/Cassandra/HBaseç­‰.
- è¯¦è§ 1.3.4 æ•°æ®è¯»å–ä¿å­˜
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt")
res2: org.apache.spark.rdd.RDD[String] = /opt/module/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[3] at textFile at <console>:25
scala> 
```

###### 1.3.2.3 ä»å…¶ä»–åˆ›å»ºRDD
- è¯¦è§1.3.2.3 RDD è½¬æ¢


##### 1.3.2.3 RDD è½¬æ¢
- RDDæ•´ä½“åˆ†ä¸º`Value`ç±»å‹å’Œ`Key-Value`ç±»å‹

##### 1.3.2.3.1 Value ç±»å‹
###### 1.3.2.3.1.1 `map(func)` Method
- ä½œç”¨ : è¿”å›ä¸€ä¸ªæ–°RDD,è¯¥RDDç”±æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ç»è¿‡funcå‡½æ•°è½¬æ¢åç»„æˆ.
- åˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- æ‰“å°RDDæœ€ç»ˆç»“æœ
```
scala> rdd.map((_,1)).collect
res4: Array[(Int, Int)] = Array((511,1), (611,1), (711,1))
scala> 
```
- å°†æ‰€æœ‰å…ƒç´ RDD*2,æœ€ç»ˆç»“æœ
```
scala> rdd.map((_*2)).collect
res5: Array[Int] = Array(1022, 1222, 1422)
scala> 
```
###### 1.3.2.3.1.2 `mapPartitions(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†ç‹¬ç«‹åœ°åœ¨RDDæ¯ä¸€ä¸ªåˆ†ç‰‡ä¸Šè¿è¡Œ,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcå‡½æ•°ç±»å‹å¿…é¡»æ˜¯Iterator[T] => Iterator[U]
- å‡è®¾æœ‰Nä¸ªå…ƒç´ ,æœ‰Mä¸ªåˆ†åŒº,é‚£ä¹ˆmapå‡½æ•°å°†è¢«è°ƒç”¨Næ¬¡,è€ŒmapPartitionsè¢«è°ƒç”¨Mæ¬¡,ä¸€ä¸ªå‡½æ•°ä¸€æ¬¡å¤„ç†æ‰€æœ‰åˆ†åŒº.
```
scala> rdd.mapPartitions(_.map(_*2)).collect
res11: Array[Int] = Array(1022, 1222, 1422)
scala> 
```

###### 1.3.2.3.1.3 `mapPartitionsWithIndex(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmapPartitions,ä½†funcå¸¦æœ‰ä¸€ä¸ªæ•´æ•°å‚æ•°è¡¨ç¤ºåˆ†ç‰‡ç´¢å¼•å€¼,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcçš„å‡½æ•°ç±»å‹å¿…é¡»æ˜¯(Int, Interator[T]) => Iterator[U];
```
scala> rdd.mapPartitionsWithIndex((index,items)=>(items.map((index,_)))).collect
res13: Array[(Int, Int)] = Array((1,511), (2,611), (3,711))
scala> 
```

###### 1.3.2.3.1.4 `flatMap(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ å¯ä»¥è¢«æ˜ å°„ä¸º0æˆ–å¤šä¸ªè¾“å‡ºå…ƒç´ (æ‰€ä»¥funcåº”è¯¥è¿”å›ä¸€ä¸ªåºåˆ—,è€Œä¸æ˜¯å•ä¸€å…ƒç´ )
```
scala> val text = sc.textFile("/core_flow/spark/input/wordcount/wordcount_001.txt")
text: org.apache.spark.rdd.RDD[String] = /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[15] at textFile at <console>:24
scala> text.flatMap(_.split(" ")).collect
res16: Array[String] = Array(hadoop, spark, hive, hadoop, spark, hadoop, hbase, flume, hive, scala, java, oozie)
scala> 
```

###### 1.3.2.3.1.5 `map()`ä¸`mapPartition()`åŒºåˆ«
- 1.map() : æ¯æ¬¡å¤„ç†ä¸€æ¡æ•°æ®
- 2.mapPartition() : æ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®,è¿™ä¸ªåˆ†åŒºçš„æ•°æ®å¤„ç†å®Œå,åŸRDDä¸­åˆ†åŒºçš„æ•°æ®æ‰èƒ½é‡Šæ”¾,å¯èƒ½å¯¼è‡´OOM.
- 3.å¼€å‘æŒ‡å¯¼ : å½“å†…å­˜ç©ºé—´è¾ƒå¤§çš„æ—¶å€™å»ºè®®ä½¿ç”¨mapPartition(),ä»¥æé«˜å¤„ç†æ•ˆç‡.

###### 1.3.2.3.1.6 `glom` Method
- ä½œç”¨ : å°†æ¯ä¸€ä¸ªåˆ†åŒºå½¢æˆä¸€ä¸ªæ•°ç»„,å½¢æˆæ–°çš„RDDç±»å‹æ—¶RDD[Array[T]]
```
scala> rdd.glom.collect
res17: Array[Array[Int]] = Array(Array(), Array(511), Array(611), Array(711))   
scala> 
```

###### 1.3.2.3.1.7 `groupBy(func)` Method
- ä½œç”¨ : åˆ†ç»„æŒ‰ç…§ä¼ å…¥å‡½æ•°çš„è¿”å›å€¼è¿›è¡Œåˆ†ç»„,å°†ç›¸åŒçš„keyå¯¹åº”çš„å€¼æ”¾å…¥ä¸€ä¸ªè¿­ä»£å™¨.
```
scala> rdd.groupBy(_ % 2).collect
res18: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(611, 711, 511)))    
scala> 
```

###### 1.3.2.3.1.8 `filter(func)` Method
- ä½œç”¨ : è¿‡æ»¤è¿”å›ä¸€ä¸ªæ–°çš„RDD,è¯¥RDDç”±ç»è¿‡funcå‡½æ•°è®¡ç®—åè¿”å›å€¼ä¸ºtrueçš„è¾“å…¥å…ƒç´ ç»„æˆ.
```
scala> rdd.filter(_%3==0).collect
res20: Array[Int] = Array(711)
scala> 
```

###### 1.3.2.3.1.9 `sample(withReplacement,fraction,seed)` Method
- ä½œç”¨ : ä»¥æŒ‡å®šéšæœºç§å­éšæœºæŠ½æ ·å‡ºæ•°é‡ä¸ºfractionçš„æ•°æ®,withReplacementè¡¨ç¤ºæ˜¯æŠ½å‡ºçš„æ•°æ®æ˜¯å¦æ”¾å›,trueä¸ºæœ‰æ”¾å›çš„æŠ½æ ·,falseä¸ºæ— æ”¾å›çš„æŠ½æ ·,seedç”¨äºæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­.
```
scala> val rdd = sc.parallelize(1 to 100)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at parallelize at <console>:24
scala> rdd.sample(false,0.1,3).collect
res22: Array[Int] = Array(1, 33, 37, 50, 59, 69, 75, 78, 85, 98) 
scala> 
```

###### 1.3.2.3.1.10 `distinct([numTasks]))` Method
- ä½œç”¨ : å¯¹æºRDDè¿›è¡Œå»é‡åè¿”å›ä¸€ä¸ªæ–°çš„RDD,é»˜è®¤æƒ…å†µä¸‹,åªæœ‰8ä¸ªå¹¶è¡Œä»»åŠ¡æ¥æ“ä½œ,ä½†æ˜¯å¯ä»¥ä¼ å…¥ä¸€ä¸ªå¯é€‰çš„numTaskså‚æ•°æ”¹å˜å®ƒ.
- ä½¿ç”¨distinct()å¯¹å…¶å»é‡æ“ä½œ.
```
scala> rdd.distinct(4).collect
res23: Array[Int] = Array(84, 100, 96, 52, 56, 4, 76, 16, 28, 80, 48, 32, 36, 24, 64, 92, 40, 72, 8, 12, 20, 60, 44, 88, 68, 13, 41, 61, 81, 21, 77, 53, 97, 25, 29, 65, 73, 57, 93, 33, 37, 45, 1, 89, 17, 69, 9, 85, 49, 5, 34, 82, 66, 22, 54, 98, 46, 30, 14, 50, 62, 42, 74, 90, 6, 70, 18, 38, 86, 58, 78, 26, 94, 10, 2, 19, 39, 15, 47, 71, 55, 95, 79, 59, 11, 35, 27, 75, 51, 23, 63, 83, 67, 3, 7, 91, 31, 87, 43, 99)
scala> 
```
###### 1.3.2.3.1.11 `coalesce(numPartitions)` Method
- ä½œç”¨ : ç¼©å‡åˆ†åŒºæ•°,ç”¨äºå¤§æ•°æ®é›†è¿‡æ»¤å,æé«˜å°æ•°æ®é›†çš„æ‰§è¡Œæ•ˆç‡.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶ç¼©å‡åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at <console>:24

scala> rdd.partitions.size
res24: Int = 4

scala> val coalesceRDD = rdd.coalesce(3)
coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[28] at coalesce at <console>:26

scala> coalesceRDD.partitions.size
res25: Int = 3
scala> 
```
###### 1.3.2.3.1.12 `repartition(numPartitions)` Method
- ä½œç”¨ : æ ¹æ®åˆ†åŒºæ•°,é‡æ–°é€šè¿‡ç½‘ç»œéšæœºæ´—ç‰Œæ‰€æœ‰æ•°æ®.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶é‡æ–°åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:24

scala> rdd.partitions.size
res26: Int = 4

scala> val rerdd = rdd.repartition(2)
rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at repartition at <console>:26

scala> rerdd.partitions.size
res27: Int = 2
scala> 
```

###### 1.3.2.3.1.13 `coalesce`ä¸`repartition`åŒºåˆ«
> 1.`coalesce`é‡æ–°åˆ†åŒº,å¯ä»¥é€‰æ‹©æ˜¯å¦è¿›è¡Œshuffleè¿‡ç¨‹,ç”±å‚æ•°`shuffle: Boolean = false/true`å†³å®š.
> 
> 2.`repartition`å®é™…ä¸Šæ˜¯è°ƒç”¨coalesce,è¿›è¡Œshuffleè¿‡ç¨‹,æºç æ¼”ç¤º:
``` scala
def repartition(numpartitions: int)(implicit ord: ordering[t] = null): rdd[t] = withscope {
coalesce(numpartitions, shuffle = true)
}
```
###### 1.3.2.3.1.14 `sortBy(func,[ascending],[numTasks])` Method
- ä½œç”¨ : ä½¿ç”¨funcå…ˆå¯¹æ•°æ®è¿›è¡Œå¤„ç†,æŒ‰ç…§å¤„ç†åçš„æ•°æ®æ¯”è¾ƒç»“æœæ’åº,é»˜è®¤ä¸ºæ­£åº.
- åˆ›å»ºRDD,æŒ‰ç…§ä¸åŒè§„åˆ™è¿›è¡Œæ’åº | æŒ‰ç…§è‡ªèº«å¤§å°æ’åº / æŒ‰ç…§ä¸3ä½™æ•°å¤§å°æ’åº / æŒ‰ç…§å€’åºæ’åº
```
scala>  val rdd = sc.parallelize(List(2,1,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at <console>:24

scala> rdd.sortBy(x => x).collect()
res29: Array[Int] = Array(1, 2, 3, 4)

scala> rdd.sortBy(x => x%3).collect()
res30: Array[Int] = Array(3, 1, 4, 2)

scala> rdd.sortBy(x => x,false).collect()
res31: Array[Int] = Array(4, 3, 2, 1)

scala> 
```

###### 1.3.2.3.1.15 `pipe(command,[envVars])` Method
- ä½œç”¨ : ç®¡é“é’ˆå¯¹æ¯ä¸ªåˆ†åŒº,éƒ½æ‰§è¡Œä¸€ä¸ªshellè„šæœ¬,è¿”å›è¾“å‡ºRDD.
- åˆ›å»ºè„šæœ¬,ä½¿ç”¨ç®¡é“å°†è„šæœ¬ä½œç”¨äºRDDä¸Š
```
[root@systemhub511 ~]# vim /opt/module/spark/input/pipe.sh
[root@systemhub511 ~]# chmod 777 /opt/module/spark/input/pipe.sh
```
- vim `pipe.sh`
``` powershell
#!/bin/
shecho"Start"
while read LINE;do
	echo ">>>" ${LINE}
done
```
```
scala> rdd.pipe("/opt/module/spark/pipe.sh").collect
res18: Array[String] = Array(Start, >>>hi, >>>Hello, >>>how, >>>are, >>>you)
scala> 
```



##### 1.3.2.3.2 åŒValueç±»å‹äº¤äº’

###### 1.3.2.3.2.1 `union(otherDataset)` Method
- ä½œç”¨ : å¯¹æºRDDå’Œå‚æ•°RDDæ±‚å¹¶é›†åè¿”å›ä¸€ä¸ªæ–°RDD | åˆ›å»ºä¸¤ä¸ªRDDè¿›è¡Œå¹¶é›†è®¡ç®—
```
scala> var rdd1 = sc.parallelize(1 to 5)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> var rdd2 = sc.parallelize(5 to 10)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> rdd1.union(rdd2).collect
res0: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10)
scala> 
```

###### 1.3.2.3.2.2 `subtract(otherDataset)` Method
- ä½œç”¨ : è®¡ç®—å·®çš„ä¸€ç§å‡½æ•°,å»é™¤ä¸¤ä¸ªRDDä¸­ç›¸åŒå…ƒç´ ,ä¸åŒçš„RDDå°†ä¿ç•™ä¸‹æ¥
```
scala> rdd1.subtract(rdd2).collect
res0: Array[Int] = Array(2, 4, 1, 3)
scala> 
```

###### 1.3.2.3.2.3 `intersection(otherDataset)` Method
- ä½œç”¨ : å¯¹æºRDDå’Œå‚æ•°RDDæ±‚äº¤é›†å,è¿”å›ä¸€ä¸ªæ–°çš„RDD
```
scala> rdd1.intersection(rdd2).collect
res1: Array[Int] = Array(5)
scala> 
```
###### 1.3.2.3.2.4 `cartesian(otherDataset)` Method
- ä½œç”¨ : ç¬›å¡å°”ç§¯ `(å°½é‡é¿å…ä½¿ç”¨)`
```
scala> rdd1.cartesian(rdd2).collect
res2: Array[(Int, Int)] = Array((1,5), (1,6), (1,7), (2,5), (2,6), (2,7), (1,8), (1,9), (1,10), (2,8), (2,9), (2,10), (3,5), (3,6), (3,7), (4,5), (4,6), (4,7), (5,5), (5,6), (5,7), (3,8), (3,9), (3,10), (4,8), (4,9), (4,10), (5,8), (5,9), (5,10))
scala> 
```

###### 1.3.2.3.2.5 `zip(otherDataset)` Method
- ä½œç”¨ : å°†ä¸¤ä¸ªRDDç»„åˆæˆKey/Valueå½¢å¼RDD,é»˜è®¤ä¸¤ä¸ªRDDçš„partitionæ•°é‡ä»¥åŠå…ƒç´ æ•°é‡éƒ½ç›¸åŒ,å¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸.
```
scala> rdd1.zip(rdd2).collect
res4: Array[(Int, Int)] = Array((1,6), (2,7), (3,8), (4,9), (5,10))
scala> 
```

##### 1.3.2.3.3 Key-Value ç±»å‹

###### 1.3.2.3.3.1 `partitionBy` Method
- ä½œç”¨ : å¯¹pairRDDè¿›è¡Œåˆ†åŒºæ“ä½œ,å¦‚æœåŸæœ‰çš„partionRDDå’Œç°æœ‰çš„partionRDDæ˜¯ä¸€è‡´çš„è¯å°±ä¸è¿›è¡Œåˆ†åŒº,å¦åˆ™ä¼šç”ŸæˆShuffleRDD,å³ä¼šäº§ç”Ÿshuffleè¿‡ç¨‹.
```
scala> val rdd1 = sc.parallelize(Array((1,"A"),(2,"B"),(3,"C"),(4,"D")),4)
rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> rdd1.mapPartitionsWithIndex((i,t)=>t.map((i,_))).collect
res3: Array[(Int, (Int, String))] = Array((0,(1,A)), (1,(2,B)), (2,(3,C)), (3,(4,D)))

scala> rdd1.partitionBy(new org.apache.spark.HashPartitioner(2))
res5: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[3] at partitionBy at <console>:27

scala> res5.partitions.size
res6: Int = 2
scala> 
```

###### 1.3.2.3.3.2 `reduceByKey(func,[numTasks])` Method
- åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ª(K,V)çš„RDD,ä½¿ç”¨æŒ‡å®šreduceå‡½æ•°,å°†ç›¸åŒkeyå€¼èšåˆåˆ°ä¸€èµ·,reduceä»»åŠ¡ä¸ªæ•°å¯ä»¥é€šè¿‡ç¬¬äºŒä¸ªå¯é€‰å‚æ•°æ¥è®¾ç½®.
```
scala> val rdd = sc.parallelize(List(("female",1),("male",5),("female",5),("male",2)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:24

scala> rdd.reduceByKey((x,y)=>x+y).collect
res7: Array[(String, Int)] = Array((female,6), (male,7))
scala> 
```

###### 1.3.2.3.3.3 `groupByKey` Method
- ä½œç”¨ : groupByKeyä¹Ÿæ˜¯å¯¹æ¯ä¸ªkeyè¿›è¡Œæ“ä½œ,ä½†åªç”Ÿæˆä¸€ä¸ªseq.
```
scala> rdd.groupByKey(2).collect
res8: Array[(String, Iterable[Int])] = Array((female,CompactBuffer(5, 1)), (male,CompactBuffer(5, 2)))

scala> 
```

###### 1.3.2.3.3.4 `reduceByKey`ä¸`groupByKey` åŒºåˆ«
> 1.reduceByKey : æŒ‰ç…§keyè¿›è¡Œèšåˆ,åœ¨shuffleä¹‹å‰æœ‰combine(é¢„èšåˆ)æ“ä½œ,è¿”å›ç»“æœæ˜¯RDD[k,v]
> 
> 2.groupByKey : æŒ‰ç…§keyè¿›è¡Œåˆ†ç»„,ç›´æ¥è¿›è¡Œshuffle
> 
> 3.å¼€å‘æŒ‡å¯¼ : reduceByKeyæ¯”groupByKey,å»ºè®®ä½¿ç”¨reduceByKey,ä½†æ˜¯éœ€è¦æ³¨æ„æ˜¯å¦ä¼šå½±å“ä¸šåŠ¡é€»è¾‘.

###### 1.3.2.3.3.5 `aggregateByKey` Method
> å‚æ•° : `(zeroValue:U,[partitioner:Partitioner])(seqOp: (U, V) => U,combOp: (U, U) => U)`
> 
> 1.ä½œç”¨ : åœ¨kvå¯¹çš„RDDä¸­,æŒ‰keyå°†valueè¿›è¡Œåˆ†ç»„åˆå¹¶,åˆå¹¶æ—¶å°†æ¯ä¸ªvalueå’Œåˆå§‹å€¼ä½œä¸ºseqå‡½æ•°å‚æ•°è¿›è¡Œè®¡ç®—,è¿”å›ç»“æœä½œä¸ºä¸€ä¸ªæ–°çš„kvå¯¹,ç„¶åå†å°†ç»“æœæŒ‰ç…§keyè¿›è¡Œåˆå¹¶,æœ€åå°†æ¯ä¸ªåˆ†ç»„çš„valueä¼ é€’ç»™combineå‡½æ•°è¿›è¡Œè®¡ç®—(å…ˆå°†å‰ä¸¤ä¸ªvalueè¿›è¡Œè®¡ç®—,å°†è¿”å›ç»“æœå’Œä¸‹ä¸€ä¸ªvalueä¼ ç»™combineå‡½æ•°,ä»¥æ­¤ç±»æ¨),å°†keyä¸è®¡ç®—ç»“æœä½œä¸ºä¸€ä¸ªæ–°çš„kvå¯¹è¾“å‡º.
> 
> 2.å‚æ•°æè¿° : 
> `zeroValue` : ç»™æ¯ä¸€ä¸ªåˆ†åŒºä¸­çš„æ¯ä¸€ä¸ªkeyä¸€ä¸ªåˆå§‹å€¼.
> `seqOp` : å‡½æ•°ç”¨äºåœ¨æ¯ä¸€ä¸ªåˆ†åŒºä¸­ç”¨åˆå§‹å€¼é€æ­¥è¿­ä»£value
> `combOp` : å‡½æ•°ç”¨äºåˆå¹¶æ¯ä¸ªåˆ†åŒºä¸­çš„ç»“æœ

- åˆ›å»ºä¸€ä¸ªpairRDD,å–å‡ºæ¯ä¸ªåˆ†åŒºç›¸åŒkeyå¯¹åº”å€¼çš„æœ€å¤§å€¼ç„¶åç›¸åŠ .
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_014.jpg)

```
scala> val rdd = sc.parallelize(List(("a",3),("a",2),("c",4),("b",3),("c",6),("c",8)),2)
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:24

scala> rdd.aggregateByKey(0)(math.max(_,_),_+_).collect
res9: Array[(String, Int)] = Array((b,3), (a,3), (c,12))
scala> 
```
```
scala> rdd.aggregateByKey(0)(_+_,_+_).collect
res10: Array[(String, Int)] = Array((b,3), (a,5), (c,18))

scala> rdd.reduceByKey(_+_).collect
res11: Array[(String, Int)] = Array((b,3), (a,5), (c,18))
scala> 
```

###### 1.3.2.3.3.6 `foldByKey` Method
- å‚æ•° : `(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]`
- ä½œç”¨ : `aggregateByKey`çš„ç®€åŒ–æ“ä½œ,seqopå’Œcombopç›¸åŒ
```
scala> rdd.foldByKey(0)(_+_).collect
res12: Array[(String, Int)] = Array((b,3), (a,5), (c,18))
scala> 
```

###### 1.3.2.3.3.7 `combineByKey[C]` Method
- å‚æ•° : `(createCombiner:V=>C,mergeValue:(C,V)=>C,mergeCombiners:(C,C)=>C) `
- ä½œç”¨ : é’ˆå¯¹ç›¸åŒK,å°†Våˆå¹¶æˆä¸€ä¸ªé›†åˆ.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_015.jpg)

- å‚æ•°æè¿° : 
> 1.`createCombiner:combineByKey()`ä¼šéå†åˆ†åŒºä¸­çš„æ‰€æœ‰å…ƒç´ ,å› æ­¤æ¯ä¸ªå…ƒç´ çš„é”®è¦ä¹ˆè¿˜æ²¡æœ‰é‡åˆ°è¿‡,è¦ä¹ˆå°±å’Œä¹‹å‰çš„æŸä¸ªå…ƒç´ çš„é”®ç›¸åŒã€‚å¦‚æœè¿™æ˜¯ä¸€ä¸ªæ–°çš„å…ƒç´ ,combineByKey()ä¼šä½¿ç”¨ä¸€ä¸ªå«ä½œcreateCombiner()å‡½æ•°æ¥åˆ›å»ºé‚£ä¸ªé”®å¯¹åº”çš„ç´¯åŠ å™¨åˆå§‹å€¼.
> 
> 2.`mergeValue` : å¦‚æœè¿™æ˜¯ä¸€ä¸ªåœ¨å¤„ç†å½“å‰åˆ†åŒºä¹‹å‰å·²ç»é‡åˆ°çš„é”®,å®ƒä¼šä½¿ç”¨mergeValue()æ–¹æ³•å°†è¯¥é”®çš„ç´¯åŠ å™¨å¯¹åº”çš„å½“å‰å€¼ä¸è¿™ä¸ªæ–°çš„å€¼è¿›è¡Œåˆå¹¶.
> 
> 3.`mergeCombiners` : ç”±äºæ¯ä¸ªåˆ†åŒºéƒ½æ˜¯ç‹¬ç«‹å¤„ç†,å› æ­¤å¯¹äºåŒä¸€ä¸ªé”®å¯ä»¥æœ‰å¤šä¸ªç´¯åŠ å™¨,å¦‚æœæœ‰ä¸¤ä¸ªæˆ–è€…æ›´å¤šçš„åˆ†åŒºéƒ½æœ‰å¯¹åº”åŒä¸€ä¸ªé”®çš„ç´¯åŠ å™¨,å°±éœ€è¦ä½¿ç”¨ç”¨æˆ·æä¾›çš„mergeCombiners()æ–¹æ³•å°†å„ä¸ªåˆ†åŒºçš„ç»“æœè¿›è¡Œåˆå¹¶.
```
scala> rdd.combineByKey((_,1),(acc:(Int,Int),v)=>(acc._1+v,acc._2+1),(acc1:(Int,Int),acc2:(Int,Int))=>(acc1._1+acc2._1,acc1._2+acc2._2)).collect
res15: Array[(String, (Int, Int))] = Array((b,(3,1)), (a,(5,2)), (c,(18,3)))    
scala> 
```

###### 1.3.2.3.3.8 `sortByKey([ascending],[numTasks])` Method
- ä½œç”¨ : åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨,Kå¿…é¡»å®ç°Orderedæ¥å£,è¿”å›ä¸€ä¸ªæŒ‰ç…§keyè¿›è¡Œæ’åºçš„(K,V)çš„RDD
```
scala>  rdd.sortByKey().collect
res17: Array[(String, Int)] = Array((a,3), (a,2), (b,3), (c,6), (c,8), (c,4))

scala> rdd.sortByKey(false).collect
res19: Array[(String, Int)] = Array((c,4), (c,6), (c,8), (b,3), (a,3), (a,2))
scala> 
```

###### 1.3.2.3.3.9 `mapValues` Method
- é’ˆå¯¹äº(K,V)å½¢å¼çš„ç±»å‹åªå¯¹Vè¿›è¡Œæ“ä½œ
```
scala> rdd.mapValues(_*2).collect
res20: Array[(String, Int)] = Array((a,6), (a,4), (c,8), (b,6), (c,12), (c,16))
scala>
```

###### 1.3.2.3.3.10 `join(otherDataset,[numTasks])` Method
- ä½œç”¨ : åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ªç›¸åŒkeyå¯¹åº”çš„æ‰€æœ‰å…ƒç´ å¯¹åœ¨ä¸€èµ·çš„(K,(V,W))çš„RDD
```
scala> val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[20] at parallelize at <console>:24

scala> val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))
rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[21] at parallelize at <console>:24

scala> rdd.join(rdd1).collect
res21: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) 

scala> rdd.leftOuterJoin(rdd1).collect
res22: Array[(Int, (String, Option[Int]))] = Array((1,(a,Some(4))), (2,(b,Some(5))), (3,(c,Some(6))))

scala> rdd.rightOuterJoin(rdd1).collect
res23: Array[(Int, (Option[String], Int))] = Array((1,(Some(a),4)), (2,(Some(b),5)), (3,(Some(c),6)))
scala> 
```

###### 1.3.2.3.3.11 `cogroup(otherDataset,[numTasks])` Method
- ä½œç”¨ : åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ª`(K,(Iterable<V>,Iterable<W>))`ç±»å‹çš„RDD
```
scala> rdd.cogroup(rdd1).collect
res24: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))

scala>
```

##### 1.3.2.4 Action
###### 1.3.2.4.1 `reduce(func)` Method
- ä½œç”¨ : é€šè¿‡funcå‡½æ•°èšé›†RDDä¸­çš„æ‰€æœ‰å…ƒç´ ,å…ˆèšåˆåˆ†åŒºå†…æ•°æ®,å†èšåˆåˆ†åŒºé—´æ•°æ®
```
scala> val rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at <console>:24

scala> rdd.reduce(_+_)
res25: Int = 55
scala>
```

###### 1.3.2.4.2 `collect()` Method
- ä½œç”¨ : åœ¨é©±åŠ¨ç¨‹åºä¸­,ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´ 
```
scala> rdd.collect
res26: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
scala>
```

###### 1.3.2.4.3 `count()` Method
- ä½œç”¨ : è¿”å›RDDä¸­å…ƒç´ çš„ä¸ªæ•°
```
scala> rdd.count
res27: Long = 10
scala> 
```

###### 1.3.2.4.4 `first()` Method
- ä½œç”¨ : è¿”å›RDDä¸­ç¬¬ä¸€ä¸ªå…ƒç´ 
```
scala> rdd.first
res28: Int = 1
scala> 
```

###### 1.3.2.4.5 `take(n)` Method
- ä½œç”¨ : è¿”å›ä¸€ä¸ªç”±RDDå‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„
```
scala> rdd.take(2)
res30: Array[Int] = Array(1, 2)
scala> 
```

###### 1.3.2.4.6 `takeOrdered(n)` Method
- ä½œç”¨ : è¿”å›è¯¥RDDæ’åºåçš„å‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„
```
scala> rdd.takeOrdered(3)
res31: Array[Int] = Array(1, 2, 3) 
scala> 
```

###### 1.3.2.4.7 `aggregate` Method
> å‚æ•° : `(zeroValue: U)(seqOp: (U, T) â‡’U, combOp: (U, U) â‡’U)`
> 
> ä½œç”¨ : aggregateå‡½æ•°å°†æ¯ä¸ªåˆ†åŒºé‡Œé¢çš„å…ƒç´ é€šè¿‡seqOpå’Œåˆå§‹å€¼è¿›è¡Œèšåˆ,ç„¶åç”¨combineå‡½æ•°å°†æ¯ä¸ªåˆ†åŒºçš„ç»“æœå’Œåˆå§‹å€¼(zeroValue)è¿›è¡Œcombineæ“ä½œ,è¿™ä¸ªå‡½æ•°æœ€ç»ˆè¿”å›çš„ç±»å‹ä¸éœ€è¦å’ŒRDDä¸­å…ƒç´ ç±»å‹ä¸€è‡´.
```
scala> rdd.aggregate(0)(_+_,_+_)
res32: Int = 55
scala>
```

###### 1.3.2.4.8 `fold(num)(func)` Method
- ä½œç”¨ : æŠ˜å æ“ä½œ,aggregateçš„ç®€åŒ–æ“ä½œ,seqopå’Œcombopä¸€æ ·
```
scala> rdd.fold(0)(_+_)
res34: Int = 55
scala> 
```

###### 1.3.2.4.9 `saveAsTextFile(path)` Method
- ä½œç”¨ : å°†æ•°æ®é›†å…ƒç´ ä»¥textfileçš„å½¢å¼ä¿å­˜åˆ°HDFSæ–‡ä»¶ç³»ç»Ÿæˆ–è€…å…¶ä»–æ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿ,å¯¹äºæ¯ä¸ªå…ƒç´ ,Sparkå°†ä¼šè°ƒç”¨toStringæ–¹æ³•,å°†å®ƒè£…æ¢ä¸ºæ–‡ä»¶ä¸­çš„æ–‡æœ¬.

###### 1.3.2.4.10 `saveAsSequenceFile(path)` Method
- ä½œç”¨ : å°†æ•°æ®é›†ä¸­çš„å…ƒç´ ä»¥Hadoop sequencefileæ ¼å¼ä¿å­˜åˆ°æŒ‡å®šç›®å½•ä¸‹,å¯ä»¥ä½¿HDFSæˆ–è€…å…¶ä»–Hadoopæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿ.

###### 1.3.2.4.11 `saveAsObjectFile(path)` Method
- ä½œç”¨ : ç”¨äºå°†RDDä¸­å…ƒç´ åºåˆ—åŒ–æˆå¯¹è±¡,å­˜å‚¨åˆ°æ–‡ä»¶ä¸­.

###### 1.3.2.4.12 `countByKey()` Method
- ä½œç”¨ : é’ˆå¯¹(K,V)ç±»å‹RDD,è¿”å›ä¸€ä¸ª(K,Int)çš„map,è¡¨ç¤ºæ¯ä¸€ä¸ªkeyå¯¹åº”çš„å…ƒç´ ä¸ªæ•°.
```
scala> val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[35] at parallelize at <console>:24

scala> rdd.countByKey
res35: scala.collection.Map[Int,Long] = Map(3 -> 2, 1 -> 3, 2 -> 1)
scala> 
```

###### 1.3.2.4.13 `foreach(func)` Method
- ä½œç”¨ : åœ¨æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Š,è¿è¡Œå‡½æ•°funcè¿›è¡Œæ›´æ–°
```
scala> rdd.foreach(print)
```


##### 1.3.2.5 RDD å‡½æ•°ä¼ é€’
> åœ¨å®é™…å¼€å‘ä¸­å¾€å¾€éœ€è¦å¼€å‘è€…å®šä¹‰ä¸€äº›å¯¹äºRDDæ“ä½œ,é‚£ä¹ˆæ­¤æ—¶éœ€è¦ä¸»è¦çš„æ˜¯,åˆå§‹åŒ–å·¥ä½œæ˜¯åœ¨Driverç«¯è¿›è¡Œ,è€Œå®é™…è¿è¡Œç¨‹åºæ˜¯åœ¨Executorç«¯è¿›è¡Œ,è¿™å°±æ¶‰åŠåˆ°äº†è·¨è¿›ç¨‹é€šä¿¡,è·¨è¿›ç¨‹é€šä¿¡æ˜¯éœ€è¦åºåˆ—åŒ–æ“ä½œ.

###### 1.3.2.5.1 ä¼ é€’æ–¹æ³•
> åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æ‰€è°ƒç”¨çš„æ–¹æ³•`isMatch()`æ˜¯å®šä¹‰åœ¨Searchè¿™ä¸ªç±»ä¸­,å®é™…ä¸Šè°ƒç”¨çš„æ˜¯`this.isMatch()`,`this`è¡¨ç¤ºSearchè¿™ä¸ªç±»çš„å¯¹è±¡,ç¨‹åºåœ¨è¿è¡Œè¿‡ç¨‹ä¸­éœ€è¦å°†Searchå¯¹è±¡åºåˆ—åŒ–ä»¥åä¼ é€’åˆ°Executorç«¯.
###### 1.3.2.5.2 ä¼ é€’å±æ€§
> åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æ‰€è°ƒç”¨çš„æ–¹æ³•`query`æ˜¯å®šä¹‰åœ¨Searchè¿™ä¸ªç±»ä¸­çš„å­—æ®µ,å®é™…ä¸Šè°ƒç”¨çš„æ˜¯`this.query`,thisè¡¨ç¤ºSearchè¿™ä¸ªç±»çš„å¯¹è±¡,ç¨‹åºåœ¨è¿è¡Œè¿‡ç¨‹ä¸­éœ€è¦å°†Searchå¯¹è±¡åºåˆ—åŒ–ä»¥åä¼ é€’åˆ°Executorç«¯.

- Create `Search.scala`
``` scala
package com.geekparkhub.core.spark.application.methods
import org.apache.spark.rdd.RDD

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * Search
  * <p>
  */

class Search(query: String) extends Serializable {

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²æ•°æ®
  def isMatch(s: String): Boolean = {
    s.contains(query)
  }

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²RDD
  def getMatch1(rdd: RDD[String]): RDD[String] = {
    rdd.filter(isMatch)
  }

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²RDD
  def getMatche2(rdd: RDD[String]): RDD[String] = {
    rdd.filter(x => x.contains(query))
  }
}
```

- Create `TransFormAction.scala`
``` scala
package com.geekparkhub.core.spark.application.methods
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * TransFormAction
  * <p>
  */

object TransFormAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("TransFormAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val word: RDD[String] = sc.parallelize(Array("abc", "dcd"))

    // åˆ›å»ºSearchå¯¹è±¡
    val search = new Search("a")

    // è°ƒç”¨æ–¹æ³•
    val searched: RDD[String] = search.getMatch1(word)

    // å¾ªç¯è¾“å‡º
    searched.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

##### 1.3.2.6 RDD ä¾èµ–å…³ç³»
###### 1.3.2.6.1 Lineage
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_016.jpg)

> RDDåªæ”¯æŒç²—ç²’åº¦è½¬æ¢,å³åœ¨å¤§é‡è®°å½•ä¸Šæ‰§è¡Œçš„å•ä¸ªæ“ä½œ,å°†åˆ›å»ºRDDçš„ä¸€ç³»åˆ—Lineage(è¡€ç»Ÿ)è®°å½•ä¸‹æ¥,ä»¥ä¾¿æ¢å¤ä¸¢å¤±çš„åˆ†åŒº,RDDçš„Lineageä¼šè®°å½•RDDçš„å…ƒæ•°æ®ä¿¡æ¯å’Œè½¬æ¢è¡Œä¸º,å½“è¯¥RDDçš„éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶,å®ƒå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯æ¥é‡æ–°è¿ç®—å’Œæ¢å¤ä¸¢å¤±çš„æ•°æ®åˆ†åŒº.

- åˆ›å»ºRDDä¾èµ–å…³ç³»
```
scala> sc.textFile("/core_flow/spark/input/wordcount/wordcount_001.txt")
res0: org.apache.spark.rdd.RDD[String] = /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25

scala> res0.flatMap(_.split(" "))
res2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:27

scala> res2.map((_,1))
res3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:29

scala> res3.reduceByKey(_+_)
res4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:31

scala>
```
- åˆ†åˆ«æŸ¥çœ‹å››ä¸ªRDDä¾èµ–å…³ç³»
- res0.toDebugString
```
scala> res0.toDebugString
res5: String =
(2) /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res2.toDebugString
```
scala> res2.toDebugString
res6: String =
(2) MapPartitionsRDD[2] at flatMap at <console>:27 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res3.toDebugString
```
scala> res3.toDebugString
res7: String =
(2) MapPartitionsRDD[3] at map at <console>:29 []
 |  MapPartitionsRDD[2] at flatMap at <console>:27 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res4.toDebugString
```
scala> res4.toDebugString
res8: String =
(2) ShuffledRDD[4] at reduceByKey at <console>:31 []
 +-(2) MapPartitionsRDD[3] at map at <console>:29 []
    |  MapPartitionsRDD[2] at flatMap at <console>:27 []
    |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
    |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
###### 1.3.2.6.2 çª„ä¾èµ–
- çª„ä¾èµ–æŒ‡çš„æ˜¯æ¯ä¸€ä¸ªçˆ¶RDDçš„Partitionæœ€å¤šè¢«å­RDDçš„ä¸€ä¸ªPartitionä½¿ç”¨.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_017.jpg)


###### 1.3.2.6.3 å®½ä¾èµ–
- å®½ä¾èµ–æŒ‡çš„æ˜¯å¤šä¸ªå­RDDçš„Partitionä¼šä¾èµ–åŒä¸€ä¸ªçˆ¶RDDçš„Partition,ä¼šå¼•èµ·shuffleè¿‡ç¨‹.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_018.jpg)

###### 1.3.2.6.4 DAG
- DAG(Directed Acyclic Graph)å«åšæœ‰å‘æ— ç¯å›¾,åŸå§‹çš„RDDé€šè¿‡ä¸€ç³»åˆ—çš„è½¬æ¢å°±å°±å½¢æˆäº†DAG,æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»çš„ä¸åŒå°†DAGåˆ’åˆ†æˆä¸åŒçš„Stage.
- å¯¹äºçª„ä¾èµ–,partitionçš„è½¬æ¢å¤„ç†åœ¨Stageä¸­å®Œæˆè®¡ç®—,å¯¹äºå®½ä¾èµ–,ç”±äºæœ‰Shuffleçš„å­˜åœ¨,åªèƒ½åœ¨parent RDDå¤„ç†å®Œæˆå,æ‰èƒ½å¼€å§‹æ¥ä¸‹æ¥çš„è®¡ç®—,å› æ­¤å®½ä¾èµ–æ˜¯åˆ’åˆ†Stageä¾æ®.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_019.jpg)


###### 1.3.2.6.5 ä»»åŠ¡åˆ’åˆ†(é‡ç‚¹)
- RDDä»»åŠ¡åˆ‡åˆ†ä¸­é—´åˆ†ä¸º : `Application` / `Job` / `Stage` /  `Task`
- Application : åˆå§‹åŒ–ä¸€ä¸ªSparkContextå³ç”Ÿæˆä¸€ä¸ªApplication.
- Job : ä¸€ä¸ªActionç®—å­å°±ä¼šç”Ÿæˆä¸€ä¸ªJob.
- Stage : æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»çš„ä¸åŒå°†Jobåˆ’åˆ†æˆä¸åŒçš„Stage,é‡åˆ°ä¸€ä¸ªå®½ä¾èµ–åˆ™åˆ’åˆ†ä¸€ä¸ªStage.
- Task : Stageæ˜¯ä¸€ä¸ªTaskSet,å°†Stageåˆ’åˆ†çš„ç»“æœå‘é€åˆ°ä¸åŒçš„Executoræ‰§è¡Œå³ä¸ºä¸€ä¸ªTask.
- Application`->`Job`->`Stage`->`Task æ¯ä¸€å±‚éƒ½æ˜¯1å¯¹nçš„å…³ç³»

##### 1.3.2.6.7 RDDç¼“å­˜
- RDDé€šè¿‡persistæ–¹æ³•æˆ–cacheæ–¹æ³•å¯ä»¥å°†å‰é¢çš„è®¡ç®—ç»“æœç¼“å­˜,é»˜è®¤æƒ…å†µä¸‹`persist()`ä¼šæŠŠæ•°æ®ä»¥åºåˆ—åŒ–å½¢å¼ç¼“å­˜åœ¨JVM çš„å †ç©ºé—´ä¸­.
- ä½†æ˜¯å¹¶ä¸æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ç«‹å³ç¼“å­˜,è€Œæ˜¯è§¦å‘åé¢çš„actionæ—¶,è¯¥RDDå°†ä¼šè¢«ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹çš„å†…å­˜ä¸­,å¹¶ä¾›åé¢é‡ç”¨
- ç¼“å­˜æœ‰å¯èƒ½ä¸¢å¤±æˆ–è€…å­˜å‚¨å­˜å‚¨äºå†…å­˜çš„æ•°æ®ç”±äºå†…å­˜ä¸è¶³è€Œè¢«åˆ é™¤,RDDçš„ç¼“å­˜å®¹é”™æœºåˆ¶ä¿è¯äº†å³ä½¿ç¼“å­˜ä¸¢å¤±ä¹Ÿèƒ½ä¿è¯è®¡ç®—çš„æ­£ç¡®æ‰§è¡Œ,é€šè¿‡åŸºäºRDDçš„ä¸€ç³»åˆ—è½¬æ¢,ä¸¢å¤±çš„æ•°æ®ä¼šè¢«é‡ç®—,ç”±äºRDDçš„å„ä¸ªPartitionæ˜¯ç›¸å¯¹ç‹¬ç«‹,å› æ­¤åªéœ€è¦è®¡ç®—ä¸¢å¤±çš„éƒ¨åˆ†å³å¯,å¹¶ä¸éœ€è¦é‡ç®—å…¨éƒ¨Partition.

##### 1.3.2.6.8 RDDCheckPoint
- Sparkä¸­å¯¹äºæ•°æ®çš„ä¿å­˜é™¤äº†æŒä¹…åŒ–æ“ä½œä¹‹å¤–,è¿˜æä¾›äº†ä¸€ç§æ£€æŸ¥ç‚¹çš„æœºåˆ¶,æ£€æŸ¥ç‚¹(æœ¬è´¨æ˜¯é€šè¿‡å°†RDDå†™å…¥Diskåšæ£€æŸ¥ç‚¹)æ˜¯ä¸ºäº†é€šè¿‡lineageåšå®¹é”™çš„è¾…åŠ©,lineageè¿‡é•¿ä¼šé€ æˆå®¹é”™æˆæœ¬è¿‡é«˜,è¿™æ ·å°±ä¸å¦‚åœ¨ä¸­é—´é˜¶æ®µåšæ£€æŸ¥ç‚¹å®¹é”™,å¦‚æœä¹‹åæœ‰èŠ‚ç‚¹å‡ºç°é—®é¢˜è€Œä¸¢å¤±åˆ†åŒº,ä»åšæ£€æŸ¥ç‚¹çš„RDDå¼€å§‹é‡åšLineage,å°±ä¼šå‡å°‘å¼€é”€,æ£€æŸ¥ç‚¹é€šè¿‡å°†æ•°æ®å†™å…¥åˆ°HDFSæ–‡ä»¶ç³»ç»Ÿå®ç°äº†RDDçš„æ£€æŸ¥ç‚¹åŠŸèƒ½.
- ä¸ºå½“å‰RDDè®¾ç½®æ£€æŸ¥ç‚¹,è¯¥å‡½æ•°å°†ä¼šåˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶çš„æ–‡ä»¶,å¹¶å­˜å‚¨åˆ°checkpointç›®å½•ä¸­,è¯¥ç›®å½•æ˜¯ç”¨SparkContext.setCheckpointDir()è®¾ç½®çš„,åœ¨checkpointçš„è¿‡ç¨‹ä¸­,è¯¥RDDæ‰€æœ‰ä¾èµ–äºçˆ¶RDDä¸­çš„ä¿¡æ¯å°†å…¨éƒ¨è¢«ç§»é™¤,å¯¹RDDè¿›è¡Œcheckpointæ“ä½œå¹¶ä¸ä¼šé©¬ä¸Šè¢«æ‰§è¡Œ,å¿…é¡»æ‰§è¡ŒActionæ“ä½œæ‰èƒ½è§¦å‘.
- è®¾ç½®æ£€æŸ¥ç‚¹
```
scala> sc.setCheckpointDir("hdfs://systemhub511:9000/core_flow/spark/checkpoint")
```
- åˆ›å»ºRDD
```
scala>  val rdd = sc.parallelize(Array("systemhub511"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:24
```
- å°†RDDè½¬æ¢ä¸ºæºå¸¦å½“å‰æ—¶é—´æˆ³å¹¶åšcheckpoint
```
scala> val check = rdd.map(_+System.currentTimeMillis)
check: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at <console>:26
scala> 
```
- å¤šæ¬¡æ‰“å°ç»“æœ
```
scala> check.collect
res10: Array[String] = Array(systemhub5111559138263898)

scala> check.collect
res11: Array[String] = Array(systemhub5111559138266443)

scala> check.collect
res12: Array[String] = Array(systemhub5111559138267862)

scala> 
```

#### 1.3.3 Key-Value RDD æ•°æ®åˆ†åŒº
- Sparkç›®å‰æ”¯æŒHashåˆ†åŒºå’ŒRangeåˆ†åŒº,å¼€å‘è€…ä¹Ÿå¯ä»¥è‡ªå®šä¹‰åˆ†åŒº,Hashåˆ†åŒºä¸ºå½“å‰é»˜è®¤åˆ†åŒº,Sparkä¸­åˆ†åŒºå™¨ç›´æ¥å†³å®šäº†RDDä¸­åˆ†åŒºçš„ä¸ªæ•°ã€RDDä¸­æ¯æ¡æ•°æ®ç»è¿‡Shuffleè¿‡ç¨‹å±äºå“ªä¸ªåˆ†åŒºå’ŒReduceçš„ä¸ªæ•°.

##### 1.3.3.1 è·å–RDD åˆ†åŒº
- æŸ¥çœ‹RDDåˆ†åŒºå™¨
```
scala> rdd.partitioner
res14: Option[org.apache.spark.Partitioner] = None
```
##### 1.3.3.2 Hash åˆ†åŒº
- HashPartitioneråˆ†åŒºçš„åŸç† : å¯¹äºç»™å®šçš„key,è®¡ç®—å…¶hashCode,å¹¶é™¤ä»¥åˆ†åŒºä¸ªæ•°å–ä½™,å¦‚æœä½™æ•°å°äº0,åˆ™ç”¨ä½™æ•°+åˆ†åŒºçš„ä¸ªæ•°,å¦åˆ™åŠ 0,æœ€åè¿”å›çš„å€¼å°±æ˜¯è¿™ä¸ªkeyæ‰€å±çš„åˆ†åŒºID.
- Hashåˆ†åŒºå®æ“
```
scala> val nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)
nopar: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:25

scala> nopar.mapPartitionsWithIndex((index,iter)=>{Iterator(index.toString+":"+iter.mkString("|"))}).collect
res15: Array[String] = Array(0:, 1:(1,3), 2:(1,2), 3:(2,4), 4:, 5:(2,3), 6:(3,6), 7:(3,8))

scala> val hashpar = nopar.partitionBy(new org.apache.spark.HashPartitioner(7))
hashpar: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[9] at partitionBy at <console>:27

scala> hashpar.count
res20: Long = 6

scala> hashpar.partitioner
res21: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@7)

scala> hashpar.mapPartitions(iter => Iterator(iter.length)).collect()
res22: Array[Int] = Array(0, 2, 2, 2, 0, 0, 0)
scala> 
```

##### 1.3.3.3 Ranger åˆ†åŒº
- HashPartitioneråˆ†åŒº`å¼Šç«¯` : å¯èƒ½å¯¼è‡´æ¯ä¸ªåˆ†åŒºä¸­æ•°æ®é‡ä¸å‡åŒ€,æç«¯æƒ…å†µä¸‹ä¼šå¯¼è‡´æŸäº›åˆ†åŒºæ‹¥æœ‰RDDå…¨éƒ¨æ•°æ®.
- RangePartitionerä½œç”¨ : å°†ä¸€å®šèŒƒå›´å†…æ•°æ˜ å°„åˆ°æŸä¸€ä¸ªåˆ†åŒºå†…,å°½é‡ä¿è¯æ¯ä¸ªåˆ†åŒºä¸­æ•°æ®é‡å‡åŒ€,è€Œä¸”åˆ†åŒºä¸åˆ†åŒºä¹‹é—´æ˜¯æœ‰åº,ä¸€ä¸ªåˆ†åŒºä¸­å…ƒç´ è‚¯å®šéƒ½æ˜¯æ¯”å¦ä¸€ä¸ªåˆ†åŒºå†…å…ƒç´ å°æˆ–è€…å¤§,ä½†æ˜¯åˆ†åŒºå†…å…ƒç´ æ˜¯ä¸èƒ½ä¿è¯é¡ºåº,ç®€å•çš„è¯´å°±æ˜¯å°†ä¸€å®šèŒƒå›´å†…çš„æ•°æ˜ å°„åˆ°æŸä¸€ä¸ªåˆ†åŒºå†….
- å®ç°è¿‡ç¨‹ : 
- 1.å…ˆä»æ•´ä¸ªRDDä¸­æŠ½å–å‡ºæ ·æœ¬æ•°æ®,å°†æ ·æœ¬æ•°æ®æ’åº,è®¡ç®—å‡ºæ¯ä¸ªåˆ†åŒºæœ€å¤§keyå€¼,å½¢æˆä¸€ä¸ª`Array[KEY]`ç±»å‹çš„æ•°ç»„å˜é‡`rangeBounds`.
- 2.åˆ¤æ–­keyåœ¨`rangeBounds`ä¸­æ‰€å¤„çš„èŒƒå›´,ç»™å‡ºè¯¥keyå€¼åœ¨ä¸‹ä¸€ä¸ªRDDä¸­åˆ†åŒºidä¸‹æ ‡,è¯¥åˆ†åŒºå™¨è¦æ±‚RDDä¸­KEYç±»å‹å¿…é¡»æ˜¯å¯æ’åº.


##### 1.3.3.4 è‡ªå®šä¹‰ åˆ†åŒº
- è¦å®ç°è‡ªå®šä¹‰åˆ†åŒºå™¨,éœ€è¦ç»§æ‰¿`org.apache.spark.Partitioner`ç±»å¹¶å®ç°ä¸‹é¢ä¸‰ä¸ªæ–¹æ³•
- 1.`numPartitions: Int` : è¿”å›åˆ›å»ºå‡ºæ¥çš„åˆ†åŒºæ•°
- 2.`getPartition(key: Any): Int` :  è¿”å›ç»™å®šé”®çš„åˆ†åŒºç¼–å·(0åˆ°numPartitions-1)
- 3.`equals()` : Java åˆ¤æ–­ç›¸ç­‰æ€§çš„æ ‡å‡†æ–¹æ³•,è¿™ä¸ªæ–¹æ³•çš„å®ç°éå¸¸é‡è¦,Sparkéœ€è¦ç”¨è¿™ä¸ªæ–¹æ³•æ¥æ£€æŸ¥åˆ†åŒºå™¨å¯¹è±¡æ˜¯å¦å’Œå…¶ä»–åˆ†åŒºå™¨å®ä¾‹ç›¸åŒ,è¿™æ ·Sparkæ‰å¯ä»¥åˆ¤æ–­ä¸¤ä¸ªRDDçš„åˆ†åŒºæ–¹å¼æ˜¯å¦ç›¸åŒ.
- 4.å®šä¹‰è‡ªå®šä¹‰åˆ†åŒºç±» | Create `CustomerPartitioner.scala`
``` scala
package com.geekparkhub.core.spark.application.partitioner

import org.apache.spark.Partitioner

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * CustomerPartitioner
  * <p>
  */

class CustomerPartitioner(partitions: Int) extends Partitioner {
  override def numPartitions: Int = partitions
  override def getPartition(key: Any): Int = {
    0
  }
}
```
- Create `PartitionerAction.scala`
```
package com.geekparkhub.core.spark.application.partitioner

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * PartitionerAction
  * <p>
  */

object PartitionerAction {

  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("TransFormAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val word: RDD[String] = sc.parallelize(Array("abc", "dcd"))

    // å°†å…ƒç´ è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOne: RDD[(String, Int)] = word.map((_, 1))

    // è‡ªå®šä¹‰åˆ†åŒº
    val partitioned: RDD[(String, Int)] = wordAndOne.partitionBy(new CustomerPartitioner(2))

    // æŸ¥çœ‹åˆ†åŒºååˆ†åŒºç»“æœ
    val indexAndData: RDD[(Int, (String, Int))] = partitioned.mapPartitionsWithIndex((i,t)=>t.map((i,_)))

    // æ‰“å°æ•°æ®
    indexAndData.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
- Log Println
```
(0,(abc,1))
(0,(dcd,1))
```

#### 1.3.4 æ•°æ®è¯»å–&ä¿å­˜
- Sparkæ•°æ®è¯»å–åŠæ•°æ®ä¿å­˜å¯ä»¥ä»ä¸¤ä¸ªç»´åº¦æ¥ä½œåŒºåˆ† : æ–‡ä»¶æ ¼å¼ä»¥åŠæ–‡ä»¶ç³»ç»Ÿ
- æ–‡ä»¶æ ¼å¼åˆ†ä¸º : Textæ–‡ä»¶ / Jsonæ–‡ä»¶ / Csvæ–‡ä»¶ / Sequenceæ–‡ä»¶ä»¥åŠObjectæ–‡ä»¶
- æ–‡ä»¶ç³»ç»Ÿåˆ†ä¸º : æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ / HDFS / HBASEä»¥åŠæ•°æ®åº“

##### 1.3.4.1 æ–‡ä»¶ç±»æ•°æ®è¯»å–&ä¿å­˜
###### 1.3.4.1 Text File
- 1.æ•°æ®è¯»å– : `textFile(String)` 
```
scala> sc.textFile("hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt")
res23: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[12] at textFile at <console>:26

scala> res23.toDebugString
res25: String =
(2) hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[12] at textFile at <console>:26 []
 |  hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[11] at textFile at <console>:26 []
scala> 
```
- 2.æ•°æ®ä¿å­˜ : `saveAsTextFile(String)`
```
scala> hdfsFile.saveAsTextFile("/core_flow/spark/output/wordcount/")
```

###### 1.3.4.2 Json File
- å¦‚æœJSONæ–‡ä»¶ä¸­æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªJSONè®°å½•,é‚£ä¹ˆå¯ä»¥é€šè¿‡å°†JSONæ–‡ä»¶å½“åšæ–‡æœ¬æ–‡ä»¶æ¥è¯»å–,ç„¶ååˆ©ç”¨ç›¸å…³çš„JSONåº“å¯¹æ¯ä¸€æ¡æ•°æ®è¿›è¡ŒJSONè§£æ.
- ä½¿ç”¨RDDè¯»å–JSONæ–‡ä»¶å¤„ç†å¾ˆå¤æ‚,åŒæ—¶SparkSQLé›†æˆäº†å¾ˆå¥½çš„å¤„ç†JSONæ–‡ä»¶æ–¹å¼,æ‰€ä»¥åº”ç”¨ä¸­å¤šæ˜¯é‡‡ç”¨SparkSQLå¤„ç†JSONæ–‡ä»¶.
- 1.å¯¼å…¥è§£æjsonæ‰€éœ€åŒ…å
```
scala> import scala.util.parsing.json.JSON
import scala.util.parsing.json.JSON
scala> 
```
- 2.åœ¨HDFSåˆ›å»ºå­˜æ”¾JSONç›®å½•
```
[root@systemhub511 ~]# hadoop fs -mkdir -p /core_flow/spark/json/001
```
- 3.ä¸Šä¼ jsonæ–‡ä»¶åˆ°HDFS
```
[root@systemhub511 ~]# hadoop fs -put /opt/module/spark/examples/src/main/resources/people.json /core_flow/spark/json/001/
```
- 4.è¯»å–æ–‡ä»¶
```
scala> val json = sc.textFile("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
json: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/json/001/people.json MapPartitionsRDD[14] at textFile at <console>:26
scala>
```
- 5.è§£æjsonæ•°æ®
```
scala> val result = json.map(JSON.parseFull)
result: org.apache.spark.rdd.RDD[Option[Any]] = MapPartitionsRDD[15] at map at <console>:28
scala>
```
- 6.æ‰“å°è§£æç»“æœ
```
scala> result.collect
res26: Array[Option[Any]] = Array(Some(Map(name -> Michael)), Some(Map(name -> Andy, age -> 30.0)), Some(Map(name -> Justin, age -> 19.0)))
scala> 
```

###### 1.3.4.3 Sequence File
- SequenceFileæ–‡ä»¶æ˜¯Hadoopç”¨æ¥å­˜å‚¨äºŒè¿›åˆ¶å½¢å¼çš„key-valueå¯¹è€Œè®¾è®¡ä¸€ç§å¹³é¢æ–‡ä»¶(FlatFile).
- Sparkæœ‰ä¸“é—¨ç”¨æ¥è¯»å–SequenceFileæ¥å£,åœ¨SparkContextä¸­,å¯ä»¥è°ƒç”¨`sequenceFile[keyClass, valueClass](path)` | SequenceFileæ–‡ä»¶åªé’ˆå¯¹`PairRDD`
- 1.åˆ›å»ºRDD
```
scala>  val rdd = sc.parallelize(Array((1,2),(3,4),(5,6)))
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[16] at parallelize at <console>:26
scala> 
```
- 2.å°†RDDä¿å­˜ä¸ºSequenceæ–‡ä»¶
```
scala> rdd.saveAsSequenceFile("file:///opt/module/spark/seqFile")
```
- 3.æŸ¥çœ‹è¯¥æ–‡ä»¶
```
[root@systemhub511 ~]# cd /opt/module/spark/seqFile/
[root@systemhub511 seqFile]# ll -a
æ€»ç”¨é‡ 28
drwxr-xr-x.  2 root          root          4096 5æœˆ  29 23:57 .
drwxr-xr-x. 21 geekdeveloper geekdeveloper 4096 5æœˆ  30 00:05 ..
-rw-r--r--.  1 root          root            92 5æœˆ  29 23:57 part-00000
-rw-r--r--.  1 root          root            12 5æœˆ  29 23:57 .part-00000.crc
-rw-r--r--.  1 root          root           108 5æœˆ  29 23:57 part-00003
-rw-r--r--.  1 root          root            12 5æœˆ  29 23:57 .part-00003.crc
-rw-r--r--.  1 root          root             0 5æœˆ  29 23:57 _SUCCESS
-rw-r--r--.  1 root          root             8 5æœˆ  29 23:57 ._SUCCESS.crc
[root@systemhub511 seqFile]# cat part-00000
SEQ org.apache.hadoop.io.IntWritable org.apache.hadoop.io.IntWritabler[-oï¿½ï¿½ï¿½]hï¿½~uï¿½ï¿½ï¿½
[root@systemhub511 seqFile]#
```
- 4.è¯»å–Sequenceæ–‡ä»¶
```
scala>  val seq = sc.sequenceFile[Int,Int]("file:///opt/module/spark/seqFile")
seq: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at sequenceFile at <console>:26
scala>
```
- 5.æ‰“å°è¯»å–åçš„Sequenceæ–‡ä»¶
```
scala> seq.collect
res14: Array[(Int, Int)] = Array((1,2), (3,4), (5,6))
```

###### 1.3.4.4 ObjectFile
- å¯¹è±¡æ–‡ä»¶æ˜¯å°†å¯¹è±¡åºåˆ—åŒ–åä¿å­˜æ–‡ä»¶,é‡‡ç”¨Javaåºåˆ—åŒ–æœºåˆ¶,å¯ä»¥é€šè¿‡`objectFile[k,v](path)`å‡½æ•°æ¥æ”¶ä¸€ä¸ªè·¯å¾„,è¯»å–å¯¹è±¡æ–‡ä»¶,è¿”å›å¯¹åº”RDD,ä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨`saveAsObjectFile()`å®ç°å¯¹å¯¹è±¡æ–‡ä»¶è¾“å‡º,å› ä¸ºæ˜¯åºåˆ—åŒ–æ‰€ä»¥è¦æŒ‡å®šç±»å‹.
- 1.åˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(1,2,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at <console>:26
scala> 
```
- 2.å°†RDDä¿å­˜ä¸ºObjectæ–‡ä»¶
```
scala> rdd.saveAsObjectFile("file:///opt/module/spark/objectFile")
```
- 3.æŸ¥çœ‹è¯¥æ–‡ä»¶
```
[root@systemhub511 ~]# cd /opt/module/spark/objectFile/
[root@systemhub511 objectFile]# ll
æ€»ç”¨é‡ 8
-rw-r--r--. 1 root root 138 5æœˆ  30 00:05 part-00000
-rw-r--r--. 1 root root 138 5æœˆ  30 00:05 part-00003
-rw-r--r--. 1 root root   0 5æœˆ  30 00:05 _SUCCESS
[root@systemhub511 objectFile]# cat part-00000
SEQ!org.apache.hadoop.io.NullWritable"org.apache.hadoop.io.BytesWritableï¿½ ï¿½Lï¿½hï¿½l:Tï¿½ï¿½ï¿½#ï¿½ï¿½ur[IMï¿½`&vê²¥xp
[root@systemhub511 objectFile]#
```
- 4.è¯»å–Objectæ–‡ä»¶
```
scala> val objFile = sc.objectFile[(Int)]("file:///opt/module/spark/objectFile")
objFile: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[24] at objectFile at <console>:26
scala>
```
- 5.æ‰“å°è¯»å–åçš„Sequenceæ–‡ä»¶
```
objFile.collect
res19: Array[Int] = Array(1, 2, 3, 4)
```

##### 1.3.4.2 æ–‡ä»¶ç³»ç»Ÿæ•°æ®è¯»å–&ä¿å­˜

###### 1.3.4.1 HDFS
> Sparkæ•´ä¸ªç”Ÿæ€ç³»ç»Ÿä¸Hadoopæ˜¯å®Œå…¨å…¼å®¹,æ‰€ä»¥å¯¹äºHadoopæ‰€æ”¯æŒçš„æ–‡ä»¶ç±»å‹æˆ–è€…æ•°æ®åº“ç±»å‹,Sparkä¹ŸåŒæ ·æ”¯æŒ.
> å¦å¤–ç”±äºHadoopçš„APIæœ‰æ–°æ—§ä¸¤ä¸ªç‰ˆæœ¬,æ‰€ä»¥Sparkä¸ºäº†èƒ½å¤Ÿå…¼å®¹Hadoopæ‰€æœ‰ç‰ˆæœ¬,ä¹Ÿæä¾›äº†ä¸¤å¥—åˆ›å»ºæ“ä½œæ¥å£.
> å¯¹äºå¤–éƒ¨å­˜å‚¨åˆ›å»ºæ“ä½œè€Œè¨€,hadoopRDDå’ŒnewHadoopRDDæ˜¯æœ€ä¸ºæŠ½è±¡çš„ä¸¤ä¸ªå‡½æ•°æ¥å£,ä¸»è¦åŒ…å«ä»¥ä¸‹å››ä¸ªå‚æ•°  : 
> 
> 1.`è¾“å…¥æ ¼å¼(InputFormat)` : åˆ¶å®šæ•°æ®è¾“å…¥ç±»å‹,å¦‚TextInputFormatç­‰,æ–°æ—§ä¸¤ä¸ªç‰ˆæœ¬æ‰€å¼•ç”¨ç‰ˆæœ¬åˆ†åˆ«æ˜¯`org.apache.hadoop.mapred.InputFormat`å’Œ`org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)`
> 2.é”®ç±»å‹ : æŒ‡å®š[K,V]é”®å€¼å¯¹ä¸­Kç±»å‹
> 3.å€¼ç±»å‹: æŒ‡å®š[K,V]é”®å€¼å¯¹ä¸­Vç±»å‹
> 4.åˆ†åŒºå€¼ : æŒ‡å®šç”±å¤–éƒ¨å­˜å‚¨ç”ŸæˆRDDçš„partitionæ•°é‡æœ€å°å€¼,å¦‚æœæ²¡æœ‰æŒ‡å®šç³»ç»Ÿä¼šä½¿ç”¨é»˜è®¤å€¼`defaultMinSplits`.
> 
> å…¶ä»–åˆ›å»ºæ“ä½œAPIæ¥å£éƒ½æ˜¯ä¸ºäº†æ–¹ä¾¿æœ€ç»ˆSparkç¨‹åºå¼€å‘è€…è€Œè®¾ç½®çš„,æ˜¯è¿™ä¸¤ä¸ªæ¥å£é«˜æ•ˆå®ç°ç‰ˆæœ¬,ä¾‹å¦‚å¯¹äºtextFileè€Œè¨€,åªæœ‰pathè¿™ä¸ªæŒ‡å®šæ–‡ä»¶è·¯å¾„å‚æ•°,å…¶ä»–å‚æ•°åœ¨ç³»ç»Ÿå†…éƒ¨æŒ‡å®šäº†é»˜è®¤å€¼.
> 1.åœ¨Hadoopä¸­ä»¥å‹ç¼©å½¢å¼å­˜å‚¨æ•°æ®,ä¸éœ€è¦æŒ‡å®šè§£å‹æ–¹å¼å°±èƒ½å¤Ÿè¿›è¡Œè¯»å–,å› ä¸ºHadoopæœ¬èº«æœ‰ä¸€ä¸ªè§£å‹å™¨ä¼šæ ¹æ®å‹ç¼©æ–‡ä»¶åç¼€æ¨æ–­è§£å‹ç®—æ³•è¿›è¡Œè§£å‹.
> 2.å¦‚æœç”¨Sparkä»Hadoopä¸­è¯»å–æŸç§ç±»å‹æ•°æ®ä¸çŸ¥é“æ€ä¹ˆè¯»å–çš„æ—¶å€™,ä¸Šç½‘æŸ¥æ‰¾ä¸€ä¸ªä½¿ç”¨map-reduceæ—¶å€™æ˜¯æ€ä¹ˆè¯»å–è¿™ç§è¿™ç§æ•°æ®,ç„¶åå†å°†å¯¹åº”çš„è¯»å–æ–¹å¼æ”¹å†™æˆä¸Šé¢çš„hadoopRDDå’ŒnewAPIHadoopRDDä¸¤ä¸ªç±»å³å¯.


###### 1.3.4.2 MySQLæ•°æ®åº“ è¿æ¥
- æ”¯æŒé€šè¿‡JavaJDBCè®¿é—®å…³ç³»å‹æ•°æ®åº“,éœ€è¦é€šè¿‡JdbcRDDè¿›è¡Œ
- 0.æ·»åŠ mysqlä¾èµ–
``` xml
<dependencies>
 <dependency>
  <groupId>mysql</groupId>
  <artifactId>mysql-connector-java</artifactId>
  <version>8.0.15</version>
 </dependency>
</dependencies>
```
- 1.Mysqlè¯»å– | Create `JDBCConnection.scala`
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import java.sql.DriverManager

import org.apache.spark.deploy.worker.DriverWrapper
import org.apache.spark.rdd.JdbcRDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * JDBCConnection
  * <p>
  */

object JDBCConnection {

  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("JDBCConnection")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // å®šä¹‰JDBCè¿æ¥å±æ€§ä¿¡æ¯
    val driver = "com.mysql.jdbc.Driver"
    val url = "jdbc:mysql://systemhub711:3306/company"
    val userName = "root"
    val passWd = "ax04854"

    // åˆ›å»ºJDBC RDD
    val JdbcRDD = new JdbcRDD[(Int, String)](sc, () => {
      Class.forName(driver)
      DriverManager.getConnection(url, userName, passWd)
    }, "select id,name from staff where ? <= id and id <= ?",
      1,
      10,
      1,
      x => {
        (x.getInt(1), x.getString(2))
      }
    )

    // æ‰“å°JdbcRDDç»“æœ
    JdbcRDD.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
- 2.Mysqlå†™å…¥ | Create `JBDCinsertData.scala`
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * JBDCinsertData
  * <p>
  */

object JBDCinsertData {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("JBDCRead")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºæ•°æ®
    val data = sc.parallelize(List("Female", "Male", "Female"))

    // è°ƒç”¨æ·»åŠ æ•°æ®æ–¹æ³•
    data.foreachPartition(insertData)
  }

  // æ·»åŠ æ•°æ®æ–¹æ³•
  def insertData(iterator: Iterator[String]): Unit = {
    Class.forName("com.mysql.jdbc.Driver").newInstance()
    val conn = java.sql.DriverManager.getConnection("jdbc:mysql://systemhub711:3306/company", "root", "000000")
    iterator.foreach(data => {
      val ps = conn.prepareStatement("insert into staff(name) values(?)")
      ps.setString(1, data)
      ps.executeUpdate()
    })
  }
}
```


###### 1.3.4.3 HBase æ•°æ®åº“
- ç”±äº`org.apache.hadoop.hbase.mapreduce.TableInputFormat`ç±»çš„å®ç°,Sparkå¯ä»¥é€šè¿‡Hadoopè¾“å…¥æ ¼å¼è®¿é—®HBase,è¿™ä¸ªè¾“å…¥æ ¼å¼ä¼šè¿”å›é”®å€¼å¯¹æ•°æ®,å…¶ä¸­é”®çš„ç±»å‹ä¸º`org. apache.hadoop.hbase.io.ImmutableBytesWritable`,è€Œå€¼çš„ç±»å‹ä¸º`org.apache.hadoop.hbase.client.Result`.
- 0.æ·»åŠ HBASEä¾èµ–
```xml
<dependency>
  <groupId>org.apache.hbase</groupId>
  <artifactId>hbase-server</artifactId>
  <version>1.3.1</version>
</dependency>
<dependency>
  <groupId>org.apache.hbase</groupId>
  <artifactId>hbase-client</artifactId>
  <version>1.3.1</version>
</dependency>
```
- 1.HBaseè¯»å–æ•°æ®
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.{NewHadoopRDD, RDD}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * HbaseConnection
  * <p>
  */

object HbaseConnection {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("HbaseConnection")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    //æ„å»ºHBaseé…ç½®ä¿¡æ¯
    val conf: Configuration = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", "systemhub511,systemhub611,systemhub711")
    conf.set(TableInputFormat.INPUT_TABLE, "test")

    // è¯»å–HBASEæ•°æ®
    val hbaseRDD = new NewHadoopRDD(sc, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result], conf)

    // è·å–RowKey
    val value: RDD[String] = hbaseRDD.map(x => Bytes.toString(x._2.getRow))

    // è¾“å‡ºæ•°æ®
    value.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

- 2.HBaseå†™å…¥æ•°æ®
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * HbaseWrite
  * <p>
  */

object HbaseWrite {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("HbaseWrite")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val initialRDD: RDD[(Int, String, Int)] = sc.parallelize(List((1, "apple", 11), (2, "banana", 12), (3, "pear", 13)))

    // åˆ›å»ºJobConf
    val conf = new JobConf()
    conf.set("hbase.zookeeper.quorum", "systemhub511,systemhub611,systemhub711")
    conf.setOutputFormat(classOf[TableOutputFormat[ImmutableBytesWritable]])
    conf.set(TableOutputFormat.OUTPUT_TABLE, "test")

    // å®šä¹‰ Hbase æ·»åŠ æ•°æ®æ–¹æ³•
    def convert(triple: (Int, String, Int)): (ImmutableBytesWritable, Put) = {
      val put = new Put(Bytes.toBytes(triple._1))
      put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(triple._2))
      put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(triple._3))(new ImmutableBytesWritable, put)
    }

    // è½¬æ¢RDD
    val writRDD: RDD[(ImmutableBytesWritable, Put)] = initialRDD.map(convert)

    // å†™å…¥HBASE
    writRDD.saveAsHadoopDataset(conf)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

#### 1.3.5 RDD ç¼–ç¨‹è¿›é˜¶
##### 1.3.5.1 ç´¯åŠ å™¨
> ç´¯åŠ å™¨ç”¨æ¥å¯¹ä¿¡æ¯è¿›è¡Œèšåˆ,é€šå¸¸åœ¨å‘Sparkä¼ é€’å‡½æ•°æ—¶,æ¯”å¦‚ä½¿ç”¨`map()`å‡½æ•°æˆ–è€…ç”¨`filter()`ä¼ æ¡ä»¶æ—¶,å¯ä»¥ä½¿ç”¨é©±åŠ¨å™¨ç¨‹åºä¸­å®šä¹‰å˜é‡,ä½†æ˜¯é›†ç¾¤ä¸­è¿è¡Œæ¯ä¸ªä»»åŠ¡éƒ½ä¼šå¾—åˆ°è¿™äº›å˜é‡çš„ä¸€ä»½æ–°å‰¯æœ¬,æ›´æ–°è¿™äº›å‰¯æœ¬çš„å€¼ä¹Ÿä¸ä¼šå½±å“é©±åŠ¨å™¨ä¸­çš„å¯¹åº”å˜é‡,å¦‚æœæƒ³å®ç°æ‰€æœ‰åˆ†ç‰‡å¤„ç†æ—¶æ›´æ–°å…±äº«å˜é‡çš„åŠŸèƒ½,é‚£ä¹ˆç´¯åŠ å™¨å¯ä»¥å®ç°æƒ³è¦çš„æ•ˆæœ.

###### 1.3.5.1.1 ç³»ç»Ÿç´¯åŠ å™¨
> é€šè¿‡åœ¨é©±åŠ¨å™¨ä¸­è°ƒç”¨S`parkContext.accumulator(initialValue`)æ–¹æ³•,åˆ›å»ºå‡ºå­˜æœ‰åˆå§‹å€¼çš„ç´¯åŠ å™¨,è¿”å›å€¼ä¸º`org.apache.spark.Accumulator[T]`å¯¹è±¡,å…¶ä¸­Tæ˜¯åˆå§‹å€¼initialValueçš„ç±»å‹,Sparké—­åŒ…é‡Œçš„æ‰§è¡Œå™¨ä»£ç å¯ä»¥ä½¿ç”¨ç´¯åŠ å™¨çš„`+=`æ–¹æ³•(åœ¨Javaä¸­æ˜¯add)å¢åŠ ç´¯åŠ å™¨çš„å€¼,é©±åŠ¨å™¨ç¨‹åºå¯ä»¥è°ƒç”¨ç´¯åŠ å™¨çš„valueå±æ€§(åœ¨Javaä¸­ä½¿ç”¨value()æˆ–setValue())æ¥è®¿é—®ç´¯åŠ å™¨çš„å€¼.
> 
> å·¥ä½œèŠ‚ç‚¹ä¸Šä»»åŠ¡ä¸èƒ½è®¿é—®ç´¯åŠ å™¨å€¼,ä»è¿™äº›ä»»åŠ¡çš„è§’åº¦æ¥çœ‹,ç´¯åŠ å™¨æ˜¯ä¸€ä¸ªåªå†™å˜é‡.
> 
> å¯¹äºè¦åœ¨è¡ŒåŠ¨æ“ä½œä¸­ä½¿ç”¨ç´¯åŠ å™¨,Sparkåªä¼šæŠŠæ¯ä¸ªä»»åŠ¡å¯¹å„ç´¯åŠ å™¨çš„ä¿®æ”¹åº”ç”¨ä¸€æ¬¡,å› æ­¤,å¦‚æœæƒ³è¦ä¸€ä¸ªæ— è®ºåœ¨å¤±è´¥è¿˜æ˜¯é‡å¤è®¡ç®—æ—¶éƒ½ç»å¯¹å¯é çš„ç´¯åŠ å™¨,å¿…é¡»æŠŠå®ƒæ”¾åœ¨foreach()è¿™æ ·çš„è¡ŒåŠ¨æ“ä½œä¸­,è½¬åŒ–æ“ä½œä¸­ç´¯åŠ å™¨å¯èƒ½ä¼šå‘ç”Ÿä¸æ­¢ä¸€æ¬¡æ›´æ–°.
``` scala
package com.geekparkhub.core.spark.application.methods

import org.apache.spark.rdd.RDD
import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AccuAction
  * <p>
  */

object AccuAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("AccuAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // ç´¯åŠ å™¨
    val sum: LongAccumulator = sc.longAccumulator("sum")

    // åˆ›å»ºRDD
    val value: RDD[Int] = sc.parallelize(Array(1, 2, 3, 4))

    val word: RDD[(Int, Int)] = value.map(x => {
      // æ·»åŠ ç´¯åŠ 
      sum.add(x)
      (x, 1)
    })

    word.collect().foreach(println)

    println(sum.value)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
###### 1.3.5.1.2 è‡ªå®šä¹‰ç´¯åŠ å™¨
> è‡ªå®šä¹‰ç´¯åŠ å™¨ç±»å‹åŠŸèƒ½åœ¨1.Xç‰ˆæœ¬ä¸­å°±å·²ç»æä¾›,ä½†æ˜¯ä½¿ç”¨èµ·æ¥æ¯”è¾ƒéº»çƒ¦,åœ¨2.0ç‰ˆæœ¬å,ç´¯åŠ å™¨çš„æ˜“ç”¨æ€§æœ‰äº†è¾ƒå¤§æ”¹è¿›,è€Œä¸”å®˜æ–¹è¿˜æä¾›äº†ä¸€ä¸ªæ–°æŠ½è±¡ç±» : `AccumulatorV2`æ¥æä¾›æ›´åŠ å‹å¥½è‡ªå®šä¹‰ç±»å‹ç´¯åŠ å™¨çš„å®ç°æ–¹å¼,å®ç°è‡ªå®šä¹‰ç±»å‹ç´¯åŠ å™¨éœ€è¦ç»§æ‰¿`AccumulatorV2`å¹¶è‡³å°‘è¦†å†™ä¸‹ä¾‹ä¸­å‡ºç°çš„æ–¹æ³•,
``` scala
package com.geekparkhub.core.spark.application.methods

import org.apache.spark.util.AccumulatorV2

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AccumulatorAction
  * <p>
  */

class AccumulatorAction extends AccumulatorV2[Int,Int]{

  var sum  = 0

  // åˆ¤æ–­æ˜¯å¦ä¸ºç©º
  override def isZero: Boolean = sum == 0

  // å¤åˆ¶æ–¹æ³•
  override def copy(): AccumulatorV2[Int, Int] = {
    val accumulatorAction = new AccumulatorAction
    accumulatorAction.sum = this.sum
    accumulatorAction
  }

  // é‡ç½®æ–¹æ³•
  override def reset(): Unit = 0

  // ç´¯åŠ æ–¹æ³•
  override def add(v: Int): Unit = sum += v

  // åˆå¹¶æ–¹æ³•
  override def merge(other: AccumulatorV2[Int, Int]): Unit = sum += other.value

  // è¿”å›å€¼
  override def value: Int = sum
}
```

##### 1.3.5.2 å¹¿æ’­å˜é‡ (è°ƒä¼˜ç­–ç•¥)
> å¹¿æ’­å˜é‡ç”¨æ¥é«˜æ•ˆåˆ†å‘è¾ƒå¤§å¯¹è±¡,å‘æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»å€¼,ä»¥ä¾›ä¸€ä¸ªæˆ–å¤šä¸ªSparkæ“ä½œä½¿ç”¨.
> 
> æ¯”å¦‚,å¦‚æœåº”ç”¨éœ€è¦å‘æ‰€æœ‰èŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»æŸ¥è¯¢è¡¨,ç”šè‡³æ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ä¸­çš„ä¸€ä¸ªå¾ˆå¤§çš„ç‰¹å¾å‘é‡,å¹¿æ’­å˜é‡ç”¨èµ·æ¥éƒ½å¾ˆé¡ºæ‰‹,åœ¨å¤šä¸ªå¹¶è¡Œæ“ä½œä¸­ä½¿ç”¨åŒä¸€ä¸ªå˜é‡,ä½†æ˜¯Sparkä¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ†åˆ«å‘é€.
> 
> ä½¿ç”¨å¹¿æ’­å˜é‡è¿‡ç¨‹ : 
> 1.é€šè¿‡å¯¹ä¸€ä¸ªç±»å‹Tçš„å¯¹è±¡è°ƒç”¨`SparkContext.broadcast`åˆ›å»ºå‡º`Broadcast[T]`å¯¹è±¡,ä»»ä½•å¯åºåˆ—åŒ–ç±»å‹éƒ½å¯ä»¥è¿™ä¹ˆå®ç°.
> 2.é€šè¿‡valueå±æ€§è®¿é—®è¯¥å¯¹è±¡å€¼(åœ¨Javaä¸­ä¸º`value()`æ–¹æ³•).
> 3.å˜é‡åªä¼šè¢«å‘åˆ°å„ä¸ªèŠ‚ç‚¹ä¸€æ¬¡,åº”ä½œä¸ºåªè¯»å€¼å¤„ç†(ä¿®æ”¹è¿™ä¸ªå€¼ä¸ä¼šå½±å“åˆ°åˆ«çš„èŠ‚ç‚¹).
```
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)

scala> 
```


### ğŸ”¥ 1.4 Spark SQL ğŸ”¥
#### 1.4.1 Spark SQL æ¦‚è¿°
##### 1.4.1.1 ä»€ä¹ˆæ˜¯ Spark SQL
> Spark SQLæ˜¯Sparkç”¨æ¥å¤„ç†ç»“æ„åŒ–æ•°æ®æ¨¡å—,å®ƒæä¾›äº†2ä¸ªç¼–ç¨‹æŠ½è±¡ : `DataFrame`å’Œ`DataSet`,å¹¶ä¸”ä½œä¸ºåˆ†å¸ƒå¼SQLæŸ¥è¯¢å¼•æ“ä½œç”¨.
> 
> å·²ç»å­¦ä¹ äº†Hive,å®ƒæ˜¯å°†Hive SQLè½¬æ¢æˆMapReduceç„¶åæäº¤åˆ°é›†ç¾¤ä¸Šæ‰§è¡Œ,å¤§å¤§ç®€åŒ–äº†ç¼–å†™MapReducç¨‹åºå¤æ‚æ€§,ç”±äºMapReduceè®¡ç®—æ¨¡å‹æ‰§è¡Œæ•ˆç‡æ¯”è¾ƒæ…¢,æ‰€ä»¥Spark SQLåº”è¿è€Œç”Ÿ,å®ƒæ˜¯å°†Spark SQLè½¬æ¢æˆRDD,ç„¶åæäº¤åˆ°é›†ç¾¤æ‰§è¡Œ,æ‰§è¡Œæ•ˆç‡éå¸¸å¿«.

##### 1.4.1.2 Spark SQL ç‰¹ç‚¹
- æ˜“æ•´åˆ
- ç»Ÿä¸€æ•°æ®è®¿é—®æ–¹å¼
- å…¼å®¹Hive
- æ ‡å‡†æ•°æ®è¿æ¥

##### 1.4.1.3 ä»€ä¹ˆæ˜¯ DataFrame
> ä¸RDDç±»ä¼¼,DataFrameä¹Ÿæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ•°æ®å®¹å™¨,ç„¶è€ŒDataFrameæ›´åƒä¼ ç»Ÿæ•°æ®åº“çš„äºŒç»´è¡¨æ ¼,é™¤äº†æ•°æ®ä»¥å¤–,è¿˜è®°å½•æ•°æ®çš„ç»“æ„ä¿¡æ¯,å³schema,åŒæ—¶ä¸Hiveç±»ä¼¼,DataFrameä¹Ÿæ”¯æŒåµŒå¥—æ•°æ®ç±»å‹(struct / array / map).
> 
> ä»APIæ˜“ç”¨æ€§è§’åº¦ä¸Šçœ‹,DataFrame APIæä¾›æ˜¯ä¸€å¥—é«˜å±‚çš„å…³ç³»æ“ä½œ,æ¯”å‡½æ•°å¼RDD APIè¦æ›´åŠ å‹å¥½,é—¨æ§›æ›´ä½.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_020.jpg)
> ä¸Šå›¾ç›´è§‚åœ°ä½“ç°äº†DataFrameå’ŒRDDåŒºåˆ«,å·¦ä¾§RDD[Person]è™½ç„¶ä»¥Personä¸ºç±»å‹å‚æ•°,ä½†Sparkæ¡†æ¶æœ¬èº«ä¸äº†è§£Personç±»å†…éƒ¨ç»“æ„,è€Œå³ä¾§DataFrameå´æä¾›äº†è¯¦ç»†çš„ç»“æ„ä¿¡æ¯,ä½¿å¾—Spark SQLå¯ä»¥æ¸…æ¥šåœ°çŸ¥é“è¯¥æ•°æ®é›†ä¸­åŒ…å«å“ªäº›åˆ—,æ¯åˆ—åç§°å’Œç±»å‹å„æ˜¯ä»€ä¹ˆ,DataFrameæ˜¯ä¸ºæ•°æ®æä¾›äº†Schemaè§†å›¾,å¯ä»¥æŠŠå®ƒå½“åšæ•°æ®åº“ä¸­ä¸€å¼ æ•°æ®è¡¨.
> 
> DataFrameä¹Ÿæ˜¯æ‡’æ‰§è¡Œ,æ€§èƒ½ä¸Šæ¯”RDDè¦é«˜è¦åŸå›  : ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’,æŸ¥è¯¢è®¡åˆ’é€šè¿‡Spark `catalyst optimiser`è¿›è¡Œä¼˜åŒ–.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_021.jpg)
> ä¸ºäº†è¯´æ˜æŸ¥è¯¢ä¼˜åŒ–,ä¸Šå›¾å±•ç¤ºçš„äººå£æ•°æ®åˆ†æç¤ºä¾‹,å›¾ä¸­æ„é€ äº†ä¸¤ä¸ªDataFrame,å°†å®ƒä»¬joinä¹‹ååˆåšäº†ä¸€æ¬¡filteræ“ä½œ,å¦‚æœåŸå°ä¸åŠ¨åœ°æ‰§è¡Œè¿™ä¸ªæ‰§è¡Œè®¡åˆ’,æœ€ç»ˆçš„æ‰§è¡Œæ•ˆç‡æ˜¯ä¸é«˜çš„,å› ä¸ºjoinæ˜¯ä¸€ä¸ªä»£ä»·è¾ƒå¤§æ“ä½œ,ä¹Ÿå¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¾ƒå¤§æ•°æ®é›†,å¦‚æœèƒ½å°†filterä¸‹æ¨åˆ°joinä¸‹æ–¹,å…ˆå¯¹DataFrameè¿›è¡Œè¿‡æ»¤,å†joinè¿‡æ»¤åçš„è¾ƒå°çš„ç»“æœé›†,ä¾¿å¯ä»¥æœ‰æ•ˆç¼©çŸ­æ‰§è¡Œæ—¶é—´.
> è€ŒSpark SQLçš„æŸ¥è¯¢ä¼˜åŒ–å™¨æ­£æ˜¯è¿™æ ·åšçš„,ç®€è€Œè¨€ä¹‹é€»è¾‘æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å°±æ˜¯ä¸€ä¸ªåˆ©ç”¨åŸºäºå…³ç³»ä»£æ•°çš„ç­‰ä»·å˜æ¢,å°†é«˜æˆæœ¬çš„æ“ä½œæ›¿æ¢ä¸ºä½æˆæœ¬æ“ä½œçš„è¿‡ç¨‹.

##### 1.4.1.4 ä»€ä¹ˆæ˜¯ DataSet
> 1.DataSetæ˜¯DataframeAPIæ‰©å±•,æ˜¯SparkSQLæœ€æ–°æ•°æ®æŠ½è±¡.
> 
> 2.å‹å¥½APIé£æ ¼,æ—¢å…·æœ‰ç±»å‹å®‰å…¨æ£€æŸ¥ä¹Ÿå…·æœ‰Dataframeçš„æŸ¥è¯¢ä¼˜åŒ–ç‰¹æ€§.
> 
> 3.Datasetæ”¯æŒç¼–è§£ç å™¨,å½“éœ€è¦è®¿é—®éå †ä¸Šçš„æ•°æ®æ—¶å¯ä»¥é¿å…ååºåˆ—åŒ–æ•´ä¸ªå¯¹è±¡,æé«˜äº†æ•ˆç‡.
> 
> 4.æ ·ä¾‹ç±»è¢«ç”¨æ¥åœ¨Datasetä¸­å®šä¹‰æ•°æ®ç»“æ„ä¿¡æ¯,æ ·ä¾‹ç±»ä¸­æ¯ä¸ªå±æ€§çš„åç§°ç›´æ¥æ˜ å°„åˆ°DataSetä¸­çš„å­—æ®µåç§°.
> 
> 5.Dataframeæ˜¯Datasetçš„ç‰¹åˆ—,`DataFrame=Dataset[Row]`,æ‰€ä»¥å¯ä»¥é€šè¿‡asæ–¹æ³•å°†Dataframeè½¬æ¢ä¸ºDataset,Rowæ˜¯ä¸€ä¸ªç±»å‹,è·ŸCar / Personè¿™äº›ç±»å‹ä¸€æ ·,æ‰€æœ‰è¡¨ç»“æ„ä¿¡æ¯éƒ½ç”¨Rowæ¥è¡¨ç¤º.
> 
> 6.DataSetæ˜¯å¼ºç±»å‹,æ¯”å¦‚å¯ä»¥æœ‰`Dataset[Car]`,`Dataset[Person]`.
> 
> 7.DataFrameåªæ˜¯çŸ¥é“å­—æ®µ,ä½†æ˜¯ä¸çŸ¥é“å­—æ®µç±»å‹,æ‰€ä»¥åœ¨æ‰§è¡Œè¿™äº›æ“ä½œæ—¶æ˜¯æ²¡åŠæ³•åœ¨ç¼–è¯‘çš„æ—¶å€™æ£€æŸ¥æ˜¯å¦ç±»å‹å¤±è´¥,æ¯”å¦‚å¯ä»¥å¯¹ä¸€ä¸ªStringè¿›è¡Œå‡æ³•æ“ä½œ,åœ¨æ‰§è¡Œæ—¶æ‰æŠ¥é”™,è€ŒDataSetä¸ä»…ä»…çŸ¥é“å­—æ®µ,è€Œä¸”çŸ¥é“å­—æ®µç±»å‹,æ‰€ä»¥æœ‰æ›´ä¸¥æ ¼çš„é”™è¯¯æ£€æŸ¥,å°±è·ŸJSONå¯¹è±¡å’Œç±»å¯¹è±¡ä¹‹é—´çš„ç±»æ¯”.



#### 1.4.2 Spark SQL ç¼–ç¨‹
##### 1.4.2.1 SparkSession æ–°èµ·å§‹ç‚¹
> åœ¨è€ç‰ˆæœ¬ä¸­,SparkSQLæä¾›ä¸¤ç§SQLæŸ¥è¯¢èµ·å§‹ç‚¹ : 
> SQLContext : ç”¨äºSparkæä¾›SQLæŸ¥è¯¢.
> HiveContext : ç”¨äºè¿æ¥HiveæŸ¥è¯¢.
> 
> SparkSessionæ˜¯Sparkæœ€æ–°SQLæŸ¥è¯¢èµ·å§‹ç‚¹,å®è´¨ä¸Šæ˜¯SQLContextå’ŒHiveContextç»„åˆ,æ‰€ä»¥åœ¨SQLContextå’ŒHiveContextä¸Šå¯ç”¨APIåœ¨SparkSessionä¸ŠåŒæ ·æ˜¯å¯ä»¥ä½¿ç”¨,SparkSessionå†…éƒ¨å°è£…äº†`sparkContext`,æ‰€ä»¥è®¡ç®—å®é™…ä¸Šæ˜¯ç”±sparkContextå®Œæˆ.

##### 1.4.2.2 DataFrame
###### 1.4.2.2.1 åˆ›å»º
> åœ¨SparkSQLä¸­`SparkSession`æ˜¯åˆ›å»ºDataFrameå’Œæ‰§è¡ŒSQLå…¥å£.
> åˆ›å»ºDataFrameæœ‰ä¸‰ç§æ–¹å¼ : 
> 1.é€šè¿‡Sparkæ•°æ®æºè¿›è¡Œåˆ›å»º.
> 2.ä»å·²å­˜åœ¨çš„RDDè¿›è¡Œè½¬æ¢.
> 3.ä»Hive Tableè¿›è¡ŒæŸ¥è¯¢è¿”å›.

- 1.ä»Sparkæ•°æ®æºè¿›è¡Œåˆ›å»º
- æŸ¥çœ‹Sparkæ•°æ®æºè¿›è¡Œåˆ›å»ºæ–‡ä»¶æ ¼å¼
```
scala> spark.read.
csv      jdbc   load     options   parquet   table   textFile      
format   json   option   orc       schema    text 
scala> spark.read.
```
- 2.è¯»å–jsonæ–‡ä»¶åˆ›å»ºDataFrameå±•ç¤ºç»“æœ
```
scala> val jsonflow = spark.read.json("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
jsonflow: org.apache.spark.sql.DataFrame = [age: bigint, name: string]=

scala> jsonflow.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> 
```
- 3.RDDè¿›è¡Œè½¬æ¢ | è½»è½»1.4.2.5
- 4.Hive Tableè¿›è¡ŒæŸ¥è¯¢è¿”å› | 

###### 1.4.2.2.2 SQLé£æ ¼è¯­æ³•(ä¸»è¦)
- å¯¹DataFrameåˆ›å»ºä¸´æ—¶è¡¨
- ä¸´æ—¶è¡¨æ˜¯SessionèŒƒå›´å†…,Sessioné€€å‡ºå,è¡¨å°±ä¼šå¤±æ•ˆ,å¦‚æœæƒ³åº”ç”¨èŒƒå›´å†…æœ‰æ•ˆ,å¯ä»¥ä½¿ç”¨å…¨å±€è¡¨,æ³¨æ„ä½¿ç”¨å…¨å±€è¡¨æ—¶éœ€è¦å…¨è·¯å¾„è®¿é—®,å¦‚ : `global_temp.people`
```
scala> jsonflow.createTempView("people")
```
- é€šè¿‡SQLè¯­å¥å®ç°æŸ¥è¯¢å…¨è¡¨ç»“æœå±•ç¤º
```
scala> val sqlDF = spark.sql("SELECT * FROM people")
sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> sqlDF.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> 
```
- å¯¹äºDataFrameåˆ›å»ºå…¨å±€è¡¨
```
scala> jsonflow.createGlobalTempView("peoples")
```
- é€šè¿‡SQLè¯­å¥å®ç°æŸ¥è¯¢å…¨è¡¨ç»“æœå±•ç¤º
```
scala> spark.sql("SELECT * FROM global_temp.peoples").show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> spark.newSession().sql("SELECT * FROM global_temp.peoples").show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala>
```
###### 1.4.2.2.3 DSLé£æ ¼è¯­æ³•(æ¬¡è¦)
- æŸ¥çœ‹DataFrame Schemaä¿¡æ¯
```
scala> jsonflow.printSchema
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

scala> 
```
- åªæŸ¥çœ‹nameåˆ—æ•°æ®
```
scala> jsonflow.select("name").show
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

scala> 
```
- æŸ¥çœ‹nameåˆ—æ•°æ®ä»¥åŠage+1æ•°æ®
```
scala> jsonflow.select($"name",$"age" + 1).show()
+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+

scala>
```
- æŸ¥çœ‹ageå¤§äº21æ•°æ®
```
scala> jsonflow.filter($"age" > 21).show()
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

scala> 
```
- æŒ‰ç…§ageåˆ†ç»„,æŸ¥çœ‹æ•°æ®æ¡æ•°
```
scala> jsonflow.groupBy("age").count().show()
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+

scala> 
```
###### 1.4.2.2.4 RDDè½¬æ¢ä¸ºDateFrame
> å¦‚æœéœ€è¦RDDä¸DFæˆ–è€…DSä¹‹é—´æ“ä½œ,éœ€è¦å¼•å…¥`import spark.implicits._`
> sparkå¹¶ä¸æ˜¯åŒ…å,è€Œæ˜¯sparkSessionå¯¹è±¡åç§°.

- å¯¼å…¥éšå¼è½¬æ¢å¹¶åˆ›å»ºRDD
```
scala> import spark.implicits._
import spark.implicits._

scala> val peopleRDD = sc.textFile("hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt")
peopleRDD: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[30] at textFile at <console>:27

scala>
```
- 1.é€šè¿‡æ‰‹åŠ¨è½¬æ¢
```
scala> peopleRDD.map{x=>{val split = x.split(",");(split(0),split(1).trim)}}.toDF("name","age")
res11: org.apache.spark.sql.DataFrame = [name: string, age: string]
scala> 
```
- 2.é€šè¿‡åå°„è½¬æ¢ (éœ€è¦ç”¨åˆ°æ ·ä¾‹ç±») 
- åˆ›å»ºæ ·ä¾‹ç±»,æ ¹æ®æ ·ä¾‹ç±»å°†RDDè½¬æ¢ä¸ºDataFrame
```
scala> case class People(name:String, age:Int)
defined class People

scala> peopleRDD.map{x=>{val split = x.split(",");People(split(0),split(1).trim.toInt)}}.toDF
res17: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> res17.toDF
res18: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala> 
```
- 3.é€šè¿‡ç¼–ç¨‹æ–¹å¼è½¬æ¢
- å¯¼å…¥æ‰€éœ€ç±»å‹
```
scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._
scala> 
```
- åˆ›å»ºSchema
```
scala> val structType: StructType = StructType(StructField("name",StringType) :: StructField("age",IntegerType) :: Nil)
structType: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))
scala> 
```
- å¯¼å…¥æ‰€éœ€ç±»å‹
```
scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row
scala> 
```
- æ ¹æ®æŒ‡å®šç±»å‹åˆ›å»ºäºŒå…ƒç»„RDD
```
scala> val data = peopleRDD.map{x => val para = x.split(",");Row(para(0),para(1).trim.toInt)}
scala> 
```
- æ ¹æ®æ•°æ®åŠæŒ‡å®šschemaåˆ›å»ºDataFrame
``` 
scala> val dataFrame = spark.createDataFrame(data, structType)
```
- Create SqlAction.scala
``` scala
package com.geekparkhub.core.spark.application.sparksql

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{IntegerType, StructField, StructType}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * SqlAction
  * <p>
  */

object SqlAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("SqlAction").getOrCreate()

    // åˆ›å»ºSC
    val sc: SparkContext = sparkSession.sparkContext

    // åˆ›å»º RDD
    val rdd: RDD[Int] = sc.parallelize(Array(1,2,3,4,5))

    // å°†Intç±»å‹RDDè½¬æ¢ä¸ºRowç±»å‹RDD
    val rowRDD: RDD[Row] = rdd.map(x => {Row(x)})

    // æ•°æ®è¾“å‡º
    rowRDD.collect().foreach(println)

    // åˆ›å»ºå…ƒæ•°æ®ä¿¡æ¯
    val structType = new StructType
    val structTypes: StructType = structType.add(StructField("id", IntegerType))
    val dataFrame: DataFrame = sparkSession.createDataFrame(rowRDD,structTypes)

    // å¯¼å…¥éšå¼è½¬æ¢
    import sparkSession.implicits._

    // DSLé£æ ¼ æ•°æ®æŸ¥è¯¢
    dataFrame.select("id").show()
    
    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```
###### 1.4.2.2.5 DateFrameè½¬æ¢ä¸ºRDD
- ç›´æ¥è°ƒç”¨rddå³å¯.
- åˆ›å»ºDataFrame
```
scala> val df = spark.read.json("/core_flow/spark/json/001/people.json")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]                
scala> 
```
- å°†DataFrameè½¬æ¢ä¸ºRDD
```
scala> val dfToRDD = df.rdd
dfToRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[6] at rdd at <console>:29
scala>
```
- æ‰“å°RDD
```
scala> dfToRDD.collect
res0: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])
scala>
```

##### 1.4.2.3 DataSet
- Datasetæ˜¯å…·æœ‰å¼ºç±»å‹çš„æ•°æ®é›†åˆ,éœ€è¦æä¾›å¯¹åº”ç±»å‹ä¿¡æ¯.
###### 1.4.2.3.1 åˆ›å»º
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- åˆ›å»ºDataSet
```
scala> val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- æŸ¥çœ‹ç»“æœ
```
scala> caseClassDS.show
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+

scala> 
```

###### 1.4.2.3.2 RDDè½¬æ¢ä¸ºDataSet
- SparkSQLèƒ½å¤Ÿè‡ªåŠ¨å°†åŒ…å«æœ‰caseç±»RDDè½¬æ¢æˆDataFrame,caseç±»å®šä¹‰äº†tableç»“æ„,caseç±»å±æ€§é€šè¿‡åå°„å˜æˆäº†è¡¨åˆ—å,Caseç±»å¯ä»¥åŒ…å«è¯¸å¦‚Seqsæˆ–è€…Arrayç­‰å¤æ‚ç»“æ„.
- åˆ›å»ºRDD
```
scala> val peopleRDD = sc.textFile("examples/src/main/resources/people.txt")
peopleRDD: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[8] at textFile at <console>:28

scala>
```
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- å°†RDDè½¬åŒ–ä¸ºDataSet
```
scala> peopleRDD.map(line => {val para = line.split(",");Person(para(0),para(1).trim.toInt)}).toDS
res2: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```

###### 1.4.2.3.3 DataSetè½¬æ¢ä¸ºRDD
- è°ƒç”¨rddæ–¹æ³•å³å¯.
- åˆ›å»ºä¸€ä¸ªDataSet
```
scala> val DS= Seq(Person("Andy", 32)).toDS()
DS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- å°†DataSetè½¬æ¢ä¸ºRDD
```
scala> DS.rdd
res3: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[12] at rdd at <console>:28

scala> res3.collect
res4: Array[Person] = Array(Person(Andy,32))
scala> 
```

##### 1.4.2.4 DataFrameä¸DataSetç›¸äº’æ“ä½œ
###### 1.4.2.4.1 DataFrameè½¬Dataset
- æ­¤æ–¹æ³•å°±æ˜¯åœ¨ç»™å‡ºæ¯ä¸€åˆ—ç±»å‹å,ä½¿ç”¨asæ–¹æ³•è½¬æˆDataset,è¿™åœ¨æ•°æ®ç±»å‹æ˜¯DataFrameåˆéœ€è¦é’ˆå¯¹å„ä¸ªå­—æ®µå¤„ç†æ—¶æä¸ºæ–¹ä¾¿,åœ¨ä½¿ç”¨ä¸€äº›ç‰¹æ®Šçš„æ“ä½œæ—¶,ä¸€å®šè¦åŠ ä¸Š`import spark.implicits._`ä¸ç„¶toDFã€toDSæ— æ³•ä½¿ç”¨.
- åˆ›å»ºDateFrame
```
scala> val df = spark.read.json("./examples/src/main/resources/people.json")
```
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- å°†DateFrameè½¬åŒ–ä¸ºDataSet
```
scala> df.as[Person]
res14:  org.apache.spark.sql.Dataset[Person]  =  [age:  bigint,  name: string]
scala> 
```

###### 1.4.2.4.2 Datasetè½¬DataFrame
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala>
```
- åˆ›å»ºDataSet
```
scala> val ds = Seq(Person("Andy", 32)).toDS()
ds: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- å°†DataSetè½¬åŒ–ä¸ºDataFrameå¹¶å±•ç¤ºç»“æœ
```
scala> val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: bigint]

scala> df.show
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+
scala> 
```


##### 1.4.2.5 RDD / DataFrame / DataSet
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_023.jpg)
> åœ¨SparkSQLä¸­Sparkä¸ºæä¾›äº†ä¸¤ä¸ªæ–°æŠ½è±¡,åˆ†åˆ«æ˜¯`DataFrame`å’Œ`DataSet`.
> ä»–ä»¬å’ŒRDDæœ‰ä»€ä¹ˆåŒºåˆ«? é¦–å…ˆä»ç‰ˆæœ¬çš„äº§ç”Ÿä¸Šæ¥çœ‹ : 
```
RDD (Spark1.0) â€”> Dataframe(Spark1.3) â€”> Dataset(Spark1.6)
```
> å¦‚æœåŒæ ·æ•°æ®éƒ½ç»™åˆ°è¿™ä¸‰ä¸ªæ•°æ®ç»“æ„,ä»–ä»¬åˆ†åˆ«è®¡ç®—ä¹‹å,éƒ½ä¼šç»™å‡ºç›¸åŒç»“æœ,ä¸åŒæ˜¯æ‰§è¡Œæ•ˆç‡å’Œæ‰§è¡Œæ–¹å¼.
> åœ¨åæœŸSparkç‰ˆæœ¬ä¸­,DataSetä¼šé€æ­¥å–ä»£RDDå’ŒDataFrameæˆä¸ºå”¯ä¸€çš„APIæ¥å£.

###### 1.4.2.5.1 ä¸‰è€…å…±æ€§
- 1.RDD / DataFrame / Datasetå…¨éƒ½æ˜¯sparkå¹³å°ä¸‹åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†,ä¸ºå¤„ç†è¶…å¤§å‹æ•°æ®æä¾›ä¾¿åˆ©.
- 2.ä¸‰è€…éƒ½æœ‰æƒ°æ€§æœºåˆ¶,åœ¨è¿›è¡Œåˆ›å»º / è½¬æ¢,å¦‚mapæ–¹æ³•æ—¶ä¸ä¼šç«‹å³æ‰§è¡Œ,åªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶,ä¸‰è€…æ‰ä¼šå¼€å§‹éå†è¿ç®—.
- 3.ä¸‰è€…éƒ½ä¼šæ ¹æ®sparkå†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—,è¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§,ä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º.
- 4.ä¸‰è€…éƒ½æœ‰partitionæ¦‚å¿µ.
- 5.ä¸‰è€…æœ‰è®¸å¤šå…±åŒå‡½æ•°,å¦‚filter,æ’åºç­‰.
- 6.åœ¨å¯¹DataFrameå’ŒDatasetè¿›è¡Œæ“ä½œè®¸å¤šæ“ä½œéƒ½éœ€è¦è¿™ä¸ªåŒ…è¿›è¡Œæ”¯æŒ`importspark.implicits._`
- 7.DataFrameå’ŒDatasetå‡å¯ä½¿ç”¨æ¨¡å¼åŒ¹é…è·å–å„ä¸ªå­—æ®µå€¼å’Œç±»å‹.
- DataFrame : 
```
DF.map{
 caseRow(col1:String,col2:Int)=>
  println(col1);println(col2)
   col1
    case_=> ""
}
```
- Dataset : 
```
// å®šä¹‰å­—æ®µåå’Œç±»å‹
caseclassColtest(col1:String,col2:Int)extendsSerializable
DS.map{
 caseColtest(col1:String,col2:Int)=>
  println(col1);println(col2)
   col1
    case_=> ""
}
```

###### 1.4.2.5.2 ä¸‰è€…åŒºåˆ«
- `1. RDD` :
- RDDä¸€èˆ¬å’Œspark mlibåŒæ—¶ä½¿ç”¨
- RDDä¸æ”¯æŒsparksqlæ“ä½œ
- `2. DataFrame`
- ä¸RDDå’ŒDatasetä¸åŒ,DataFrameæ¯ä¸€è¡Œç±»å‹å›ºå®šä¸ºRow,æ¯ä¸€åˆ—çš„å€¼æ²¡æ³•ç›´æ¥è®¿é—®,åªæœ‰é€šè¿‡è§£ææ‰èƒ½è·å–å„ä¸ªå­—æ®µå€¼.
```
DF.foreach{
 line=>
  valcol1=line.getAs[String]("col1")
  valcol2=line.getAs[String]("col2")
}
```
- DataFrameä¸Datasetä¸€èˆ¬ä¸ä¸spark mlibåŒæ—¶ä½¿ç”¨
- DataFrameä¸Datasetå‡æ”¯æŒsparksqlæ“ä½œ,æ¯”å¦‚select,groupby,è¿˜èƒ½æ³¨å†Œä¸´æ—¶è¡¨/è§†çª—,è¿›è¡Œsqlè¯­å¥æ“ä½œ.
- DataFrameä¸Datasetæ”¯æŒä¸€äº›ç‰¹åˆ«æ–¹ä¾¿ä¿å­˜æ–¹å¼,æ¯”å¦‚ä¿å­˜æˆcsv,å¯ä»¥å¸¦ä¸Šè¡¨å¤´,è¿™æ ·æ¯ä¸€åˆ—å­—æ®µåä¸€ç›®äº†ç„¶.
- `3.`Dataset`
- Datasetå’ŒDataFrameæ‹¥æœ‰å®Œå…¨ç›¸åŒçš„æˆå‘˜å‡½æ•°,åŒºåˆ«åªæ˜¯æ¯ä¸€è¡Œæ•°æ®ç±»å‹ä¸åŒ.
- DataFrameä¹Ÿå¯ä»¥å«`Dataset[Row]`,æ¯ä¸€è¡Œçš„ç±»å‹æ˜¯Row,ä¸è§£ææ¯ä¸€è¡Œç©¶ç«Ÿæœ‰å“ªäº›å­—æ®µ,å„ä¸ªå­—æ®µåˆæ˜¯ä»€ä¹ˆç±»å‹éƒ½æ— ä»å¾—çŸ¥,åªèƒ½ç”¨ä¸Šé¢æåˆ°`getAS`æ–¹æ³•æˆ–è€…å…±æ€§ä¸­çš„ç¬¬ä¸ƒæ¡æåˆ°çš„æ¨¡å¼åŒ¹é…æ‹¿å‡ºç‰¹å®šå­—æ®µ,è€ŒDatasetä¸­,æ¯ä¸€è¡Œæ˜¯ä»€ä¹ˆç±»å‹æ˜¯ä¸ä¸€å®š,åœ¨è‡ªå®šä¹‰äº†case classä¹‹åå¯ä»¥å¾ˆè‡ªç”±è·å¾—æ¯ä¸€è¡Œä¿¡æ¯.
- Datasetåœ¨éœ€è¦è®¿é—®åˆ—ä¸­æŸä¸ªå­—æ®µæ—¶æ˜¯éå¸¸æ–¹ä¾¿,ç„¶è€Œå¦‚æœè¦å†™ä¸€äº›é€‚é…æ€§å¾ˆå¼ºå‡½æ•°æ—¶,å¦‚æœä½¿ç”¨Dataset,è¡Œç±»å‹åˆä¸ç¡®å®š,å¯èƒ½æ˜¯å„ç§case class,æ— æ³•å®ç°é€‚é…,è¿™æ—¶å€™ç”¨DataFrameå³Dataset[Row]å°±èƒ½æ¯”è¾ƒå¥½è§£å†³é—®é¢˜.

##### 1.4.2.6 SparkSQL Application
- SQL & DSLé£æ ¼æ•°æ®æŸ¥è¯¢
``` scala
package com.geekparkhub.core.spark.application.sparksql

import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * SqlAction
  * <p>
  */

object SqlAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("SqlAction").getOrCreate()

    // å¯¼å…¥éšå¼è½¬æ¢
    import sparkSession.implicits._

    // åˆ›å»ºDF
    val df: DataFrame = sparkSession.read.json("/Volumes/GEEK-SYSTEM/Technical_Framework/spark/projects/spark_server/spark-sql/data/people.json")

    // SQLé£æ ¼ æ•°æ®æŸ¥è¯¢ | åˆ›å»ºä¸´æ—¶è¡¨
    df.createTempView("PEOPLE")
    sparkSession.sql("SELECT * FROM PEOPLE").show()

    // DSLé£æ ¼ æ•°æ®æŸ¥è¯¢
    df.select("name").show()

    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```

##### 1.4.2.7 è‡ªå®šä¹‰å‡½æ•°
- åœ¨Shellçª—å£ä¸­å¯é€šè¿‡`spark.udf`åŠŸèƒ½è‡ªå®šä¹‰å‡½æ•°.

###### 1.4.2.7.1 è‡ªå®šä¹‰UDFå‡½æ•°
- åˆ›å»ºDF
```
scala> val df = spark.read.json("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala>
```
- æ³¨å†ŒUDF
```
scala> spark.udf.register("addName",(x:String) => "Name:" + x)
res1: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))

scala>
```
- åˆ›å»ºæ•°æ®è¡¨
```
scala> df.createOrReplaceTempView("people")
```
- æŸ¥è¯¢æ•°æ®è¡¨
```
scala> spark.sql("Select addName(name),age from people").show()
+-----------------+----+
|UDF:addName(name)| age|
+-----------------+----+
|     Name:Michael|null|
|        Name:Andy|  30|
|      Name:Justin|  19|
+-----------------+----+
scala>
```
###### 1.4.2.7.2 è‡ªå®šä¹‰UDAFèšåˆå‡½æ•°
- å¼ºç±»å‹Datasetå’Œå¼±ç±»å‹DataFrameéƒ½æä¾›äº†ç›¸å…³èšåˆå‡½æ•°,å¦‚count(),countDistinct(),avg(),max(),min(),é™¤æ­¤ä¹‹å¤–è¿˜å¯ä»¥è®¾å®šè‡ªå®šä¹‰èšåˆå‡½æ•°.
- å¼±ç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•° : é€šè¿‡ç»§æ‰¿`UserDefinedAggregateFunction`æ¥å®ç°è‡ªå®šä¹‰èšåˆå‡½æ•°.
- ä¸‹é¢å±•ç¤º æ±‚å¹³å‡å·¥èµ„è‡ªå®šä¹‰èšåˆå‡½æ•°
- Create `AvgAction.scala`
``` scala
package com.geekparkhub.core.spark.application.aggregation

import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DoubleType, LongType, StructField, StructType}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AvgAction
  * <p>
  */

object AvgAction extends UserDefinedAggregateFunction {

  // å®šä¹‰è¾“å…¥æ•°æ®ç±»å‹
  override def inputSchema: StructType = StructType(StructField("input", LongType) :: Nil)

  // ç¼“å­˜ä¸­é—´å€¼ç±»å‹
  override def bufferSchema: StructType = StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)

  // å®šä¹‰è¾“å‡ºæ•°æ®ç±»å‹
  override def dataType: DataType = DoubleType

  // å‡½æ•°ç¨³å®šå‚æ•°
  override def deterministic: Boolean = true

  // åˆå§‹åŒ–ç¼“å­˜æ•°æ®
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }

  // åœ¨æ‰§è¡Œå™¨ä¹‹å†…æ›´æ–°
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    buffer(0) = buffer.getLong(0) + input.getLong(0)
    buffer(1) = buffer.getLong(1) + 1L
  }

  // åœ¨æ‰§è¡Œå™¨ä¹‹å¤–åˆå¹¶
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)

  }

  // æ‰§è¡Œæ•°æ®è®¡ç®—
  override def evaluate(buffer: Row): Any = buffer.getLong(0).toDouble / buffer.getLong(1)
}
```
- Create `UdafAction.scala`
```
package com.geekparkhub.core.spark.application.aggregation

import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * UdafAction
  * <p>
  */

object UdafAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("UdafAction").getOrCreate()

    // åˆ›å»ºDF
    val df: DataFrame = sparkSession.read.json("/Volumes/GEEK-SYSTEM/Technical_Framework/spark/projects/spark_server/spark-sql/data/people.json")

    // SQLé£æ ¼ æ•°æ®æŸ¥è¯¢ | åˆ›å»ºä¸´æ—¶è¡¨
    df.createTempView("PEOPLE")

    // æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
    sparkSession.udf.register("AvgAction", AvgAction)

    // ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
    sparkSession.sql("SELECT AvgAction(age) FROM PEOPLE").show()

    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```


- å¼ºç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•° : é€šè¿‡ç»§æ‰¿`Aggregator`æ¥å®ç°å¼ºç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•°,åŒæ ·æ˜¯æ±‚å¹³å‡å·¥èµ„.

#### 1.4.3 Spark SQL æ•°æ®æº
##### 1.4.3.1 é€šç”¨åŠ è½½ / ä¿å­˜æ–¹æ³•
###### 1.4.3.1.1 æ‰‹åŠ¨æŒ‡å®šé€‰é¡¹
- Spark SQL DataFrameæ¥å£æ”¯æŒå¤šç§æ•°æ®æºæ“ä½œ,ä¸€ä¸ªDataFrameå¯ä»¥è¿›è¡ŒRDDsæ–¹å¼æ“ä½œ,ä¹Ÿå¯ä»¥è¢«æ³¨å†Œä¸ºä¸´æ—¶è¡¨,æŠŠDataFrameæ³¨å†Œä¸ºä¸´æ—¶è¡¨ä¹‹å,å°±å¯ä»¥å¯¹è¯¥DataFrameæ‰§è¡ŒSQLæŸ¥è¯¢.
- Spark SQLé»˜è®¤æ•°æ®æºä¸ºParquetæ ¼å¼,æ•°æ®æºä¸ºParquetæ–‡ä»¶æ—¶,Spark SQLå¯ä»¥æ–¹ä¾¿æ‰§è¡Œæ‰€æœ‰æ“ä½œ,ä¿®æ”¹é…ç½®é¡¹`spark.sql.sources.default`,å¯ä¿®æ”¹é»˜è®¤æ•°æ®æºæ ¼å¼ : 
```
scala> val df = spark.read.load("examples/src/main/resources/users.parquet") df.select("name","favorite_color").write.save("namesAndFavColors.parquet")
scala> 
```
- å½“æ•°æ®æºæ ¼å¼ä¸æ˜¯parquetæ ¼å¼æ–‡ä»¶æ—¶,éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ•°æ®æºæ ¼å¼,æ•°æ®æºæ ¼å¼éœ€è¦æŒ‡å®šå…¨åï¼ˆä¾‹å¦‚ï¼š`org.apache.spark.sql.parquet`ï¼‰,å¦‚æœæ•°æ®æºæ ¼å¼ä¸ºå†…ç½®æ ¼å¼,åˆ™åªéœ€è¦æŒ‡å®šç®€ç§°å®š`json`,`parquet`,`jdbc`,`orc`,`libsvm`,`csv`,`text`æ¥æŒ‡å®šæ•°æ®æ ¼å¼
- å¯ä»¥é€šè¿‡SparkSessionæä¾›çš„read.loadæ–¹æ³•ç”¨äºé€šç”¨åŠ è½½æ•°æ®ï¼Œä½¿ç”¨`write`å’Œ`save`ä¿å­˜æ•°æ®.
```
scala> val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
scala> peopleDF.write.format("parquet").save("hdfs://hadoop102:9000/namesAndAges.parquet")
scala> 
```
- é™¤æ­¤ä¹‹å¤–,å¯ä»¥ç›´æ¥è¿è¡ŒSQLåœ¨æ–‡ä»¶ä¸Š.
```
scala> val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs://systemhub511:9000/namesAndAges.parquet`")

scala> sqlDF.show()

scala> val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")peopleDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> peopleDF.write.format("parquet").save("hdfs://hadoop102:9000/namesAndAges.parquet")

scala> peopleDF.show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> val sqlDF = spark.sql("SELECT * FROM  parquet.`hdfs://systemhub511:9000/namesAndAges.parquet`")
sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> sqlDF.show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
```


###### 1.4.3.1.2 æ–‡ä»¶ä¿å­˜é€‰é¡¹
- å¯ä»¥é‡‡ç”¨SaveModeæ‰§è¡Œå­˜å‚¨æ“ä½œ,SaveModeå®šä¹‰äº†å¯¹æ•°æ®å¤„ç†æ¨¡å¼,éœ€è¦æ³¨æ„çš„æ˜¯,è¿™äº›ä¿å­˜æ¨¡å¼ä¸ä½¿ç”¨ä»»ä½•é”å®š,ä¸æ˜¯åŸå­æ“ä½œ,æ­¤å¤–å½“ä½¿ç”¨Overwriteæ–¹å¼æ‰§è¡Œæ—¶,åœ¨è¾“å‡ºæ–°æ•°æ®ä¹‹å‰åŸæ•°æ®å°±å·²ç»è¢«åˆ é™¤,SaveModeè¯¦ç»†ä»‹ç»å¦‚ä¸‹è¡¨ : 

| Scala / Java      |     Any Language |   Meaning   |
| :--------: | :--------:| :------: |
| SaveMode.ErrorIfExists(default)    |   "error"(default) |  å¦‚æœæ–‡ä»¶å­˜åœ¨,åˆ™æŠ¥é”™  |
| SaveMode.Append    |   "append" |  è¿½åŠ   |
| SaveMode.Overwrite    |   "overwrite" |  è¦†å†™  |
| SaveMode.Ignore    |   "ignore" |  æ•°æ®å­˜åœ¨,åˆ™å¿½ç•¥  |
-  æºç å‡ºå¤„
- `org.apache.spark.sql.DataFrameWriter` & `org.apache.spark.sql.SaveMode` 
```
/** 
 * Specifies the behavior when data or table already exists. Options include:
 *   - `overwrite`: overwrite the existing data.
 *   - `append`: append the data.
 *   - `ignore`: ignore the operation (i.e. no-op).
 *   - `error`: default option, throw an exception at runtime.
 *   
 * @since 1.4.0
 */ 
def mode(saveMode: String): DataFrameWriter[T] = {
  this.mode = saveMode.toLowerCase match {
    case "overwrite" => SaveMode.Overwrite
    case "append" => SaveMode.Append
    case "ignore" => SaveMode.Ignore
    case "error" | "default" => SaveMode.ErrorIfExists
    case _ => throw new IllegalArgumentException(s"Unknown save mode: $saveMode. " + "Accepted save modes are 'overwrite', 'append', 'ignore', 'error'.")
 }         
 this
}
```

#####1.4.3.2 JSONæ–‡ä»¶
- Spark SQLèƒ½å¤Ÿè‡ªåŠ¨æ¨æµ‹JSONæ•°æ®é›†ç»“æ„,å¹¶å°†å®ƒåŠ è½½ä¸º`Dataset[Row]`,å¯ä»¥é€šè¿‡`SparkSession.read.json()`åŠ è½½JSON æ–‡ä»¶.
- JSONæ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªä¼ ç»ŸJSONæ–‡ä»¶,è€Œæ˜¯æ¯ä¸€è¡Œéƒ½å¾—æ˜¯ä¸€ä¸ªJSONä¸².
```
scala> import spark.implicits._

scala> val path = "examples/src/main/resources/people.json"

scala> val peopleDF = spark.read.json(path)

scala> peopleDF.createOrReplaceTempView("people")

scala> val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE  age BETWEEN 13 AND 19")

scala> teenagerNamesDF.show()
+------+
|  name|
+------+
|Justin|
+------+

scala> val otherPeopleDataset = spark.createDataset("""{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)

scala> val otherPeople = spark.read.json(otherPeopleDataset)

scala> otherPeople.show()
+---------------+----+
| address 	|  name  |
+---------------+----+
|[Columbus,Ohio]| Yin|
```

#####1.4.3.3 Parquetæ–‡ä»¶
- Parquetæ˜¯ä¸€ç§æµè¡Œåˆ—å¼å­˜å‚¨æ ¼å¼,å¯ä»¥é«˜æ•ˆåœ°å­˜å‚¨å…·æœ‰åµŒå¥—å­—æ®µè®°å½•,Parquetæ ¼å¼ç»å¸¸åœ¨Hadoopç”Ÿæ€åœˆä¸­è¢«ä½¿ç”¨,å®ƒä¹Ÿæ”¯æŒSpark SQLå…¨éƒ¨æ•°æ®ç±»å‹,Spark SQLæä¾›äº†ç›´æ¥è¯»å–å’Œå­˜å‚¨Parquetæ ¼å¼æ–‡ä»¶çš„æ–¹æ³•.
```
scala> importing spark.implicits._

scala> import spark.implicits._	

scala> val peopleDF = spark.read.json("examples/src/main/resources/people.json")

scala> peopleDF.write.parquet("hdfs://systemhub511:9000/people.parquet")

scala> val parquetFileDF =         spark.read.parquet("hdfs://systemhub511:9000/people.parquet")

scala> parquetFileDF.createOrReplaceTempView("parquetFile")

scala> val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")

scala> namesDF.map(attributes => "Name: " + attributes(0)).show()

+------------+
|       value|
+------------+
|Name: Justin|
+------------+
```

#####1.4.3.4 JDBC
- Spark SQLå¯ä»¥é€šè¿‡JDBCä»å…³ç³»å‹æ•°æ®åº“ä¸­è¯»å–æ•°æ®æ–¹å¼åˆ›å»ºDataFrame,é€šè¿‡å¯¹DataFrameä¸€ç³»åˆ—çš„è®¡ç®—å,è¿˜å¯ä»¥å°†æ•°æ®å†å†™å›å…³ç³»å‹æ•°æ®åº“ä¸­.
- æ³¨æ„,éœ€è¦å°†ç›¸å…³æ•°æ®åº“é©±åŠ¨æ”¾åˆ°sparkç±»è·¯å¾„ä¸‹.
```
[root@systemhub711 ~]# cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /opt/module/spark/jars/
```
```
scala> val jdbcDF = spark.read.format("jdbc").option("url","jdbc:mysql://systemhub711:3306/company").option("dbtable","staff").option("user","root").option("password","ax01465").load()
jdbcDF: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]

scala> jdbcDF.show

+---+-------+------+
| id|   name|   sex|
+---+-------+------+
|  1|test001|  male|
|  2|test002|female|
|  3|test003|female|
|  4|test004|  male|
|  5|test005|female|
|  6|test006|  male|
|  7|test007|female|
|  8|test008|female|
|  9|test009|female|
| 10|test010|female|
| 11|test011|female|
| 12|test012|  male|
| 13| Female|  null|
| 14|   Male|  null|
| 15| Female|  null|
+---+-------+------+
scala> 
```
```
scala> jdbcDF.write.format("jdbc").option("url", "jdbc:mysql://systemhub711:3306/company").option("dbtable","rddtable2").option("user","root").option("password","ax01465").save()
```
#####1.4.3.5 Hive DataBase
> Apache Hiveæ˜¯Hadoopä¸ŠSQLå¼•æ“,Spark SQLç¼–è¯‘æ—¶å¯ä»¥åŒ…å«Hiveæ”¯æŒ,ä¹Ÿå¯ä»¥ä¸åŒ…å«,åŒ…å«Hiveæ”¯æŒçš„Spark SQLå¯ä»¥æ”¯æŒHiveè¡¨è®¿é—®ã€UDF(ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°)ä»¥åŠHiveæŸ¥è¯¢è¯­è¨€(HiveQL/HQL)ç­‰,éœ€è¦å¼ºè°ƒçš„ä¸€ç‚¹æ˜¯,å¦‚æœè¦åœ¨Spark SQLä¸­åŒ…å«Hiveåº“,å¹¶ä¸éœ€è¦äº‹å…ˆå®‰è£…Hive,ä¸€èˆ¬æ¥è¯´,æœ€å¥½è¿˜æ˜¯åœ¨ç¼–è¯‘Spark SQLæ—¶å¼•å…¥Hiveæ”¯æŒ,è¿™æ ·å°±å¯ä»¥ä½¿ç”¨è¿™äº›ç‰¹æ€§äº†,å¦‚æœä¸‹è½½çš„æ˜¯äºŒè¿›åˆ¶ç‰ˆæœ¬Spark,å®ƒåº”è¯¥å·²ç»åœ¨ç¼–è¯‘æ—¶æ·»åŠ äº†Hiveæ”¯æŒ.
> 
> è‹¥è¦æŠŠSpark SQLè¿æ¥åˆ°ä¸€ä¸ªéƒ¨ç½²å¥½Hiveä¸Š,ä½ å¿…é¡»æŠŠhive-site.xmlå¤åˆ¶åˆ°Sparké…ç½®æ–‡ä»¶ç›®å½•ä¸­`($SPARK_HOME/conf)`,å³ä½¿æ²¡æœ‰éƒ¨ç½²å¥½Hive,Spark SQLä¹Ÿå¯ä»¥è¿è¡Œ,éœ€è¦æ³¨æ„çš„æ˜¯,å¦‚æœæ²¡æœ‰éƒ¨ç½²å¥½Hive,Spark SQLä¼šåœ¨å½“å‰å·¥ä½œç›®å½•ä¸­åˆ›å»ºå‡ºHiveå…ƒæ•°æ®ä»“åº“,å«ä½œ`metastore_db`,æ­¤å¤–å¦‚æœå°è¯•ä½¿ç”¨HiveQLä¸­çš„`CREATE TABLE (å¹¶éCREATE EXTERNAL TABLE)`è¯­å¥æ¥åˆ›å»ºè¡¨,è¿™äº›è¡¨ä¼šè¢«æ”¾åœ¨é»˜è®¤çš„æ–‡ä»¶ç³»ç»Ÿä¸­çš„`/user/hive/warehouse`ç›®å½•ä¸­(å¦‚æœclasspathä¸­æœ‰é…å¥½çš„hdfs-site.xml,é»˜è®¤æ–‡ä»¶ç³»ç»Ÿå°±æ˜¯HDFS,å¦åˆ™å°±æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ).

###### 1.4.3.5.1 å†…åµŒHiveåº”ç”¨
- å¦‚æœè¦ä½¿ç”¨å†…åµŒHive,ä»€ä¹ˆéƒ½ä¸ç”¨åš,ç›´æ¥ç”¨å°±å¯ä»¥äº†,`--conf:spark.sql.warehouse.dir=`
- å¦‚æœä½¿ç”¨æ˜¯å†…éƒ¨çš„Hive,åœ¨Spark2.0ä¹‹å,`spark.sql.warehouse.dir`ç”¨äºæŒ‡å®šæ•°æ®ä»“åº“åœ°å€,å¦‚æœéœ€è¦æ˜¯ç”¨HDFSä½œä¸ºè·¯å¾„,é‚£ä¹ˆéœ€è¦å°†`core-site.xml`å’Œ`hdfs-site.xml`åŠ å…¥åˆ°Spark confç›®å½•,å¦åˆ™åªä¼šåˆ›å»ºmasterèŠ‚ç‚¹ä¸Šwarehouseç›®å½•,æŸ¥è¯¢æ—¶ä¼šå‡ºç°æ–‡ä»¶æ‰¾ä¸åˆ°çš„é—®é¢˜,è¿™æ˜¯éœ€è¦å‘ä½¿ç”¨HDFS,åˆ™éœ€è¦å°†metastoreåˆ é™¤,é‡å¯é›†ç¾¤.
```
scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+

scala> spark.sql("create table hivetest(id int)")
19/06/03 02:03:17 WARN metastore.HiveMetaStore: Location: file:/opt/module/spark/spark-warehouse/hivetest specified for non-external table:hivetest
res5: org.apache.spark.sql.DataFrame = []

scala> spark.sql("show tables").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default| hivetest|      false|
+--------+---------+-----------+

scala> spark.sql("select * from hivetest").show()
+---+
| id|
+---+
+---+
scala> 
```

###### 1.4.3.5.2 å¤–éƒ¨Hiveåº”ç”¨
- å¦‚æœæƒ³è¿æ¥å¤–éƒ¨å·²ç»éƒ¨ç½²å¥½çš„Hive,éœ€è¦é€šè¿‡ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ : 
- 1.å¯åŠ¨HiveæœåŠ¡
```
[root@systemhub711 spark]# /opt/module/hive/bin/hive
```
- 2.å°†Hiveä¸­çš„`hive-site.xml`æ‹·è´æˆ–è€…è½¯è¿æ¥åˆ°Sparkå®‰è£…ç›®å½•ä¸‹confç›®å½•ä¸‹
```
[root@systemhub711 spark]# cp /opt/module/hive/conf/hive-site.xml ./conf/
```
- 3.å¼€å¯spark shellç»ˆç«¯ | æ˜¾ç¤ºæ‰€æœ‰æ•°æ®è¡¨å¹¶æŸ¥çœ‹æŸå¼ æ•°æ®è¡¨æ•°æ®
```
scala> spark.sql("show tables").show
+--------+--------------------+-----------+
|database|           tableName|isTemporary|
+--------+--------------------+-----------+
| default|            business|      false|
| default|                dept|      false|
| default|      dept_partition|      false|
| default|                 emp|      false|
| default|             emp_sex|      false|
| default|hive_hbase_emp_table|      false|
| default|       hive_workflow|      false|
| default|            location|      false|
| default|          movie_info|      false|
| default|multitasking_hive...|      false|
| default|         person_info|      false|
| default| relevance_hbase_emp|      false|
| default|               score|      false|
| default|          staff_hive|      false|
| default|                test|      false|
| default|             test001|      false|
| default|             test002|      false|
| default|             test003|      false|
| default|             test004|      false|
| default|             test005|      false|
+--------+--------------------+-----------+
only showing top 20 rows

scala> spark.sql("select * from emp").show
+-----+-----+---------+----+----------+--------+-----+------+
|empno|ename|      job| mgr|  hiredate|     sal| comm|deptno|
+-----+-----+---------+----+----------+--------+-----+------+
| 7369|SMITH|CLERKSKLD|7902|1980-12-17|   800.0| 20.0|  null|
| 7499|ALLTE|SALESMANS|7689|1987-02-23|  1600.0|300.0|    30|
| 7521|WAROS|SJDHHJDJX|7869|1984-06-12| 1250.18|500.0|    30|
| 7566|JOSSS|JDHYHDSDS|4545|1874-05-15| 2894.25| 20.0|  null|
| 7654|SOCTD|MANSJUSSD|4855|1996-02-14|  2852.3| 30.0|  null|
| 7698|ADAMS|JUSHHWESD|4552|1985-05-16|25524.02| 30.0|  null|
| 7782|JAMSK|KIHNGSEHN|7769|1991-06-23|  1100.0| 20.0|  null|
| 7788|FOESS|CLAEDFDFD|7698|1994-09-17|   950.0| 30.0|  null|
| 7939|KINGS|CLADDJHEW|7566|1993-07-12|  3000.0| 20.0|  null|
+-----+-----+---------+----+----------+--------+-----+------+
scala> 
```

###### 1.4.3.5.3 è¿è¡ŒSpark SQL CLI
- Spark SQL CLIå¯ä»¥å¾ˆæ–¹ä¾¿åœ¨æœ¬åœ°è¿è¡ŒHiveå…ƒæ•°æ®æœåŠ¡ä»¥åŠä»å‘½ä»¤è¡Œæ‰§è¡ŒæŸ¥è¯¢ä»»åŠ¡.
- åœ¨Sparkç›®å½•ä¸‹æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤å¯åŠ¨Spark SQL CLI
```
[root@systemhub711 spark]# bin/spark-sql
spark-sql (default)> show tables;
database        tableName       isTemporary
default business        false
default dept    false
default dept_partition  false
default emp     false
default emp_sex false
default hive_hbase_emp_table    false
default hive_workflow   false
default location        false
default movie_info      false
default multitasking_hive_workflow      false
default person_info     false
default relevance_hbase_emp     false
default score   false
default staff_hive      false
default test    false
default test001 false
default test002 false
default test003 false
default test004 false
default test005 false
default test006 false
default test007 false
default test008 false
default test_buck       false
default test_bucket     false
Time taken: 6.2 seconds, Fetched 25 row(s)
19/06/03 02:17:58 INFO CliDriver: Time taken: 6.2 seconds, Fetched 25 row(s)
spark-sql (default)> 
```
###### 1.4.3.5.4 ä½¿ç”¨IDEAè¿æ¥SparkSQL for Hive 
- pom.xml å…¬å…±ä¾èµ–ä¿¡æ¯
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub.core.spark</groupId>
    <artifactId>spark_server</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>

    <modules>
        <module>spark-common</module>
        <module>spark-core</module>
        <module>spark-sql</module>
    </modules>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.15</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-server</artifactId>
            <version>1.3.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>1.3.1</version>
        </dependency>
    </dependencies>
</project>
```
- Create `SparkHiveAction.scala`
``` scala
package com.geekparkhub.core.spark.application.sparksql

import org.apache.spark.sql.SparkSession

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * SparkHiveAction
  * <p>
  */

object SparkHiveAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder()
      .enableHiveSupport()
      .master("local[*]")
      .appName("SparkHiveAction")
      .getOrCreate()

    // å±•ç¤ºæ•°æ®è¡¨ä¿¡æ¯
    sparkSession.sql("show tables").show()

    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```
- è¿è¡ŒæŸ¥çœ‹ç»“æœ
```
+--------+--------------------+-----------+
|database|           tableName|isTemporary|
+--------+--------------------+-----------+
| default|            business|      false|
| default|                dept|      false|
| default|      dept_partition|      false|
| default|                 emp|      false|
| default|             emp_sex|      false|
| default|hive_hbase_emp_table|      false|
| default|       hive_workflow|      false|
| default|            location|      false|
| default|          movie_info|      false|
| default|multitasking_hive...|      false|
| default|         person_info|      false|
| default| relevance_hbase_emp|      false|
| default|               score|      false|
| default|          staff_hive|      false|
| default|                test|      false|
| default|             test001|      false|
| default|             test002|      false|
| default|             test003|      false|
| default|             test004|      false|
| default|             test005|      false|
+--------+--------------------+-----------+
only showing top 20 rows
```

#### 1.4.4 Spark SQL å®ä¾‹

### ğŸ”¥ 1.5 Spark Streaming ğŸ”¥
#### 1.5.1 Spark Streaming æ¦‚è¿°
##### 1.5.1.1 Spark Streaming æ˜¯ä»€ä¹ˆ
> Spark Streamingç”¨äºæµå¼æ•°æ®å¤„ç†,Spark Streamingæ”¯æŒæ•°æ®è¾“å…¥æºå¾ˆå¤š,ä¾‹å¦‚ : Kafka / Flume / Twitter / ZeroMQå’Œç®€å•TCPå¥—æ¥å­—ç­‰ç­‰,æ•°æ®è¾“å…¥åå¯ä»¥ç”¨Sparké«˜åº¦æŠ½è±¡åŸè¯­å¦‚ : map / reduce / join / windowç­‰è¿›è¡Œè¿ç®—,è€Œç»“æœä¹Ÿèƒ½ä¿å­˜åœ¨å¾ˆå¤šåœ°æ–¹,å¦‚HDFS,æ•°æ®åº“ç­‰,å¦å¤–Spark Streamingä¹Ÿèƒ½å’ŒMLlib(æœºå™¨å­¦ä¹ )ä»¥åŠGraphxå®Œç¾èåˆ.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_024.jpg)

> Spark Streamingå’ŒSparkåŸºäºRDDæ¦‚å¿µå¾ˆç›¸ä¼¼,Spark Streamingä½¿ç”¨`ç¦»æ•£åŒ–æµ`(`discretized stream`)ä½œä¸ºæŠ½è±¡è¡¨ç¤º,å«ä½œ`DStream`,DStreamæ˜¯éšæ—¶é—´æ¨ç§»è€Œæ”¶åˆ°çš„æ•°æ®åºåˆ—,åœ¨å†…éƒ¨æ¯ä¸ªæ—¶é—´åŒºé—´æ”¶åˆ°æ•°æ®éƒ½ä½œä¸ºRDDå­˜åœ¨,è€ŒDStreamæ˜¯ç”±è¿™äº›RDDæ‰€ç»„æˆçš„åºåˆ—(å› æ­¤å¾—åâ€œ`ç¦»æ•£åŒ–`â€).
> 
> DStreamå¯ä»¥ä»å„ç§è¾“å…¥æºåˆ›å»º,æ¯”å¦‚Flume / Kafkaæˆ–è€…HDFS,åˆ›å»ºå‡ºæ¥DStreamæ”¯æŒä¸¤ç§æ“ä½œ,ä¸€ç§æ˜¯è½¬åŒ–æ“ä½œ(`transformation`),ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„DStream,å¦ä¸€ç§æ˜¯è¾“å‡ºæ“ä½œ(`output operation`),å¯ä»¥æŠŠæ•°æ®å†™å…¥å¤–éƒ¨ç³»ç»Ÿä¸­,DStreamæä¾›äº†è®¸å¤šä¸RDDæ‰€æ”¯æŒçš„æ“ä½œç›¸ç±»ä¼¼æ“ä½œæ”¯æŒ,è¿˜å¢åŠ äº†ä¸æ—¶é—´ç›¸å…³æ–°æ“ä½œ,æ¯”å¦‚æ»‘åŠ¨çª—å£.

##### 1.5.1.2 Spark Streaming ç‰¹ç‚¹
- 1.æ˜“ç”¨
- 2.å®¹é”™
- 3.æ˜“æ•´åˆåˆ°Sparkä½“ç³»

##### 1.5.1.3 Spark Streaming æ¶æ„

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_025.jpg)

#### 1.5.2 DataStream å…¥é—¨
##### 1.5.2.1 WordCount æ¡ˆä¾‹
- 1.éœ€æ±‚ : ä½¿ç”¨`netcat`å·¥å…·å‘9999ç«¯å£ä¸æ–­å‘é€æ•°æ®,é€šè¿‡SparkStreamingè¯»å–ç«¯å£æ•°æ®å¹¶ç»Ÿè®¡ä¸åŒå•è¯å‡ºç°æ¬¡æ•°.
- 2.è¿½åŠ ä¾èµ–ä¿¡æ¯
``` xml
<dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>spark-streaming_2.11</artifactId>
 <version>2.1.1</version>
</dependency>
```
- 3. Create `StreamWordCounAction.scala`
``` scala
package com.geekparkhub.core.spark.application.example

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * StreamWordCounAction
  * <p>
  */

object StreamWordCounAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("loacl[*]").setAppName("StreamWordCounAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // åˆ›å»º DStream
    val lineDStream: ReceiverInputDStream[String] = ssc.socketTextStream("systemhub511", 9999)

    // å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå•è¯
    val wordDStream: DStream[String] = lineDStream.flatMap(_.split(" "))

    // å°†å•è¯ä½è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_, 1))

    // ç»Ÿè®¡å•è¯å‡ºç°ä¸ªæ•°
    val DStreamResult: DStream[(String, Int)] = wordAndOneDStream.reduceByKey(_ + _)

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    DStreamResult.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```

- 4.å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡(åŒ…æ‹¬SparkæœåŠ¡)
```
[root@systemhub511 ~]# start-cluster.sh
[root@systemhub511 spark]# sbin/start-all.sh
[root@systemhub511 spark]# sbin/start-history-server.sh
```

- 5.å¯åŠ¨ç¨‹åºå¹¶é€šè¿‡NetCatå‘é€æ•°æ®
```
[root@systemhub511 spark]# nc -lk 9999
hello
spark
io
io
io
```
- 6.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559563323000 ms
-------------------------------------------
(hello,1)

-------------------------------------------
Time: 1559563326000 ms
-------------------------------------------
(spark,1)

-------------------------------------------
Time: 1559563329000 ms
-------------------------------------------

Time: 1559563341000 ms
-------------------------------------------
(io,1)

-------------------------------------------
Time: 1559563344000 ms
-------------------------------------------
(io,2)
```
##### 1.5.2.2 WordCount è§£æ
> `Discretized Stream`æ˜¯Spark StreamingåŸºç¡€æŠ½è±¡,ä»£è¡¨æŒç»­æ€§æ•°æ®æµå’Œç»è¿‡å„ç§SparkåŸè¯­æ“ä½œåçš„ç»“æœæ•°æ®æµ,åœ¨å†…éƒ¨å®ç°ä¸Š`DStream`æ˜¯ä¸€ç³»åˆ—è¿ç»­çš„RDDæ¥è¡¨ç¤º,æ¯ä¸ªRDDå«æœ‰ä¸€æ®µæ—¶é—´é—´éš”å†…çš„æ•°æ®.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_026.jpg)

> å¯¹æ•°æ®æ“ä½œä¹Ÿæ˜¯æŒ‰ç…§RDDä¸ºå•ä½æ¥è¿›è¡Œ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_027.jpg)

> è®¡ç®—è¿‡ç¨‹ç”±Spark engineæ¥å®Œæˆ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_028.jpg)

#### 1.5.3 DataStream åˆ›å»º
> Spark  StreamingåŸç”Ÿæ”¯æŒä¸€äº›ä¸åŒæ•°æ®æº,ä¸€äº›æ ¸å¿ƒæ•°æ®æºå·²ç»è¢«æ‰“åŒ…åˆ°Spark Streamingçš„Mavenå·¥ä»¶ä¸­,è€Œå…¶ä»–ä¸€äº›åˆ™å¯ä»¥é€šè¿‡spark-streaming-kafkaç­‰é™„åŠ å·¥ä»¶è·å–,æ¯ä¸ªæ¥æ”¶å™¨éƒ½ä»¥Sparkæ‰§è¡Œå™¨ç¨‹åºä¸­ä¸€ä¸ªé•¿æœŸè¿è¡Œçš„ä»»åŠ¡å½¢å¼è¿è¡Œ,å› æ­¤ä¼šå æ®åˆ†é…ç»™åº”ç”¨CPUæ ¸å¿ƒ.
> 
> æ­¤å¤–è¿˜éœ€è¦æœ‰å¯ç”¨çš„CPUæ ¸å¿ƒæ¥å¤„ç†æ•°æ®,è¿™æ„å‘³ç€å¦‚æœè¦è¿è¡Œå¤šä¸ªæ¥æ”¶å™¨,å°±å¿…é¡»è‡³å°‘æœ‰å’Œæ¥æ”¶å™¨æ•°ç›®ç›¸åŒçš„æ ¸å¿ƒæ•°,è¿˜è¦åŠ ä¸Šç”¨æ¥å®Œæˆè®¡ç®—æ‰€éœ€è¦çš„æ ¸å¿ƒæ•°,ä¾‹å¦‚å¦‚æœæƒ³è¦åœ¨æµè®¡ç®—åº”ç”¨ä¸­è¿è¡Œ10ä¸ªæ¥æ”¶å™¨,é‚£ä¹ˆè‡³å°‘éœ€è¦ä¸ºåº”ç”¨åˆ†é…11ä¸ªCPUæ ¸å¿ƒ,æ‰€ä»¥å¦‚æœåœ¨æœ¬åœ°æ¨¡å¼è¿è¡Œ,ä¸è¦ä½¿ç”¨`local`æˆ–è€…`local[1]`

##### 1.5.3.1 æ–‡ä»¶æ•°æ®æº
> æ–‡ä»¶æ•°æ®æµ : èƒ½å¤Ÿè¯»å–æ‰€æœ‰HDFS APIå…¼å®¹æ–‡ä»¶ç³»ç»Ÿæ–‡ä»¶,é€šè¿‡`fileStream`æ–¹æ³•è¿›è¡Œè¯»å–,Spark Streamingå°†ä¼šç›‘æ§`dataDirectory`ç›®å½•å¹¶ä¸æ–­å¤„ç†ç§»åŠ¨è¿›æ¥çš„æ–‡ä»¶,ä½†æ˜¯ç›®å‰ä¸æ”¯æŒåµŒå¥—ç›®å½•.

###### 1.5.3.1.1 ç”¨æ³•åŠè¯´æ˜
- æ³¨æ„äº‹é¡¹ : 
- 1.æ–‡ä»¶éœ€è¦æœ‰ç›¸åŒæ•°æ®æ ¼å¼
- 2.æ–‡ä»¶è¿›å…¥dataDirectoryæ–¹å¼éœ€è¦é€šè¿‡ç§»åŠ¨æˆ–è€…é‡å‘½åæ¥å®ç°
- 3.ä¸€æ—¦æ–‡ä»¶ç§»åŠ¨è¿›ç›®å½•,åˆ™ä¸èƒ½å†ä¿®æ”¹,å³ä¾¿ä¿®æ”¹ä¹Ÿä¸ä¼šè¯»å–æ–°æ•°æ®
```
streamingContext.textFileStream(dataDirectory)
```

###### 1.5.3.1.2 æ¡ˆä¾‹å®æ“
- 1.åœ¨HDFSä¸Šåˆ›å»ºç”¨äºè¢«ç›‘å¬ç›®å½•
```
[root@systemhub511 spark]# hadoop fs -mkdir /core_flow/spark/filestream
```
- 2.åˆ›å»ºä¸‰ä¸ªæ–‡ä»¶
```
[root@systemhub511 filestream]# vim a.txt
[root@systemhub511 filestream]# vim b.txt
[root@systemhub511 filestream]# vim c.txt
```
- 3.Create `FileStreamAction.scala`
``` scala
package com.geekparkhub.core.spark.application.datastream

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * FileStreamAction
  * <p>
  */

object FileStreamAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("FileStreamAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // ç›‘æ§æ–‡ä»¶å¤¹ DStream
    val fileDStream: DStream[String] = ssc.textFileStream("hdfs://systemhub511:9000/core_flow/spark/filestream/")

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    fileDStream.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```
- 4.å¯åŠ¨ç¨‹åº
```
-------------------------------------------
Time: 1559566113000 ms
-------------------------------------------

-------------------------------------------
Time: 1559566116000 ms
-------------------------------------------

-------------------------------------------
Time: 1559566119000 ms
-------------------------------------------
```

- 5.ä¸Šä¼ æ–‡ä»¶
```
[root@systemhub511 spark]# hadoop fs -put /opt/module/datas/spark_flow/filestream/a.txt /core_flow/spark/filestream/

[root@systemhub511 spark]# hadoop fs -put /opt/module/datas/spark_flow/filestream/b.txt /core_flow/spark/filestream/

[root@systemhub511 spark]# hadoop fs -put /opt/module/datas/spark_flow/filestream/c.txt /core_flow/spark/filestream/
```
- 6.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559566146000 ms
-------------------------------------------
SparkStreaming
SparkStreaming
SparkStream
DStream

-------------------------------------------
Time: 1559566155000 ms
-------------------------------------------
textFileStream
textFileStream
StreamingContext

-------------------------------------------
Time: 1559566164000 ms
-------------------------------------------
awaitTermination
hadoop
```

##### 1.5.3.2 RDD é˜Ÿåˆ—
###### 1.5.3.2.1 ç”¨æ³•åŠè¯´æ˜
> æµ‹è¯•è¿‡ç¨‹ä¸­,å¯ä»¥é€šè¿‡ä½¿ç”¨`ssc.queueStream(queueOfRDDs)`æ¥åˆ›å»ºDStream,æ¯ä¸€ä¸ªæ¨é€åˆ°è¿™ä¸ªé˜Ÿåˆ—ä¸­çš„RDD,éƒ½ä¼šä½œä¸ºä¸€ä¸ªDStreamå¤„ç†.
###### 1.5.3.2.2 æ¡ˆä¾‹å®æ“
- 1.éœ€æ±‚ : å¾ªç¯åˆ›å»ºRDD,å°†RDDæ”¾å…¥é˜Ÿåˆ—,é€šè¿‡SparkStreamåˆ›å»ºDstreamè®¡ç®—WordCoun.
- 2.Create `QueuStreamAction.scala`
``` scala
package com.geekparkhub.core.spark.application.datastream

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.collection.mutable

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * QueuStreamAction
  * <p>
  */

object QueuStreamAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("QueuStreamAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // åˆ›å»ºRDDé˜Ÿåˆ—
    val rddQueue = new mutable.Queue[RDD[Int]]()

    // åˆ›å»º rddDStream
    val rddDStream: InputDStream[Int] = ssc.queueStream(rddQueue,false)

    // ç»Ÿè®¡è®¡ç®—
    val result: DStream[Int] = rddDStream.reduce(_ + _)

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    result.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()

    // å¾ªç¯åˆ›å»ºRDD
    for (i <- 1 to 5) {
      rddQueue += ssc.sparkContext.makeRDD(1 to 100, 10)
      Thread.sleep(2000)
    }
    ssc.awaitTermination()
  }
}
```
 - 3.å¯åŠ¨ç¨‹åº æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
 -------------------------------------------
Time: 1559567436000 ms
-------------------------------------------
5050

-------------------------------------------
Time: 1559567439000 ms
-------------------------------------------
10100

-------------------------------------------
Time: 1559567442000 ms
-------------------------------------------
5050

-------------------------------------------
Time: 1559567445000 ms
-------------------------------------------
5050
```

##### 1.5.3.3 è‡ªå®šä¹‰æ•°æ®æº
###### 1.5.3.3.1 ç”¨æ³•åŠè¯´æ˜
- éœ€è¦ç»§æ‰¿`Receiver`,å¹¶å®ç°`onStart` & `onStop`æ–¹æ³•æ¥è‡ªå®šä¹‰æ•°æ®æºé‡‡é›†.
###### 1.5.3.3.2 æ¡ˆä¾‹å®æ“	
- éœ€æ±‚ : è‡ªå®šä¹‰æ•°æ®æº,å®ç°ç›‘æ§æŸä¸ªç«¯å£å·,è·å–è¯¥ç«¯å£å·å†…å®¹.
- 1.Create `CustomizeReceiver.scala`
``` scala
package com.geekparkhub.core.spark.application.datastream

import java.io.{BufferedReader, InputStreamReader}
import java.net.Socket
import java.nio.charset.StandardCharsets

import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.receiver.Receiver

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * CustomizeReceiver
  * <p>
  */

class CustomizeReceiver(hostName: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_ONLY) {

  // å¼€å§‹è¯»å–æ•°æ®
  override def onStart(): Unit = {
    new Thread("receiver") {
      override def run(): Unit = {
        receiver()
      }
    }.start()
  }

  // è¯»å–æ•°æ®
  def receiver(): Unit = {
    try {
      // åˆ›å»º Socket
      val socket = new Socket(hostName, port)
      // å®šä¹‰å˜é‡,ç”¨æ¥æ¥æ”¶ç«¯å£ä¼ è¿‡æ¥çš„æ•°æ®
      var input: String = null
      // åˆ›å»ºBufferedReaderç”¨äºè¯»å–ç«¯å£ä¼ æ¥çš„æ•°æ®
      val reader = new BufferedReader(new InputStreamReader(socket.getInputStream, StandardCharsets.UTF_8))
      // èµ‹å€¼
      input = reader.readLine()
      while (input != null) {
        store(input)
        input = reader.readLine()
      }
      // è·³å‡ºå¾ªç¯åˆ™å…³é—­èµ„æº
      reader.close()
      socket.close()

      // é‡å¯æµå¼ä»»åŠ¡
      restart("restart")
    } catch {
      case e: Exception => restart("restart")
    }
  }

  // ç»“æŸè¯»å–æ•°æ®
  override def onStop(): Unit = {}
}
```
- 2.Create `CustomizeReceiverAction.scala`
``` scala
package com.geekparkhub.core.spark.application.datastream

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * CustomizeReceiverAction
  * <p>
  */

object CustomizeReceiverAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("CustomizeReceiverAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    val lineDStream: ReceiverInputDStream[String] = ssc.receiverStream(new CustomizeReceiver("systemhub511", 9999))

    // å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå•è¯
    val wordDStream: DStream[String] = lineDStream.flatMap(_.split(" "))

    // å°†å•è¯ä½è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_, 1))

    // ç»Ÿè®¡å•è¯å‡ºç°ä¸ªæ•°
    val DStreamResult: DStream[(String, Int)] = wordAndOneDStream.reduceByKey(_ + _)

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    DStreamResult.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```
- 3.å¯åŠ¨ç¨‹åºå¹¶é€šè¿‡NetCatå‘é€æ•°æ®
```
[root@systemhub511 spark]# nc -lk 9999CustomizeReceiverAction
CustomizeReceiverAction
CustomizeReceiver      
CustomizeReceiver      
CustomizeReceiv
```
- 4.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559570220000 ms
-------------------------------------------

Time: 1559570226000 ms
-------------------------------------------
(CustomizeReceiverAction,1)

-------------------------------------------
Time: 1559570229000 ms
-------------------------------------------
(CustomizeReceiverAction,1)
(CustomizeReceiver,1)

-------------------------------------------
Time: 1559570232000 ms
-------------------------------------------
(CustomizeReceiver,1)

-------------------------------------------
Time: 1559570238000 ms
-------------------------------------------
(CustomizeReceiv,1)
```



##### 1.5.3.4 Kafkaæ•°æ®æº
###### 1.5.3.4.1 ç”¨æ³•åŠè¯´æ˜
- åœ¨å·¥ç¨‹ä¸­éœ€è¦å¼•å…¥Mavenå·¥ä»¶`spark-streaming-kafka_2.10`æ¥ä½¿ç”¨å®ƒ,åŒ…å†…æä¾›çš„KafkaUtilså¯¹è±¡å¯ä»¥åœ¨StreamingContextå’ŒJavaStreamingContextä¸­ä»¥Kafkaæ¶ˆæ¯åˆ›å»ºå‡ºDStream.
- 
- ç”±äºKafkaUtilså¯ä»¥è®¢é˜…å¤šä¸ªä¸»é¢˜,å› æ­¤å®ƒåˆ›å»ºå‡ºçš„DStreamç”±æˆå¯¹çš„ä¸»é¢˜å’Œæ¶ˆæ¯ç»„æˆ,è¦åˆ›å»ºå‡ºä¸€ä¸ªæµæ•°æ®,éœ€è¦ä½¿ç”¨StreamingContextå®ä¾‹ã€ä¸€ä¸ªç”±é€—å·éš”å¼€çš„ZooKeeperä¸»æœºåˆ—è¡¨å­—ç¬¦ä¸²ã€æ¶ˆè´¹è€…ç»„çš„åå­—(å”¯ä¸€åå­—),ä»¥åŠä¸€ä¸ªä»ä¸»é¢˜åˆ°é’ˆå¯¹è¿™ä¸ªä¸»é¢˜çš„æ¥æ”¶å™¨çº¿ç¨‹æ•°çš„æ˜ å°„è¡¨æ¥è°ƒç”¨createStream()æ–¹æ³•.

###### 1.5.3.4.2 æ¡ˆä¾‹å®æ“
- éœ€æ±‚ : é€šè¿‡SparkStreamingä»Kafkaè¯»å–æ•°æ®,å¹¶å°†è¯»å–è¿‡æ¥æ•°æ®åšç®€å•è®¡ç®—(WordCount),æœ€ç»ˆå°†ä¿¡æ¯æ‰“å°è‡³æ§åˆ¶å°.
- 1.è¿½åŠ ä¾èµ–ä¿¡æ¯
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub.core.spark</groupId>
    <artifactId>spark_server</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>

    <modules>
        <module>spark-common</module>
        <module>spark-core</module>
        <module>spark-sql</module>
        <module>spark-streaming</module>
    </modules>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.1</version>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.15</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-server</artifactId>
            <version>1.3.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>1.3.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka_2.11</artifactId>
            <version>1.6.3</version>
        </dependency>
    </dependencies>
</project>
```
- 2.Create `KafkaSparkStreamingAction.scala`
```
package com.geekparkhub.core.spark.application.datastream

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * KafkaSparkStreamingAction
  * <p>
  */

object KafkaSparkStreamingAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("KafkaSparkStreamingAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // å£°æ˜ Kafkaå‚æ•°
    val zookeeper = "systemhub511:2181,systemhub611:2181,systemhub711:2181"
    val topic = "topic001"
    val consumerGroup = "spark"

    // å®šä¹‰ Kafkaå‚æ•°
    val kafkaPara: Map[String, String] = Map[String, String](
      ConsumerConfig.GROUP_ID_CONFIG -> consumerGroup,
      "zookeeper.connect" -> zookeeper,
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer"
    )

    // åˆ›å»º KafkaDStream
    val KafkaDStream: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](ssc, kafkaPara, Map(topic -> 1), StorageLevel.MEMORY_ONLY)

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    KafkaDStream.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```

- 3.æ£€ç´¢æ‰€æœ‰ kafka topic
```
[root@systemhub511 kafka]# bin/kafka-topics.sh --list --zookeeper systemhub511:2181
__consumer_offsets
ct
topic001
topic002
topic003
topic004
```
- 4.å¼€å¯æ•°æ®ç”Ÿäº§è€…æœåŠ¡
```
[root@systemhub511 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>
```
- 5.åˆæ¬¡å¯åŠ¨ç¨‹åº | è¿è¡Œæ—¶å‡ºç°å¼‚å¸¸
```
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/Logging
```
- 5.1 è§£å†³æ–¹æ¡ˆ : åœ¨é¡¹ç›®ä¸­åˆ›å»º`org.apache.spark`åŒ…,å¹¶åˆ›å»º`Logging`å®ä½“å³å¯
```
package org.apache.spark

import org.apache.log4j.{LogManager, PropertyConfigurator}
import org.slf4j.{Logger, LoggerFactory}
import org.slf4j.impl.StaticLoggerBinder
import org.apache.spark.annotation.DeveloperApi
import org.apache.spark.util.Utils

/** :: DeveloperApi ::
  * Utility trait for classes that want to log data. Creates a SLF4J logger for the class and allows
  * logging messages at different levels using methods that only evaluate parameters lazily if the
  * log level is enabled.
  *
  * NOTE: DO NOT USE this class outside of Spark. It is intended as an internal utility.
  * This will likely be changed or removed in future releases.
  */
@DeveloperApi
trait Logging {
  // Make the log field transient so that objects with Logging can
  // be serialized and used on another machine
  @transient private var log_ : Logger = null

  // Method to get the logger name for this object
  protected def logName = {
    // Ignore trailing $'s in the class names for Scala objects
    this.getClass.getName.stripSuffix("$")
  }

  // Method to get or create the logger for this object
  protected def log: Logger = {
    if (log_ == null) {
      initializeIfNecessary()
      log_ = LoggerFactory.getLogger(logName)
    }
    log_
  }

  // Log methods that take only a String
  protected def logInfo(msg: => String) {
    if (log.isInfoEnabled) log.info(msg)
  }

  protected def logDebug(msg: => String) {
    if (log.isDebugEnabled) log.debug(msg)
  }

  protected def logTrace(msg: => String) {
    if (log.isTraceEnabled) log.trace(msg)
  }

  protected def logWarning(msg: => String) {
    if (log.isWarnEnabled) log.warn(msg)
  }

  protected def logError(msg: => String) {
    if (log.isErrorEnabled) log.error(msg)
  }

  // Log methods that take Throwables (Exceptions/Errors) too
  protected def logInfo(msg: => String, throwable: Throwable) {
    if (log.isInfoEnabled) log.info(msg, throwable)
  }

  protected def logDebug(msg: => String, throwable: Throwable) {
    if (log.isDebugEnabled) log.debug(msg, throwable)
  }

  protected def logTrace(msg: => String, throwable: Throwable) {
    if (log.isTraceEnabled) log.trace(msg, throwable)
  }

  protected def logWarning(msg: => String, throwable: Throwable) {
    if (log.isWarnEnabled) log.warn(msg, throwable)
  }

  protected def logError(msg: => String, throwable: Throwable) {
    if (log.isErrorEnabled) log.error(msg, throwable)
  }

  protected def isTraceEnabled(): Boolean = {
    log.isTraceEnabled
  }

  private def initializeIfNecessary() {
    if (!Logging.initialized) {
      Logging.initLock.synchronized {
        if (!Logging.initialized) {
          initializeLogging()
        }
      }
    }
  }

  private def initializeLogging() {
    // Don't use a logger in here, as this is itself occurring during initialization of a logger
    // If Log4j 1.2 is being used, but is not initialized, load a default properties file
    val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr
    // This distinguishes the log4j 1.2 binding, currently
    // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently
    // org.apache.logging.slf4j.Log4jLoggerFactory
    val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass)

    lazy val isInInterpreter: Boolean = {
      try {
        val interpClass = classForName("org.apache.spark.repl.Main")
        interpClass.getMethod("interp").invoke(null) != null
      } catch {
        case _: ClassNotFoundException => false
      }
    }

    def classForName(className: String): Class[_] = {
      Class.forName(className, true, getContextOrSparkClassLoader)
      // scalastyle:on classforname
    }

    def getContextOrSparkClassLoader: ClassLoader =
      Option(Thread.currentThread().getContextClassLoader).getOrElse(getSparkClassLoader)

    def getSparkClassLoader: ClassLoader = getClass.getClassLoader

    if (usingLog4j12) {
      val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements
      if (!log4j12Initialized) {
        // scalastyle:off println
        if (isInInterpreter) {
          val replDefaultLogProps = "org/apache/spark/log4j-defaults-repl.properties"
          Option(Utils.getSparkClassLoader.getResource(replDefaultLogProps)) match {
            case Some(url) =>
              PropertyConfigurator.configure(url)
              System.err.println(s"Using Spark's repl log4j profile: $replDefaultLogProps")
              System.err.println("To adjust logging level use sc.setLogLevel(\"INFO\")")
            case None =>
              System.err.println(s"Spark was unable to load $replDefaultLogProps")
          }
        } else {
          val defaultLogProps = "org/apache/spark/log4j-defaults.properties"
          Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match {
            case Some(url) =>
              PropertyConfigurator.configure(url)
              System.err.println(s"Using Spark's default log4j profile: $defaultLogProps")
            case None =>
              System.err.println(s"Spark was unable to load $defaultLogProps")
          }
        }
        // scalastyle:on println
      }
    }
    Logging.initialized = true

    // Force a call into slf4j to initialize it. Avoids this happening from multiple threads
    // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html
    log
  }
}

private object Logging {
  @volatile private var initialized = false
  val initLock = new Object()
  try {
    // We use reflection here to handle the case where users remove the
    // slf4j-to-jul bridge order to route their logs to JUL.
    val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler")
    bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null)
    val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean]
    if (!installed) {
      bridgeClass.getMethod("install").invoke(null)
    }
  } catch {
    case e: ClassNotFoundException => // can't log anything yet so just fail silently
  }
}
```
- 6.å†æ¬¡å¯åŠ¨ç¨‹åº
```
-------------------------------------------
Time: 1559624490000 ms
-------------------------------------------
```
- 7.å‘Kafkaæ•°æ®ç”Ÿäº§è€…å†™å…¥æ•°æ®
```
[root@systemhub511 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>top001
>top002
>top003
>top004
>top005
>top006
```
- 8.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559624499000 ms
-------------------------------------------
(null,top001)

-------------------------------------------
Time: 1559624502000 ms
-------------------------------------------
(null,top002)

-------------------------------------------
Time: 1559624505000 ms
-------------------------------------------
(null,top003)

-------------------------------------------
Time: 1559624508000 ms
-------------------------------------------
(null,top004)

-------------------------------------------
Time: 1559624511000 ms
-------------------------------------------
(null,top005)

-------------------------------------------
Time: 1559624514000 ms
-------------------------------------------
(null,top006)
```

#### 1.5.4  DataStream è½¬æ¢
- DStreamä¸Šçš„åŸè¯­ä¸RDDç±»ä¼¼,åˆ†ä¸º`Transformations(è½¬æ¢)`å’Œ`Output Operations()è¾“å‡º)`ä¸¤ç§,æ­¤å¤–è½¬æ¢æ“ä½œä¸­è¿˜æœ‰ä¸€äº›æ¯”è¾ƒç‰¹æ®ŠåŸè¯­,å¦‚ : `updateStateByKey()`ã€`transform()`ä»¥åŠå„ç§Windowç›¸å…³åŸè¯­.

##### 1.5.4.1 æ— çŠ¶æ€è½¬åŒ–æ“ä½œ
> æ— çŠ¶æ€è½¬åŒ–æ“ä½œå°±æ˜¯æŠŠç®€å•çš„RDDè½¬åŒ–æ“ä½œåº”ç”¨åˆ°æ¯ä¸ªæ‰¹æ¬¡ä¸Š,ä¹Ÿå°±æ˜¯è½¬åŒ–DStreamä¸­çš„æ¯ä¸€ä¸ªRDD,éƒ¨åˆ†æ— çŠ¶æ€è½¬åŒ–æ“ä½œåˆ—åœ¨äº†ä¸‹è¡¨ä¸­,æ³¨æ„é’ˆå¯¹é”®å€¼å¯¹çš„DStreamè½¬åŒ–æ“ä½œ(æ¯”å¦‚`reduceByKey()`)è¦æ·»åŠ `import StreamingContext._`æ‰èƒ½åœ¨Scalaä¸­ä½¿ç”¨.

| å‡½æ•°åç§° |   ä½œç”¨ |   Scalaå®ä¾‹ | ç”¨æ¥æ“ä½œDStream[T]ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•° å‡½æ•°ç­¾å |
| :--------: | :--------:| :------: | :------: |
| map()    |   å¯¹DStreamä¸­çš„æ¯ä¸ªå…ƒç´ åº”ç”¨åˆ°ç»™å®šå‡½æ•°,è¿”å›ç”±å„ä¸ªå…ƒç´ è¾“å‡ºçš„å…ƒç´ çš„DStream. |  ds.map(x => x + 1)  |  f:(T) -> U  |
| flatMap()    |   å¯¹DStreamä¸­çš„æ¯ä¸ªå…ƒç´ åº”ç”¨ç»™å®šå‡½æ•°,è¿”å›æœ‰å„ä¸ªå…ƒç´ è¾“å‡ºè¿­ä»£å™¨ç»„æˆçš„DStream. |  ds.flatMap(x => x.split(" "))  |  f: T -> Iterable[U] |
| filter()    |   è¿”å›ç”±ç»™å®šDStreamä¸­é€šè¿‡ç­›é€‰çš„å…ƒç´ ç»„æˆçš„DStream. |  ds.filter(x => x != 1)  |  f: T -> Boolean  |
| repartition()    |   æ”¹å˜DStreamåˆ†åŒºæ•° |  ds.repartition(10)  |  N/A  |
| reduceByKey()    |   å°†æ¯ä¸ªæ‰¹æ¬¡ä¸­é—´ç›¸åŒçš„è®°å½•è§„çº¦ |  ds.reduceByKey((x,y) => x + y)  |  f:T , T -> T  |
| groupByKey()    | å°†æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„è®°å½•æ ¹æ®é”®åˆ†ç»„   | ds.groupByKey()  |  N/A  |

> éœ€è¦è®°ä½çš„æ˜¯,å°½ç®¡è¿™äº›å‡½æ•°çœ‹èµ·æ¥åƒä½œç”¨åœ¨æ•´ä¸ªæµä¸Šä¸€æ ·,ä½†äº‹å®ä¸Šæ¯ä¸ªDStreamåœ¨å†…éƒ¨æ˜¯ç”±è®¸å¤šRDD(æ‰¹æ¬¡)ç»„æˆ,ä¸”æ— çŠ¶æ€è½¬åŒ–æ“ä½œæ˜¯åˆ†åˆ«åº”ç”¨åˆ°æ¯ä¸ªRDDä¸Šçš„,ä¾‹å¦‚`reduceByKey()`ä¼šå½’çº¦æ¯ä¸ªæ—¶é—´åŒºé—´ä¸­æ•°æ®,ä½†ä¸ä¼šå½’çº¦ä¸åŒåŒºé—´ä¹‹é—´æ•°æ®.
> 
> ä¸¾ä¸ªä¾‹å­,åœ¨ä¹‹å‰çš„wordcountç¨‹åºä¸­,åªä¼šç»Ÿè®¡1ç§’å†…æ¥æ”¶åˆ°çš„æ•°æ®å•è¯ä¸ªæ•°,è€Œä¸ä¼šç´¯åŠ .
> 
> æ— çŠ¶æ€è½¬åŒ–æ“ä½œä¹Ÿèƒ½åœ¨å¤šä¸ªDStreamé—´æ•´åˆæ•°æ®,ä¸è¿‡ä¹Ÿæ˜¯åœ¨å„ä¸ªæ—¶é—´åŒºé—´å†…,ä¾‹å¦‚é”®å€¼å¯¹DStreamæ‹¥æœ‰å’ŒRDDä¸€æ ·çš„ä¸è¿æ¥ç›¸å…³çš„è½¬åŒ–æ“ä½œ,ä¹Ÿå°±æ˜¯`cogroup()` / `join()` / `leftOuterJoin()`ç­‰,å¯ä»¥åœ¨DStreamä¸Šä½¿ç”¨è¿™äº›æ“ä½œ,è¿™æ ·å°±å¯¹æ¯ä¸ªæ‰¹æ¬¡åˆ†åˆ«æ‰§è¡Œäº†å¯¹åº”çš„RDDæ“ä½œ.
> è¿˜å¯ä»¥åƒåœ¨å¸¸è§„çš„Sparkä¸­ä¸€æ ·ä½¿ç”¨DStreamçš„`union()`æ“ä½œå°†å®ƒå’Œå¦ä¸€ä¸ªDStream çš„å†…å®¹åˆå¹¶èµ·æ¥,ä¹Ÿå¯ä»¥ä½¿ç”¨StreamingContext.union()æ¥åˆå¹¶å¤šä¸ªæµ.

##### 1.5.4.2 æœ‰çŠ¶æ€è½¬åŒ–æ“ä½œ
###### 1.5.4.2.1 UpdateStateByKey
> UpdateStateByKeyåŸè¯­ç”¨äºè®°å½•å†å²è®°å½•,æœ‰æ—¶éœ€è¦åœ¨DStreamä¸­è·¨æ‰¹æ¬¡ç»´æŠ¤çŠ¶æ€(ä¾‹å¦‚æµè®¡ç®—ä¸­ç´¯åŠ wordcount),é’ˆå¯¹è¿™ç§æƒ…å†µ,`updateStateByKey()`æä¾›äº†å¯¹ä¸€ä¸ªçŠ¶æ€å˜é‡çš„è®¿é—®,ç”¨äºé”®å€¼å¯¹å½¢å¼DStream,ç»™å®šä¸€ä¸ªç”±(é”®,äº‹ä»¶)å¯¹æ„æˆDStream,å¹¶ä¼ é€’ä¸€ä¸ªæŒ‡å®šå¦‚ä½•æ ¹æ®æ–°çš„äº‹ä»¶æ›´æ–°æ¯ä¸ªé”®å¯¹åº”çŠ¶æ€çš„å‡½æ•°,å®ƒå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªæ–°çš„DStream,å…¶å†…éƒ¨æ•°æ®ä¸º(é”®,çŠ¶æ€) å¯¹.
> 
> `updateStateByKey()`ç»“æœä¼šæ˜¯ä¸€ä¸ªæ–°çš„DStream,å…¶å†…éƒ¨çš„RDDåºåˆ—æ˜¯ç”±æ¯ä¸ªæ—¶é—´åŒºé—´å¯¹åº”çš„(é”®,çŠ¶æ€)å¯¹ç»„æˆ.
> 
> `updateStateByKey`æ“ä½œå¯ä»¥åœ¨ç”¨æ–°ä¿¡æ¯è¿›è¡Œæ›´æ–°æ—¶ä¿æŒä»»æ„çŠ¶æ€,ä¸ºä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ï¼Œåªéœ€è¦åšä¸‹é¢ä¸¤æ­¥ : 
> 1. å®šä¹‰çŠ¶æ€,çŠ¶æ€å¯ä»¥æ˜¯ä¸€ä¸ªä»»æ„æ•°æ®ç±»å‹.
> 2.å®šä¹‰çŠ¶æ€æ›´æ–°å‡½æ•°,ç”¨æ­¤å‡½æ•°é˜æ˜å¦‚ä½•ä½¿ç”¨ä¹‹å‰çŠ¶æ€å’Œæ¥è‡ªè¾“å…¥æµæ–°å€¼å¯¹çŠ¶æ€è¿›è¡Œæ›´æ–°.
> ä½¿ç”¨`updateStateByKey`éœ€è¦å¯¹æ£€æŸ¥ç‚¹ç›®å½•è¿›è¡Œé…ç½®,ä¼šä½¿ç”¨æ£€æŸ¥ç‚¹æ¥ä¿å­˜çŠ¶æ€.
> æ›´æ–°ç‰ˆçš„wordcount

- 1.å®šä¹‰æœ‰çŠ¶æ€è½¬åŒ–æ“ä½œ | Create `UpdateStateByKeyWordCounAction.scala`
``` scala
package com.geekparkhub.core.spark.application.example

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * UpdateStateByKeyWordCounAction
  * <p>
  */

object UpdateStateByKeyWordCounAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("UpdateStateByKeyWordCounAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // åˆ›å»ºç¼“å­˜ç›®å½•æ£€æŸ¥ç«™
    ssc.checkpoint("./ck")

    // åˆ›å»º DStream
    val lineDStream: ReceiverInputDStream[String] = ssc.socketTextStream("systemhub511", 9999)

    // å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå•è¯
    val wordDStream: DStream[String] = lineDStream.flatMap(_.split(" "))

    // å°†å•è¯ä½è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_, 1))

    /**
      * å®šä¹‰æ›´æ–°çŠ¶æ€æ–¹æ³•
      * å‚æ•° valuesä¸ºå½“å‰æ‰¹æ¬¡å•è¯é¢‘åº¦
      * å‚æ•° stateä¸ºä»¥å¾€æ‰¹æ¬¡å•è¯é¢‘åº¦
      */
    val updateFunc = (values: Seq[Int], state: Option[Int]) => {
      val count: Int = values.sum
      val perCount: Int = state.getOrElse(0)
      Some(count + perCount)
    }

    // ç»Ÿè®¡å•è¯å‡ºç°ä¸ªæ•°
    val DStreamResult: DStream[(String, Int)] = wordAndOneDStream.updateStateByKey(updateFunc)

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    DStreamResult.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```
- 2.å¯åŠ¨ç¨‹åºå¹¶é€šè¿‡NetCatå‘é€æ•°æ®
```
[root@systemhub511 spark]# nc -kl 9999
hello
hello
hello
geek
geek
hello
hey
hey
test
test
hello
```
- 3.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559643684000 ms
-------------------------------------------
(hello,1)

-------------------------------------------
Time: 1559643687000 ms
-------------------------------------------
(hello,1)

-------------------------------------------
Time: 1559643690000 ms
-------------------------------------------
(hello,3)

-------------------------------------------
Time: 1559643693000 ms
-------------------------------------------
(hello,3)
(geek,1)

-------------------------------------------
Time: 1559643696000 ms
-------------------------------------------
(hello,3)
(geek,2)

-------------------------------------------
Time: 1559643699000 ms
-------------------------------------------
(hello,4)
(geek,2)

-------------------------------------------
Time: 1559643702000 ms
-------------------------------------------
(hello,4)
(geek,2)
(hey,1)

-------------------------------------------
Time: 1559643705000 ms
-------------------------------------------
(hello,4)
(geek,2)
(hey,2)

-------------------------------------------
Time: 1559643708000 ms
-------------------------------------------
(hello,4)
(test,2)
(geek,2)
(hey,2)

-------------------------------------------
Time: 1559643711000 ms
-------------------------------------------
(hello,5)
(test,2)
(geek,2)
(hey,2)
```

###### 1.5.4.2.2 Window Operations
> `Window Operations`æœ‰ç‚¹ç±»ä¼¼äºStormä¸­`State`,å¯ä»¥è®¾ç½®çª—å£å¤§å°å’Œæ»‘åŠ¨çª—å£é—´éš”æ¥åŠ¨æ€è·å–å½“å‰Steamingå…è®¸çŠ¶æ€.
> 
> åŸºäºçª—å£æ“ä½œä¼šåœ¨ä¸€ä¸ªæ¯”`StreamingContext`çš„æ‰¹æ¬¡é—´éš”æ›´é•¿æ—¶é—´èŒƒå›´å†…,é€šè¿‡æ•´åˆå¤šä¸ªæ‰¹æ¬¡ç»“æœ,è®¡ç®—å‡ºæ•´ä¸ªçª—å£çš„ç»“æœ.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_029.jpg)

> æ‰€æœ‰åŸºäºçª—å£æ“ä½œéƒ½éœ€è¦ä¸¤ä¸ªå‚æ•°,åˆ†åˆ«ä¸ºçª—å£æ—¶é•¿ä»¥åŠæ»‘åŠ¨æ­¥é•¿,ä¸¤è€…éƒ½å¿…é¡»æ˜¯`StreamContext` æ‰¹æ¬¡é—´éš”æ•´æ•°å€,çª—å£æ—¶é•¿æ§åˆ¶æ¯æ¬¡è®¡ç®—æœ€è¿‘å¤šå°‘ä¸ªæ‰¹æ¬¡æ•°æ®,å…¶å®å°±æ˜¯æœ€è¿‘çš„windowDuration/batchIntervalä¸ªæ‰¹æ¬¡.
> 
> å¦‚æœæœ‰ä¸€ä¸ªä»¥10ç§’ä¸ºæ‰¹æ¬¡é—´éš”æºDStream,è¦åˆ›å»ºä¸€ä¸ªæœ€è¿‘30ç§’æ—¶é—´çª—å£(å³æœ€è¿‘3ä¸ªæ‰¹æ¬¡),å°±åº”å½“æŠŠwindowDurationè®¾ä¸º30ç§’,è€Œæ»‘åŠ¨æ­¥é•¿é»˜è®¤å€¼ä¸æ‰¹æ¬¡é—´éš”ç›¸ç­‰,ç”¨æ¥æ§åˆ¶å¯¹æ–°çš„DStreamè¿›è¡Œè®¡ç®—é—´éš”,å¦‚æœæºDStreamæ‰¹æ¬¡é—´éš”ä¸º10ç§’,å¹¶ä¸”åªå¸Œæœ›æ¯ä¸¤ä¸ªæ‰¹æ¬¡è®¡ç®—ä¸€æ¬¡çª—å£ç»“æœ,å°±åº”è¯¥æŠŠæ»‘åŠ¨æ­¥é•¿è®¾ç½®ä¸º20ç§’.
> 
> å‡è®¾æ¯éš”åç§’å¯¹æŒç»­30ç§’æ•°æ®ç”Ÿæˆwordcount,ä¸ºåšåˆ°è¿™ä¸ªéœ€è¦åœ¨æŒç»­30ç§’æ•°æ®çš„(word,1)å¯¹DStreamä¸Šåº”ç”¨reduceByKey,ä½¿ç”¨æ“ä½œ`reduceByKeyAndWindow`
```
reduce last 30 seconds of data, every 10 secondwindowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x -y, 30, 20)
```
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_030.jpg)

> å…³äºWindowæ“ä½œæœ‰å¦‚ä¸‹åŸè¯­ : 
> 1.`window(windowLength,slideInterval)` : åŸºäºå¯¹æºDStreamçª—åŒ–æ‰¹æ¬¡è¿›è¡Œè®¡ç®—è¿”å›ä¸€ä¸ªæ–°çš„Dstream.
> 
> 2.`countByWindow(windowLength,slideInterval)` ï¼šè¿”å›æ»‘åŠ¨çª—å£è®¡æ•°æµä¸­çš„å…ƒç´ .
> 
> 3.`reduceByWindow(func,windowLength,slideInterval)` ï¼šé€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°æ•´åˆæ»‘åŠ¨åŒºé—´æµå…ƒç´ æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„å•å…ƒç´ æµ.
> 
> 4.`reduceByKeyAndWindow(func,windowLength,slideInterval, [numTasks])` ï¼šå½“(K,V)å¯¹çš„DStreamä¸Šè°ƒç”¨æ­¤å‡½æ•°,ä¼šè¿”å›ä¸€ä¸ªæ–°(K,V)å¯¹çš„DStream,æ­¤å¤„é€šè¿‡å¯¹æ»‘åŠ¨çª—å£ä¸­æ‰¹æ¬¡æ•°æ®ä½¿ç”¨reduceå‡½æ•°æ¥æ•´åˆæ¯ä¸ªkeyçš„valueå€¼,`Note`:é»˜è®¤æƒ…å†µä¸‹,è¿™ä¸ªæ“ä½œä½¿ç”¨Sparké»˜è®¤æ•°é‡å¹¶è¡Œä»»åŠ¡(æœ¬åœ°æ˜¯2),åœ¨é›†ç¾¤æ¨¡å¼ä¸­ä¾æ®é…ç½®å±æ€§(`spark.default.parallelism`)æ¥åšgrouping,å¯ä»¥é€šè¿‡è®¾ç½®å¯é€‰å‚æ•°`numTasks`æ¥è®¾ç½®ä¸åŒæ•°é‡tasks.
> 
> 5.`reduceByKeyAndWindow(func,invFunc,windowLength,slideInterval, [numTasks])` ï¼šè¿™ä¸ªå‡½æ•°æ˜¯ä¸Šè¿°å‡½æ•°æ›´é«˜æ•ˆç‰ˆæœ¬,æ¯ä¸ªçª—å£reduceå€¼éƒ½æ˜¯é€šè¿‡ç”¨å‰ä¸€ä¸ªçª—çš„reduceå€¼æ¥é€’å¢è®¡ç®—,é€šè¿‡reduceè¿›å…¥åˆ°æ»‘åŠ¨çª—å£æ•°æ®å¹¶â€åå‘reduceâ€ç¦»å¼€çª—å£çš„æ—§æ•°æ®æ¥å®ç°è¿™ä¸ªæ“ä½œ,ä¸€ä¸ªä¾‹å­æ˜¯éšç€çª—å£æ»‘åŠ¨å¯¹keysçš„â€œåŠ â€â€œå‡â€è®¡æ•°,è¿™ä¸ªå‡½æ•°åªé€‚ç”¨äºâ€å¯é€†çš„reduceå‡½æ•°â€,ä¹Ÿå°±æ˜¯è¿™äº›reduceå‡½æ•°æœ‰ç›¸åº”çš„â€åreduceâ€å‡½æ•°(ä»¥å‚æ•°`invFunc`å½¢å¼ä¼ å…¥),å¦‚å‰è¿°å‡½æ•°reduceä»»åŠ¡æ•°é‡é€šè¿‡å¯é€‰å‚æ•°æ¥é…ç½®,æ³¨æ„ä¸ºäº†ä½¿ç”¨è¿™ä¸ªæ“ä½œ,æ£€æŸ¥ç‚¹å¿…é¡»å¯ç”¨.
> 
> 6.`countByValueAndWindow(windowLength,slideInterval, [numTasks])` ï¼šå¯¹(K,V)å¯¹çš„DStreamè°ƒç”¨,è¿”å›(K,Long)å¯¹çš„æ–°DStream,å…¶ä¸­æ¯ä¸ªkeyå€¼æ˜¯å…¶åœ¨æ»‘åŠ¨çª—å£ä¸­é¢‘ç‡,å¦‚ä¸Šå¯é…ç½®reduceä»»åŠ¡æ•°é‡.
> 
> `reduceByWindow()`å’Œ`reduceByKeyAndWindow()`å¯ä»¥å¯¹æ¯ä¸ªçª—å£æ›´é«˜æ•ˆåœ°è¿›è¡Œå½’çº¦æ“ä½œ,å®ƒä»¬æ¥æ”¶ä¸€ä¸ªå½’çº¦å‡½æ•°,åœ¨æ•´ä¸ªçª—å£ä¸Šæ‰§è¡Œ,æ¯”å¦‚+é™¤æ­¤ä»¥å¤–,å®ƒä»¬è¿˜æœ‰ä¸€ç§ç‰¹æ®Šå½¢å¼,é€šè¿‡åªè€ƒè™‘æ–°è¿›å…¥çª—å£æ•°æ®å’Œç¦»å¼€çª—å£æ•°æ®,è®©Sparkå¢é‡è®¡ç®—å½’çº¦ç»“æœ,è¿™ç§ç‰¹æ®Šå½¢å¼éœ€è¦æä¾›å½’çº¦å‡½æ•°çš„ä¸€ä¸ªé€†å‡½æ•°,æ¯”å¦‚+å¯¹åº”çš„é€†å‡½æ•°ä¸º-,å¯¹äºè¾ƒå¤§çš„çª—å£,æä¾›é€†å‡½æ•°å¯ä»¥å¤§å¤§æé«˜æ‰§è¡Œæ•ˆç‡.
> 
> `countByWindow()`å’Œ`countByValueAndWindow()`ä½œä¸ºå¯¹æ•°æ®è¿›è¡Œè®¡æ•°æ“ä½œçš„ç®€å†™,`countByWindow()`è¿”å›ä¸€ä¸ªè¡¨ç¤ºæ¯ä¸ªçª—å£ä¸­å…ƒç´ ä¸ªæ•°çš„DStream,è€Œ`countByValueAndWindow()`è¿”å›çš„DStreamåˆ™åŒ…å«çª—å£ä¸­æ¯ä¸ªå€¼çš„ä¸ªæ•°.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_031.jpg)

- 1.Create `WindowOperationsWordCountAction.scala`
``` scala
package com.geekparkhub.core.spark.application.example

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}


/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * WindowOperationsWordCountAction
  * <p>
  */

object WindowOperationsWordCountAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»º SparkConf
    val sc: SparkConf = new SparkConf().setMaster("local[*]").setAppName("WindowOperationsWordCountAction")

    //åˆ›å»º StreamingContext
    val ssc = new StreamingContext(sc, Seconds(3))

    // åˆ›å»ºç¼“å­˜ç›®å½•æ£€æŸ¥ç«™
    ssc.checkpoint("./ck")

    // åˆ›å»º DStream
    val lineDStream: ReceiverInputDStream[String] = ssc.socketTextStream("systemhub511", 9999)

    // å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå•è¯
    val wordDStream: DStream[String] = lineDStream.flatMap(_.split(" "))

    // å°†å•è¯ä½è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_, 1))

    // ç»Ÿè®¡å•è¯å‡ºç°ä¸ªæ•°
    val DStreamResult: DStream[(String, Int)] = wordAndOneDStream.reduceByKeyAndWindow((x: Int, y: Int) => x + y, Seconds(6), Seconds(3))

    // è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    DStreamResult.print()

    // å¯åŠ¨æµå¼ä»»åŠ¡
    ssc.start()
    ssc.awaitTermination()
  }
}
```

- 2.å¯åŠ¨ç¨‹åºå¹¶é€šè¿‡NetCatå‘é€æ•°æ®
```
[root@systemhub511 spark]# nc -kl 9999
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
sparkstreaming sparkcore sparksql spark
```

- 3.æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
```
-------------------------------------------
Time: 1559652894000 ms
-------------------------------------------

-------------------------------------------
Time: 1559652897000 ms
-------------------------------------------
(sparksql,1)
(sparkcore,1)
(spark,1)
(sparkstreaming,1)

-------------------------------------------
Time: 1559652900000 ms
-------------------------------------------
(sparksql,3)
(sparkcore,3)
(spark,3)
(sparkstreaming,3)

-------------------------------------------
Time: 1559652903000 ms
-------------------------------------------
(sparksql,7)
(sparkcore,7)
(spark,7)
(sparkstreaming,7)

-------------------------------------------
Time: 1559652906000 ms
-------------------------------------------
(sparksql,8)
(sparkcore,8)
(spark,8)
(sparkstreaming,8)

-------------------------------------------
Time: 1559652909000 ms
-------------------------------------------
(sparksql,3)
(sparkcore,3)
(spark,3)
(sparkstreaming,3)

-------------------------------------------
Time: 1559652912000 ms
-------------------------------------------

-------------------------------------------
```

##### 1.5.4.3 å…¶ä»–é‡è¦æ“ä½œ
###### 1.5.4.3.1 Transform
> TransformåŸè¯­å…è®¸DStreamä¸Šæ‰§è¡Œä»»æ„`RDD-to-RDD`å‡½æ•°,å³ä½¿è¿™äº›å‡½æ•°å¹¶æ²¡æœ‰åœ¨DStreamAPIä¸­æš´éœ²å‡ºæ¥,é€šè¿‡è¯¥å‡½æ•°å¯ä»¥æ–¹ä¾¿çš„æ‰©å±•Spark API,è¯¥å‡½æ•°æ¯ä¸€æ‰¹æ¬¡è°ƒåº¦ä¸€æ¬¡,å…¶å®ä¹Ÿå°±æ˜¯å¯¹DStreamä¸­çš„RDDåº”ç”¨è½¬æ¢.

###### 1.5.4.3.2 Join
> è¿æ¥æ“ä½œ(`leftOuterJoin` / `rightOuterJoin` / `fullOuterJoin`)ä¹Ÿå¯ä»¥è¿æ¥`Stream-Stream`,`windows-stream to windows-stream` / `stream-dataset`


#### 1.5.5 DataStream è¾“å‡º
> è¾“å‡ºæ“ä½œæŒ‡å®šå¯¹æµæ•°æ®ç»è½¬åŒ–æ“ä½œå¾—åˆ°çš„æ•°æ®æ‰€è¦æ‰§è¡Œçš„æ“ä½œ(ä¾‹å¦‚æŠŠç»“æœæ¨å…¥å¤–éƒ¨æ•°æ®åº“æˆ–è¾“å‡ºåˆ°å±å¹•ä¸Š),ä¸RDDä¸­æƒ°æ€§æ±‚å€¼ç±»ä¼¼,å¦‚æœä¸€ä¸ªDStreamåŠå…¶æ´¾ç”ŸDStreaméƒ½æ²¡æœ‰è¢«æ‰§è¡Œè¾“å‡ºæ“ä½œ,é‚£ä¹ˆè¿™äº›DStreamå°±éƒ½ä¸ä¼šè¢«æ±‚å€¼,å¦‚æœStreamingContextä¸­æ²¡æœ‰è®¾å®šè¾“å‡ºæ“ä½œ,æ•´ä¸ªcontextå°±éƒ½ä¸ä¼šå¯åŠ¨.
> 
> è¾“å‡ºæ“ä½œå¦‚ä¸‹ : 
> 1.`print()` ï¼šåœ¨è¿è¡Œæµç¨‹åºé©±åŠ¨ç»“ç‚¹ä¸Šæ‰“å°DStreamä¸­æ¯ä¸€æ‰¹æ¬¡æ•°æ®æœ€å¼€å§‹10ä¸ªå…ƒç´ ,è¿™ç”¨äºå¼€å‘å’Œè°ƒè¯•,åœ¨Python APIä¸­,åŒæ ·çš„æ“ä½œå«print().
> 
> 2.`saveAsTextFiles(prefix, [suffix])` ï¼šä»¥textæ–‡ä»¶å½¢å¼å­˜å‚¨è¿™ä¸ªDStreamçš„å†…å®¹,æ¯ä¸€æ‰¹æ¬¡å­˜å‚¨æ–‡ä»¶ååŸºäºå‚æ•°ä¸­çš„prefixå’Œsuffix,`prefix-Time_IN_MS[.suffix]`
> 
> 3.`saveAsObjectFiles(prefix, [suffix])` ï¼šä»¥Javaå¯¹è±¡åºåˆ—åŒ–æ–¹å¼å°†Streamä¸­çš„æ•°æ®ä¿å­˜ä¸ºSequenceFiles,æ¯ä¸€æ‰¹æ¬¡å­˜å‚¨æ–‡ä»¶ååŸºäºå‚æ•°ä¸­çš„ä¸º`prefix-TIME_IN_MS[.suffix]`,Pythonä¸­ç›®å‰ä¸å¯ç”¨.
> 
> 4.`saveAsHadoopFiles(prefix, [suffix])` ï¼šå°†Streamä¸­æ•°æ®ä¿å­˜ä¸ºHadoopfiles, æ¯ä¸€æ‰¹æ¬¡å­˜å‚¨æ–‡ä»¶ååŸºäºå‚æ•°ä¸­çš„ä¸º`prefix-TIME_IN_MS[.suffix]`,Python APIPythonä¸­ç›®å‰ä¸å¯ç”¨.
> 
> 5.`foreachRDD(func)` ï¼šè¿™æ˜¯æœ€é€šç”¨çš„è¾“å‡ºæ“ä½œ,å³å°†å‡½æ•°funcç”¨äºäº§ç”Ÿäºstreamæ¯ä¸€ä¸ªRDD,å…¶ä¸­å‚æ•°ä¼ å…¥å‡½æ•°funcåº”è¯¥å®ç°å°†æ¯ä¸€ä¸ªRDDä¸­æ•°æ®æ¨é€åˆ°å¤–éƒ¨ç³»ç»Ÿ,å¦‚å°†RDDå­˜å…¥æ–‡ä»¶æˆ–è€…é€šè¿‡ç½‘ç»œå°†å…¶å†™å…¥æ•°æ®åº“,æ³¨æ„å‡½æ•°funcåœ¨è¿è¡Œæµåº”ç”¨é©±åŠ¨ä¸­è¢«æ‰§è¡Œ,åŒæ—¶å…¶ä¸­ä¸€èˆ¬å‡½æ•°RDDæ“ä½œä»è€Œå¼ºåˆ¶å…¶å¯¹äºæµRDDçš„è¿ç®—.
> 
> é€šç”¨è¾“å‡ºæ“ä½œ`foreachRDD()`,å®ƒç”¨æ¥å¯¹DStreamä¸­çš„RDDè¿è¡Œä»»æ„è®¡ç®—,è¿™å’Œ`transform()`æœ‰äº›ç±»ä¼¼,éƒ½å¯ä»¥è®¿é—®ä»»æ„RDD,åœ¨foreachRDD()ä¸­,å¯ä»¥é‡ç”¨Sparkä¸­å®ç°çš„æ‰€æœ‰è¡ŒåŠ¨æ“ä½œ,æ¯”å¦‚å¸¸è§çš„ç”¨ä¾‹ä¹‹ä¸€æ˜¯æŠŠæ•°æ®å†™åˆ°è¯¸å¦‚MySQLçš„å¤–éƒ¨æ•°æ®åº“ä¸­.
> 
> `æ³¨æ„`: 
> 1.è¿æ¥ä¸èƒ½å†™åœ¨driverå±‚é¢
> 2.å¦‚æœå†™åœ¨foreachåˆ™æ¯ä¸ªRDDéƒ½åˆ›å»º,å¾—ä¸å¿å¤±
> 3.å¢åŠ foreachPartition,åœ¨åˆ†åŒºåˆ›å»º


#### 1.5.5 DataStream Program
##### 1.5.5.1 ç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡
> `ç´¯åŠ å™¨(Accumulators)`å’Œ`å¹¿æ’­å˜é‡(Broadcast variables)`ä¸èƒ½ä»Spark Streamingçš„æ£€æŸ¥ç‚¹ä¸­æ¢å¤,å¦‚æœå¯ç”¨æ£€æŸ¥å¹¶ä¹Ÿä½¿ç”¨äº†ç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡,é‚£ä¹ˆå¿…é¡»åˆ›å»ºç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡çš„å»¶è¿Ÿå•å®ä¾‹ä»è€Œåœ¨é©±åŠ¨å› å¤±æ•ˆé‡å¯åå¯ä»¥è¢«é‡æ–°å®ä¾‹åŒ–.

##### 1.5.5.2 DataFrame ans SQL Operations
> å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨æµæ•°æ®ä¸Šä½¿ç”¨DataFrameså’ŒSQL,å¿…é¡»ä½¿ç”¨SparkContextæ¥åˆ›å»ºStreamingContextè¦ç”¨çš„SQLContext,æ­¤å¤–è¿™ä¸€è¿‡ç¨‹å¯ä»¥åœ¨é©±åŠ¨å¤±æ•ˆåé‡å¯,é€šè¿‡åˆ›å»ºä¸€ä¸ªå®ä¾‹åŒ–SQLContextå•å®ä¾‹æ¥å®ç°è¿™ä¸ªå·¥ä½œ,æ¯ä¸ªRDDè¢«è½¬æ¢ä¸ºDataFrame,ä»¥ä¸´æ—¶è¡¨æ ¼é…ç½®å¹¶ç”¨SQLè¿›è¡ŒæŸ¥è¯¢.

##### 1.5.5.3 Caching / Persistence
> å’ŒRDDsç±»ä¼¼,DStreamsåŒæ ·å…è®¸å¼€å‘è€…å°†æµæ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­,ä¹Ÿå°±æ˜¯è¯´åœ¨DStreamä¸Šä½¿ç”¨`persist()`æ–¹æ³•å°†ä¼šDStreamsä¸­çš„æ¯ä¸ªRDDä¿å­˜åœ¨å†…å­˜ä¸­,å½“DStreamä¸­æ•°æ®è¦è¢«å¤šæ¬¡è®¡ç®—æ—¶,è¿™ä¸ªéå¸¸æœ‰ç”¨(å¦‚åœ¨åŒæ ·æ•°æ®ä¸Šçš„å¤šæ¬¡æ“ä½œ),å¯¹äºåƒ`reduceByWindow`å’Œ`reduceByKeyAndWindow`ä»¥åŠåŸºäºçŠ¶æ€çš„(updateStateByKey)è¿™ç§æ“ä½œ,ä¿å­˜æ˜¯éšå«é»˜è®¤çš„,å› æ­¤å³ä½¿å¼€å‘è€…æ²¡æœ‰è°ƒç”¨persist(),ç”±åŸºäºçª—æ“ä½œäº§ç”Ÿçš„DStreamsä¼šè‡ªåŠ¨ä¿å­˜åœ¨å†…å­˜ä¸­.


## ğŸ”¥ 2. Spark é«˜é˜¶ ğŸ”¥
### 2.1 Spark å†…æ ¸è§£æ

#### 2.1.1 Spark å†…æ ¸æ¦‚è¿°	
> Sparkå†…æ ¸æ³›æŒ‡Sparkæ ¸å¿ƒè¿è¡Œæœºåˆ¶,åŒ…æ‹¬Sparkæ ¸å¿ƒç»„ä»¶çš„è¿è¡Œæœºåˆ¶ã€Sparkä»»åŠ¡è°ƒåº¦æœºåˆ¶ã€Sparkå†…å­˜ç®¡ç†æœºåˆ¶ã€Sparkæ ¸å¿ƒåŠŸèƒ½çš„è¿è¡ŒåŸç†ç­‰,ç†Ÿç»ƒæŒæ¡Sparkå†…æ ¸åŸç†,èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°å®ŒæˆSparkä»£ç è®¾è®¡,å¹¶èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬å‡†ç¡®é”å®šé¡¹ç›®è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜çš„ç—‡ç»“æ‰€åœ¨.
##### 2.1.1.1 Sparkæ ¸å¿ƒç»„ä»¶å›é¡¾
###### 2.1.1.1.1 Driver
> Sparké©±åŠ¨å™¨èŠ‚ç‚¹,ç”¨äºæ‰§è¡ŒSparkä»»åŠ¡ä¸­çš„mainæ–¹æ³•,è´Ÿè´£å®é™…ä»£ç çš„æ‰§è¡Œå·¥ä½œ.
> 
> Driveråœ¨Sparkä½œä¸šæ‰§è¡Œæ—¶ä¸»è¦è´Ÿè´£ : 
> 
> 1.å°†ç”¨æˆ·ç¨‹åºè½¬åŒ–ä¸ºä»»åŠ¡(job).
> 2.åœ¨Executorä¹‹é—´è°ƒåº¦ä»»åŠ¡(task).
> 3.è·Ÿè¸ªExecutorçš„æ‰§è¡Œæƒ…å†µ.
> 4.é€šè¿‡UIå±•ç¤ºæŸ¥è¯¢è¿è¡Œæƒ…å†µ.

###### 2.1.1.1.2 Executor
> Spark ExecutorèŠ‚ç‚¹æ˜¯ä¸€ä¸ªJVMè¿›ç¨‹,è´Ÿè´£åœ¨Sparkä½œä¸šä¸­è¿è¡Œå…·ä½“ä»»åŠ¡,ä»»åŠ¡å½¼æ­¤ä¹‹é—´ç›¸äº’ç‹¬ç«‹,Sparkåº”ç”¨å¯åŠ¨æ—¶,ExecutorèŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨,å¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ªSparkåº”ç”¨ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨,å¦‚æœæœ‰ExecutorèŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒ,Sparkåº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œ,ä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šçš„ä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»–ExecutorèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œ.
> 
> Executoræœ‰ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ : 
> 
> 1.è´Ÿè´£è¿è¡Œç»„æˆSparkåº”ç”¨ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™Driverè¿›ç¨‹.
> 2.å®ƒä»¬é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨(Block Manager)ä¸ºç”¨æˆ·ç¨‹åºä¸­è¦æ±‚ç¼“å­˜çš„RDDæä¾›å†…å­˜å¼å­˜å‚¨,RDDæ˜¯ç›´æ¥ç¼“å­˜åœ¨Executorè¿›ç¨‹å†…,å› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—.

##### 2.1.1.2 Spark é€šç”¨è¿è¡Œæµç¨‹æ¦‚è¿°
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_032.jpg)

> Sparké€šç”¨è¿è¡Œæµç¨‹,ä¸è®ºSparkä»¥ä½•ç§æ¨¡å¼è¿›è¡Œéƒ¨ç½²,ä»»åŠ¡æäº¤å,éƒ½ä¼šå…ˆå¯åŠ¨Driverè¿›ç¨‹,éšåDriverè¿›ç¨‹å‘é›†ç¾¤ç®¡ç†å™¨æ³¨å†Œåº”ç”¨ç¨‹åº,ä¹‹åé›†ç¾¤ç®¡ç†å™¨æ ¹æ®æ­¤ä»»åŠ¡çš„é…ç½®æ–‡ä»¶åˆ†é…Executorå¹¶å¯åŠ¨,å½“Driveræ‰€éœ€çš„èµ„æºå…¨éƒ¨æ»¡è¶³å,Driverå¼€å§‹æ‰§è¡Œmainå‡½æ•°,SparkæŸ¥è¯¢ä¸ºæ‡’æ‰§è¡Œ,å½“æ‰§è¡Œåˆ°actionç®—å­æ—¶å¼€å§‹åå‘æ¨ç®—,æ ¹æ®å®½ä¾èµ–è¿›è¡Œstageåˆ’åˆ†,éšåæ¯ä¸€ä¸ªstageå¯¹åº”ä¸€ä¸ªtaskset,tasksetä¸­æœ‰å¤šä¸ªtask,æ ¹æ®æœ¬åœ°åŒ–åŸåˆ™,taskä¼šè¢«åˆ†å‘åˆ°æŒ‡å®šçš„Executorå»æ‰§è¡Œ,åœ¨ä»»åŠ¡æ‰§è¡Œçš„è¿‡ç¨‹ä¸­,Executorä¹Ÿä¼šä¸æ–­ä¸Driverè¿›è¡Œé€šä¿¡,æŠ¥å‘Šä»»åŠ¡è¿è¡Œæƒ…å†µ.

#### 2.1.2 Spark éƒ¨ç½²æ¨¡å¼
> Sparkæ”¯æŒ`3`ç§é›†ç¾¤ç®¡ç†å™¨ (ClusterManager),åˆ†åˆ«ä¸º : 
> 
> 1.`Standalone` :` ç‹¬ç«‹æ¨¡å¼`,SparkåŸç”Ÿç®€å•é›†ç¾¤ç®¡ç†å™¨,è‡ªå¸¦å®Œæ•´æœåŠ¡,å¯å•ç‹¬éƒ¨ç½²åˆ°ä¸€ä¸ªé›†ç¾¤ä¸­,æ— éœ€ä¾èµ–ä»»ä½•å…¶ä»–èµ„æºç®¡ç†ç³»ç»Ÿ,ä½¿ç”¨Standaloneå¯ä»¥å¾ˆæ–¹ä¾¿åœ°æ­å»ºä¸€ä¸ªé›†ç¾¤.
> 
> 2.`Apache Mesos` ï¼šä¸€ä¸ªå¼ºå¤§åˆ†å¸ƒå¼èµ„æºç®¡ç†æ¡†æ¶,å®ƒå…è®¸å¤šç§ä¸åŒçš„æ¡†æ¶éƒ¨ç½²åœ¨å…¶ä¸Š,åŒ…æ‹¬yarn.
> 
> 3.`HadoopYARN` ï¼šç»Ÿä¸€èµ„æºç®¡ç†æœºåˆ¶,å¯ä»¥è¿è¡Œå¤šå¥—è®¡ç®—æ¡†æ¶,å¦‚MapReduce/Stormç­‰,æ ¹æ®driveråœ¨é›†ç¾¤ä¸­çš„ä½ç½®ä¸åŒ,åˆ†ä¸ºYarnClientå’ŒYarnCluster.
> 
> 4.å®é™…ä¸Šé™¤äº†ä¸Šè¿°è¿™äº›é€šç”¨çš„é›†ç¾¤ç®¡ç†å™¨å¤–,Sparkå†…éƒ¨ä¹Ÿæä¾›äº†ä¸€äº›æ–¹ä¾¿ç”¨æˆ·æµ‹è¯•å’Œå­¦ä¹ çš„ç®€å•é›†ç¾¤éƒ¨ç½²æ¨¡å¼,ç”±äºåœ¨å®é™…å·¥å‚ç¯å¢ƒä¸‹ä½¿ç”¨çš„ç»å¤§å¤šæ•°çš„é›†ç¾¤ç®¡ç†å™¨æ˜¯HadoopYARN,å› æ­¤æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹æ˜¯HadoopYARNæ¨¡å¼ä¸‹çš„Sparké›†ç¾¤éƒ¨ç½².
> 
> 5.Sparkè¿è¡Œæ¨¡å¼å–å†³äºä¼ é€’ç»™SparkContextçš„MASTERç¯å¢ƒå˜é‡çš„å€¼,ä¸ªåˆ«æ¨¡å¼è¿˜éœ€è¦è¾…åŠ©çš„ç¨‹åºæ¥å£æ¥é…åˆä½¿ç”¨,ç›®å‰æ”¯æŒçš„Masterå­—ç¬¦ä¸²åŠURL.
> 
> 6.Sparkè¿è¡Œæ¨¡å¼é…ç½®è¡¨
> | Master URL |  Meaning |
> | :--------: | :--------:|
> | `local`    |   åœ¨æœ¬åœ°è¿è¡Œ,åªæœ‰ä¸€ä¸ªå·¥ä½œè¿›ç¨‹,æ— å¹¶è¡Œè®¡ç®—èƒ½åŠ›. |
> | `local[K]`    |   åœ¨æœ¬åœ°è¿è¡Œ,æœ‰Kä¸ªå·¥ä½œè¿›ç¨‹,é€šå¸¸è®¾ç½®Kä¸ºæœºå™¨CPUæ ¸å¿ƒæ•°é‡. |
> | `local[*]`    |   åœ¨æœ¬åœ°è¿è¡Œ,å·¥ä½œè¿›ç¨‹æ•°é‡ç­‰äºæœºå™¨CPUæ ¸å¿ƒæ•°é‡. |
> | `spark://HOST:PORT`    |   ä»¥Standaloneæ¨¡å¼è¿è¡Œ,è¿™æ˜¯Sparkè‡ªèº«æä¾›é›†ç¾¤è¿è¡Œæ¨¡å¼,é»˜è®¤ç«¯å£å·:7077,è¯¦ç»†æ–‡æ¡£è§:`Spark standalone cluster` |
> | `mesos://HOST:PORT`    |   åœ¨Mesosé›†ç¾¤ä¸Šè¿è¡Œ,Driverè¿›ç¨‹å’ŒWorkerè¿›ç¨‹è¿è¡Œåœ¨Mesosé›†ç¾¤ä¸Š,éƒ¨ç½²æ¨¡å¼å¿…é¡»ä½¿ç”¨å›ºå®šå€¼:`--deploy-mode cluster`,è¯¦ç»†æ–‡æ¡£è§:`MesosClusterDispatcher` |
> | `yarn-client`    |   åœ¨Yarné›†ç¾¤ä¸Šè¿è¡Œ,Driverè¿›ç¨‹åœ¨æœ¬åœ°,Workè¿›ç¨‹åœ¨Yarné›†ç¾¤ä¸Š,éƒ¨ç½²æ¨¡å¼å¿…é¡»ä½¿ç”¨å›ºå®šå€¼:`--deploy-modeclient`,Yarné›†ç¾¤åœ°å€å¿…é¡»åœ¨`HADOOP_CONF_DIRorYARN_CONF_DIR`å˜é‡é‡Œå®šä¹‰. |
> | `yarn-cluster`    |   åœ¨Yarné›†ç¾¤ä¸Šè¿è¡Œ,Driverè¿›ç¨‹åœ¨Yarné›†ç¾¤ä¸Š,Workè¿›ç¨‹ä¹Ÿåœ¨Yarné›†ç¾¤ä¸Š,éƒ¨ç½²æ¨¡å¼å¿…é¡»ä½¿ç”¨å›ºå®šå€¼:`--deploy-mode cluster`,Yarné›†ç¾¤åœ°å€å¿…é¡»åœ¨`HADOOP_CONF_DIRorYARN_CONF_DIR`å˜é‡é‡Œå®šä¹‰. |
> 
> ç”¨æˆ·åœ¨æäº¤ä»»åŠ¡ç»™Sparkå¤„ç†æ—¶,ä»¥ä¸‹ä¸¤ä¸ªå‚æ•°å…±åŒå†³å®šäº†Sparkè¿è¡Œæ–¹å¼.
> 
> 1.`â€“master MASTER_URL` ï¼šå†³å®šäº†Sparkä»»åŠ¡æäº¤ç»™å“ªç§é›†ç¾¤å¤„ç†.
> 
> 2.`â€“deploy-mode DEPLOY_MODE` ï¼šå†³å®šäº†Driverçš„è¿è¡Œæ–¹å¼,å¯é€‰å€¼ä¸ºClientæˆ–è€…Cluster

##### 2.1.2.1 Standalone æ¨¡å¼è¿è¡Œæœºåˆ¶
> Standaloneé›†ç¾¤æœ‰å››ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†,åˆ†åˆ«æ˜¯ : 
> 
> 1.`Driver` ï¼šæ˜¯ä¸€ä¸ªè¿›ç¨‹,ç¼–å†™Sparkåº”ç”¨ç¨‹åºå°±è¿è¡Œåœ¨Driverä¸Š,ç”±Driverè¿›ç¨‹æ‰§è¡Œ.
> 
> 2.`Master` ï¼šæ˜¯ä¸€ä¸ªè¿›ç¨‹,ä¸»è¦è´Ÿè´£èµ„æºè°ƒåº¦å’Œåˆ†é…,å¹¶è¿›è¡Œé›†ç¾¤ç›‘æ§ç­‰èŒè´£.
> 
> 3.`Worker` ï¼šæ˜¯ä¸€ä¸ªè¿›ç¨‹,ä¸€ä¸ªWorkerè¿è¡Œåœ¨é›†ç¾¤ä¸­çš„ä¸€å°æœåŠ¡å™¨ä¸Š,ä¸»è¦è´Ÿè´£ä¸¤ä¸ªèŒè´£,ä¸€ä¸ªæ˜¯ç”¨è‡ªå·±å†…å­˜å­˜å‚¨RDDæŸä¸ªæˆ–æŸäº›partition,å¦ä¸€ä¸ªæ˜¯å¯åŠ¨å…¶ä»–è¿›ç¨‹å’Œçº¿ç¨‹(Executor),å¯¹RDDä¸Šçš„partitionè¿›è¡Œå¹¶è¡Œçš„å¤„ç†å’Œè®¡ç®—.
> 
> 4.`Executor` ï¼šæ˜¯ä¸€ä¸ªè¿›ç¨‹,ä¸€ä¸ªWorkerä¸Šå¯ä»¥è¿è¡Œå¤šä¸ªExecutor,Executoré€šè¿‡å¯åŠ¨å¤šä¸ªçº¿ç¨‹(task)æ¥æ‰§è¡Œå¯¹RDDçš„partitionè¿›è¡Œå¹¶è¡Œè®¡ç®—,ä¹Ÿå°±æ˜¯æ‰§è¡ŒRDDå®šä¹‰,ä¾‹å¦‚mapã€flatMapã€reduceç­‰ç®—å­æ“ä½œ.

###### 2.1.2.1.1 Standalone Client æ¨¡å¼

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_033.jpg)
> åœ¨Standalone Clientæ¨¡å¼ä¸‹,Driveråœ¨ä»»åŠ¡æäº¤çš„æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œ,Driverå¯åŠ¨åå‘Masteræ³¨å†Œåº”ç”¨ç¨‹åº,Masteræ ¹æ®submitè„šæœ¬èµ„æºéœ€æ±‚æ‰¾åˆ°å†…éƒ¨èµ„æºè‡³å°‘å¯ä»¥å¯åŠ¨ä¸€ä¸ªExecutorçš„æ‰€æœ‰Worker,ç„¶ååœ¨è¿™äº›Workerä¹‹é—´åˆ†é…Executor,Workerä¸ŠExecutorå¯åŠ¨åä¼šå‘Driveråå‘æ³¨å†Œ,æ‰€æœ‰Executoræ³¨å†Œå®Œæˆå,Driverå¼€å§‹æ‰§è¡Œmainå‡½æ•°,ä¹‹åæ‰§è¡Œåˆ°Actionç®—å­æ—¶,å¼€å§‹åˆ’åˆ†stage,æ¯ä¸ªstageç”Ÿæˆå¯¹åº”çš„taskSet,ä¹‹åå°†taskåˆ†å‘åˆ°å„ä¸ªExecutorä¸Šæ‰§è¡Œ.
###### 2.1.2.1.2 Standalone Cluster æ¨¡å¼

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_034.jpg)

> åœ¨Standalone Clusteræ¨¡å¼ä¸‹,ä»»åŠ¡æäº¤å,Masterä¼šæ‰¾åˆ°ä¸€ä¸ªWorkerå¯åŠ¨Driverè¿›ç¨‹,Driverå¯åŠ¨åå‘Masteræ³¨å†Œåº”ç”¨ç¨‹åº,Masteræ ¹æ®submitè„šæœ¬èµ„æºéœ€æ±‚æ‰¾åˆ°å†…éƒ¨èµ„æºè‡³å°‘å¯ä»¥å¯åŠ¨ä¸€ä¸ªExecutorçš„æ‰€æœ‰Worker,ç„¶ååœ¨è¿™äº›Workerä¹‹é—´åˆ†é…Executor,Workerä¸Šçš„Executorå¯åŠ¨åä¼šå‘Driveråå‘æ³¨å†Œ,æ‰€æœ‰Executoræ³¨å†Œå®Œæˆå,Driverå¼€å§‹æ‰§è¡Œmainå‡½æ•°,ä¹‹åæ‰§è¡Œåˆ°Actionç®—å­æ—¶,å¼€å§‹åˆ’åˆ†stage,æ¯ä¸ªstageç”Ÿæˆå¯¹åº”çš„taskSet,ä¹‹åå°†taskåˆ†å‘åˆ°å„ä¸ªExecutorä¸Šæ‰§è¡Œ.
> 
> æ³¨æ„ : Standaloneä¸¤ç§æ¨¡å¼ä¸‹(`Client` / `Cluster`),Masteråœ¨æ¥åˆ°Driveræ³¨å†ŒSparkåº”ç”¨ç¨‹åºè¯·æ±‚å,ä¼šè·å–å…¶æ‰€ç®¡ç†çš„å‰©ä½™èµ„æºèƒ½å¤Ÿå¯åŠ¨ä¸€ä¸ªExecutorçš„æ‰€æœ‰Worker,ç„¶ååœ¨è¿™äº›Workerä¹‹é—´åˆ†å‘Executor,æ­¤æ—¶åˆ†å‘åªè€ƒè™‘Workerä¸Šèµ„æºæ˜¯å¦è¶³å¤Ÿä½¿ç”¨,ç›´åˆ°å½“å‰åº”ç”¨ç¨‹åºæ‰€éœ€æ‰€æœ‰Executoréƒ½åˆ†é…å®Œæ¯•,Executoråå‘æ³¨å†Œå®Œæ¯•å,Driverå¼€å§‹æ‰§è¡Œmainç¨‹åº.

##### 2.1.2.2 YARNæ¨¡å¼ è¿è¡Œæœºåˆ¶ 

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_035.jpg)
> åœ¨YARNClientæ¨¡å¼ä¸‹,Driveråœ¨ä»»åŠ¡æäº¤æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œ,Driverå¯åŠ¨åä¼šå’ŒResourceManageré€šè®¯ç”³è¯·å¯åŠ¨ApplicationMaster,éšåResourceManageråˆ†é…container,åœ¨åˆé€‚çš„NodeManagerä¸Šå¯åŠ¨ApplicationMaster,æ­¤æ—¶çš„ApplicationMasterçš„åŠŸèƒ½ç›¸å½“äºä¸€ä¸ªExecutorLaucher,åªè´Ÿè´£å‘ResourceManagerç”³è¯·Executorå†…å­˜.
> 
> ResourceManageræ¥åˆ°ApplicationMasterèµ„æºç”³è¯·åä¼šåˆ†é…container,ç„¶åApplicationMasteråœ¨èµ„æºåˆ†é…æŒ‡å®šNodeManagerä¸Šå¯åŠ¨Executorè¿›ç¨‹,Executorè¿›ç¨‹å¯åŠ¨åä¼šå‘Driveråå‘æ³¨å†Œ,Executorå…¨éƒ¨æ³¨å†Œå®ŒæˆåDriverå¼€å§‹æ‰§è¡Œmainå‡½æ•°,ä¹‹åæ‰§è¡Œåˆ°Actionç®—å­æ—¶,è§¦å‘ä¸€ä¸ªjob,å¹¶æ ¹æ®å®½ä¾èµ–å¼€å§‹åˆ’åˆ†stage,æ¯ä¸ªstageç”Ÿæˆå¯¹åº”çš„taskSet,ä¹‹åå°†taskåˆ†å‘åˆ°å„ä¸ªExecutorä¸Šæ‰§è¡Œ.

###### 2.1.2.2.1 YARN Cluster æ¨¡å¼

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_036.jpg)

> åœ¨YARNClusteræ¨¡å¼ä¸‹,ä»»åŠ¡æäº¤åä¼šå’ŒResourceManageré€šè®¯ç”³è¯·å¯åŠ¨ApplicationMaster,éšåResourceManageråˆ†é…container,åœ¨åˆé€‚çš„NodeManagerä¸Šå¯åŠ¨ApplicationMaster,æ­¤æ—¶çš„ApplicationMasterå°±æ˜¯Driver.
> 
> Driverå¯åŠ¨åå‘ResourceManagerç”³è¯·Executorå†…å­˜,ResourceManageræ¥åˆ°ApplicationMasterçš„èµ„æºç”³è¯·åä¼šåˆ†é…container,ç„¶ååœ¨åˆé€‚çš„NodeManagerä¸Šå¯åŠ¨Executorè¿›ç¨‹,Executorè¿›ç¨‹å¯åŠ¨åä¼šå‘Driveråå‘æ³¨å†Œ,Executorå…¨éƒ¨æ³¨å†Œå®ŒæˆåDriverå¼€å§‹æ‰§è¡Œmainå‡½æ•°,ä¹‹åæ‰§è¡Œåˆ°Actionç®—å­æ—¶,è§¦å‘ä¸€ä¸ªjob,å¹¶æ ¹æ®å®½ä¾èµ–å¼€å§‹åˆ’åˆ†stage,æ¯ä¸ªstageç”Ÿæˆå¯¹åº”çš„taskSet,ä¹‹åå°†taskåˆ†å‘åˆ°å„ä¸ªExecutorä¸Šæ‰§è¡Œ.

#### 2.1.3 Spark é€šè®¯æ¶æ„
##### 2.1.3.1 Spark é€šä¿¡æ¶æ„ æ¦‚è¿°
> Spark2.xç‰ˆæœ¬ä½¿ç”¨Nettyé€šè®¯æ¡†æ¶ä½œä¸ºå†…éƒ¨é€šè®¯ç»„ä»¶,sparkåŸºäºNettyæ–°rpcæ¡†æ¶å€Ÿé‰´äº†Akkaä¸­çš„è®¾è®¡,å®ƒæ˜¯åŸºäºActoræ¨¡å‹,å¦‚ä¸‹å›¾æ‰€ç¤º.
> 
> Actoræ¨¡å‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_037.jpg)
> Sparké€šè®¯æ¡†æ¶ä¸­å„ä¸ªç»„ä»¶(Client/Master/Worker)å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªä¸ªç‹¬ç«‹çš„å®ä½“,å„ä¸ªå®ä½“ä¹‹é—´é€šè¿‡æ¶ˆæ¯æ¥è¿›è¡Œé€šä¿¡,å…·ä½“å„ä¸ªç»„ä»¶ä¹‹é—´çš„å…³ç³»å›¾å¦‚ä¸‹ : 
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_038.jpg)
> Endpoint(Client/Master/Worker)æœ‰1ä¸ªInBoxå’ŒNä¸ªOutBox(N>=1,Nå–å†³äºå½“å‰Endpointä¸å¤šå°‘å…¶ä»–çš„Endpointè¿›è¡Œé€šä¿¡,ä¸€ä¸ªä¸å…¶é€šè®¯çš„å…¶ä»–Endpointå¯¹åº”ä¸€ä¸ªOutBox),Endpointæ¥æ”¶åˆ°çš„æ¶ˆæ¯è¢«å†™å…¥InBox,å‘é€å‡ºå»çš„æ¶ˆæ¯å†™å…¥OutBoxå¹¶è¢«å‘é€åˆ°å…¶ä»–Endpointçš„InBoxä¸­.

##### 2.1.3.2 Spark é€šè®¯æ¶æ„ è§£æ
- Sparké€šä¿¡æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤º : 
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_039.jpg)
> 1.`RpcEndpoint` ï¼šRPCç«¯ç‚¹,Sparké’ˆå¯¹æ¯ä¸ªèŠ‚ç‚¹(Client/Master/Worker)éƒ½ç§°ä¹‹ä¸ºä¸€ä¸ªRpcç«¯ç‚¹,ä¸”éƒ½å®ç°RpcEndpointæ¥å£,å†…éƒ¨æ ¹æ®ä¸åŒç«¯ç‚¹çš„éœ€æ±‚,è®¾è®¡ä¸åŒçš„æ¶ˆæ¯å’Œä¸åŒçš„ä¸šåŠ¡å¤„ç†,å¦‚æœéœ€è¦å‘é€(è¯¢é—®)åˆ™è°ƒç”¨Dispatcher.
> 
> 2.`RpcEnv` ï¼šRPCä¸Šä¸‹æ–‡ç¯å¢ƒ,æ¯ä¸ªRPCç«¯ç‚¹è¿è¡Œæ—¶ä¾èµ–çš„ä¸Šä¸‹æ–‡ç¯å¢ƒç§°ä¸ºRpcEnv.
> 
> 3.`Dispatcher` ï¼šæ¶ˆæ¯åˆ†å‘å™¨,é’ˆå¯¹äºRPCç«¯ç‚¹éœ€è¦å‘é€æ¶ˆæ¯æˆ–è€…ä»è¿œç¨‹RPCæ¥æ”¶åˆ°çš„æ¶ˆæ¯,åˆ†å‘è‡³å¯¹åº”çš„æŒ‡ä»¤æ”¶ä»¶ç®±/å‘ä»¶ç®±,å¦‚æœæŒ‡ä»¤æ¥æ”¶æ–¹æ˜¯è‡ªå·±åˆ™å­˜å…¥æ”¶ä»¶ç®±,å¦‚æœæŒ‡ä»¤æ¥æ”¶æ–¹ä¸æ˜¯è‡ªå·±,åˆ™æ”¾å…¥å‘ä»¶ç®±.
> 
> 4.`Inbox` ï¼šæŒ‡ä»¤æ¶ˆæ¯æ”¶ä»¶ç®±,ä¸€ä¸ªæœ¬åœ°RpcEndpointå¯¹åº”ä¸€ä¸ªæ”¶ä»¶ç®±,Dispatcheråœ¨æ¯æ¬¡å‘Inboxå­˜å…¥æ¶ˆæ¯æ—¶,éƒ½å°†å¯¹åº”EndpointDataåŠ å…¥å†…éƒ¨ReceiverQueueä¸­,å¦å¤–Dispatcheråˆ›å»ºæ—¶ä¼šå¯åŠ¨ä¸€ä¸ªå•ç‹¬çº¿ç¨‹è¿›è¡Œè½®è¯¢ReceiverQueue,è¿›è¡Œæ”¶ä»¶ç®±æ¶ˆæ¯æ¶ˆè´¹.
> 
> 5.`RpcEndpointRef` ï¼šRpcEndpointRefæ˜¯å¯¹è¿œç¨‹RpcEndpointçš„ä¸€ä¸ªå¼•ç”¨,å½“éœ€è¦å‘ä¸€ä¸ªå…·ä½“RpcEndpointå‘é€æ¶ˆæ¯æ—¶,ä¸€èˆ¬éœ€è¦è·å–åˆ°è¯¥RpcEndpointçš„å¼•ç”¨ç„¶åé€šè¿‡è¯¥åº”ç”¨å‘é€æ¶ˆæ¯.
> 
> 6.`OutBox` ï¼šæŒ‡ä»¤æ¶ˆæ¯å‘ä»¶ç®±,å¯¹äºå½“å‰RpcEndpointæ¥è¯´,ä¸€ä¸ªç›®æ ‡RpcEndpointå¯¹åº”ä¸€ä¸ªå‘ä»¶ç®±,å¦‚æœå‘å¤šä¸ªç›®æ ‡RpcEndpointå‘é€ä¿¡æ¯,åˆ™æœ‰å¤šä¸ªOutBox,å½“æ¶ˆæ¯æ”¾å…¥Outboxå,ç´§æ¥ç€é€šè¿‡TransportClientå°†æ¶ˆæ¯å‘é€å‡ºå»,æ¶ˆæ¯æ”¾å…¥å‘ä»¶ç®±ä»¥åŠå‘é€è¿‡ç¨‹æ˜¯åœ¨åŒä¸€ä¸ªçº¿ç¨‹ä¸­è¿›è¡Œ.
> 
> 7.`RpcAddress` ï¼šè¡¨ç¤ºè¿œç¨‹RpcEndpointRefåœ°å€,Host + Port
> 
> 8.`TransportClient` ï¼šNettyé€šä¿¡å®¢æˆ·ç«¯,ä¸€ä¸ªOutBoxå¯¹åº”ä¸€ä¸ªTransportClient,TransportClientä¸æ–­è½®è¯¢OutBox,æ ¹æ®OutBoxæ¶ˆæ¯çš„receiverä¿¡æ¯,è¯·æ±‚å¯¹åº”çš„è¿œç¨‹TransportServer.
> 
> 9.`TransportServer` ï¼šNettyé€šä¿¡æœåŠ¡ç«¯,ä¸€ä¸ªRpcEndpointå¯¹åº”ä¸€ä¸ªTransportServer,æ¥å—è¿œç¨‹æ¶ˆæ¯åè°ƒç”¨Dispatcheråˆ†å‘æ¶ˆæ¯è‡³å¯¹åº”æ”¶å‘ä»¶ç®±.
> 
> æ ¹æ®ä¸Šé¢çš„åˆ†æ,Sparké€šä¿¡æ¶æ„çš„é«˜å±‚è§†å›¾å¦‚ä¸‹å›¾æ‰€ç¤º : 

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_040.jpg)

#### 2.1.4 Spark Context è§£æ
> åœ¨Sparkä¸­ç”±SparkContextè´Ÿè´£ä¸é›†ç¾¤è¿›è¡Œé€šè®¯ã€èµ„æºçš„ç”³è¯·ä»¥åŠä»»åŠ¡çš„åˆ†é…å’Œç›‘æ§ç­‰,å½“WorkerèŠ‚ç‚¹ä¸­çš„Executorè¿è¡Œå®Œæ¯•Taskå,DriveråŒæ—¶è´Ÿè´£å°†SparkContextå…³é—­,é€šå¸¸ä¹Ÿå¯ä»¥ä½¿ç”¨SparkContextæ¥ä»£è¡¨é©±åŠ¨ç¨‹åº(Driver).
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_041.jpg)
> SparkContextæ˜¯=é€šå¾€Sparké›†ç¾¤çš„å”¯ä¸€å…¥å£,å¯ä»¥ç”¨æ¥åœ¨Sparké›†ç¾¤ä¸­åˆ›å»ºRDDã€ç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡,SparkContextä¹Ÿæ˜¯æ•´ä¸ªSparkåº”ç”¨ç¨‹åºä¸­è‡³å…³é‡è¦çš„ä¸€ä¸ªå¯¹è±¡,å¯ä»¥è¯´æ˜¯æ•´ä¸ªApplicationè¿è¡Œè°ƒåº¦çš„æ ¸å¿ƒ(ä¸åŒ…æ‹¬èµ„æºè°ƒåº¦).
> 
> SparkContextçš„æ ¸å¿ƒä½œç”¨æ˜¯åˆå§‹åŒ–Sparkåº”ç”¨ç¨‹åºè¿è¡Œæ‰€éœ€çš„æ ¸å¿ƒç»„ä»¶,åŒ…æ‹¬é«˜å±‚è°ƒåº¦å™¨(DAGScheduler)ã€åº•å±‚è°ƒåº¦å™¨(TaskScheduler)å’Œè°ƒåº¦å™¨çš„é€šä¿¡ç»ˆç«¯,(SchedulerBackend),åŒæ—¶è¿˜ä¼šè´Ÿè´£Sparkç¨‹åºå‘ClusterManagerçš„æ³¨å†Œç­‰.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_042.jpg)
> SparkContextåˆå§‹åŒ–ç»„ä»¶
> åœ¨å®é™…ç¼–ç è¿‡ç¨‹ä¸­,ä¼šå…ˆåˆ›å»ºSparkConfå®ä¾‹,å¹¶å¯¹SparkConfçš„å±æ€§è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®,éšåå°†SparkConfä½œä¸ºSparkContextç±»çš„å”¯ä¸€æ„é€ å‚æ•°ä¼ å…¥æ¥å®ŒæˆSparkContextå®ä¾‹å¯¹è±¡çš„åˆ›å»º.
> SparkContextåœ¨å®ä¾‹åŒ–è¿‡ç¨‹ä¸­ä¼šåˆå§‹åŒ–DAGSchedulerã€TaskSchedulerå’ŒSchedulerBackend,å½“RDDçš„actionç®—å­è§¦å‘äº†ä½œä¸š(Job)å,SparkContextä¼šè°ƒç”¨DAGScheduleræ ¹æ®å®½çª„ä¾èµ–å°†Jobåˆ’åˆ†æˆå‡ ä¸ªå°çš„é˜¶æ®µ(Stage),TaskSchedulerä¼šè°ƒåº¦æ¯ä¸ªStageä»»åŠ¡(Task),å¦å¤–SchedulerBackendè´Ÿè´£ç”³è¯·å’Œç®¡ç†é›†ç¾¤ä¸ºå½“å‰Applicationåˆ†é…çš„è®¡ç®—èµ„æº(å³Executor).
> 
> å¦‚æœå°†SparkApplicationæ¯”ä½œæ±½è½¦,é‚£ä¹ˆSparkContextå°±æ˜¯æ±½è½¦å¼•æ“,è€ŒSparkConfå°±æ˜¯å¼•æ“é…ç½®å‚æ•°.
> 
> ä¸‹å›¾æè¿°Spark-On-Yarnæ¨¡å¼ä¸‹åœ¨ä»»åŠ¡è°ƒåº¦æœŸé—´,ApplicationMasterã€Driverä»¥åŠExecutorå†…éƒ¨æ¨¡å—çš„äº¤äº’è¿‡ç¨‹ :  Sparkç»„ä»¶äº¤äº’è¿‡ç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_043.jpg)
> Driveråˆå§‹åŒ–SparkContextè¿‡ç¨‹ä¸­,ä¼šåˆ†åˆ«åˆå§‹åŒ–DAGScheduler / TaskScheduler / SchedulerBackendä»¥åŠHeartbeatReceiver,å¹¶å¯åŠ¨SchedulerBackendä»¥åŠHeartbeatReceiver,SchedulerBackendé€šè¿‡ApplicationMasterç”³è¯·èµ„æº,å¹¶ä¸æ–­ä»TaskSchedulerä¸­æ‹¿åˆ°åˆé€‚çš„Taskåˆ†å‘åˆ°Executoræ‰§è¡Œ,HeartbeatReceiverè´Ÿè´£æ¥æ”¶Executorå¿ƒè·³ä¿¡æ¯,ç›‘æ§Executorçš„å­˜æ´»çŠ¶å†µ,å¹¶é€šçŸ¥åˆ°TaskScheduler.


#### 2.1.5 Spark ä»»åŠ¡è°ƒåº¦æœºåˆ¶
- åœ¨å·¥å‚ç¯å¢ƒä¸‹,Sparké›†ç¾¤çš„éƒ¨ç½²æ–¹å¼ä¸€èˆ¬ä¸ºYARN-Clusteræ¨¡å¼,ä¹‹åå†…æ ¸åˆ†æå†…å®¹ä¸­é»˜è®¤é›†ç¾¤éƒ¨ç½²æ–¹å¼ä¸ºYARN-Clusteræ¨¡å¼.

##### 2.1.5.1 Spark ä»»åŠ¡æäº¤ æµç¨‹
- SparkYARN-Clusteræ¨¡å¼ä¸‹çš„ä»»åŠ¡æäº¤æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_044.jpg)
- æ—¶åºå›¾æ¸…æ™°åœ°è¯´æ˜Sparkåº”ç”¨ç¨‹åºä»æäº¤åˆ°è¿è¡Œçš„å®Œæ•´æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_045.jpg)
> æäº¤Sparkåº”ç”¨ç¨‹åº,é¦–å…ˆé€šè¿‡Clientå‘ResourceManagerè¯·æ±‚å¯åŠ¨ä¸€ä¸ªApplication,åŒæ—¶æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æºæ»¡è¶³Applicationéœ€æ±‚,å¦‚æœèµ„æºæ¡ä»¶æ»¡è¶³,åˆ™å‡†å¤‡ApplicationMasterçš„å¯åŠ¨ä¸Šä¸‹æ–‡,äº¤ç»™ResourceManager,å¹¶å¾ªç¯ç›‘æ§ApplicationçŠ¶æ€.
> 
> å½“æäº¤çš„èµ„æºé˜Ÿåˆ—ä¸­æœ‰èµ„æºæ—¶,ResourceManagerä¼šåœ¨æŸä¸ªNodeManagerä¸Šå¯åŠ¨ApplicationMasterè¿›ç¨‹,ApplicationMasterä¼šå•ç‹¬å¯åŠ¨Driveråå°çº¿ç¨‹,å½“Driverå¯åŠ¨å,ApplicationMasterä¼šé€šè¿‡æœ¬åœ°çš„RPCè¿æ¥Driver,å¹¶å¼€å§‹å‘ResourceManagerç”³è¯·Containerèµ„æºè¿è¡ŒExecutorè¿›ç¨‹(ä¸€ä¸ªExecutorå¯¹åº”ä¸ä¸€ä¸ªContainer),å½“ResourceManagerè¿”å›Containerèµ„æº,ApplicationMasteråˆ™åœ¨å¯¹åº”çš„Containerä¸Šå¯åŠ¨Executor.
> 
> Driverçº¿ç¨‹ä¸»è¦æ˜¯åˆå§‹åŒ–SparkContextå¯¹è±¡,å‡†å¤‡è¿è¡Œæ‰€éœ€çš„ä¸Šä¸‹æ–‡,ç„¶åä¸€æ–¹é¢ä¿æŒä¸ApplicationMasterçš„RPCè¿æ¥,é€šè¿‡ApplicationMasterç”³è¯·èµ„æº,å¦ä¸€æ–¹é¢æ ¹æ®ç”¨æˆ·ä¸šåŠ¡é€»è¾‘å¼€å§‹è°ƒåº¦ä»»åŠ¡,å°†ä»»åŠ¡ä¸‹å‘åˆ°å·²æœ‰çš„ç©ºé—²Executorä¸Š.
> 
> å½“ResourceManagerå‘ApplicationMasterè¿”å›Containerèµ„æºæ—¶,ApplicationMasterå°±å°è¯•åœ¨å¯¹åº”çš„Containerä¸Šå¯åŠ¨Executorè¿›ç¨‹,Executorè¿›ç¨‹èµ·æ¥å,ä¼šå‘Driveråå‘æ³¨å†Œ,æ³¨å†ŒæˆåŠŸåä¿æŒä¸Driverçš„å¿ƒè·³,åŒæ—¶ç­‰å¾…Driveråˆ†å‘ä»»åŠ¡,å½“åˆ†å‘çš„ä»»åŠ¡æ‰§è¡Œå®Œæ¯•å,å°†ä»»åŠ¡çŠ¶æ€ä¸ŠæŠ¥ç»™Driver.
> 
> ä»ä¸Šè¿°æ—¶åºå›¾å¯çŸ¥,Clientåªè´Ÿè´£æäº¤Applicationå¹¶ç›‘æ§Applicationçš„çŠ¶æ€,å¯¹äºSparkçš„ä»»åŠ¡è°ƒåº¦ä¸»è¦æ˜¯é›†ä¸­åœ¨ä¸¤ä¸ªæ–¹é¢: èµ„æºç”³è¯·å’Œä»»åŠ¡åˆ†å‘,å…¶ä¸»è¦æ˜¯é€šè¿‡ApplicationMasterã€Driverä»¥åŠExecutorä¹‹é—´æ¥å®Œæˆ.

##### 2.1.5.2 Spark ä»»åŠ¡è°ƒåº¦ æ¦‚è¿°
> å½“Driverèµ·æ¥å,Driveråˆ™ä¼šæ ¹æ®ç”¨æˆ·ç¨‹åºé€»è¾‘å‡†å¤‡ä»»åŠ¡,å¹¶æ ¹æ®Executorèµ„æºæƒ…å†µé€æ­¥åˆ†å‘ä»»åŠ¡,åœ¨è¯¦ç»†é˜è¿°ä»»åŠ¡è°ƒåº¦å‰,é¦–å…ˆè¯´æ˜ä¸‹Sparké‡Œçš„å‡ ä¸ªæ¦‚å¿µ,ä¸€ä¸ªSparkåº”ç”¨ç¨‹åºåŒ…æ‹¬Jobã€Stageä»¥åŠTaskä¸‰ä¸ªæ¦‚å¿µ.
> 1.Jobæ˜¯ä»¥Actionæ–¹æ³•ä¸ºç•Œ,é‡åˆ°ä¸€ä¸ªActionæ–¹æ³•åˆ™è§¦å‘ä¸€ä¸ªJob.
> 2.Stageæ˜¯Jobçš„å­é›†,ä»¥RDDå®½ä¾èµ–(å³Shuffle)ä¸ºç•Œ,é‡åˆ°Shuffleåšä¸€æ¬¡åˆ’åˆ†.
> 3.Taskæ˜¯Stageçš„å­é›†,ä»¥å¹¶è¡Œåº¦(åˆ†åŒºæ•°)æ¥è¡¡é‡,åˆ†åŒºæ•°æ˜¯å¤šå°‘,åˆ™æœ‰å¤šå°‘ä¸ªtask.
> 
> Sparkä»»åŠ¡è°ƒåº¦æ€»ä½“æ¥è¯´åˆ†ä¸¤è·¯è¿›è¡Œ,ä¸€è·¯æ˜¯Stageçº§è°ƒåº¦,ä¸€è·¯æ˜¯Taskçº§è°ƒåº¦,æ€»ä½“è°ƒåº¦æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º : Sparkä»»åŠ¡è°ƒåº¦æ¦‚è§ˆ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_046.jpg)
> Spark RDDé€šè¿‡å…¶Transactionsæ“ä½œ,å½¢æˆäº†RDDè¡€ç¼˜å…³ç³»å›¾,å³DAG,æœ€åé€šè¿‡Actionçš„è°ƒç”¨,è§¦å‘Jobå¹¶è°ƒåº¦æ‰§è¡Œ.
> DAGSchedulerè´Ÿè´£Stageçº§çš„è°ƒåº¦,ä¸»è¦æ˜¯å°†jobåˆ‡åˆ†æˆè‹¥å¹²Stages,å¹¶å°†æ¯ä¸ªStageæ‰“åŒ…æˆTaskSetäº¤ç»™TaskSchedulerè°ƒåº¦.
> TaskSchedulerè´Ÿè´£Taskçº§çš„è°ƒåº¦,å°†DAGSchedulerç»™è¿‡æ¥çš„TaskSetæŒ‰ç…§æŒ‡å®šçš„è°ƒåº¦ç­–ç•¥åˆ†å‘åˆ°Executorä¸Šæ‰§è¡Œ,è°ƒåº¦è¿‡ç¨‹ä¸­SchedulerBackendè´Ÿè´£æä¾›å¯ç”¨èµ„æº,å…¶ä¸­SchedulerBackendæœ‰å¤šç§å®ç°,åˆ†åˆ«å¯¹æ¥ä¸åŒçš„èµ„æºç®¡ç†ç³»ç»Ÿ.

##### 2.1.5.3 Spark Stageçº§ è°ƒåº¦
> Sparkçš„ä»»åŠ¡è°ƒåº¦æ˜¯ä»DAGåˆ‡å‰²å¼€å§‹ï¼Œä¸»è¦æ˜¯ç”±DAGScheduleræ¥å®Œæˆã€‚å½“é‡åˆ°ä¸€ä¸ªActionæ“ä½œåå°±ä¼šè§¦å‘ä¸€ä¸ªJobçš„è®¡ç®—ï¼Œå¹¶äº¤ç»™DAGScheduleræ¥æäº¤ï¼Œä¸‹å›¾æ˜¯æ¶‰åŠåˆ°Jobæäº¤çš„ç›¸å…³æ–¹æ³•è°ƒç”¨æµç¨‹å›¾ : Jobæäº¤è°ƒç”¨æ ˆ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_047.jpg)
> Jobç”±æœ€ç»ˆRDDå’ŒActionæ–¹æ³•å°è£…è€Œæˆ,SparkContextå°†Jobäº¤ç»™DAGScheduleræäº¤,å®ƒä¼šæ ¹æ®RDDè¡€ç¼˜å…³ç³»æ„æˆçš„DAGè¿›è¡Œåˆ‡åˆ†,å°†ä¸€ä¸ªJobåˆ’åˆ†ä¸ºè‹¥å¹²Stages,å…·ä½“åˆ’åˆ†ç­–ç•¥æ˜¯ï¼Œç”±æœ€ç»ˆRDDä¸æ–­é€šè¿‡ä¾èµ–å›æº¯åˆ¤æ–­çˆ¶ä¾èµ–æ˜¯å¦æ˜¯å®½ä¾èµ–,å³ä»¥Shuffleä¸ºç•Œ,åˆ’åˆ†Stage,çª„ä¾èµ–çš„RDDä¹‹é—´è¢«åˆ’åˆ†åˆ°åŒä¸€ä¸ªStageä¸­,å¯ä»¥è¿›è¡Œpipelineå¼è®¡ç®—,å¦‚ä¸Šå›¾ç´«è‰²æµç¨‹éƒ¨åˆ†,åˆ’åˆ†çš„Stagesåˆ†ä¸¤ç±»,ä¸€ç±»å«åšResultStage,ä¸ºDAGæœ€ä¸‹æ¸¸çš„Stage,ç”±Actionæ–¹æ³•å†³å®š,å¦ä¸€ç±»å«åšShuffleMapStage,ä¸ºä¸‹æ¸¸Stageå‡†å¤‡æ•°æ®,ä¸‹é¢çœ‹ä¸€ä¸ªç®€å•ä¾‹å­WordCount.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_048.jpg)
> Jobç”±saveAsTextFileè§¦å‘,è¯¥Jobç”±RDD-3å’ŒsaveAsTextFileæ–¹æ³•ç»„æˆ,æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»ä»RDD-3å¼€å§‹å›æº¯æœç´¢,ç›´åˆ°æ²¡æœ‰ä¾èµ–çš„RDD-0,åœ¨å›æº¯æœç´¢è¿‡ç¨‹ä¸­,RDD-3ä¾èµ–RDD-2,å¹¶ä¸”æ˜¯å®½ä¾èµ–,æ‰€ä»¥åœ¨RDD-2å’ŒRDD-3ä¹‹é—´åˆ’åˆ†Stage,RDD-3è¢«åˆ’åˆ°æœ€åä¸€ä¸ªStage,å³ResultStageä¸­,RDD-2ä¾èµ–RDD-1,RDD-1ä¾èµ–RDD-0,è¿™äº›ä¾èµ–éƒ½æ˜¯çª„ä¾èµ–,æ‰€ä»¥å°†RDD-0ã€RDD-1å’ŒRDD-2åˆ’åˆ†åˆ°åŒä¸€ä¸ªStage,å³ShuffleMapStageä¸­,å®é™…æ‰§è¡Œçš„æ—¶å€™,æ•°æ®è®°å½•ä¼šä¸€æ°”å‘µæˆåœ°æ‰§è¡ŒRDD-0åˆ°RDD-2çš„è½¬åŒ–,ä¸éš¾çœ‹å‡ºå…¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ·±åº¦ä¼˜å…ˆæœç´¢ç®—æ³•.
> 
> ä¸€ä¸ªStageæ˜¯å¦è¢«æäº¤,éœ€è¦åˆ¤æ–­å®ƒçš„çˆ¶Stageæ˜¯å¦æ‰§è¡Œ,åªæœ‰åœ¨çˆ¶Stageæ‰§è¡Œå®Œæ¯•æ‰èƒ½æäº¤å½“å‰Stage,å¦‚æœä¸€ä¸ªStageæ²¡æœ‰çˆ¶Stage,é‚£ä¹ˆä»è¯¥Stageå¼€å§‹æäº¤,Stageæäº¤æ—¶ä¼šå°†Taskä¿¡æ¯(åˆ†åŒºä¿¡æ¯ä»¥åŠæ–¹æ³•ç­‰)åºåˆ—åŒ–å¹¶è¢«æ‰“åŒ…æˆTaskSetäº¤ç»™TaskScheduler,ä¸€ä¸ªPartitionå¯¹åº”ä¸€ä¸ªTask,å¦ä¸€æ–¹é¢TaskSchedulerä¼šç›‘æ§Stageçš„è¿è¡ŒçŠ¶æ€,åªæœ‰Executorä¸¢å¤±æˆ–è€…Taskç”±äºFetchå¤±è´¥æ‰éœ€è¦é‡æ–°æäº¤å¤±è´¥çš„Stageä»¥è°ƒåº¦è¿è¡Œå¤±è´¥ä»»åŠ¡,å…¶ä»–ç±»å‹çš„Taskå¤±è´¥ä¼šåœ¨TaskSchedulerçš„è°ƒåº¦è¿‡ç¨‹ä¸­é‡è¯•.
> 
> ç›¸å¯¹æ¥è¯´DAGScheduleråšäº‹æƒ…è¾ƒä¸ºç®€å•,ä»…ä»…æ˜¯åœ¨Stageå±‚é¢ä¸Šåˆ’åˆ†DAG,æäº¤Stageå¹¶ç›‘æ§ç›¸å…³çŠ¶æ€ä¿¡æ¯,TaskScheduleråˆ™ç›¸å¯¹è¾ƒä¸ºå¤æ‚,ä¸‹é¢è¯¦ç»†é˜è¿°å…¶ç»†èŠ‚.

##### 2.1.5.4 Spark Taskçº§ è°ƒåº¦
> SparkTaskè°ƒåº¦æ˜¯ç”±TaskScheduleræ¥å®Œæˆ,ç”±å‰æ–‡å¯çŸ¥,DAGSchedulerå°†Stageæ‰“åŒ…åˆ°TaskSetäº¤ç»™TaskScheduler,TaskSchedulerä¼šå°†TaskSetå°è£…ä¸ºTaskSetManageråŠ å…¥åˆ°è°ƒåº¦é˜Ÿåˆ—ä¸­,TaskSetManagerç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º : TaskManagerç»“æ„

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_049.jpg)
> TaskSetManagerè´Ÿè´£ç›‘æ§ç®¡ç†åŒä¸€ä¸ªStageä¸­çš„Tasks,TaskSchedulerå°±æ˜¯ä»¥TaskSetManagerä¸ºå•å…ƒæ¥è°ƒåº¦ä»»åŠ¡.
> 
> TaskScheduleråˆå§‹åŒ–åä¼šå¯åŠ¨SchedulerBackend,å®ƒè´Ÿè´£è·Ÿå¤–ç•Œæ‰“äº¤é“,æ¥æ”¶Executorçš„æ³¨å†Œä¿¡æ¯,å¹¶ç»´æŠ¤Executorçš„çŠ¶æ€,æ‰€ä»¥è¯´SchedulerBackendæ˜¯ç®¡â€œç²®é£Ÿâ€çš„,åŒæ—¶å®ƒåœ¨å¯åŠ¨åä¼šå®šæœŸåœ°å»â€œè¯¢é—®â€TaskScheduleræœ‰æ²¡æœ‰ä»»åŠ¡è¦è¿è¡Œ,ä¹Ÿå°±æ˜¯è¯´å®ƒä¼šå®šæœŸåœ°â€œé—®â€TaskSchedulerâ€œæˆ‘æœ‰è¿™ä¹ˆä½™é‡,ä½ è¦ä¸è¦å•Šâ€,TaskScheduleråœ¨SchedulerBackendâ€œé—®â€å®ƒçš„æ—¶å€™,ä¼šä»è°ƒåº¦é˜Ÿåˆ—ä¸­æŒ‰ç…§æŒ‡å®šçš„è°ƒåº¦ç­–ç•¥é€‰æ‹©TaskSetManagerå»è°ƒåº¦è¿è¡Œ,å¤§è‡´æ–¹æ³•è°ƒç”¨æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º : taskè°ƒåº¦æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_050.jpg)
> å°†TaskSetManageråŠ å…¥rootPoolè°ƒåº¦æ± ä¸­ä¹‹å,è°ƒç”¨SchedulerBackendçš„riviveOffersæ–¹æ³•ç»™driverEndpointå‘é€ReviveOfferæ¶ˆæ¯,driverEndpointæ”¶åˆ°ReviveOfferæ¶ˆæ¯åè°ƒç”¨makeOffersæ–¹æ³•,è¿‡æ»¤å‡ºæ´»è·ƒçŠ¶æ€çš„Executor(è¿™äº›Executoréƒ½æ˜¯ä»»åŠ¡å¯åŠ¨æ—¶åå‘æ³¨å†Œåˆ°Driverçš„Executor),ç„¶åå°†Executorå°è£…æˆWorkerOfferå¯¹è±¡,å‡†å¤‡å¥½è®¡ç®—èµ„æº(WorkerOffer)å,taskScheduleråŸºäºè¿™äº›èµ„æºè°ƒç”¨resourceOfferåœ¨Executorä¸Šåˆ†é…task.

###### 2.1.5.4.1 è°ƒåº¦ç­–ç•¥
> TaskSchedulerä¼šå…ˆæŠŠDAGSchedulerç»™è¿‡æ¥çš„TaskSetå°è£…æˆTaskSetManageræ‰”åˆ°ä»»åŠ¡é˜Ÿåˆ—é‡Œ,ç„¶åå†ä»ä»»åŠ¡é˜Ÿåˆ—é‡ŒæŒ‰ç…§ä¸€å®šçš„è§„åˆ™æŠŠå®ƒä»¬å–å‡ºæ¥åœ¨SchedulerBackendç»™è¿‡æ¥çš„Executorä¸Šè¿è¡Œmè¿™ä¸ªè°ƒåº¦è¿‡ç¨‹å®é™…ä¸Šè¿˜æ˜¯æ¯”è¾ƒç²—ç²’åº¦,æ˜¯é¢å‘TaskSetManager.
> 
> è°ƒåº¦é˜Ÿåˆ—å±‚æ¬¡ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º : FIFOè°ƒåº¦ç­–ç•¥å†…å­˜ç»“æ„
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_051.jpg)
> TaskScheduleræ˜¯ä»¥æ ‘çš„æ–¹å¼æ¥ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—,æ ‘ä¸­çš„èŠ‚ç‚¹ç±»å‹ä¸ºSchdulable,å¶å­èŠ‚ç‚¹ä¸ºTaskSetManager,éå¶å­èŠ‚ç‚¹ä¸ºPool,ä¸‹å›¾æ˜¯å®ƒä»¬ä¹‹é—´çš„ç»§æ‰¿å…³ç³» : ä»»åŠ¡é˜Ÿåˆ—ç»§æ‰¿å…³ç³»
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_052.jpg)
> TaskScheduleræ”¯æŒä¸¤ç§è°ƒåº¦ç­–ç•¥,ä¸€ç§æ˜¯FIFO,ä¹Ÿæ˜¯é»˜è®¤çš„è°ƒåº¦ç­–ç•¥,å¦ä¸€ç§æ˜¯FAIR,åœ¨TaskScheduleråˆå§‹åŒ–è¿‡ç¨‹ä¸­ä¼šå®ä¾‹åŒ–rootPool,è¡¨ç¤ºæ ‘çš„æ ¹èŠ‚ç‚¹,æ˜¯Poolç±»å‹.
> 
> 1.`FIFOè°ƒåº¦ç­–ç•¥` | FIFOè°ƒåº¦ç­–ç•¥æ‰§è¡Œæ­¥éª¤å¦‚ä¸‹
> å¯¹s1å’Œs2ä¸¤ä¸ªSchedulableçš„ä¼˜å…ˆçº§(Schedulableç±»ä¸€ä¸ªå±æ€§,è®°ä¸ºpriority,å€¼è¶Šå°,ä¼˜å…ˆçº§è¶Šé«˜).
> å¦‚æœä¸¤ä¸ªSchedulableçš„ä¼˜å…ˆçº§ç›¸åŒ,åˆ™å¯¹s1,s2æ‰€å±çš„Stageèº«ä»½è¿›è¡Œæ ‡è¯†è¿›è¡Œæ¯”è¾ƒ(Schedulableç±»çš„ä¸€ä¸ªå±æ€§,è®°ä¸ºpriority,å€¼è¶Šå°,ä¼˜å…ˆçº§è¶Šé«˜)
> å¦‚æœæ¯”è¾ƒçš„ç»“æœå°äº0,åˆ™ä¼˜å…ˆè°ƒåº¦s1,å¦åˆ™ä¼˜å…ˆè°ƒåº¦s2.
> FIFOè°ƒåº¦ç­–ç•¥å†…å­˜ç»“æ„
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_053.jpg)
> 2.`FAIR`è°ƒåº¦ç­–ç•¥ | FAIRè°ƒåº¦ç­–ç•¥çš„æ ‘ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_054.jpg)
> FAIRæ¨¡å¼ä¸­æœ‰ä¸€ä¸ªrootPoolå’Œå¤šä¸ªå­Pool,å„ä¸ªå­Poolä¸­å­˜å‚¨ç€æ‰€æœ‰å¾…åˆ†é…çš„TaskSetMagager.
> 
> å¯ä»¥é€šè¿‡åœ¨Propertiesä¸­æŒ‡å®š`spark.scheduler.pool`å±æ€§,æŒ‡å®šè°ƒåº¦æ± ä¸­çš„æŸä¸ªè°ƒåº¦æ± ä½œä¸ºTaskSetManagerçš„çˆ¶è°ƒåº¦æ± ,å¦‚æœæ ¹è°ƒåº¦æ± ä¸å­˜åœ¨æ­¤å±æ€§å€¼å¯¹åº”çš„è°ƒåº¦æ± ,ä¼šåˆ›å»ºä»¥æ­¤å±æ€§å€¼ä¸ºåç§°çš„è°ƒåº¦æ± ä½œä¸ºTaskSetManagerçš„çˆ¶è°ƒåº¦æ± ,å¹¶å°†æ­¤è°ƒåº¦æ± ä½œä¸ºæ ¹è°ƒåº¦æ± çš„å­è°ƒåº¦æ± .
> 
> åœ¨FAIRæ¨¡å¼ä¸­,éœ€è¦å…ˆå¯¹å­Poolè¿›è¡Œæ’åº,å†å¯¹å­Poolé‡Œé¢çš„TaskSetMagagerè¿›è¡Œæ’åº,å› ä¸ºPoolå’ŒTaskSetMagageréƒ½ç»§æ‰¿äº†Schedulableç‰¹è´¨,å› æ­¤ä½¿ç”¨ç›¸åŒçš„æ’åºç®—æ³•.
> 
> æ’åºè¿‡ç¨‹çš„æ¯”è¾ƒæ˜¯åŸºäºFair-shareæ¥æ¯”è¾ƒçš„,æ¯ä¸ªè¦æ’åºçš„å¯¹è±¡åŒ…å«ä¸‰ä¸ªå±æ€§ : runningTaskså€¼(æ­£åœ¨è¿è¡Œçš„Taskæ•°)ã€minShareå€¼ã€weightå€¼,æ¯”è¾ƒæ—¶ä¼šç»¼åˆè€ƒé‡runningTaskså€¼,minShareå€¼ä»¥åŠweightå€¼.
> 
> æ³¨æ„minShareã€weightçš„å€¼å‡åœ¨å…¬å¹³è°ƒåº¦é…ç½®æ–‡ä»¶`fairscheduler.xml`ä¸­è¢«æŒ‡å®š,è°ƒåº¦æ± åœ¨æ„å»ºé˜¶æ®µä¼šè¯»å–æ­¤æ–‡ä»¶çš„ç›¸å…³é…ç½®.
> 1.å¦‚æœAå¯¹è±¡çš„runningTaskså¤§äºå®ƒçš„minShare,Bå¯¹è±¡çš„runningTaskså°äºå®ƒçš„minShare,é‚£ä¹ˆBæ’åœ¨Aå‰é¢(runningTasksæ¯”minShareå°çš„å…ˆæ‰§è¡Œ).
> 
> 2.å¦‚æœAã€Bå¯¹è±¡çš„runningTaskséƒ½å°äºå®ƒä»¬çš„minShare,é‚£ä¹ˆå°±æ¯”è¾ƒrunningTasksä¸minShareçš„æ¯”å€¼(minShareä½¿ç”¨ç‡),è°å°è°æ’å‰é¢(minShareä½¿ç”¨ç‡ä½çš„å…ˆæ‰§è¡Œ).
> 
> 3.å¦‚æœAã€Bå¯¹è±¡çš„runningTaskséƒ½å¤§äºå®ƒä»¬çš„minShare,é‚£ä¹ˆå°±æ¯”è¾ƒrunningTasksä¸weightçš„æ¯”å€¼(æƒé‡ä½¿ç”¨ç‡),è°å°è°æ’å‰é¢(æƒé‡ä½¿ç”¨ç‡ä½çš„å…ˆæ‰§è¡Œ).
> 
> 4.å¦‚æœä¸Šè¿°æ¯”è¾ƒå‡ç›¸ç­‰ï¼Œåˆ™æ¯”è¾ƒåå­—.
> æ•´ä½“ä¸Šæ¥è¯´å°±æ˜¯é€šè¿‡minShareå’Œweightè¿™ä¸¤ä¸ªå‚æ•°æ§åˆ¶æ¯”è¾ƒè¿‡ç¨‹,å¯ä»¥åšåˆ°è®©minShareä½¿ç”¨ç‡å’Œæƒé‡ä½¿ç”¨ç‡å°‘(å®é™…è¿è¡Œtaskæ¯”ä¾‹è¾ƒå°‘)çš„å…ˆè¿è¡Œ.
> FAIRæ¨¡å¼æ’åºå®Œæˆå,æ‰€æœ‰çš„TaskSetManagerè¢«æ”¾å…¥ä¸€ä¸ªArrayBufferé‡Œ,ä¹‹åä¾æ¬¡è¢«å–å‡ºå¹¶å‘é€ç»™Executoræ‰§è¡Œ.
> ä»è°ƒåº¦é˜Ÿåˆ—ä¸­æ‹¿åˆ°TaskSetManagerå,ç”±äºTaskSetManagerå°è£…äº†ä¸€ä¸ªStageçš„æ‰€æœ‰Task,å¹¶è´Ÿè´£ç®¡ç†è°ƒåº¦è¿™äº›Task,é‚£ä¹ˆæ¥ä¸‹æ¥çš„å·¥ä½œå°±æ˜¯TaskSetManageræŒ‰ç…§ä¸€å®šçš„è§„åˆ™ä¸€ä¸ªä¸ªå–å‡ºTaskç»™TaskScheduler,TaskSchedulerå†äº¤ç»™SchedulerBackendå»å‘åˆ°Executorä¸Šæ‰§è¡Œ.

###### 2.1.5.4.2 æœ¬åœ°åŒ–è°ƒåº¦
> DAGScheduleråˆ‡å‰²Job,åˆ’åˆ†Stage,é€šè¿‡è°ƒç”¨submitStageæ¥æäº¤ä¸€ä¸ªStageå¯¹åº”çš„tasks,submitStageä¼šè°ƒç”¨submitMissingTasks,submitMissingTasksç¡®å®šæ¯ä¸ªéœ€è¦è®¡ç®—çš„taskçš„preferredLocations,é€šè¿‡è°ƒç”¨`getPreferrdeLocations()`å¾—åˆ°partitionçš„ä¼˜å…ˆä½ç½®,ç”±äºä¸€ä¸ªpartitionå¯¹åº”ä¸€ä¸ªtask,æ­¤partitionçš„ä¼˜å…ˆä½ç½®å°±æ˜¯taskçš„ä¼˜å…ˆä½ç½®,å¯¹äºè¦æäº¤åˆ°TaskSchedulerçš„TaskSetä¸­çš„æ¯ä¸€ä¸ªtask,è¯¥taskä¼˜å…ˆä½ç½®ä¸å…¶å¯¹åº”çš„partitionå¯¹åº”çš„ä¼˜å…ˆä½ç½®ä¸€è‡´.
> 
> ä»è°ƒåº¦é˜Ÿåˆ—ä¸­æ‹¿åˆ°TaskSetManagerå,é‚£ä¹ˆæ¥ä¸‹æ¥çš„å·¥ä½œå°±æ˜¯TaskSetManageræŒ‰ç…§ä¸€å®šçš„è§„åˆ™ä¸€ä¸ªä¸ªå–å‡ºtaskç»™TaskScheduler,TaskSchedulerå†äº¤ç»™SchedulerBackendå»å‘åˆ°Executorä¸Šæ‰§è¡Œ,TaskSetManagerå°è£…äº†ä¸€ä¸ªStageçš„æ‰€æœ‰task,å¹¶è´Ÿè´£ç®¡ç†è°ƒåº¦è¿™äº›task.
> 
> æ ¹æ®æ¯ä¸ªtaskçš„ä¼˜å…ˆä½ç½®ï¼Œç¡®å®štaskçš„Localityçº§åˆ«ï¼ŒLocalityä¸€å…±æœ‰äº”ç§ï¼Œä¼˜å…ˆçº§ç”±é«˜åˆ°ä½é¡ºåº : Sparkæœ¬åœ°åŒ–ç­‰çº§

> | åç§°      |     è§£æ |
> | :--------: | :--------:|
> | `PROCESS_LOCAL`    |   è¿›ç¨‹æœ¬åœ°åŒ–,taskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªExecutorä¸­,æ€§èƒ½æœ€å¥½. |
> | `NODE_LOCAL`    |   èŠ‚ç‚¹æœ¬åœ°åŒ–,taskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­,ä½†æ˜¯taskå’Œæ•°æ®ä¸åœ¨åŒä¸€ä¸ªExecutorä¸­,æ•°æ®éœ€è¦åœ¨è¿›ç¨‹é—´è¿›è¡Œä¼ è¾“. |
> | `RACK_LOCAL`    |   æœºæ¶æœ¬åœ°åŒ–,taskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªæœºæ¶çš„ä¸¤ä¸ªèŠ‚ç‚¹ä¸Š,æ•°æ®éœ€è¦é€šè¿‡ç½‘ç»œåœ¨èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œä¼ è¾“. |
> | `NO_PREF`    |   å¯¹äºtaskæ¥è¯´,ä»å“ªé‡Œè·å–éƒ½ä¸€æ ·,æ²¡æœ‰å¥½åä¹‹åˆ† |
> | `ANY`    |   taskå’Œæ•°æ®å¯ä»¥åœ¨é›†ç¾¤çš„ä»»ä½•åœ°æ–¹,è€Œä¸”ä¸åœ¨ä¸€ä¸ªæœºæ¶ä¸­,æ€§èƒ½æœ€å·® |
> 
> åœ¨è°ƒåº¦æ‰§è¡Œæ—¶,Sparkè°ƒåº¦æ€»æ˜¯ä¼šå°½é‡è®©æ¯ä¸ªtaskä»¥æœ€é«˜çš„æœ¬åœ°æ€§çº§åˆ«æ¥å¯åŠ¨,å½“ä¸€ä¸ªtaskä»¥Xæœ¬åœ°æ€§çº§åˆ«å¯åŠ¨,ä½†æ˜¯è¯¥æœ¬åœ°æ€§çº§åˆ«å¯¹åº”çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½æ²¡æœ‰ç©ºé—²èµ„æºè€Œå¯åŠ¨å¤±è´¥,æ­¤æ—¶å¹¶ä¸ä¼šé©¬ä¸Šé™ä½æœ¬åœ°æ€§çº§åˆ«å¯åŠ¨è€Œæ˜¯åœ¨æŸä¸ªæ—¶é—´é•¿åº¦å†…å†æ¬¡ä»¥Xæœ¬åœ°æ€§çº§åˆ«æ¥å¯åŠ¨è¯¥task,è‹¥è¶…è¿‡é™æ—¶æ—¶é—´åˆ™é™çº§å¯åŠ¨,å»å°è¯•ä¸‹ä¸€ä¸ªæœ¬åœ°æ€§çº§åˆ«,ä¾æ¬¡ç±»æ¨.
> 
> å¯ä»¥é€šè¿‡è°ƒå¤§æ¯ä¸ªç±»åˆ«çš„æœ€å¤§å®¹å¿å»¶è¿Ÿæ—¶é—´,åœ¨ç­‰å¾…é˜¶æ®µå¯¹åº”çš„Executorå¯èƒ½å°±ä¼šæœ‰ç›¸åº”çš„èµ„æºå»æ‰§è¡Œæ­¤task,è¿™å°±åœ¨ä¸€å®šç¨‹åº¦ä¸Šæåˆ°äº†è¿è¡Œæ€§èƒ½.

###### 2.1.5.4.3 å¤±è´¥é‡è¯•ä¸é»‘åå•æœºåˆ¶
> é™¤äº†é€‰æ‹©åˆé€‚çš„Taskè°ƒåº¦è¿è¡Œå¤–,è¿˜éœ€è¦ç›‘æ§Taskçš„æ‰§è¡ŒçŠ¶æ€,ä¸å¤–éƒ¨æ‰“äº¤é“çš„æ˜¯SchedulerBackend,Taskè¢«æäº¤åˆ°Executorå¯åŠ¨æ‰§è¡Œå,Executorä¼šå°†æ‰§è¡ŒçŠ¶æ€ä¸ŠæŠ¥ç»™SchedulerBackend,SchedulerBackendåˆ™å‘Šè¯‰TaskScheduler,TaskScheduleræ‰¾åˆ°è¯¥Taskå¯¹åº”çš„TaskSetManager,å¹¶é€šçŸ¥åˆ°è¯¥TaskSetManager,è¿™æ ·TaskSetManagerå°±çŸ¥é“Taskçš„å¤±è´¥ä¸æˆåŠŸçŠ¶æ€,å¯¹äºå¤±è´¥çš„Task,ä¼šè®°å½•å®ƒå¤±è´¥çš„æ¬¡æ•°,å¦‚æœå¤±è´¥æ¬¡æ•°è¿˜æ²¡æœ‰è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°,é‚£ä¹ˆå°±æŠŠå®ƒæ”¾å›å¾…è°ƒåº¦çš„Taskæ± å­ä¸­,å¦åˆ™æ•´ä¸ªApplicationå¤±è´¥.
> 
> åœ¨è®°å½•Taskå¤±è´¥æ¬¡æ•°è¿‡ç¨‹ä¸­,ä¼šè®°å½•å®ƒä¸Šä¸€æ¬¡å¤±è´¥æ‰€åœ¨çš„Executor Idå’ŒHost,è¿™æ ·ä¸‹æ¬¡å†è°ƒåº¦è¿™ä¸ªTaskæ—¶,ä¼šä½¿ç”¨é»‘åå•æœºåˆ¶,é¿å…å®ƒè¢«è°ƒåº¦åˆ°ä¸Šä¸€æ¬¡å¤±è´¥çš„èŠ‚ç‚¹ä¸Š,èµ·åˆ°ä¸€å®šçš„å®¹é”™ä½œç”¨,é»‘åå•è®°å½•Taskä¸Šä¸€æ¬¡å¤±è´¥æ‰€åœ¨çš„Executor Idå’ŒHost,ä»¥åŠå…¶å¯¹åº”çš„â€œæ‹‰é»‘â€æ—¶é—´,â€œæ‹‰é»‘â€æ—¶é—´æ˜¯æŒ‡è¿™æ®µæ—¶é—´å†…ä¸è¦å†å¾€è¿™ä¸ªèŠ‚ç‚¹ä¸Šè°ƒåº¦è¿™ä¸ªTaskäº†.

#### 2.1.6 Spark Shuffle è§£æ
##### 2.1.6.1 Shuffle æ ¸å¿ƒè¦ç‚¹
###### 2.1.6.1.1 ShuffleMapStageä¸FinalStage
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_055.jpg)
> åœ¨åˆ’åˆ†stageæ—¶,æœ€åä¸€ä¸ªstageç§°ä¸ºFinalStage,å®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªResultStageå¯¹è±¡,å‰é¢çš„æ‰€æœ‰stageè¢«ç§°ä¸ºShuffleMapStage.
> 
> ShuffleMapStageçš„ç»“æŸä¼´éšç€shuffleæ–‡ä»¶çš„å†™ç£ç›˜.
> ResultStageåŸºæœ¬ä¸Šå¯¹åº”ä»£ç ä¸­çš„actionç®—å­,å³å°†ä¸€ä¸ªå‡½æ•°åº”ç”¨åœ¨RDDçš„å„ä¸ªpartitionçš„æ•°æ®é›†ä¸Š,æ„å‘³ç€ä¸€ä¸ªjobçš„è¿è¡Œç»“æŸ.

###### 2.1.6.1.2 Shuffle ä»»åŠ¡ä¸ªæ•°
> 1.mapç«¯taskä¸ªæ•°ç¡®å®š
> Shuffleè¿‡ç¨‹ä¸­çš„taskä¸ªæ•°ç”±RDDåˆ†åŒºæ•°å†³å®šï¼Œè€ŒRDDçš„åˆ†åŒºä¸ªæ•°ä¸å‚æ•°spark.default.parallelismæœ‰å¯†åˆ‡å…³ç³»
> åœ¨YarnClusteræ¨¡å¼ä¸‹ï¼Œå¦‚æœæ²¡æœ‰æ‰‹åŠ¨è®¾ç½®`spark.default.parallelism`
> ```
park.default.parallelism=  max(æ‰€æœ‰executorä½¿ç”¨çš„coreæ€»æ•°ï¼Œ2)
> ```
> 2. reduceç«¯taskä¸ªæ•°ç¡®å®š
> Reduceç«¯è¿›è¡Œæ•°æ®çš„èšåˆ,ä¸€éƒ¨åˆ†èšåˆç®—å­å¯ä»¥æ‰‹åŠ¨æŒ‡å®šreducetaskçš„å¹¶è¡Œåº¦,å¦‚æœæ²¡æœ‰æŒ‡å®š,åˆ™ä»¥mapç«¯çš„æœ€åä¸€ä¸ªRDDçš„åˆ†åŒºæ•°ä½œä¸ºå…¶åˆ†åŒºæ•°,é‚£ä¹ˆåˆ†åŒºæ•°å°±å†³å®šäº†reduceç«¯çš„taskçš„ä¸ªæ•°.

###### 2.1.6.1.3 reduceç«¯æ•°æ®è¯»å–
> æ ¹æ®stageçš„åˆ’åˆ†,mapç«¯taskå’Œreduceç«¯taskä¸åœ¨ç›¸åŒçš„stageä¸­,maptaskä½äºShuffleMapStage,reducetaskä½äºResultStage,maptaskä¼šå…ˆæ‰§è¡Œ.
> 
> reduceç«¯çš„æ•°æ®æ‹‰å–è¿‡ç¨‹å¦‚ä¸‹ : 
> 1.map taskæ‰§è¡Œå®Œæ¯•åä¼šå°†è®¡ç®—çŠ¶æ€ä»¥åŠç£ç›˜å°æ–‡ä»¶ä½ç½®ç­‰ä¿¡æ¯å°è£…åˆ°mapStatueå¯¹è±¡ä¸­,ç„¶åç”±æœ¬è¿›ç¨‹ä¸­çš„MapOutPutTrackerWorkerå¯¹è±¡å°†mapStatuså¯¹è±¡å‘é€ç»™Driverè¿›ç¨‹çš„MapOutPutTrackerMasterå¯¹è±¡.
> 2.åœ¨reduce taskå¼€å§‹æ‰§è¡Œä¹‹å‰ä¼šå…ˆè®©æœ¬è¿›ç¨‹ä¸­çš„MapOutputTrackerWorkerå‘Driverè¿›ç¨‹ä¸­çš„MapoutPutTrakcerMasterå‘åŠ¨è¯·æ±‚,è¯·æ±‚ç£ç›˜å°æ–‡ä»¶ä½ç½®ä¿¡æ¯.
> 3.å½“æ‰€æœ‰çš„Map taskæ‰§è¡Œå®Œæ¯•å,Driverè¿›ç¨‹ä¸­çš„MapOutPutTrackerMasterå°±æŒæ¡äº†æ‰€æœ‰çš„ç£ç›˜å°æ–‡ä»¶çš„ä½ç½®ä¿¡æ¯,æ­¤æ—¶MapOutPutTrackerMasterä¼šå‘Šè¯‰MapOutPutTrackerWorkerç£ç›˜å°æ–‡ä»¶çš„ä½ç½®ä¿¡æ¯.
> 4.å®Œæˆä¹‹å‰çš„æ“ä½œä¹‹å,ç”±BlockTransforServiceå»Executoræ‰€åœ¨çš„èŠ‚ç‚¹æ‹‰æ•°æ®,é»˜è®¤ä¼šå¯åŠ¨äº”ä¸ªå­çº¿ç¨‹,æ¯æ¬¡æ‹‰å–çš„æ•°æ®é‡ä¸èƒ½è¶…è¿‡48M(reduce taskæ¯æ¬¡æœ€å¤šæ‹‰å–48Mæ•°æ®,å°†æ‹‰æ¥çš„æ•°æ®å­˜å‚¨åˆ°Executorå†…å­˜çš„20%å†…å­˜ä¸­).

##### 2.1.6.2 Hash Shuffle è§£æ
> 1.æœªç»ä¼˜åŒ–HashShuffleManager
> shuffle writeé˜¶æ®µ,ä¸»è¦å°±æ˜¯åœ¨ä¸€ä¸ªstageç»“æŸè®¡ç®—ä¹‹å,ä¸ºäº†ä¸‹ä¸€ä¸ªstageå¯ä»¥æ‰§è¡Œshuffleç±»çš„ç®—å­(æ¯”å¦‚reduceByKey),è€Œå°†æ¯ä¸ªtaskå¤„ç†çš„æ•°æ®æŒ‰keyè¿›è¡Œâ€œåˆ’åˆ†â€,æ‰€è°“â€œåˆ’åˆ†â€å°±æ˜¯å¯¹ç›¸åŒçš„keyæ‰§è¡Œhashç®—æ³•,ä»è€Œå°†ç›¸åŒkeyéƒ½å†™å…¥åŒä¸€ä¸ªç£ç›˜æ–‡ä»¶ä¸­,è€Œæ¯ä¸€ä¸ªç£ç›˜æ–‡ä»¶éƒ½åªå±äºä¸‹æ¸¸stageçš„ä¸€ä¸ªtask,åœ¨å°†æ•°æ®å†™å…¥ç£ç›˜ä¹‹å‰,ä¼šå…ˆå°†æ•°æ®å†™å…¥å†…å­˜ç¼“å†²ä¸­,å½“å†…å­˜ç¼“å†²å¡«æ»¡ä¹‹åæ‰ä¼šæº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¸­å».
> 
> ä¸‹ä¸€ä¸ªstageçš„taskæœ‰å¤šå°‘ä¸ª,å½“å‰stageçš„æ¯ä¸ªtaskå°±è¦åˆ›å»ºå¤šå°‘ä»½ç£ç›˜æ–‡ä»¶,æ¯”å¦‚ä¸‹ä¸€ä¸ªstageæ€»å…±æœ‰100ä¸ªtask,é‚£ä¹ˆå½“å‰stageçš„æ¯ä¸ªtaskéƒ½è¦åˆ›å»º100ä»½ç£ç›˜æ–‡ä»¶,å¦‚æœå½“å‰stageæœ‰50ä¸ªtask,æ€»å…±æœ‰10ä¸ªExecutor,æ¯ä¸ªExecutoræ‰§è¡Œ5ä¸ªtask,é‚£ä¹ˆæ¯ä¸ªExecutorä¸Šæ€»å…±å°±è¦åˆ›å»º500ä¸ªç£ç›˜æ–‡ä»¶,æ‰€æœ‰Executorä¸Šä¼šåˆ›å»º5000ä¸ªç£ç›˜æ–‡ä»¶,ç”±æ­¤å¯è§,æœªç»ä¼˜åŒ–çš„shuffle writeæ“ä½œæ‰€äº§ç”Ÿçš„ç£ç›˜æ–‡ä»¶çš„æ•°é‡æ˜¯æå…¶æƒŠäºº.
> 
> shuffle readé˜¶æ®µ,é€šå¸¸å°±æ˜¯ä¸€ä¸ªstageåˆšå¼€å§‹æ—¶è¦åšçš„äº‹æƒ…,æ­¤æ—¶è¯¥stageçš„æ¯ä¸€ä¸ªtaskå°±éœ€è¦å°†ä¸Šä¸€ä¸ªstageçš„è®¡ç®—ç»“æœä¸­çš„æ‰€æœ‰ç›¸åŒkey,ä»å„ä¸ªèŠ‚ç‚¹ä¸Šé€šè¿‡ç½‘ç»œéƒ½æ‹‰å–åˆ°è‡ªå·±æ‰€åœ¨çš„èŠ‚ç‚¹ä¸Š,ç„¶åè¿›è¡Œkeyçš„èšåˆæˆ–è¿æ¥ç­‰æ“ä½œ,ç”±äºshuffle writeçš„è¿‡ç¨‹ä¸­,maptaskç»™ä¸‹æ¸¸stageçš„æ¯ä¸ªreducetaskéƒ½åˆ›å»ºäº†ä¸€ä¸ªç£ç›˜æ–‡ä»¶,å› æ­¤shuffle readçš„è¿‡ç¨‹ä¸­,æ¯ä¸ªreducetaskåªè¦ä»ä¸Šæ¸¸stageçš„æ‰€æœ‰maptaskæ‰€åœ¨èŠ‚ç‚¹ä¸Š,æ‹‰å–å±äºè‡ªå·±çš„é‚£ä¸€ä¸ªç£ç›˜æ–‡ä»¶å³å¯.
> shuffle readçš„æ‹‰å–è¿‡ç¨‹æ˜¯ä¸€è¾¹æ‹‰å–ä¸€è¾¹è¿›è¡Œèšåˆ,æ¯ä¸ªshuffle read taskéƒ½ä¼šæœ‰ä¸€ä¸ªè‡ªå·±çš„bufferç¼“å†²,æ¯æ¬¡éƒ½åªèƒ½æ‹‰å–ä¸bufferç¼“å†²ç›¸åŒå¤§å°çš„æ•°æ®,ç„¶åé€šè¿‡å†…å­˜ä¸­çš„ä¸€ä¸ªMapè¿›è¡Œèšåˆç­‰æ“ä½œ,èšåˆå®Œä¸€æ‰¹æ•°æ®å,å†æ‹‰å–ä¸‹ä¸€æ‰¹æ•°æ®,å¹¶æ”¾åˆ°bufferç¼“å†²ä¸­è¿›è¡Œèšåˆæ“ä½œ,ä»¥æ­¤ç±»æ¨,ç›´åˆ°æœ€åå°†æ‰€æœ‰æ•°æ®åˆ°æ‹‰å–å®Œ,å¹¶å¾—åˆ°æœ€ç»ˆçš„ç»“æœ.
> æœªä¼˜åŒ–HashShuffleManagerå·¥ä½œåŸç†å›¾ : 
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_056.jpg)

> 2.ä¼˜åŒ–å HashShuffleManager
> ä¸ºäº†ä¼˜åŒ–HashShuffleManagerå¯ä»¥è®¾ç½®ä¸€ä¸ªå‚æ•°,`spark.shuffle.consolidateFiles`,è¯¥å‚æ•°é»˜è®¤å€¼ä¸ºfalse,å°†å…¶è®¾ç½®ä¸ºtrueå³å¯å¼€å¯ä¼˜åŒ–æœºåˆ¶,é€šå¸¸æ¥è¯´,å¦‚æœä½¿ç”¨HashShuffleManager,é‚£ä¹ˆéƒ½å»ºè®®å¼€å¯è¿™ä¸ªé€‰é¡¹.
> 
> å¼€å¯consolidateæœºåˆ¶ä¹‹å,åœ¨shuffle writeè¿‡ç¨‹ä¸­,taskå°±ä¸æ˜¯ä¸ºä¸‹æ¸¸stageçš„æ¯ä¸ªtaskåˆ›å»ºä¸€ä¸ªç£ç›˜æ–‡ä»¶äº†,æ­¤æ—¶ä¼šå‡ºç°shuffleFileGroupçš„æ¦‚å¿µ,æ¯ä¸ªshuffleFileGroupä¼šå¯¹åº”ä¸€æ‰¹ç£ç›˜æ–‡ä»¶,ç£ç›˜æ–‡ä»¶çš„æ•°é‡ä¸ä¸‹æ¸¸stageçš„taskæ•°é‡æ˜¯ç›¸åŒçš„,ä¸€ä¸ªExecutorä¸Šæœ‰å¤šå°‘ä¸ªCPU core,å°±å¯ä»¥å¹¶è¡Œæ‰§è¡Œå¤šå°‘ä¸ªtask,è€Œç¬¬ä¸€æ‰¹å¹¶è¡Œæ‰§è¡Œçš„æ¯ä¸ªtaskéƒ½ä¼šåˆ›å»ºä¸€ä¸ªshuffleFileGroup,å¹¶å°†æ•°æ®å†™å…¥å¯¹åº”çš„ç£ç›˜æ–‡ä»¶å†….
> 
> å½“Executorçš„CPU coreæ‰§è¡Œå®Œä¸€æ‰¹task,æ¥ç€æ‰§è¡Œä¸‹ä¸€æ‰¹taskæ—¶,ä¸‹ä¸€æ‰¹taskå°±ä¼šå¤ç”¨ä¹‹å‰å·²æœ‰çš„shuffleFileGroup,åŒ…æ‹¬å…¶ä¸­çš„ç£ç›˜æ–‡ä»¶,ä¹Ÿå°±æ˜¯è¯´æ­¤æ—¶taskä¼šå°†æ•°æ®å†™å…¥å·²æœ‰çš„ç£ç›˜æ–‡ä»¶ä¸­,è€Œä¸ä¼šå†™å…¥æ–°çš„ç£ç›˜æ–‡ä»¶ä¸­,å› æ­¤consolidateæœºåˆ¶å…è®¸ä¸åŒçš„taskå¤ç”¨åŒä¸€æ‰¹ç£ç›˜æ–‡ä»¶,è¿™æ ·å°±å¯ä»¥æœ‰æ•ˆå°†å¤šä¸ªtaskçš„ç£ç›˜æ–‡ä»¶è¿›è¡Œä¸€å®šç¨‹åº¦ä¸Šçš„åˆå¹¶,ä»è€Œå¤§å¹…åº¦å‡å°‘ç£ç›˜æ–‡ä»¶çš„æ•°é‡,è¿›è€Œæå‡shuffle writeçš„æ€§èƒ½.
> 
> å‡è®¾ç¬¬äºŒä¸ªstageæœ‰100ä¸ªtask,ç¬¬ä¸€ä¸ªstageæœ‰50ä¸ªtask,æ€»å…±è¿˜æ˜¯æœ‰10ä¸ªExecutor(ExecutorCPUä¸ªæ•°ä¸º1),æ¯ä¸ªExecutoræ‰§è¡Œ5ä¸ªtask,é‚£ä¹ˆåŸæœ¬ä½¿ç”¨æœªç»ä¼˜åŒ–HashShuffleManageræ—¶,æ¯ä¸ªExecutorä¼šäº§ç”Ÿ500ä¸ªç£ç›˜æ–‡ä»¶,æ‰€æœ‰Executorä¼šäº§ç”Ÿ5000ä¸ªç£ç›˜æ–‡ä»¶çš„,ä½†æ˜¯æ­¤æ—¶ç»è¿‡ä¼˜åŒ–ä¹‹å,æ¯ä¸ªExecutoråˆ›å»ºçš„ç£ç›˜æ–‡ä»¶çš„æ•°é‡çš„è®¡ç®—å…¬å¼ä¸ºï¼š`CPU coreçš„æ•°é‡*ä¸‹ä¸€ä¸ªstageçš„taskæ•°é‡`,ä¹Ÿå°±æ˜¯è¯´,æ¯ä¸ªExecutoræ­¤æ—¶åªä¼šåˆ›å»º100ä¸ªç£ç›˜æ–‡ä»¶,æ‰€æœ‰Executoråªä¼šåˆ›å»º1000ä¸ªç£ç›˜æ–‡ä»¶.
> 
> ä¼˜åŒ–åHashShuffleManagerå·¥ä½œåŸç†å›¾
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_057.jpg)



##### 2.1.6.3 Sort Shuffle è§£æ
> SortShuffleManagerçš„è¿è¡Œæœºåˆ¶ä¸»è¦åˆ†æˆä¸¤ç§,ä¸€ç§æ˜¯æ™®é€šè¿è¡Œæœºåˆ¶,å¦ä¸€ç§æ˜¯bypassè¿ è¡Œæœºåˆ¶, å½“shuffle read taskæ•°é‡å°äºç­‰äº`spark.shuffle.sort.bypassMergeThreshold`å‚æ•°å€¼æ—¶(é»˜è®¤ä¸º200),å°±ä¼šå¯ç”¨bypassæœºåˆ¶.
> 
> 1.æ™®é€šè¿è¡Œæœºåˆ¶
> åœ¨è¯¥æ¨¡å¼ä¸‹,æ•°æ®ä¼šå…ˆå†™å…¥ä¸€ä¸ªå†…å­˜æ•°æ®ç»“æ„ä¸­,æ­¤æ—¶æ ¹æ®ä¸åŒçš„shuffleç®—å­,å¯èƒ½é€‰ç”¨ä¸åŒçš„æ•°æ®ç»“æ„,å¦‚æœæ˜¯reduceByKeyè¿™ç§èšåˆç±»çš„shuffleç®—å­,é‚£ä¹ˆä¼šé€‰ç”¨Mapæ•°æ®ç»“æ„,ä¸€è¾¹é€šè¿‡Mapè¿›è¡Œèšåˆ,ä¸€è¾¹å†™å…¥å†…å­˜,å¦‚æœæ˜¯joinè¿™ç§æ™®é€šçš„shuffleç®—å­,é‚£ä¹ˆä¼šé€‰ç”¨Arrayæ•°æ®ç»“æ„,ç›´æ¥å†™å…¥å†…å­˜,æ¥ç€æ¯å†™ä¸€æ¡æ•°æ®è¿›å…¥å†…å­˜æ•°æ®ç»“æ„ä¹‹å,å°±ä¼šåˆ¤æ–­ä¸€ä¸‹,æ˜¯å¦è¾¾åˆ°äº†æŸä¸ªä¸´ç•Œé˜ˆå€¼,å¦‚æœè¾¾åˆ°ä¸´ç•Œé˜ˆå€¼çš„è¯,é‚£ä¹ˆå°±ä¼šå°è¯•å°†å†…å­˜æ•°æ®ç»“æ„ä¸­çš„æ•°æ®æº¢å†™åˆ°ç£ç›˜,ç„¶åæ¸…ç©ºå†…å­˜æ•°æ®ç»“æ„.
> 
> åœ¨æº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¹‹å‰,ä¼šå…ˆæ ¹æ®keyå¯¹å†…å­˜æ•°æ®ç»“æ„ä¸­å·²æœ‰çš„æ•°æ®è¿›è¡Œæ’åº,æ’åºè¿‡å,ä¼šåˆ†æ‰¹å°†æ•°æ®å†™å…¥ç£ç›˜æ–‡ä»¶,é»˜è®¤çš„batchæ•°é‡æ˜¯10000æ¡,ä¹Ÿå°±æ˜¯è¯´æ’åºå¥½çš„æ•°æ®,ä¼šä»¥æ¯æ‰¹1ä¸‡æ¡æ•°æ®çš„å½¢å¼åˆ†æ‰¹å†™å…¥ç£ç›˜æ–‡ä»¶,å†™å…¥ç£ç›˜æ–‡ä»¶æ˜¯é€šè¿‡Javaçš„BufferedOutputStreamå®ç°çš„,BufferedOutputStreamæ˜¯Javaçš„ç¼“å†²è¾“å‡ºæµ,é¦–å…ˆä¼šå°†æ•°æ®ç¼“å†²åœ¨å†…å­˜ä¸­,å½“å†…å­˜ç¼“å†²æ»¡æº¢ä¹‹åå†ä¸€æ¬¡å†™å…¥ç£ç›˜æ–‡ä»¶ä¸­,è¿™æ ·å¯ä»¥å‡å°‘ç£ç›˜IOæ¬¡æ•°,æå‡æ€§èƒ½.
> 
> ä¸€ä¸ªtaskå°†æ‰€æœ‰æ•°æ®å†™å…¥å†…å­˜æ•°æ®ç»“æ„çš„è¿‡ç¨‹ä¸­,ä¼šå‘ç”Ÿå¤šæ¬¡ç£ç›˜æº¢å†™æ“ä½œ,ä¹Ÿå°±ä¼šäº§ç”Ÿå¤šä¸ªä¸´æ—¶æ–‡ä»¶,æœ€åä¼šå°†ä¹‹å‰æ‰€æœ‰çš„ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½è¿›è¡Œåˆå¹¶,è¿™å°±æ˜¯mergeè¿‡ç¨‹,æ­¤æ—¶ä¼šå°†ä¹‹å‰æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶ä¸­çš„æ•°æ®è¯»å–å‡ºæ¥,ç„¶åä¾æ¬¡å†™å…¥æœ€ç»ˆçš„ç£ç›˜æ–‡ä»¶ä¹‹ä¸­,æ­¤å¤–ç”±äºä¸€ä¸ªtaskå°±åªå¯¹åº”ä¸€ä¸ªç£ç›˜æ–‡ä»¶,ä¹Ÿå°±æ„å‘³ç€è¯¥taskä¸ºä¸‹æ¸¸stageçš„taskå‡†å¤‡çš„æ•°æ®éƒ½åœ¨è¿™ä¸€ä¸ªæ–‡ä»¶ä¸­,å› æ­¤è¿˜ä¼šå•ç‹¬å†™ä¸€ä»½ç´¢å¼•æ–‡ä»¶,å…¶ä¸­æ ‡è¯†äº†ä¸‹æ¸¸å„ä¸ªtaskçš„æ•°æ®åœ¨æ–‡ä»¶ä¸­çš„start offsetä¸end offset.
> 
> SortShuffleManagerç”±äºæœ‰ä¸€ä¸ªç£ç›˜æ–‡ä»¶mergeçš„è¿‡ç¨‹,å› æ­¤å¤§å¤§å‡å°‘äº†æ–‡ä»¶æ•°é‡,æ¯”å¦‚ç¬¬ä¸€ä¸ªstageæœ‰50ä¸ªtask,æ€»å…±æœ‰10ä¸ªExecutor,æ¯ä¸ªExecutoræ‰§è¡Œ5ä¸ªtask,è€Œç¬¬äºŒä¸ªstageæœ‰100ä¸ªtask,ç”±äºæ¯ä¸ªtaskæœ€ç»ˆåªæœ‰ä¸€ä¸ªç£ç›˜æ–‡ä»¶,å› æ­¤æ­¤æ—¶æ¯ä¸ªExecutorä¸Šåªæœ‰5ä¸ªç£ç›˜æ–‡ä»¶,æ‰€æœ‰Executoråªæœ‰50ä¸ªç£ç›˜æ–‡ä»¶.
> 
> æ™®é€šè¿è¡Œæœºåˆ¶ SortShuffleManagerå·¥ä½œåŸç†å›¾
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_058.jpg)

> 2.bypassè¿è¡Œæœºåˆ¶
> bypassè¿è¡Œæœºåˆ¶çš„è§¦å‘æ¡ä»¶å¦‚ä¸‹ : 
> shuffle map taskæ•°é‡å°äº`spark.shuffle.sort.bypassMergeThreshold`å‚æ•°çš„å€¼.
> ä¸æ˜¯èšåˆç±»çš„shuffleç®—å­.
> 
> æ­¤æ—¶æ¯ä¸ªtaskä¼šä¸ºæ¯ä¸ªä¸‹æ¸¸taskéƒ½åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç£ç›˜æ–‡ä»¶,å¹¶å°†æ•°æ®æŒ‰keyè¿›è¡Œhashç„¶åæ ¹æ®keyçš„hashå€¼,å°†keyå†™å…¥å¯¹åº”çš„ç£ç›˜æ–‡ä»¶ä¹‹ä¸­,å½“ç„¶å†™å…¥ç£ç›˜æ–‡ä»¶æ—¶ä¹Ÿæ˜¯å…ˆå†™å…¥å†…å­˜ç¼“å†²,ç¼“å†²å†™æ»¡ä¹‹åå†æº¢å†™åˆ°ç£ç›˜æ–‡ä»¶çš„,æœ€ååŒæ ·ä¼šå°†æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½åˆå¹¶æˆä¸€ä¸ªç£ç›˜æ–‡ä»¶,å¹¶åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ç´¢å¼•æ–‡ä»¶.
> 
> è¯¥è¿‡ç¨‹çš„ç£ç›˜å†™æœºåˆ¶å…¶å®è·Ÿæœªç»ä¼˜åŒ–çš„HashShuffleManageræ˜¯ä¸€æ¨¡ä¸€æ ·çš„,å› ä¸ºéƒ½è¦åˆ›å»ºæ•°é‡æƒŠäººçš„ç£ç›˜æ–‡ä»¶,åªæ˜¯åœ¨æœ€åä¼šåšä¸€ä¸ªç£ç›˜æ–‡ä»¶çš„åˆå¹¶è€Œå·²,å› æ­¤å°‘é‡çš„æœ€ç»ˆç£ç›˜æ–‡ä»¶,ä¹Ÿè®©è¯¥æœºåˆ¶ç›¸å¯¹æœªç»ä¼˜åŒ–çš„HashShuffleManageræ¥è¯´,shuffle readçš„æ€§èƒ½ä¼šæ›´å¥½.
> 
> è€Œè¯¥æœºåˆ¶ä¸æ™®é€šSortShuffleManagerè¿è¡Œæœºåˆ¶çš„ä¸åŒåœ¨äº : ç¬¬ä¸€ç£ç›˜å†™æœºåˆ¶ä¸åŒ,ç¬¬äºŒï¼Œä¸ä¼šè¿›è¡Œæ’åº,ä¹Ÿå°±æ˜¯è¯´å¯ç”¨è¯¥æœºåˆ¶çš„æœ€å¤§å¥½å¤„åœ¨äº,shuffle writeè¿‡ç¨‹ä¸­,ä¸éœ€è¦è¿›è¡Œæ•°æ®çš„æ’åºæ“ä½œ,ä¹Ÿå°±èŠ‚çœæ‰äº†è¿™éƒ¨åˆ†çš„æ€§èƒ½å¼€é”€.
> 
> æ™®é€šè¿è¡Œæœºåˆ¶ SortShuffleManagerå·¥ä½œåŸç†å›¾
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_059.jpg)

#### 2.1.7 Spark å†…å­˜ç®¡ç†
> åœ¨æ‰§è¡ŒSparkåº”ç”¨ç¨‹åºæ—¶,Sparké›†ç¾¤ä¼šå¯åŠ¨Driverå’ŒExecutorä¸¤ç§JVMè¿›ç¨‹,å‰è€…ä¸ºä¸»æ§è¿›ç¨‹,è´Ÿè´£åˆ›å»ºSparkä¸Šä¸‹æ–‡,æäº¤Sparkä½œä¸š(Job),å¹¶å°†ä½œä¸šè½¬åŒ–ä¸ºè®¡ç®—ä»»åŠ¡(Task),åœ¨å„ä¸ªExecutorè¿›ç¨‹é—´åè°ƒä»»åŠ¡çš„è°ƒåº¦,åè€…è´Ÿè´£åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Šæ‰§è¡Œå…·ä½“çš„è®¡ç®—ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™Driver,åŒæ—¶ä¸ºéœ€è¦æŒä¹…åŒ–çš„RDDæä¾›å­˜å‚¨åŠŸèƒ½,ç”±äºDriverçš„å†…å­˜ç®¡ç†ç›¸å¯¹æ¥è¯´è¾ƒä¸ºç®€å•,æœ¬èŠ‚ä¸»è¦å¯¹Executor çš„å†…å­˜ç®¡ç†è¿›è¡Œåˆ†æ,ä¸‹æ–‡ä¸­çš„Sparkå†…å­˜å‡ç‰¹æŒ‡Executorå†…å­˜.

##### 2.1.7.1 å †å†…å’Œå †å¤–å†…å­˜è§„åˆ’
> ä½œä¸ºä¸€ä¸ªJVMè¿›ç¨‹,Executorå†…å­˜ç®¡ç†å»ºç«‹åœ¨JVM çš„å†…å­˜ç®¡ç†ä¹‹ä¸Š,Sparkå¯¹JVMçš„å †å†…(On-heap)ç©ºé—´è¿›è¡Œäº†æ›´ä¸ºè¯¦ç»†çš„åˆ†é…,ä»¥å……åˆ†åˆ©ç”¨å†…å­˜,åŒæ—¶Sparkå¼•å…¥äº†å †å¤–(Off-heap)å†…å­˜,ä½¿ä¹‹å¯ä»¥ç›´æ¥åœ¨å·¥ä½œèŠ‚ç‚¹çš„ç³»ç»Ÿå†…å­˜ä¸­å¼€è¾Ÿç©ºé—´,è¿›ä¸€æ­¥ä¼˜åŒ–äº†å†…å­˜çš„ä½¿ç”¨.
> 
> 1.å †å†…å†…å­˜
> å †å†…å†…å­˜çš„å¤§å°,ç”±Spark åº”ç”¨ç¨‹åºå¯åŠ¨æ—¶çš„`â€“executor-memory`æˆ–`spark.executor.memory`å‚æ•°é…ç½®,Executor å†…è¿è¡Œçš„å¹¶å‘ä»»åŠ¡å…±äº«JVMå †å†…å†…å­˜,è¿™äº›ä»»åŠ¡åœ¨ç¼“å­˜RDDæ•°æ®å’Œå¹¿æ’­(Broadcast)æ•°æ®æ—¶å ç”¨çš„å†…å­˜è¢«è§„åˆ’ä¸ºå­˜å‚¨(Storage)å†…å­˜,è€Œè¿™äº›ä»»åŠ¡åœ¨æ‰§è¡ŒShuffleæ—¶å ç”¨çš„å†…å­˜è¢«è§„åˆ’ä¸ºæ‰§è¡Œ(Execution)å†…å­˜,å‰©ä½™çš„éƒ¨åˆ†ä¸åšç‰¹æ®Šè§„åˆ’,é‚£äº›Spark å†…éƒ¨çš„å¯¹è±¡å®ä¾‹,æˆ–è€…ç”¨æˆ·å®šä¹‰çš„Sparkåº”ç”¨ç¨‹åºä¸­çš„å¯¹è±¡å®ä¾‹,å‡å ç”¨å‰©ä½™çš„ç©ºé—´,ä¸åŒçš„ç®¡ç†æ¨¡å¼ä¸‹,è¿™ä¸‰éƒ¨åˆ†å ç”¨çš„ç©ºé—´å¤§å°å„ä¸ç›¸åŒ.
> 
> 2.å †å¤–å†…å­˜
> ä¸ºäº†è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜çš„ä½¿ç”¨ä»¥åŠæé«˜Shuffleæ—¶æ’åºçš„æ•ˆç‡,Spark å¼•å…¥äº†å †å¤–(Off-heap)å†…å­˜,ä½¿ä¹‹å¯ä»¥ç›´æ¥åœ¨å·¥ä½œèŠ‚ç‚¹çš„ç³»ç»Ÿå†…å­˜ä¸­å¼€è¾Ÿç©ºé—´,å­˜å‚¨ç»è¿‡åºåˆ—åŒ–çš„äºŒè¿›åˆ¶æ•°æ®.
> 
> å †å¤–å†…å­˜æ„å‘³ç€æŠŠå†…å­˜å¯¹è±¡åˆ†é…åœ¨Javaè™šæ‹Ÿæœºçš„å †ä»¥å¤–çš„å†…å­˜,è¿™äº›å†…å­˜ç›´æ¥å—æ“ä½œç³»ç»Ÿç®¡ç†(è€Œä¸æ˜¯è™šæ‹Ÿæœº),è¿™æ ·åšçš„ç»“æœå°±æ˜¯èƒ½ä¿æŒä¸€ä¸ªè¾ƒå°çš„å †,ä»¥å‡å°‘åƒåœ¾æ”¶é›†å¯¹åº”ç”¨çš„å½±å“.

##### 2.1.7.2 å†…å­˜ç©ºé—´åˆ†é…
> 1.é™æ€å†…å­˜ç®¡ç†
> åœ¨Sparkæœ€åˆé‡‡ç”¨çš„é™æ€å†…å­˜ç®¡ç†æœºåˆ¶ä¸‹,å­˜å‚¨å†…å­˜ã€æ‰§è¡Œå†…å­˜å’Œå…¶ä»–å†…å­˜çš„å¤§å°åœ¨Spark åº”ç”¨ç¨‹åºè¿è¡ŒæœŸé—´å‡ä¸ºå›ºå®šçš„,ä½†ç”¨æˆ·å¯ä»¥åº”ç”¨ç¨‹åºå¯åŠ¨å‰è¿›è¡Œé…ç½®.
> 
> é™æ€å†…å­˜ç®¡ç†æœºåˆ¶å®ç°èµ·æ¥è¾ƒä¸ºç®€å•,ä½†å¦‚æœç”¨æˆ·ä¸ç†Ÿæ‚‰Spark çš„å­˜å‚¨æœºåˆ¶,æˆ–æ²¡æœ‰æ ¹æ®å…·ä½“çš„æ•°æ®è§„æ¨¡å’Œè®¡ç®—ä»»åŠ¡æˆ–åšç›¸åº”çš„é…ç½®,å¾ˆå®¹æ˜“é€ æˆâ€ä¸€åŠæµ·æ°´,ä¸€åŠç«ç„°â€çš„å±€é¢,å³å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜ä¸­çš„ä¸€æ–¹å‰©ä½™å¤§é‡çš„ç©ºé—´,è€Œå¦ä¸€æ–¹å´æ—©æ—©è¢«å æ»¡,ä¸å¾—ä¸æ·˜æ±°æˆ–ç§»å‡ºæ—§çš„å†…å®¹ä»¥å­˜å‚¨æ–°çš„å†…å®¹,ç”±äºæ–°çš„å†…å­˜ç®¡ç†æœºåˆ¶çš„å‡ºç°,è¿™ç§æ–¹å¼ç›®å‰å·²ç»å¾ˆå°‘æœ‰å¼€å‘è€…ä½¿ç”¨,å‡ºäºå…¼å®¹æ—§ç‰ˆæœ¬çš„åº”ç”¨ç¨‹åºçš„ç›®çš„,Sparkä»ç„¶ä¿ç•™äº†å®ƒçš„å®ç°.
> 
> 2.ç»Ÿä¸€å†…å­˜ç®¡ç†
> spark 1.6ä¹‹åå¼•å…¥çš„ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶,ä¸é™æ€å†…å­˜ç®¡ç†çš„åŒºåˆ«åœ¨äºå­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜å…±äº«åŒä¸€å—ç©ºé—´,å¯ä»¥åŠ¨æ€å ç”¨å¯¹æ–¹çš„ç©ºé—²åŒºåŸŸ,ç»Ÿä¸€å†…å­˜ç®¡ç†çš„å †å†…å†…å­˜.
> 
> å…¶ä¸­æœ€é‡è¦çš„ä¼˜åŒ–åœ¨äºåŠ¨æ€å ç”¨æœºåˆ¶,å…¶è§„åˆ™å¦‚ä¸‹ : 
> 1.è®¾å®šåŸºæœ¬çš„å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜åŒºåŸŸ`spark.storage.storageFraction å‚æ•°`,è¯¥è®¾å®šç¡®å®šäº†åŒæ–¹å„è‡ªæ‹¥æœ‰çš„ç©ºé—´çš„èŒƒå›´.
> 2.åŒæ–¹çš„ç©ºé—´éƒ½ä¸è¶³æ—¶,åˆ™å­˜å‚¨åˆ°ç¡¬ç›˜,è‹¥å·±æ–¹ç©ºé—´ä¸è¶³è€Œå¯¹æ–¹ç©ºä½™æ—¶,å¯å€Ÿç”¨å¯¹æ–¹çš„ç©ºé—´,(å­˜å‚¨ç©ºé—´ä¸è¶³æ˜¯æŒ‡ä¸è¶³ä»¥æ”¾ä¸‹ä¸€ä¸ªå®Œæ•´çš„Block)
> 3.æ‰§è¡Œå†…å­˜çš„ç©ºé—´è¢«å¯¹æ–¹å ç”¨å,å¯è®©å¯¹æ–¹å°†å ç”¨çš„éƒ¨åˆ†è½¬å­˜åˆ°ç¡¬ç›˜,ç„¶åâ€å½’è¿˜â€å€Ÿç”¨çš„ç©ºé—´.
> 4.å­˜å‚¨å†…å­˜çš„ç©ºé—´è¢«å¯¹æ–¹å ç”¨å,æ— æ³•è®©å¯¹æ–¹â€å½’è¿˜â€,å› ä¸ºéœ€è¦è€ƒè™‘Shuffleè¿‡ç¨‹ä¸­çš„å¾ˆå¤šå› ç´ ,å®ç°èµ·æ¥è¾ƒä¸ºå¤æ‚.


##### 2.1.7.3 å­˜å‚¨å†…å­˜ç®¡ç†
> 1.RDD æŒä¹…åŒ–æœºåˆ¶
> å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(RDD)ä½œä¸ºSparkæœ€æ ¹æœ¬çš„æ•°æ®æŠ½è±¡,æ˜¯åªè¯»çš„åˆ†åŒºè®°å½•(Partition)çš„é›†åˆ,åªèƒ½åŸºäºåœ¨ç¨³å®šç‰©ç†å­˜å‚¨ä¸­çš„æ•°æ®é›†ä¸Šåˆ›å»º,æˆ–è€…åœ¨å…¶ä»–å·²æœ‰çš„RDDä¸Šæ‰§è¡Œè½¬æ¢(Transformation)æ“ä½œäº§ç”Ÿä¸€ä¸ªæ–°çš„RDD,è½¬æ¢åçš„RDDä¸åŸå§‹çš„RDDä¹‹é—´äº§ç”Ÿä¾èµ–å…³ç³»,æ„æˆäº†è¡€ç»Ÿ(Lineage),å‡­å€Ÿè¡€ç»Ÿ,Sparkä¿è¯äº†æ¯ä¸€ä¸ªRDDéƒ½å¯ä»¥è¢«é‡æ–°æ¢å¤,ä½†RDDçš„æ‰€æœ‰è½¬æ¢éƒ½æ˜¯æƒ°æ€§,å³åªæœ‰å½“ä¸€ä¸ªè¿”å›ç»“æœç»™Driverçš„è¡ŒåŠ¨(Action)å‘ç”Ÿæ—¶,Sparkæ‰ä¼šåˆ›å»ºä»»åŠ¡è¯»å–RDDç„¶åçœŸæ­£è§¦å‘è½¬æ¢çš„æ‰§è¡Œ.
> 
> Taskåœ¨å¯åŠ¨ä¹‹åˆè¯»å–ä¸€ä¸ªåˆ†åŒºæ—¶,ä¼šå…ˆåˆ¤æ–­è¿™ä¸ªåˆ†åŒºæ˜¯å¦å·²ç»è¢«æŒä¹…åŒ–,å¦‚æœæ²¡æœ‰åˆ™éœ€è¦æ£€æŸ¥Checkpointæˆ–æŒ‰ç…§è¡€ç»Ÿé‡æ–°è®¡ç®—,æ‰€ä»¥å¦‚æœä¸€ä¸ªRDDä¸Šè¦æ‰§è¡Œå¤šæ¬¡è¡ŒåŠ¨,å¯ä»¥åœ¨ç¬¬ä¸€æ¬¡è¡ŒåŠ¨ä¸­ä½¿ç”¨persistæˆ–cacheæ–¹æ³•,åœ¨å†…å­˜æˆ–ç£ç›˜ä¸­æŒä¹…åŒ–æˆ–ç¼“å­˜è¿™ä¸ªRDD,ä»è€Œåœ¨åé¢çš„è¡ŒåŠ¨æ—¶æå‡è®¡ç®—é€Ÿåº¦.
> 
> RDDçš„æŒä¹…åŒ–ç”±Sparkçš„Storage æ¨¡å—è´Ÿè´£,å®ç°äº†RDD ä¸ç‰©ç†å­˜å‚¨çš„è§£è€¦åˆ,Storageæ¨¡å—è´Ÿè´£ç®¡ç†Sparkåœ¨è®¡ç®—è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ•°æ®,å°†é‚£äº›åœ¨å†…å­˜æˆ–ç£ç›˜ã€åœ¨æœ¬åœ°æˆ–è¿œç¨‹å­˜å–æ•°æ®çš„åŠŸèƒ½å°è£…äº†èµ·æ¥,åœ¨å…·ä½“å®ç°æ—¶Driverç«¯å’ŒExecutorç«¯çš„Storageæ¨¡å—æ„æˆäº†ä¸»ä»å¼çš„æ¶æ„,å³Driverç«¯çš„BlockManagerä¸ºMaster,Executorç«¯çš„BlockManagerä¸ºSlave.
> 
> Sparkä¸­7ç§å­˜å‚¨çº§åˆ«å¦‚ä¸‹ : 
> | æŒä¹…åŒ–çº§åˆ«      |     å«ä¹‰ |
> | :--------: | :--------:|
> | MEMORY_ONLY    |   ä»¥éåºåˆ—åŒ–çš„Javaå¯¹è±¡çš„æ–¹å¼æŒä¹…åŒ–åœ¨JVMå†…å­˜ä¸­,å¦‚æœå†…å­˜æ— æ³•å®Œå…¨å­˜å‚¨RDDæ‰€æœ‰çš„partition,é‚£ä¹ˆé‚£äº›æ²¡æœ‰æŒä¹…åŒ–çš„partitionå°±ä¼šåœ¨ä¸‹ä¸€æ¬¡éœ€è¦ä½¿ç”¨å®ƒä»¬çš„æ—¶å€™,é‡æ–°è¢«è®¡ç®— |
> | MEMORY_AND_DISK    |   åŒä¸Š,ä½†æ˜¯å½“æŸäº›partitionæ— æ³•å­˜å‚¨åœ¨å†…å­˜ä¸­æ—¶,ä¼šæŒä¹…åŒ–åˆ°ç£ç›˜ä¸­,ä¸‹æ¬¡éœ€è¦ä½¿ç”¨è¿™äº›partitionæ—¶,éœ€è¦ä»ç£ç›˜ä¸Šè¯»å– |
> | MEMORY_ONLY_SER    |   åŒMEMORY_ONLY,ä½†æ˜¯ä¼šä½¿ç”¨Javaåºåˆ—åŒ–æ–¹å¼,å°†Javaå¯¹è±¡åºåˆ—åŒ–åè¿›è¡ŒæŒä¹…åŒ–,å¯ä»¥å‡å°‘å†…å­˜å¼€é”€,ä½†æ˜¯éœ€è¦è¿›è¡Œååºåˆ—åŒ–,å› æ­¤ä¼šåŠ å¤§CPUå¼€é”€ |
> | MEMORY_AND_DISK_SER    |   åŒMEMORY_AND_DISK,ä½†æ˜¯ä½¿ç”¨åºåˆ—åŒ–æ–¹å¼æŒä¹…åŒ–Javaå¯¹è±¡ |
> | DISK_ONLY    |   ä½¿ç”¨éåºåˆ—åŒ–Javaå¯¹è±¡çš„æ–¹å¼æŒä¹…åŒ–,å®Œå…¨å­˜å‚¨åˆ°ç£ç›˜ä¸Š |
> | MEMORY_ONLY_2 & MEMORY_AND_DISK_2    |   å¦‚æœæ˜¯å°¾éƒ¨åŠ äº†2çš„æŒä¹…åŒ–çº§åˆ«,è¡¨ç¤ºå°†æŒä¹…åŒ–æ•°æ®å¤ç”¨ä¸€ä»½,ä¿å­˜åˆ°å…¶ä»–èŠ‚ç‚¹,ä»è€Œåœ¨æ•°æ®ä¸¢å¤±æ—¶,ä¸éœ€è¦å†æ¬¡è®¡ç®—,åªéœ€è¦ä½¿ç”¨å¤‡ä»½æ•°æ®å³å¯ |
> 
> é€šè¿‡å¯¹æ•°æ®ç»“æ„çš„åˆ†æ,å¯ä»¥çœ‹å‡ºå­˜å‚¨çº§åˆ«ä»ä¸‰ä¸ªç»´åº¦å®šä¹‰äº†RDDçš„Partition(åŒæ—¶ä¹Ÿå°±æ˜¯Block)å­˜å‚¨æ–¹å¼ : 
> 1.å­˜å‚¨ä½ç½® : ç£ç›˜ï¼å †å†…å†…å­˜ï¼å †å¤–å†…å­˜,å¦‚`MEMORY_AND_DISK`æ˜¯åŒæ—¶åœ¨ç£ç›˜å’Œå †å†…å†…å­˜ä¸Šå­˜å‚¨,å®ç°äº†å†—ä½™å¤‡ä»½,`OFF_HEAP`åˆ™æ˜¯åªåœ¨å †å¤–å†…å­˜å­˜å‚¨,ç›®å‰é€‰æ‹©å †å¤–å†…å­˜æ—¶ä¸èƒ½åŒæ—¶å­˜å‚¨åˆ°å…¶ä»–ä½ç½®.
> 2.å­˜å‚¨å½¢å¼ : Blockç¼“å­˜åˆ°å­˜å‚¨å†…å­˜å,æ˜¯å¦ä¸ºéåºåˆ—åŒ–çš„å½¢å¼,å¦‚`MEMORY_ONLY`æ˜¯éåºåˆ—åŒ–æ–¹å¼å­˜å‚¨,`OFF_HEAP`æ˜¯åºåˆ—åŒ–æ–¹å¼å­˜å‚¨.
> 3.å‰¯æœ¬æ•°é‡ : å¤§äº1 æ—¶éœ€è¦è¿œç¨‹å†—ä½™å¤‡ä»½åˆ°å…¶ä»–èŠ‚ç‚¹,å¦‚`DISK_ONLY_2`éœ€è¦è¿œç¨‹å¤‡ä»½1ä¸ªå‰¯æœ¬.

> 2.RDD ç¼“å­˜è¿‡ç¨‹
> RDDåœ¨ç¼“å­˜åˆ°å­˜å‚¨å†…å­˜ä¹‹å‰,Partitionä¸­çš„æ•°æ®ä¸€èˆ¬ä»¥è¿­ä»£å™¨(Iterator)çš„æ•°æ®ç»“æ„æ¥è®¿é—®,è¿™æ˜¯Scalaè¯­è¨€ä¸­ä¸€ç§éå†æ•°æ®é›†åˆçš„æ–¹æ³•,é€šè¿‡Iteratorå¯ä»¥è·å–åˆ†åŒºä¸­æ¯ä¸€æ¡åºåˆ—åŒ–æˆ–è€…éåºåˆ—åŒ–çš„æ•°æ®é¡¹(Record),è¿™äº›Recordçš„å¯¹è±¡å®ä¾‹åœ¨é€»è¾‘ä¸Šå ç”¨äº†JVM å †å†…å†…å­˜çš„other éƒ¨åˆ†çš„ç©ºé—´,åŒä¸€Partitionçš„ä¸åŒRecordçš„å­˜å‚¨ç©ºé—´å¹¶ä¸è¿ç»­.
> 
> RDDåœ¨ç¼“å­˜åˆ°å­˜å‚¨å†…å­˜ä¹‹å,Partitionè¢«è½¬æ¢æˆBlock,Record åœ¨å †å†…æˆ–å †å¤–å­˜å‚¨å†…å­˜ä¸­å ç”¨ä¸€å—è¿ç»­çš„ç©ºé—´,å°†Partitionç”±ä¸è¿ç»­çš„å­˜å‚¨ç©ºé—´è½¬æ¢ä¸ºè¿ç»­å­˜å‚¨ç©ºé—´çš„è¿‡ç¨‹,Sparkç§°ä¹‹ä¸º"å±•å¼€"(Unroll)

> 3.æ·˜æ±°ä¸è½ç›˜
> ç”±äºåŒä¸€ä¸ªExecutorçš„æ‰€æœ‰çš„è®¡ç®—ä»»åŠ¡å…±äº«æœ‰é™çš„å­˜å‚¨å†…å­˜ç©ºé—´,å½“æœ‰æ–°çš„Blockéœ€è¦ç¼“å­˜ä½†æ˜¯å‰©ä½™ç©ºé—´ä¸è¶³ä¸”æ— æ³•åŠ¨æ€å ç”¨æ—¶,å°±è¦å¯¹LinkedHashMapä¸­çš„æ—§Blockè¿›è¡Œæ·˜æ±°(Eviction),è€Œè¢«æ·˜æ±°çš„Blockå¦‚æœå…¶å­˜å‚¨çº§åˆ«ä¸­åŒæ—¶åŒ…å«å­˜å‚¨åˆ°ç£ç›˜çš„è¦æ±‚,åˆ™è¦å¯¹å…¶è¿›è¡Œè½ç›˜(Drop),å¦åˆ™ç›´æ¥åˆ é™¤è¯¥Block.
> 
> å­˜å‚¨å†…å­˜çš„æ·˜æ±°è§„åˆ™ä¸º : 
> è¢«æ·˜æ±°çš„æ—§Blockè¦ä¸æ–°Blockçš„MemoryModeç›¸åŒ,å³åŒå±äºå †å¤–æˆ–å †å†…å†…å­˜.
> æ–°æ—§Blockä¸èƒ½å±äºåŒä¸€ä¸ªRDD,é¿å…å¾ªç¯æ·˜æ±°.
> æ—§Blockæ‰€å±RDDä¸èƒ½å¤„äºè¢«è¯»çŠ¶æ€,é¿å…å¼•å‘ä¸€è‡´æ€§é—®é¢˜.
> éå†LinkedHashMapä¸­Block,æŒ‰ç…§æœ€è¿‘æœ€å°‘ä½¿ç”¨(LRU)çš„é¡ºåºæ·˜æ±°,ç›´åˆ°æ»¡è¶³æ–°Blockæ‰€éœ€çš„ç©ºé—´,å…¶ä¸­LRUæ˜¯LinkedHashMapç‰¹æ€§.
> è½ç›˜çš„æµç¨‹åˆ™æ¯”è¾ƒç®€å•,å¦‚æœå…¶å­˜å‚¨çº§åˆ«ç¬¦åˆ`_useDisk`ä¸ºtrueçš„æ¡ä»¶,å†æ ¹æ®å…¶`_deserialized`åˆ¤æ–­æ˜¯å¦æ˜¯éåºåˆ—åŒ–çš„å½¢å¼,è‹¥æ˜¯åˆ™å¯¹å…¶è¿›è¡Œåºåˆ—åŒ–,æœ€åå°†æ•°æ®å­˜å‚¨åˆ°ç£ç›˜,åœ¨Storageæ¨¡å—ä¸­æ›´æ–°å…¶ä¿¡æ¯.

##### 2.1.7.4 æ‰§è¡Œå†…å­˜ç®¡ç†
> æ‰§è¡Œå†…å­˜ä¸»è¦ç”¨æ¥å­˜å‚¨ä»»åŠ¡åœ¨æ‰§è¡ŒShuffleæ—¶å ç”¨çš„å†…å­˜,Shuffleæ˜¯æŒ‰ç…§ä¸€å®šè§„åˆ™å¯¹RDD æ•°æ®é‡æ–°åˆ†åŒºçš„è¿‡ç¨‹,Shuffleçš„Writeå’ŒReadä¸¤é˜¶æ®µå¯¹æ‰§è¡Œå†…å­˜çš„ä½¿ç”¨.
> 
> Shuffle Write : åœ¨mapç«¯ä¼šé‡‡ç”¨ExternalSorterè¿›è¡Œå¤–æ’,åœ¨å†…å­˜ä¸­å­˜å‚¨æ•°æ®æ—¶ä¸»è¦å ç”¨å †å†…æ‰§è¡Œç©ºé—´.
> Shuffle Read : åœ¨å¯¹reduceç«¯çš„æ•°æ®è¿›è¡Œèšåˆæ—¶,è¦å°†æ•°æ®äº¤ç»™Aggregatorå¤„ç†,åœ¨å†…å­˜ä¸­å­˜å‚¨æ•°æ®æ—¶å ç”¨å †å†…æ‰§è¡Œç©ºé—´.
> å¦‚æœéœ€è¦è¿›è¡Œæœ€ç»ˆç»“æœæ’åº,åˆ™è¦å°†å†æ¬¡å°†æ•°æ®äº¤ç»™ExternalSorterå¤„ç†,å ç”¨å †å†…æ‰§è¡Œç©ºé—´.
> åœ¨ExternalSorterå’ŒAggregatorä¸­,Sparkä¼šä½¿ç”¨ä¸€ç§å«AppendOnlyMapå“ˆå¸Œè¡¨åœ¨å †å†…æ‰§è¡Œå†…å­˜ä¸­å­˜å‚¨æ•°æ®,ä½†åœ¨Shuffleè¿‡ç¨‹ä¸­æ‰€æœ‰æ•°æ®å¹¶ä¸èƒ½éƒ½ä¿å­˜åˆ°è¯¥å“ˆå¸Œè¡¨ä¸­,å½“è¿™ä¸ªå“ˆå¸Œè¡¨å ç”¨çš„å†…å­˜ä¼šè¿›è¡Œå‘¨æœŸæ€§åœ°é‡‡æ ·ä¼°ç®—,å½“å…¶å¤§åˆ°ä¸€å®šç¨‹åº¦,æ— æ³•å†ä»MemoryManagerç”³è¯·åˆ°æ–°çš„æ‰§è¡Œå†…å­˜æ—¶,Sparkå°±ä¼šå°†å…¶å…¨éƒ¨å†…å®¹å­˜å‚¨åˆ°ç£ç›˜æ–‡ä»¶ä¸­,è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºæº¢å­˜(Spill),æº¢å­˜åˆ°ç£ç›˜çš„æ–‡ä»¶æœ€åä¼šè¢«å½’å¹¶(Merge).
> 
> Spark çš„å­˜å‚¨å†…å­˜å’Œæ‰§è¡Œå†…å­˜æœ‰ç€æˆªç„¶ä¸åŒçš„ç®¡ç†æ–¹å¼ : å¯¹äºå­˜å‚¨å†…å­˜æ¥è¯´Sparkç”¨ä¸€ä¸ªLinkedHashMapæ¥é›†ä¸­ç®¡ç†æ‰€æœ‰çš„Block,Blockç”±éœ€è¦ç¼“å­˜çš„RDDçš„Partitionè½¬åŒ–è€Œæˆ,è€Œå¯¹äºæ‰§è¡Œå†…å­˜Sparkç”¨AppendOnlyMapæ¥å­˜å‚¨Shuffleè¿‡ç¨‹ä¸­çš„æ•°æ®,åœ¨Tungsten æ’åºä¸­ç”šè‡³æŠ½è±¡æˆä¸ºé¡µå¼å†…å­˜ç®¡ç†,å¼€è¾Ÿäº†å…¨æ–°çš„JVM å†…å­˜ç®¡ç†æœºåˆ¶.

#### 2.1.8 Spark æ ¸å¿ƒç»„ä»¶è§£æ
##### 2.1.8.1 BlockManager æ•°æ®å­˜å‚¨ä¸ç®¡ç†æœºåˆ¶
> BlockManageræ˜¯æ•´ä¸ªSparkåº•å±‚è´Ÿè´£æ•°æ®å­˜å‚¨ä¸ç®¡ç†çš„ä¸€ä¸ªç»„ä»¶,Driverå’ŒExecutorçš„æ‰€æœ‰æ•°æ®éƒ½ç”±å¯¹åº”çš„BlockManagerè¿›è¡Œç®¡ç†.
> 
> Driverä¸Šæœ‰BlockManagerMaster,è´Ÿè´£å¯¹å„ä¸ªèŠ‚ç‚¹ä¸Šçš„BlockManagerå†…éƒ¨ç®¡ç†çš„æ•°æ®çš„å…ƒæ•°æ®è¿›è¡Œç»´æŠ¤,æ¯”å¦‚blockçš„å¢åˆ æ”¹ç­‰æ“ä½œ,éƒ½ä¼šåœ¨è¿™é‡Œç»´æŠ¤å¥½å…ƒæ•°æ®çš„å˜æ›´.
> 
> æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªBlockManager,æ¯ä¸ªBlockManageråˆ›å»ºä¹‹å,ç¬¬ä¸€ä»¶äº‹å³ä½¿å»å‘BlockManagerMasterè¿›è¡Œæ³¨å†Œ,æ­¤æ—¶BlockManagerMasterä¼šä¸ºå…¶é•¿éš¾å¥å¯¹åº”çš„BlockManagerInfo.
> 
> BlockManagerMasterä¸BlockManagerçš„å…³ç³»éå¸¸åƒNameNodeä¸DataNodeçš„å…³ç³»,BlockManagerMasterä¸­ä¿å­˜ä¸­BlockManagerå†…éƒ¨ç®¡ç†æ•°æ®çš„å…ƒæ•°æ®,è¿›è¡Œç»´æŠ¤ï¼Œå½“BlockManagerè¿›è¡ŒBlockå¢åˆ æ”¹ç­‰æ“ä½œæ—¶ï¼Œéƒ½ä¼šåœ¨BlockManagerMasterä¸­è¿›è¡Œå…ƒæ•°æ®çš„å˜æ›´ï¼Œè¿™ä¸NameNodeç»´æŠ¤DataNodeçš„å…ƒæ•°æ®ä¿¡æ¯ï¼ŒDataNodeä¸­æ•°æ®å‘ç”Ÿå˜åŒ–æ—¶NameNodeä¸­çš„å…ƒæ•°æ®ä¿¡æ¯ä¹Ÿä¼šç›¸åº”å˜åŒ–æ˜¯ä¸€è‡´.


##### 2.1.8.2 Spark å…±äº«å˜é‡åº•å±‚å®ç°
> Sparkä¸€ä¸ªéå¸¸é‡è¦çš„ç‰¹æ€§å°±æ˜¯å…±äº«å˜é‡.
> é»˜è®¤æƒ…å†µä¸‹,å¦‚æœåœ¨ä¸€ä¸ªç®—å­çš„å‡½æ•°ä¸­ä½¿ç”¨åˆ°äº†æŸä¸ªå¤–éƒ¨çš„å˜é‡,é‚£ä¹ˆè¿™ä¸ªå˜é‡çš„å€¼ä¼šè¢«æ‹·è´åˆ°æ¯ä¸ªtaskä¸­,æ­¤æ—¶æ¯ä¸ªtaskåªèƒ½æ“ä½œè‡ªå·±çš„é‚£ä»½å˜é‡å‰¯æœ¬,å¦‚æœå¤šä¸ªtaskæƒ³è¦å…±äº«æŸä¸ªå˜é‡,é‚£ä¹ˆè¿™ç§æ–¹å¼æ˜¯åšä¸åˆ°çš„.
> 
> Sparkä¸ºæ­¤æä¾›äº†ä¸¤ç§å…±äº«å˜é‡,ä¸€ç§æ˜¯Broadcast Variable(å¹¿æ’­å˜é‡),å¦ä¸€ç§æ˜¯Accumulator(ç´¯åŠ å˜é‡),Broadcast Variableä¼šå°†ç”¨åˆ°çš„å˜é‡,ä»…ä»…ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ‹·è´ä¸€ä»½,å³æ¯ä¸ªExecutoræ‹·è´ä¸€ä»½,æ›´å¤§çš„ç”¨é€”æ˜¯ä¼˜åŒ–æ€§èƒ½,å‡å°‘ç½‘ç»œä¼ è¾“ä»¥åŠå†…å­˜æŸè€—,Accumulatoråˆ™å¯ä»¥è®©å¤šä¸ªtaskå…±åŒæ“ä½œä¸€ä»½å˜é‡,ä¸»è¦å¯ä»¥è¿›è¡Œç´¯åŠ æ“ä½œ,Broadcast Variableæ˜¯å…±äº«è¯»å˜é‡,taskä¸èƒ½å»ä¿®æ”¹å®ƒ,è€ŒAccumulatorå¯ä»¥è®©å¤šä¸ªtaskæ“ä½œä¸€ä¸ªå˜é‡.

###### 2.1.8.2.1 å¹¿æ’­å˜é‡
> å¹¿æ’­å˜é‡å…è®¸ç¼–ç¨‹è€…åœ¨æ¯ä¸ªExecutorä¸Šä¿ç•™å¤–éƒ¨æ•°æ®çš„åªè¯»å˜é‡,è€Œä¸æ˜¯ç»™æ¯ä¸ªä»»åŠ¡å‘é€ä¸€ä¸ªå‰¯æœ¬.
> 
> æ¯ä¸ªtaskéƒ½ä¼šä¿å­˜ä¸€ä»½å®ƒæ‰€ä½¿ç”¨çš„å¤–éƒ¨å˜é‡çš„å‰¯æœ¬,å½“ä¸€ä¸ªExecutorä¸Šçš„å¤šä¸ªtaskéƒ½ä½¿ç”¨ä¸€ä¸ªå¤§å‹å¤–éƒ¨å˜é‡æ—¶,å¯¹äºExecutorå†…å­˜çš„æ¶ˆè€—æ˜¯éå¸¸å¤§çš„,å› æ­¤å¯ä»¥å°†å¤§å‹å¤–éƒ¨å˜é‡å°è£…ä¸ºå¹¿æ’­å˜é‡æ­¤æ—¶ä¸€ä¸ªExecutorä¿å­˜ä¸€ä¸ªå˜é‡å‰¯æœ¬æ­¤Executorä¸Šçš„æ‰€æœ‰taskå…±ç”¨æ­¤å˜é‡ä¸å†æ˜¯ä¸€ä¸ªtaskå•ç‹¬ä¿å­˜ä¸€ä¸ªå‰¯æœ¬è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šé™ä½äº†Sparkä»»åŠ¡çš„å†…å­˜å ç”¨.


###### 2.1.8.2.2 ç´¯åŠ å™¨
> ç´¯åŠ å™¨(accumulator) ï¼šAccumulatoræ˜¯ä»…ä»…è¢«ç›¸å…³æ“ä½œç´¯åŠ çš„å˜é‡,å› æ­¤å¯ä»¥åœ¨å¹¶è¡Œä¸­è¢«æœ‰æ•ˆåœ°æ”¯æŒ,å®ƒä»¬å¯ç”¨äºå®ç°è®¡æ•°å™¨(å¦‚MapReduce)æˆ–æ€»å’Œè®¡æ•°.


#### 2.1.9 Spark å†…æ ¸è§£ææ€»ç»“
> Sparkå†…æ ¸åŸç†å¯¹äºæ›´å¥½ä½¿ç”¨Sparkå®Œæˆå¼€å‘ä»»åŠ¡æœ‰ç€éå¸¸é‡è¦çš„ä½œç”¨,åŒæ—¶Sparkå†…æ ¸çŸ¥è¯†ä¹Ÿæ˜¯é¢è¯•è¿‡ç¨‹ä¸­ç»å¸¸è¢«é—®åˆ°çš„çŸ¥è¯†ç‚¹.
> 
> Sparkçš„éƒ¨ç½²æ¨¡å¼ã€é€šä¿¡æ¶æ„ã€ä»»åŠ¡è°ƒåº¦æœºåˆ¶ã€Shuffleè¿‡ç¨‹ã€å†…å­˜ç®¡ç†æœºåˆ¶ä»¥åŠSparkçš„æ ¸å¿ƒç»„ä»¶è¿›è¡Œäº†è¯¦ç»†åˆ†æ,è¿™äº›å†…å®¹éƒ½æ˜¯Sparkæœ€ä¸ºé‡è¦çš„æ¶æ„åŸç†,å¸Œæœ›åœ¨ä¹‹åçš„å­¦ä¹ ä¸­å¯ä»¥ä¸æ–­æ·±åŒ–å¯¹äºSparkå†…æ ¸æ¶æ„çš„ç†è§£,åœ¨æ›´é«˜çš„å±‚æ¬¡ä¸Šå»ä½¿ç”¨SparkæŠ€æœ¯æ¡†æ¶.

### 2.2 Spark æ€§èƒ½è°ƒä¼˜
#### 2.2.1 Spark æ€§èƒ½è°ƒä¼˜
#### 2.2.2 Spark æ•°æ®å€¾æ–œ
#### 2.2.3 Spark Troubleshooting

## 3. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------