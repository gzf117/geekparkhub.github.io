# å¤§æ•°æ®Sparkç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Spark Blog

@(2019-05-15)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Spark|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

## ğŸ˜ Spark Technology ä¿®ä»™ä¹‹é“ é‡‘ä»™é“æœ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/spark.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------


[TOC]


## ğŸ”¥ 1. Spark åŸºç¡€ ğŸ”¥

### 1.1 Spark æ¦‚è¿°
- Sparkæ˜¯ä¸€ç§åŸºäºå†…å­˜å¿«é€Ÿ / é€šç”¨ / å¯æ‰©å±•å¤§æ•°æ®åˆ†æå¼•æ“.
- Sparkåœ¨2009å¹´è¯ç”Ÿäº(UC Berkeley AMP Lab)åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡AMPå®éªŒå®¤,Sparkæ˜¯ä½¿ç”¨å†…å­˜è®¡ç®—çš„å¼€æºå¤§æ•°æ®å¹¶è¡Œè®¡ç®—æ¡†æ¶,å¯ä»¥åº”å¯¹å¤æ‚çš„å¤§æ•°æ®å¤„ç†åœºæ™¯,2013å¹´Sparkæˆä¸ºApacheåŸºé‡‘ä¼šæ——ä¸‹é¡¶çº§é¡¹ç›®.
- Sparkå†…æ ¸æ˜¯ç”±Scalaç¼–ç¨‹è¯­è¨€å¼€å‘,åŒæ—¶ä¹Ÿæä¾›äº†Java/Python/Rè¯­è¨€ç­‰å¼€å‘ç¼–ç¨‹æ¥å£.


#### 1.1.1 Spark æ¨¡å—
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_001.jpg)
- 1.`Spark Core` : å®ç°äº†SparkåŸºæœ¬åŠŸèƒ½,åŒ…å«ä»»åŠ¡è°ƒåº¦ / å†…å­˜ç®¡ç† / é”™è¯¯æ¢å¤ / ä¸å­˜å‚¨ç³»ç»Ÿäº¤äº’ç­‰æ¨¡å—,Spark Coreä¸­è¿˜åŒ…å«äº†å¯¹å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(Resilient Distributed DataSet,ç®€ç§°RDD)APIå®šä¹‰.
- 2.`Spark SQL` : æ˜¯Sparkç”¨æ¥æ“ä½œç»“æ„åŒ–æ•°æ®ç¨‹åºåŒ…,é€šè¿‡Spark SQL,å¯ä»¥ä½¿ç”¨SQLæˆ–è€…Apache Hiveç‰ˆæœ¬çš„SQLæ–¹è¨€(HQL)æ¥æŸ¥è¯¢æ•°æ®,Spark SQLæ”¯æŒå¤šç§æ•°æ®æº,æ¯”å¦‚Hiveè¡¨ã€Parquetä»¥åŠJSONç­‰.
- 3.`Spark Streaming` : æ˜¯Sparkæä¾›å¯¹å®æ—¶æ•°æ®è¿›è¡Œæµå¼è®¡ç®—çš„ç»„ä»¶,æä¾›äº†ç”¨æ¥æ“ä½œæ•°æ®æµçš„API,å¹¶ä¸”ä¸Spark Coreä¸­çš„RDD APIé«˜åº¦å¯¹åº”.
- 4.`Spark MLlib` : æä¾›å¸¸è§çš„æœºå™¨å­¦ä¹ (ML)åŠŸèƒ½ç¨‹åºåº“,åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰,è¿˜æä¾›äº†æ¨¡å‹è¯„ä¼°ã€æ•°æ®å¯¼å…¥ç­‰é¢å¤–æ”¯æŒåŠŸèƒ½.
- 5.`é›†ç¾¤ç®¡ç†å™¨` : Sparkè®¾è®¡ä¸ºå¯ä»¥é«˜æ•ˆåœ°åœ¨ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹åˆ°æ•°åƒä¸ªè®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä¼¸ç¼©è®¡ç®—,ä¸ºäº†å®ç°è¿™æ ·è¦æ±‚,åŒæ—¶è·å¾—æœ€å¤§çµæ´»æ€§,Sparkæ”¯æŒåœ¨å„ç§é›†ç¾¤ç®¡ç†å™¨(Cluster Manager)ä¸Šè¿è¡Œ,åŒ…æ‹¬Hadoop YARNã€ApacheMesos,ä»¥åŠSparkè‡ªå¸¦ç®€æ˜“è°ƒåº¦å™¨,å«ä½œç‹¬ç«‹è°ƒåº¦å™¨.

#### 1.1.2 Spark ç‰¹ç‚¹
- 1.`å¿«é€Ÿ` : ä¸Hadoop MapReduceç›¸æ¯”,SparkåŸºäºå†…å­˜è¿ç®—è¦å¿«100å€ä»¥ä¸Š,åŸºäºç¡¬ç›˜è¿ç®—ä¹Ÿè¦å¿«10å€ä»¥ä¸Š,Sparkå®ç°äº†é«˜æ•ˆDAGæœ‰å‘æ— ç¯å›¾æ‰§è¡Œå¼•æ“,å¯ä»¥é€šè¿‡åŸºäºå†…å­˜æ¥é«˜æ•ˆå¤„ç†æ•°æ®æµ,è®¡ç®—ä¸­é—´ç»“æœæ˜¯å­˜åœ¨äºå†…å­˜ä¸­.
- 2.`æ˜“ç”¨` : Sparkæ”¯æŒJavaã€Pythonå’ŒScalaçš„API,è¿˜æ”¯æŒè¶…è¿‡80ç§é«˜çº§ç®—æ³•,ä½¿å¼€å‘è€…å¯ä»¥å¿«é€Ÿæ„å»ºä¸åŒåº”ç”¨,è€Œä¸”Sparkæ”¯æŒäº¤äº’å¼çš„Pythonå’ŒScalaçš„Shell,å¯ä»¥éå¸¸æ–¹ä¾¿åœ°åœ¨Shellä¸­ä½¿ç”¨Sparké›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜æ–¹æ³•.
- 3.`é€šç”¨æ€§å¼º` : Sparkæä¾›äº†ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ,Sparkå¯ä»¥ç”¨äºæ‰¹å¤„ç† / äº¤äº’å¼æŸ¥è¯¢(SparkSQL) / å®æ—¶æµå¤„ç†(SparkStreaming) / æœºå™¨å­¦ä¹ (SparkMLlib) / å›¾è®¡ç®—(GraphX),è¿™äº›ä¸åŒç±»å‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨,å‡å°‘äº†å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬.
- 4.`å…¼å®¹æ€§` : Sparkå¯ä»¥éå¸¸æ–¹ä¾¿åœ°ä¸å…¶ä»–çš„å¼€æºäº§å“è¿›è¡Œèåˆ,æ¯”å¦‚Sparkå¯ä»¥ä½¿ç”¨Hadoop YARNå’ŒApacheMesosä½œä¸ºèµ„æºç®¡ç†å’Œè°ƒåº¦å™¨,å¹¶ä¸”å¯ä»¥å¤„ç†æ‰€æœ‰Hadoopæ”¯æŒçš„æ•°æ®,åŒ…æ‹¬HDFSã€HBaseç­‰,è¿™å¯¹äºå·²ç»éƒ¨ç½²Hadoopé›†ç¾¤çš„ç”¨æˆ·ç‰¹åˆ«é‡è¦,å› ä¸ºä¸éœ€è¦åšä»»ä½•æ•°æ®è¿ç§»å°±å¯ä»¥ä½¿ç”¨Sparkå¼ºå¤§å¤„ç†èƒ½åŠ›.

#### 1.1.3 Spark åº”ç”¨åœºæ™¯
- 1.Sparkå…·æœ‰ä¸°å¯Œç»„ä»¶,å¯é€‚ç”¨äºå¤šç§å¤æ‚åº”ç”¨åœºæ™¯,å¦‚SQLæŸ¥è¯¢/æœºå™¨å­¦ä¹ /å›¾å½¢è®¡ç®—/æµå¼è®¡ç®—ç­‰,åŒæ—¶Sparkå¯ä»¥ä¸Hadoopå¾ˆå¥½åœ°é›†æˆåœ¨ä¸€èµ·,ç›®å‰å·²ç»æœ‰éƒ¨åˆ†ä¸»æµå¤§æ•°æ®å‚å•†åœ¨å‘è¡Œç‰ˆHadoopç‰ˆæœ¬ä¸­åŒ…å«Spark/Cloudera/Hortonworks/MapReduceç­‰.
- 2.Sparkå¾—åˆ°äº†ä¼—å¤šå¤§æ•°æ®å…¬å¸çš„æ”¯æŒ,è¿™äº›å…¬å¸åŒ…æ‹¬Hortonworksã€IBMã€Intelã€Clouderaã€MapRã€Pivotalã€ç™¾åº¦ã€é˜¿é‡Œã€è…¾è®¯ã€äº¬ä¸œã€æºç¨‹ã€ä¼˜é…·åœŸè±†,å½“å‰ç™¾åº¦çš„Sparkå·²åº”ç”¨äºå¤§æœç´¢ã€ç›´è¾¾å·ã€ç™¾åº¦å¤§æ•°æ®ç­‰ä¸šåŠ¡,é˜¿é‡Œåˆ©ç”¨GraphXæ„å»ºäº†å¤§è§„æ¨¡å›¾è®¡ç®—å’Œå›¾æŒ–æ˜ç³»ç»Ÿ,å®ç°äº†å¾ˆå¤šç”Ÿäº§ç³»ç»Ÿçš„æ¨èç®—æ³•,è…¾è®¯Sparké›†ç¾¤è¾¾åˆ°8000å°è§„æ¨¡,æ˜¯å½“å‰å·²çŸ¥ä¸–ç•Œä¸Šæœ€å¤§çš„Sparké›†ç¾¤.


### 1.2 Spark éƒ¨ç½²
- Sparkå®˜æ–¹åœ°å€ : [spark.apache.org](http://spark.apache.org)
- Sparkå®˜æ–¹ä¸‹è½½ : [spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)
- Sparkå®˜æ–¹æ–‡æ¡£ : [spark.apache.org/docs/2.1.1/](https://spark.apache.org/docs/2.1.1/)

è§£å‹`spark-2.1.1-bin-hadoop2.7.tgz`
```
[root@systemhub511 software]# tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/
```
é‡å‘½å`spark-2.1.1-bin-hadoop2.7`
```
[root@systemhub511 module]# mv spark-2.1.1-bin-hadoop2.7/ spark
```

### 1.3 Spark è¿è¡Œæ¨¡å¼
#### ğŸ’¥ 1.3.1 Loacl Mode ğŸ’¥
##### 1.3.1.1 Loacl Mode æ¦‚è¿°
- Localæ¨¡å¼å°±æ˜¯è¿è¡Œåœ¨å•å°æœ¬åœ°è®¡ç®—æœºæ¨¡å¼,é€šå¸¸å°±æ˜¯ç”¨äºåœ¨æœ¬åœ°ä¸Šç»ƒæ‰‹æˆ–æµ‹è¯•,å®ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹é›†ä¸­æ–¹å¼è®¾ç½®Master.
- 1.`local` : æ‰€æœ‰è®¡ç®—éƒ½è¿è¡Œåœ¨ä¸€ä¸ªçº¿ç¨‹å½“ä¸­,æ²¡æœ‰ä»»ä½•å¹¶è¡Œè®¡ç®—,é€šå¸¸åœ¨æœ¬æœºæ‰§è¡Œæµ‹è¯•ä»£ç å°±ç”¨è¿™ç§æ¨¡å¼.
- 2.`local[K]` : æŒ‡å®šä½¿ç”¨å¤šå°‘ä¸ªçº¿ç¨‹æ¥è¿è¡Œè®¡ç®—,æ¯”å¦‚`local[4]`å°±æ˜¯è¿è¡Œ4ä¸ªWorkerçº¿ç¨‹,é€šå¸¸Cpuæœ‰å‡ ä¸ªCore,å°±æŒ‡å®šå‡ ä¸ªçº¿ç¨‹,æœ€å¤§åŒ–åˆ©ç”¨Cpuè®¡ç®—èƒ½åŠ›.
- 3.`local[*]` : è¿™ç§æ¨¡å¼ç›´æ¥æŒ‰ç…§Cpuæœ€å¤šCoresæ¥è®¾ç½®çº¿ç¨‹æ•°é‡.

##### 1.3.1.2 (æ±‚Ï€) & (WordCount) & (æœ¬åœ°è°ƒè¯•) å®˜æ–¹æ¡ˆä¾‹
- 1.åŸºæœ¬è¯­æ³•
```
bin/spark-submit \
--class <main-class>
--master <master-url> \
--deploy-mode <deploy-mode> \
--conf <key>=<value> \
... # other options
<application-jar> \
[application-arguments]
```
- 2.å‚æ•°è¯´æ˜
- `--master`: æŒ‡å®šMasteråœ°å€,é»˜è®¤ä¸ºLocal.
- `--class`: åº”ç”¨ä¸»å¯åŠ¨ç±»(å¦‚org.apache.spark.examples.SparkPi).
- `--deploy-mode` : æ˜¯å¦å‘å¸ƒé©±åŠ¨åˆ°workerèŠ‚ç‚¹(cluster)æˆ–è€…ä½œä¸ºä¸€ä¸ªæœ¬åœ°å®¢æˆ·ç«¯(client)(default: client)*
- `--conf` : ä»»æ„Sparké…ç½®å±æ€§,æ ¼å¼`key=value`,å¦‚æœå€¼åŒ…å«ç©ºæ ¼,å¯ä»¥åŠ å¼•å·`"key=value"`
- `application-jar` : æ‰“åŒ…å¥½åº”ç”¨jar,åŒ…å«ä¾èµ–,URLåœ¨é›†ç¾¤ä¸­å…¨å±€å¯è§,æ¯”å¦‚`hdfs://`å…±äº«å­˜å‚¨ç³»ç»Ÿ,å¦‚æœæ˜¯`file://path`,é‚£ä¹ˆæ‰€æœ‰èŠ‚ç‚¹çš„pathéƒ½åŒ…å«åŒæ ·çš„jaråŒ….
- `application-arguments` : ä¼ ç»™main()æ–¹æ³•çš„å‚æ•°.
- `--executor-memory 1G` : æŒ‡å®šæ¯ä¸ªexecutorå¯ç”¨å†…å­˜ä¸º1G
- `--total-executor-cores 2` : æŒ‡å®šæ¯ä¸ªexecutorä½¿ç”¨cpuæ ¸æ•°ä¸º2ä¸ª

- 3.æ±‚Ï€ç¨‹åº
- 3.1 æ±‚Ï€æ‰§è¡Œè¯­å¥
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 3.2 å¼€å§‹æ‰§è¡Œä»»åŠ¡
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
```
- 3.3 æŸ¥çœ‹æ‰§è¡Œç»“æœ | è¯¥ç®—æ³•æ˜¯åˆ©ç”¨`è’™ç‰¹Â·å¡ç½—ç®—æ³•`æ±‚Ï€
```
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.059446 s
Pi is roughly 3.1411463141146316
```
- 3.4 å¯åŠ¨`spark-shell`
```
[root@systemhub511 spark]# bin/spark-shell
Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = local[*], app id = local-1558677071165).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
```
- 3.5 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:4040`

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_002.jpg)

- 4.è¿è¡ŒWordCountç¨‹åº
- 4.1 åœ¨sparkæ ¹ç›®å½•åˆ›å»ºwordcountç›®å½•
```
[root@systemhub511 spark]# mkdir -p input/wordcount
```
- 4.2 åœ¨wordcountç›®å½•åˆ›å»ºæ•°æ®æ–‡ä»¶ | vim `wordcount_001.txt`
```
[root@systemhub511 spark]# cd input/wordcount/
[root@systemhub511 wordcount]# vim wordcount_001.txt
```
```
hadoop spark hive
hadoop spark hadoop
hbase flume hive
scala java oozie
```
- 4.3 æ‰§è¡ŒWordCountå¹¶æŸ¥çœ‹æ‰“å°ç»“æœ
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (spark,2), (hive,2), (hadoop,3), (oozie,1), (flume,1), (java,1), (hbase,1))

scala> 
```
- 4.4 å°†WordCountæ‰§è¡Œç»“æœè¾“å‡ºè‡³æœ¬åœ°æ–‡ä»¶
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).saveAsTextFile("./output/wordcount/")
```
- 4.5 æŸ¥çœ‹æ–‡ä»¶ç»“æœ
```
[root@systemhub511 spark]# cd output/wordcount/
[root@systemhub511 wordcount]# ll
total 4
-rw-r--r--. 1 root root 79 May 24 14:48 part-00000
-rw-r--r--. 1 root root  0 May 24 14:48 _SUCCESS
[root@systemhub511 wordcount]# cat part-00000 
(scala,1)
(spark,2)
(hive,2)
(hadoop,3)
(oozie,1)
(flume,1)
(java,1)
(hbase,1)
[root@systemhub511 wordcount]# 
```

##### 1.3.1.3 æäº¤æµç¨‹
- æäº¤ä»»åŠ¡åˆ†æ | Sparké€šç”¨è¿è¡Œç®€æ˜“æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_003.jpg)
- æäº¤ä»»åŠ¡è§’è‰² : Driver (é©±åŠ¨å™¨) & Executor (æ‰§è¡Œå™¨)
- `1. Driver (é©±åŠ¨å™¨)`
- Sparké©±åŠ¨å™¨æ˜¯æ‰§è¡Œå¼€å‘ç¨‹åºä¸­mainæ–¹æ³•è¿›ç¨‹,å®ƒè´Ÿè´£å¼€å‘äººå‘˜ç¼–å†™ç”¨æ¥åˆ›å»ºSparkContext / åˆ›å»ºRDD,ä»¥åŠè¿›è¡ŒRDDè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œä»£ç çš„æ‰§è¡Œ,å¦‚æœä½¿ç”¨spark shell,é‚£ä¹ˆå½“å¯åŠ¨Spark shellçš„æ—¶å€™,ç³»ç»Ÿåå°è‡ªå¯ä¸€ä¸ªSparké©±åŠ¨å™¨ç¨‹åº,å°±æ˜¯åœ¨Spark shellä¸­é¢„åŠ è½½ä¸€ä¸ªå«ä½œscçš„SparkContextå¯¹è±¡,å¦‚æœé©±åŠ¨å™¨ç¨‹åºç»ˆæ­¢,é‚£ä¹ˆSparkåº”ç”¨ä¹Ÿå°±ç»“æŸäº†.
- 1.1 Driverä¸»è¦è´Ÿè´£ : 1.å°†å¼€å‘è€…ç¨‹åºè½¬ä¸ºä»»åŠ¡. `->` 2.è·Ÿè¸ªExecutorè¿è¡ŒçŠ¶å†µ. `->` 3.ä¸ºæ‰§è¡Œå™¨èŠ‚ç‚¹è°ƒåº¦ä»»åŠ¡. `->` 4.WebUIå±•ç¤ºåº”ç”¨è¿è¡ŒçŠ¶å†µ.
- `2. Executor (æ‰§è¡Œå™¨)`
- Spark Executoræ˜¯ä¸€ä¸ªå·¥ä½œè¿›ç¨‹,è´Ÿè´£åœ¨Sparkä½œä¸šä¸­è¿è¡Œä»»åŠ¡,ä»»åŠ¡é—´ç›¸äº’ç‹¬ç«‹,Sparkåº”ç”¨å¯åŠ¨æ—¶,ExecutorèŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨,å¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ªSparkåº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨,å¦‚æœæœ‰ExecutorèŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒ,Sparkåº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œ,ä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»–ExecutorèŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œ.
- 2.2 Executorä¸»è¦è´Ÿè´£ : 1.è´Ÿè´£è¿è¡Œç»„æˆSparkåº”ç”¨ä»»åŠ¡,å¹¶å°†ç»“æœè¿”å›ç»™é©±åŠ¨å™¨è¿›ç¨‹. `->` 2.é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨(Block Manager)ä¸ºå¼€å‘è€…ç¨‹åºä¸­è¦æ±‚ç¼“å­˜RDDæä¾›å†…å­˜å¼å­˜å‚¨,RDDæ˜¯ç›´æ¥ç¼“å­˜åœ¨Executorè¿›ç¨‹å†…,å› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—.

##### 1.3.1.4 æ•°æ®æµç¨‹

| å‚æ•°åˆ—è¡¨      |     å‚æ•°æè¿° |
| :--------: | :--------:|
| `textFile("input")`    |   è¯»å–æœ¬åœ°æ–‡ä»¶inputæ–‡ä»¶å¤¹æ•°æ® |
| `flatMap(_.split(" "))` | å‹å¹³æ“ä½œ,æŒ‰ç…§ç©ºæ ¼åˆ†å‰²ç¬¦å°†ä¸€è¡Œæ•°æ®æ˜ å°„æˆä¸€ä¸ªä¸ªå•è¯ |
| `map((_,1))` | å¯¹æ¯ä¸€ä¸ªå…ƒç´ æ“ä½œ,å°†å•è¯æ˜ å°„ä¸ºå…ƒç»„ |
| `reduceByKey(_+_)` | æŒ‰ç…§keyå°†å€¼è¿›è¡Œèšåˆç›¸åŠ  |
| `collect` | å°†æ•°æ®æ”¶é›†åˆ°Driverç«¯å±•ç¤º |

- WordCount ç¨‹åºåˆ†æ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_004.jpg)


#### ğŸ’¥ 1.3.2 Standalone Mode ğŸ’¥
##### 1.3.2.1 Standalone Mode æ¦‚è¿°
- ç”±`Master`+`Slave`æ„å»ºè€Œæˆçš„Sparké›†ç¾¤,Sparkè¿è¡Œåœ¨é›†ç¾¤ä¸­.
- Standaloneè¿è¡Œæ¨¡å¼
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_005.jpg)

##### 1.3.2.2 StandaloneMode QuickStart
- 1.åœ¨sparkæ ¹ç›®å½•ä¸‹è¿›å…¥confç›®å½•
```
[root@systemhub511 spark]# cd conf/
```
- 2.ä¿®æ”¹é…ç½®æ–‡ä»¶åç§° | `slaves` & `spark-env.sh`
```
[root@systemhub511 conf]# mv slaves.template slaves
[root@systemhub511 conf]# mv spark-env.sh.template spark-env.sh
```
- 3.ä¿®æ”¹slaveæ–‡ä»¶,æ·»åŠ workèŠ‚ç‚¹ | vim `slaves`
```
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# A Spark Worker will be started on each of the machines listed below.
systemhub511
systemhub611
systemhub711
```
- 4.ä¿®æ”¹spark-env.shæ–‡ä»¶ | vim `spark-env.sh`
```
# Options for the daemons used in the standalone deploy mode
SPARK_MASTER_HOST=systemhub511
SPARK_MASTER_PORT=7077
```

- 5.å°†sparkåˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 6.å¯åŠ¨sparké›†ç¾¤ | `sbin/start-all.sh`
```
[root@systemhub511 spark]# sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-systemhub511.out
systemhub711: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub711.out
systemhub611: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub611.out
systemhub511: starting org.apache.spark.deploy.worker.Worker, logging to /opt/module/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-systemhub511.out
[root@systemhub511 spark]# 
```
- 7.æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
``` powershell
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
30651 org.apache.spark.deploy.worker.Worker
30443 org.apache.spark.deploy.master.Master
813 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
10369 org.apache.spark.deploy.worker.Worker
11777 sun.tools.jps.Jps
================        root@systemhub711 All Processes         ===========
8960 org.apache.spark.deploy.worker.Worker
10364 sun.tools.jps.Jps
[root@systemhub511 spark]# 
```

- 8.(æ±‚Ï€)å®˜æ–¹æ¡ˆä¾‹
- 8.1 æ‰§è¡Œè¯­å¥ | æŒ‡å®š spark master
```
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://systemhub511:7077 \
--executor-memory 1G \
--total-executor-cores 1 \
./examples/jars/spark-examples_2.11-2.1.1.jar \
100
```
- 8.2 æ‰§è¡Œå¹¶æŸ¥çœ‹ç»“æœ
```
[root@systemhub511 spark]# bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://systemhub511:7077 \
> --executor-memory 1G \
> --total-executor-cores 1 \
> ./examples/jars/spark-examples_2.11-2.1.1.jar \
> 100
INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 6.478381 s
Pi is roughly 3.1405883140588315
```

- 8.3 å¯åŠ¨`sparkshell`,å¹¶æ‰§è¡ŒWordCountç¨‹åºæŸ¥çœ‹ç»“æœ
- å‚æ•°ï¼š`--master spark://systemhub511:7077` æŒ‡å®šè¦è¿æ¥é›†ç¾¤master
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077

Spark context Web UI available at http://systemhub511:4040
Spark context available as 'sc' (master = spark://systemhub511:7077, app id = app-20190524174512-0001).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
res0: Array[(String, Int)] = Array((scala,1), (hive,2), (oozie,1), (java,1), (spark,2), (hadoop,3), (flume,1), (hbase,1))

scala> 
```

- 8.4 é€šè¿‡WebUIæŸ¥çœ‹ç¨‹åºè¿è¡Œ | `http://hostname:8088`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_006.jpg)

- 8.5 é…ç½®å†å²æœåŠ¡å™¨(JobHistoryServer)
- é‡å‘½å`spark-default.conf.template`
```
[root@systemhub511 conf]# mv spark-defaults.conf.template spark-defaults.conf
```
- 8.5.1 é…ç½®`spark-default.conf` | vim `spark-default.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
```
- 8.5.2 é…ç½®spark-env.sh | vim `spark-env.sh`
```
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- å‚æ•°æè¿° : 
```
spark.eventLog.dirï¼šApplicationåœ¨è¿è¡Œè¿‡ç¨‹ä¸­æ‰€æœ‰ä¿¡æ¯å‡è®°å½•åœ¨è¯¥å±æ€§æŒ‡å®šçš„è·¯å¾„ä¸‹.

spark.history.ui.port=18080 WEBUIè®¿é—®ç«¯å£å·ä¸º18080

spark.history.fs.logDirectory=hdfs://systemhub511:9000/directory é…ç½®äº†è¯¥å±æ€§å,åœ¨start-history-server.shæ—¶å°±æ— éœ€å†æ˜¾ç¤ºæŒ‡å®šè·¯å¾„,Spark History Serveråªå±•ç¤ºè¯¥æŒ‡å®šè·¯å¾„ä¸‹ä¿¡æ¯.

spark.history.retainedApplications=30 æŒ‡å®šä¿å­˜Applicationå†å²è®°å½•ä¸ªæ•°,å¦‚æœè¶…è¿‡è¿™ä¸ªå€¼,æ—§åº”ç”¨ç¨‹åºä¿¡æ¯å°†è¢«åˆ é™¤,è¿™ä¸ªæ˜¯å†…å­˜ä¸­åº”ç”¨æ•°,è€Œä¸æ˜¯é¡µé¢ä¸Šæ˜¾ç¤ºåº”ç”¨æ•°.
```
- 8.5.3 åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 8.5.4 å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 hadoop]# sbin/start-dfs.sh
```
- 8.5.5 æ‰‹åŠ¨åˆ›å»ºHDFS /directoryç›®å½•
```
[root@systemhub511 spark]# hadoop fs -mkdir /directory
``` 
- 8.5.6 å¯åŠ¨Sparké›†ç¾¤
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 8.5.6 å¯åŠ¨Sparkå†å²æœåŠ¡
```
[root@systemhub511 spark]# sbin/start-history-server.sh
```
- 8.5.7 å¯åŠ¨`sparkshell`
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077
sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```
- 8.5.8 æŸ¥çœ‹å†å²æœåŠ¡ | `http://hostname:18080`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_007.jpg)

##### 1.3.2.3 Spark HA é«˜å¯ç”¨
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_008.jpg)

- 1.åœæ­¢é›†ç¾¤æ‰€æœ‰æœåŠ¡
- 2.é…ç½®spark-env.sh | vim `spark-env.sh`
```
# SPARK_MASTER_HOST=systemhub511
# SPARK_MASTER_PORT=7077
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=systemhub511,systemhub611,systemhub711 -Dspark.deploy.zookeeper.dir=/spark"
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=30 -Dspark.history.fs.logDirectory=hdfs://systemhub511:9000/directory"
```
- 3.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```
- 4.å¯åŠ¨Hadoop HDFS
```
[root@systemhub511 spark]# /opt/module/hadoop/sbin/start-dfs.sh
```
- 5.å¯åŠ¨Zookeeperé›†ç¾¤
```
[root@systemhub511 spark]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub611 ~]# /opt/module/zookeeper/bin/zkServer.sh start
[root@systemhub711 ~]# /opt/module/zookeeper/bin/zkServer.sh start
```
- 6.åœ¨systemhub511å¯åŠ¨å…¨éƒ¨æœåŠ¡èŠ‚ç‚¹
```
[root@systemhub511 spark]# sbin/start-all.sh
```
- 7.åœ¨systemhub611å•ç‹¬å¯åŠ¨masterå¤‡ä»½èŠ‚ç‚¹
```
[root@systemhub611 ~]# /opt/module/spark/sbin/start-master.sh
```
- 8.è®¿é—®SparkHAé›†ç¾¤
```
[root@systemhub511 spark]# bin/spark-shell --master spark://systemhub511:7077,systemhub611:7077
```
`http://systemhub511:8080` | systemhub511èŠ‚ç‚¹çŠ¶æ€ä¸º`ALIVE`
`http://systemhub611:8080` | systemhub611èŠ‚ç‚¹çŠ¶æ€ä¸º`STANDBY`
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_009.jpg)

- 9.æ•…éšœè½¬ç§»æµ‹è¯•
- æ‰‹åŠ¨æ€æ­»systemhub511æœåŠ¡å™¨Masterè¿›ç¨‹,å¹¶æŸ¥çœ‹systemhub511æ˜¯å¦å°†ä»»åŠ¡è½¬ç§»ç»™systemhub611å¤‡ä»½èŠ‚ç‚¹ä½œä¸ºä¸»èŠ‚ç‚¹.
- 9.1 æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€
```
[root@systemhub511 spark]# jps.sh
================        root@systemhub511 All Processes         ===========
32242 org.apache.hadoop.hdfs.server.namenode.NameNode
11206 org.apache.spark.deploy.master.Master
11368 org.apache.spark.deploy.worker.Worker
9705 org.apache.zookeeper.server.quorum.QuorumPeerMain
32444 org.apache.hadoop.hdfs.server.datanode.DataNode
5228 sun.tools.jps.Jps
================        root@systemhub611 All Processes         ===========
9157 org.apache.spark.deploy.master.Master
8901 org.apache.spark.deploy.worker.Worker
2822 sun.tools.jps.Jps
30214 org.apache.hadoop.hdfs.server.datanode.DataNode
7495 org.apache.zookeeper.server.quorum.QuorumPeerMain
================        root@systemhub711 All Processes         ===========
5312 org.apache.spark.deploy.worker.Worker
31568 sun.tools.jps.Jps
26869 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
26647 org.apache.hadoop.hdfs.server.datanode.DataNode
4014 org.apache.zookeeper.server.quorum.QuorumPeerMain
[root@systemhub511 spark]# 
```

- 9.2 Kill systemhub511 Masterä¸»èŠ‚ç‚¹
```
[root@systemhub511 spark]# kill -9 11206
```
- 9.3 systemhub511èŠ‚ç‚¹å·²å®•æœº | systemhub611å¤‡ä»½èŠ‚ç‚¹çŠ¶æ€å·²è½¬åŒ–ä¸ºALIVEä¸»èŠ‚ç‚¹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_010.jpg)


#### ğŸ’¥ 1.3.3 Yarn Mode ğŸ’¥
##### 1.3.3.1 Yarn Mode æ¦‚è¿°
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_011.jpg)
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Yarn,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤.
- ä¸¤ç§æ¨¡å¼`yarn-client`å’Œ`yarn-cluster`,ä¸»è¦åŒºåˆ«åœ¨äº : Driverç¨‹åºè¿è¡ŒèŠ‚ç‚¹
- `yarn-client` : Driverç¨‹åºè¿è¡Œåœ¨å®¢æˆ·ç«¯,é€‚ç”¨äºäº¤äº’è°ƒè¯•,ç«‹å³çœ‹åˆ°appè¾“å‡º.
- `yarn-cluster` : Driverç¨‹åºè¿è¡Œåœ¨ç”±RM(ResourceManager)å¯åŠ¨AP(APPMaster)é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ.

##### 1.3.3.2 YarnMode QuickStart

- 1.é…ç½®spark-env.sh | vim `spark-env.sh`
```
YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
```
- vim `spark-defaults.conf`
```
spark.master                     spark://systemhub511:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://systemhub511:9000/directory
spark.yarn.historyServer.address=systemhub511:18080
spark.history.ui.port=18080
```
- vim `yarn-site.xml`
``` xml
<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„ç‰©ç†å†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true -->
<property>
  <name>yarn.nodemanager.pmem-check-enabled</name>
  <value>false</value>
</property>

<!--æ˜¯å¦å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ£€æŸ¥æ¯ä¸ªä»»åŠ¡æ­£ä½¿ç”¨çš„è™šæ‹Ÿå†…å­˜é‡,å¦‚æœä»»åŠ¡è¶…å‡ºåˆ†é…å€¼,åˆ™ç›´æ¥å°†å…¶æ€æ‰,é»˜è®¤æ˜¯true-->
<property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
</property>
```

- 2.åˆ†å‘è‡³å…¶ä»–èŠ‚ç‚¹é›†ç¾¤
```
[root@systemhub511 module]# scp -r spark/ root@systemhub611:/opt/module/
[root@systemhub511 module]# scp -r spark/ root@systemhub711:/opt/module/
```

- 3.æäº¤ä»»åŠ¡åˆ°Yarnæ‰§è¡Œ
```
bin/spark-submit \ 
--class org.apache.spark.examples.SparkPi \ 
--master yarn \ 
--deploy-mode client \ 
./examples/jars/spark-examples_2.11-2.1.1.jar\ 
100
```

#### ğŸ’¥ 1.3.4 Mesos Mode ğŸ’¥
##### 1.3.4.1 Mesos Mode æ¦‚è¿°
- Sparkå®¢æˆ·ç«¯ç›´æ¥è¿æ¥Mesos,ä¸éœ€è¦é¢å¤–æ„å»ºSparké›†ç¾¤,å›½å†…åº”ç”¨æ¯”è¾ƒå°‘,æ›´å¤šæ˜¯è¿ç”¨yarnè°ƒåº¦.


#### ğŸ’¥ 1.3.5 è¿è¡Œæ¨¡å¼å¯¹æ¯” ğŸ’¥

| æ¨¡å¼      |     é›†ç¾¤æ•°é‡ |   é›†ç¾¤è¿›ç¨‹   |   æ‰€å±è€…   |
| :--------: | :--------:| :------: | :------: |
| Loacl Mode    |   1 |  æ—   |  Spark  |
| Standalone Mode    |   3 |  Master & Worker  |  Spark  |
| Yarn Mode    |   1 |  Yarn & HDFS  |  Hadoop  |

#### ğŸ’¥ 1.3.6 WordCount å®ä¾‹ ğŸ’¥
- Spark Shellä»…åœ¨æµ‹è¯•å’ŒéªŒè¯ç¨‹åºæ—¶ä½¿ç”¨çš„è¾ƒå¤š,åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šå¸¸ä¼šåœ¨IDEä¸­ç¼–åˆ¶ç¨‹åº,ç„¶åæ‰“æˆjaråŒ…æäº¤åˆ°é›†ç¾¤,æœ€å¸¸ç”¨æ˜¯åˆ›å»ºMavenå·¥ç¨‹,åˆ©ç”¨Mavenæ¥ç®¡ç†jaråŒ…ä¾èµ–.
- 1.JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
- 2.çˆ¶å·¥ç¨‹é…ç½®ä¿¡æ¯ | pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.geekparkhub.core.spark</groupId>
    <artifactId>spark_server</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>

    <modules>
        <module>spark-common</module>
    </modules>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.1.1</version>
        </dependency>
    </dependencies>

</project>
```
- 3.åˆ›å»ºå­æ¨¡å— spark-common | å­æ¨¡å—é…ç½®ä¿¡æ¯ pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>spark_server</artifactId>
        <groupId>com.geekparkhub.core.spark</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>spark-common</artifactId>

    <build>
        <finalName>WordCount</finalName>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

</project>
```

- 4.åœ¨`spark-common`å­æ¨¡å—ä¸­åˆ›å»ºscalaæºç ç›®å½• | Create `WordCount.scala`
``` scala
package com.geekparkhub.core.spark.application.wordcount

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * WordCountApplication
  * <p>
  */

object WordCount {
  def main(args: Array[String]): Unit = {

    /**
      * Create SparkConf
      * åˆ›å»º SparkConf
      */
    val sparkConf = new SparkConf().setMaster(args(0)).setAppName("WordCountApplication")

    /**
      * Create SparkContext
      * åˆ›å»º SparkContext
      */
    val sc = new SparkContext()

    /**
      * Read file
      * è¯»å–æ–‡ä»¶
      */
    val line: RDD[String] = sc.textFile(args(1))

    /**
      * To flatten
      * å‹å¹³
      */
    val word: RDD[String] = line.flatMap(_.split(" "))

    /**
      * Word conversion dual group
      * å•è¯è½¬æ¢äºŒå…ƒç»„
      */
    val wordAndOne: RDD[(String, Int)] = word.map((_, 1))

    /**
      * Count the total number of words
      * ç»Ÿè®¡å•è¯æ€»æ•°
      */
    val wordCount: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)

    /**
      * Write out the file
      * å†™å‡ºæ–‡ä»¶
      */
    wordCount.saveAsTextFile(args(2))

    /**
      * Close resource
      * å…³é—­èµ„æº
      */
    sc.stop()
  }
}
```
- 5.å°†`spark-common`å­æ¨¡å—æ‰“è‡³æˆjaråŒ…ä¸Šä¼ è‡³systemhub511æœåŠ¡å™¨
- 6.å¯åŠ¨HDFS | åœ¨HDFSåˆ›å»ºå¤šçº§ç›®å½•
```
[root@systemhub511 ~]# hadoop fs -mkdir -p /core_flow/spark/input/wordcount
```

- 7.å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ è‡³HDFSç›®å½•
```
hadoop fs -put /opt/module/spark/input/wordcount/wordcount_001.txt /core_flow/spark/input/wordcount
```
- 8.Yarnæ‰§è¡Œæäº¤ä»»åŠ¡è‡³
```
bin/spark-submit \
--class com.geekparkhub.core.spark.application.wordcount.WordCount \
--master yarn \
./lib_jar/WordCount.jar yarn \
/core_flow/spark/input/wordcount/wordcount_001.txt \
/core_flow/spark/output/wordcount
```
- 9.æŸ¥çœ‹ä»»åŠ¡æ±‡æ€»ç»“æœ
- 9.1 `hadoop fs -ls -R`
```
[root@systemhub511 spark]# hadoop fs -ls -R /core_flow/spark/output/wordcount/
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/_SUCCESS
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00000
-rw-r--r--   3 root supergroup /core_flow/spark/output/wordcount/part-00001
[root@systemhub511 spark]# 
```
- 9.2 part-00000
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00000
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(scala,1)
(hive,2)
(oozie,1)
(java,1)
[root@systemhub511 spark]# 
```
- 9.3 part-00001
```
[root@systemhub511 spark]# hadoop fs -cat /core_flow/spark/output/wordcount/part-00001
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
(spark,2)
(hadoop,3)
(flume,1)
(hbase,1)
[root@systemhub511 spark]# 
```


### ğŸ”¥ 1.3 Spark Core ğŸ”¥
#### 1.3.1 RDD æ¦‚è¿°
##### 1.3.1.1 ä»€ä¹ˆæ˜¯RDD
> `RDD` (`Resilient Distributed Dataset`)`å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†`æ˜¯Sparkä¸­æœ€åŸºæœ¬æ•°æ®æŠ½è±¡,ä»£ç ä¸­æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»,å®ƒä»£è¡¨ä¸€ä¸ªå¼¹æ€§/ä¸å¯å˜/å¯åˆ†åŒº/é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆ.

##### 1.3.1.2 RDD å±æ€§
```
 * Internally, each RDD is characterized by five main properties:
 *
 *  - 1. A list of partitions
 *  - 2. A function for computing each split
 *  - 3. A list of dependencies on other RDDs
 *  - 4. Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - 5. Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
```
> 1.ä¸€ç»„åˆ†åŒº(Partition),å³æ•°æ®é›†åŸºæœ¬ç»„æˆå•ä½;
> 2.ä¸€ä¸ªè®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å‡½æ•°;
> 3.RDDä¹‹é—´ä¾èµ–å…³ç³»;
> 4.ä¸€ä¸ªPartitioner,å³RDDåˆ†ç‰‡å‡½æ•°;
> 5.ä¸€ä¸ªåˆ—è¡¨,å­˜å‚¨å­˜å–æ¯ä¸ªPartitionçš„ä¼˜å…ˆä½ç½®(preferred location)

##### 1.3.1.3 RDD ç‰¹ç‚¹
> RDDè¡¨ç¤ºåªè¯»åˆ†åŒºæ•°æ®é›†,å¯¹RDDè¿›è¡Œæ”¹åŠ¨,åªèƒ½é€šè¿‡RDDè½¬æ¢æ“ä½œ,ç”±ä¸€ä¸ªRDDå¾—åˆ°ä¸€ä¸ªæ–°çš„RDD,æ–°çš„RDDåŒ…å«äº†ä»å…¶ä»–RDDè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´å­˜åœ¨ä¾èµ–,RDDæ‰§è¡Œæ˜¯æŒ‰ç…§è¡€ç¼˜å…³ç³»å»¶æ—¶è®¡ç®—,å¦‚æœè¡€ç¼˜å…³ç³»è¾ƒé•¿,å¯ä»¥é€šè¿‡æŒä¹…åŒ–RDDæ¥åˆ‡æ–­è¡€ç¼˜å…³ç³».

###### 1.3.1.3.1 å¼¹æ€§
- å­˜å‚¨å¼¹æ€§ : å†…å­˜ä¸ç£ç›˜çš„è‡ªåŠ¨åˆ‡æ¢.
- å®¹é”™å¼¹æ€§ : æ•°æ®ä¸¢å¤±å¯ä»¥è‡ªåŠ¨æ¢å¤.
- è®¡ç®—å¼¹æ€§ : è®¡ç®—å‡ºé”™é‡è¯•æœºåˆ¶.
- åˆ†ç‰‡å¼¹æ€§ : å¯æ ¹æ®éœ€è¦é‡æ–°åˆ†ç‰‡.


###### 1.3.1.3.2 åˆ†åŒº
> RDDé€»è¾‘ä¸Šæ˜¯åˆ†åŒºçš„,æ¯ä¸ªåˆ†åŒºæ•°æ®æ˜¯æŠ½è±¡å­˜åœ¨çš„,è®¡ç®—æ—¶ä¼šé€šè¿‡ä¸€ä¸ªcomputeå‡½æ•°å¾—åˆ°æ¯ä¸ªåˆ†åŒºæ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å·²æœ‰æ–‡ä»¶ç³»ç»Ÿæ„å»º,åˆ™computeå‡½æ•°æ˜¯è¯»å–æŒ‡å®šæ–‡ä»¶ç³»ç»Ÿä¸­æ•°æ®,å¦‚æœRDDæ˜¯é€šè¿‡å…¶ä»–RDDè½¬æ¢è€Œæ¥,åˆ™computeå‡½æ•°æ˜¯æ‰§è¡Œè½¬æ¢é€»è¾‘å°†å…¶ä»–RDDæ•°æ®è¿›è¡Œè½¬æ¢.

###### 1.3.1.3.3 åªè¯»
> RDDæ˜¯åªè¯»çš„,è¦æƒ³æ”¹å˜RDDä¸­æ•°æ®,åªèƒ½åœ¨ç°æœ‰RDDåŸºç¡€ä¸Šåˆ›å»ºæ–°çš„RDD.
> 
> ç”±ä¸€ä¸ªRDDè½¬æ¢åˆ°å¦ä¸€ä¸ªRDD,å¯ä»¥é€šè¿‡ä¸°å¯Œçš„æ“ä½œç®—å­å®ç°,ä¸å†åƒMapReduceé‚£æ ·åªèƒ½å†™mapå’Œreduce.
> 
> RDDæ“ä½œç®—å­åŒ…æ‹¬ä¸¤ç±»,ä¸€ç±»æ˜¯`transformations`,å®ƒæ˜¯ç”¨æ¥å°†RDDè¿›è¡Œè½¬åŒ–,æ„å»ºRDDçš„è¡€ç¼˜å…³ç³»,å¦ä¸€ç±»æ˜¯`actions`,å®ƒæ˜¯ç”¨æ¥è§¦å‘RDDè®¡ç®—å¾—åˆ°RDDç›¸å…³è®¡ç®—ç»“æœæˆ–è€…å°†RDDä¿å­˜æ–‡ä»¶ç³»ç»Ÿä¸­.

###### 1.3.1.3.4 ä¾èµ–
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_012.jpg)

> å¦‚å›¾æ‰€ç¤º,RDDsé€šè¿‡æ“ä½œç®—å­è¿›è¡Œè½¬æ¢,è½¬æ¢å¾—åˆ°æ–°RDDåŒ…å«äº†ä»å…¶ä»–RDDsè¡ç”Ÿæ‰€å¿…éœ€çš„ä¿¡æ¯,RDDsä¹‹é—´ç»´æŠ¤ç€è¿™ç§è¡€ç¼˜å…³ç³»,ä¹Ÿç§°ä¹‹ä¸ºä¾èµ–.
> 
> ä¾èµ–åŒ…æ‹¬ä¸¤ç§,ä¸€ç§æ˜¯çª„ä¾èµ–,RDDsä¹‹é—´åˆ†åŒºæ˜¯ä¸€ä¸€å¯¹åº”,å¦ä¸€ç§æ˜¯å®½ä¾èµ–,ä¸‹æ¸¸RDDçš„æ¯ä¸ªåˆ†åŒºä¸ä¸Šæ¸¸RDD(ä¹Ÿç§°ä¹‹ä¸ºçˆ¶RDD)çš„æ¯ä¸ªåˆ†åŒºéƒ½æœ‰å…³,æ˜¯å¤šå¯¹å¤šå…³ç³».

###### 1.3.1.3.5 ç¼“å­˜
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_013.jpg)

> å¦‚æœåœ¨åº”ç”¨ç¨‹åºä¸­å¤šæ¬¡ä½¿ç”¨åŒä¸€ä¸ªRDDæ—¶,å¯ä»¥å°†è¯¥RDDç¼“å­˜èµ·æ¥,è¯¥RDDåªæœ‰åœ¨ç¬¬ä¸€æ¬¡è®¡ç®—æ—¶ä¼šæ ¹æ®è¡€ç¼˜å…³ç³»å¾—åˆ°åˆ†åŒºæ•°æ®,åœ¨åç»­å…¶ä»–åœ°æ–¹ç”¨åˆ°è¯¥RDDæ—¶,ä¼šç›´æ¥ä»ç¼“å­˜å¤„å–è€Œä¸ç”¨å†æ ¹æ®è¡€ç¼˜å…³ç³»è®¡ç®—,è¿™æ ·å°±åŠ é€ŸåæœŸçš„é‡ç”¨.
> 
> å¦‚å›¾æ‰€ç¤º,RDD-1ç»è¿‡ä¸€ç³»åˆ—è½¬æ¢åå¾—åˆ°RDD-nå¹¶ä¿å­˜åˆ°HDFS,RDD-1åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ä¼šæœ‰ä¸ªä¸­é—´ç»“æœ,å¦‚æœå°†å…¶ç¼“å­˜åˆ°å†…å­˜,é‚£ä¹ˆåœ¨éšåRDD-1è½¬æ¢åˆ°RDD-mè¿™ä¸€è¿‡ç¨‹ä¸­,å°±ä¸ä¼šè®¡ç®—å…¶ä¹‹å‰çš„RDD-0.

###### 1.3.1.3.6 CheckPoint
> è™½ç„¶RDDè¡€ç¼˜å…³ç³»å¤©ç„¶åœ°å¯ä»¥å®ç°å®¹é”™,å½“RDDæŸä¸ªåˆ†åŒºæ•°æ®å¤±è´¥æˆ–ä¸¢å¤±,å¯ä»¥é€šè¿‡è¡€ç¼˜å…³ç³»é‡å»º,ä½†æ˜¯å¯¹äºé•¿æ—¶é—´è¿­ä»£å‹åº”ç”¨æ¥è¯´éšç€è¿­ä»£è¿›è¡Œ,RDDsä¹‹é—´è¡€ç¼˜å…³ç³»ä¼šè¶Šæ¥è¶Šé•¿,ä¸€æ—¦åœ¨åç»­è¿­ä»£è¿‡ç¨‹ä¸­å‡ºé”™,åˆ™éœ€è¦é€šè¿‡éå¸¸é•¿çš„è¡€ç¼˜å…³ç³»å»é‡å»º,åŠ¿å¿…å½±å“æ€§èƒ½.
> 
> ä¸ºæ­¤,RDDæ”¯æŒcheckpointå°†æ•°æ®ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨ä¸­,è¿™æ ·å°±å¯ä»¥åˆ‡æ–­ä¹‹å‰è¡€ç¼˜å…³ç³»,å› ä¸ºcheckpointåçš„RDDä¸éœ€è¦çŸ¥é“å®ƒçš„çˆ¶RDDs,å®ƒå¯ä»¥ä»checkpointå¤„æ‹¿åˆ°æ•°æ®.


#### 1.3.2 RDD ç¼–ç¨‹
##### 1.3.2.1 ç¼–ç¨‹æ¨¡å‹
> åœ¨Sparkä¸­,RDDè¢«è¡¨ç¤ºä¸ºå¯¹è±¡,é€šè¿‡å¯¹è±¡æ–¹æ³•è°ƒç”¨RDDè¿›è¡Œè½¬æ¢,ç»è¿‡ä¸€ç³»åˆ—çš„`transformations`å®šä¹‰RDDä¹‹å,å°±å¯ä»¥è°ƒç”¨`actions`è§¦å‘RDDè®¡ç®—,`action`å¯ä»¥æ˜¯å‘åº”ç”¨ç¨‹åºè¿”å›ç»“æœ(count,collectç­‰),æˆ–è€…æ˜¯å‘å­˜å‚¨ç³»ç»Ÿä¿å­˜æ•°æ®(saveAsTextFileç­‰).
> åœ¨Sparkä¸­,åªæœ‰é‡åˆ°`action`æ‰ä¼šæ‰§è¡ŒRDDè®¡ç®—(å³å»¶è¿Ÿè®¡ç®—),è¿™æ ·åœ¨è¿è¡Œæ—¶å¯ä»¥é€šè¿‡ç®¡é“æ–¹å¼ä¼ è¾“å¤šä¸ªè½¬æ¢.
> ä½¿ç”¨Sparkå¼€å‘è€…éœ€è¦ç¼–å†™ä¸€ä¸ªDriverç¨‹åº,å®ƒè¢«æäº¤åˆ°é›†ç¾¤ä»¥è°ƒåº¦è¿è¡ŒWorker,Driverä¸­å®šä¹‰äº†ä¸€ä¸ªæˆ–å¤šä¸ªRDD.å¹¶è°ƒç”¨RDDä¸Šçš„action.Workeråˆ™æ‰§è¡ŒRDDåˆ†åŒºè®¡ç®—ä»»åŠ¡.
##### 1.3.2.2 RDD åˆ›å»º
- Sparkåˆ›å»ºRDDåˆ›å»ºæ–¹å¼å¯ä»¥åˆ†ä¸ºä¸‰ç§:
- 1.ä»é›†åˆä¸­åˆ›å»ºRDD
- 2.ä»å¤–éƒ¨å­˜å‚¨åˆ›å»ºRDD
- 3.ä»å…¶ä»–RDDåˆ›å»º
###### 1.3.2.1 é›†åˆåˆ›å»ºRDD
- ä»é›†åˆä¸­åˆ›å»ºRDD,Sparkä¸»è¦æä¾›äº†ä¸¤ç§å‡½æ•° : `parallelize`å’Œ`makeRDD`
- 1.ä½¿ç”¨`parallelize()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- 2.ä½¿ç”¨`makeRDD()`ä»é›†åˆåˆ›å»ºRDD
```
scala> val makerdd = sc.makeRDD(Array(511,611,711))
makerdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:24
scala> makerdd.collect
res1: Array[Int] = Array(511, 611, 711)
scala> 
```
###### 1.3.2.2 å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿæ•°æ®é›†åˆ›å»ºRDD
- é™¤äº†åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ,è¿˜æœ‰æ‰€æœ‰Hadoopæ”¯æŒæ•°æ®é›†,æ¯”å¦‚HDFS/Cassandra/HBaseç­‰.
- è¯¦è§ 1.3.4 æ•°æ®è¯»å–ä¿å­˜
```
scala> sc.textFile("/opt/module/spark/input/wordcount/wordcount_001.txt")
res2: org.apache.spark.rdd.RDD[String] = /opt/module/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[3] at textFile at <console>:25
scala> 
```

###### 1.3.2.3 ä»å…¶ä»–åˆ›å»ºRDD
- è¯¦è§1.3.2.3 RDD è½¬æ¢


##### 1.3.2.3 RDD è½¬æ¢
- RDDæ•´ä½“åˆ†ä¸º`Value`ç±»å‹å’Œ`Key-Value`ç±»å‹

##### 1.3.2.3.1 Value ç±»å‹
###### 1.3.2.3.1.1 `map(func)` Method
- ä½œç”¨ : è¿”å›ä¸€ä¸ªæ–°RDD,è¯¥RDDç”±æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ç»è¿‡funcå‡½æ•°è½¬æ¢åç»„æˆ.
- åˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(511,611,711))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24
scala> rdd.collect
res0: Array[Int] = Array(511, 611, 711)
scala> 
```
- æ‰“å°RDDæœ€ç»ˆç»“æœ
```
scala> rdd.map((_,1)).collect
res4: Array[(Int, Int)] = Array((511,1), (611,1), (711,1))
scala> 
```
- å°†æ‰€æœ‰å…ƒç´ RDD*2,æœ€ç»ˆç»“æœ
```
scala> rdd.map((_*2)).collect
res5: Array[Int] = Array(1022, 1222, 1422)
scala> 
```
###### 1.3.2.3.1.2 `mapPartitions(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†ç‹¬ç«‹åœ°åœ¨RDDæ¯ä¸€ä¸ªåˆ†ç‰‡ä¸Šè¿è¡Œ,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcå‡½æ•°ç±»å‹å¿…é¡»æ˜¯Iterator[T] => Iterator[U]
- å‡è®¾æœ‰Nä¸ªå…ƒç´ ,æœ‰Mä¸ªåˆ†åŒº,é‚£ä¹ˆmapå‡½æ•°å°†è¢«è°ƒç”¨Næ¬¡,è€ŒmapPartitionsè¢«è°ƒç”¨Mæ¬¡,ä¸€ä¸ªå‡½æ•°ä¸€æ¬¡å¤„ç†æ‰€æœ‰åˆ†åŒº.
```
scala> rdd.mapPartitions(_.map(_*2)).collect
res11: Array[Int] = Array(1022, 1222, 1422)
scala> 
```

###### 1.3.2.3.1.3 `mapPartitionsWithIndex(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmapPartitions,ä½†funcå¸¦æœ‰ä¸€ä¸ªæ•´æ•°å‚æ•°è¡¨ç¤ºåˆ†ç‰‡ç´¢å¼•å€¼,å› æ­¤åœ¨ç±»å‹ä¸ºTçš„RDDä¸Šè¿è¡Œæ—¶,funcçš„å‡½æ•°ç±»å‹å¿…é¡»æ˜¯(Int, Interator[T]) => Iterator[U];
```
scala> rdd.mapPartitionsWithIndex((index,items)=>(items.map((index,_)))).collect
res13: Array[(Int, Int)] = Array((1,511), (2,611), (3,711))
scala> 
```

###### 1.3.2.3.1.4 `flatMap(func)` Method
- ä½œç”¨ : ç±»ä¼¼äºmap,ä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ å¯ä»¥è¢«æ˜ å°„ä¸º0æˆ–å¤šä¸ªè¾“å‡ºå…ƒç´ (æ‰€ä»¥funcåº”è¯¥è¿”å›ä¸€ä¸ªåºåˆ—,è€Œä¸æ˜¯å•ä¸€å…ƒç´ )
```
scala> val text = sc.textFile("/core_flow/spark/input/wordcount/wordcount_001.txt")
text: org.apache.spark.rdd.RDD[String] = /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[15] at textFile at <console>:24
scala> text.flatMap(_.split(" ")).collect
res16: Array[String] = Array(hadoop, spark, hive, hadoop, spark, hadoop, hbase, flume, hive, scala, java, oozie)
scala> 
```

###### 1.3.2.3.1.5 `map()`ä¸`mapPartition()`åŒºåˆ«
- 1.map() : æ¯æ¬¡å¤„ç†ä¸€æ¡æ•°æ®
- 2.mapPartition() : æ¯æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®,è¿™ä¸ªåˆ†åŒºçš„æ•°æ®å¤„ç†å®Œå,åŸRDDä¸­åˆ†åŒºçš„æ•°æ®æ‰èƒ½é‡Šæ”¾,å¯èƒ½å¯¼è‡´OOM.
- 3.å¼€å‘æŒ‡å¯¼ : å½“å†…å­˜ç©ºé—´è¾ƒå¤§çš„æ—¶å€™å»ºè®®ä½¿ç”¨mapPartition(),ä»¥æé«˜å¤„ç†æ•ˆç‡.

###### 1.3.2.3.1.6 `glom` Method
- ä½œç”¨ : å°†æ¯ä¸€ä¸ªåˆ†åŒºå½¢æˆä¸€ä¸ªæ•°ç»„,å½¢æˆæ–°çš„RDDç±»å‹æ—¶RDD[Array[T]]
```
scala> rdd.glom.collect
res17: Array[Array[Int]] = Array(Array(), Array(511), Array(611), Array(711))   
scala> 
```

###### 1.3.2.3.1.7 `groupBy(func)` Method
- ä½œç”¨ : åˆ†ç»„æŒ‰ç…§ä¼ å…¥å‡½æ•°çš„è¿”å›å€¼è¿›è¡Œåˆ†ç»„,å°†ç›¸åŒçš„keyå¯¹åº”çš„å€¼æ”¾å…¥ä¸€ä¸ªè¿­ä»£å™¨.
```
scala> rdd.groupBy(_ % 2).collect
res18: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(611, 711, 511)))    
scala> 
```

###### 1.3.2.3.1.8 `filter(func)` Method
- ä½œç”¨ : è¿‡æ»¤è¿”å›ä¸€ä¸ªæ–°çš„RDD,è¯¥RDDç”±ç»è¿‡funcå‡½æ•°è®¡ç®—åè¿”å›å€¼ä¸ºtrueçš„è¾“å…¥å…ƒç´ ç»„æˆ.
```
scala> rdd.filter(_%3==0).collect
res20: Array[Int] = Array(711)
scala> 
```

###### 1.3.2.3.1.9 `sample(withReplacement,fraction,seed)` Method
- ä½œç”¨ : ä»¥æŒ‡å®šéšæœºç§å­éšæœºæŠ½æ ·å‡ºæ•°é‡ä¸ºfractionçš„æ•°æ®,withReplacementè¡¨ç¤ºæ˜¯æŠ½å‡ºçš„æ•°æ®æ˜¯å¦æ”¾å›,trueä¸ºæœ‰æ”¾å›çš„æŠ½æ ·,falseä¸ºæ— æ”¾å›çš„æŠ½æ ·,seedç”¨äºæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­.
```
scala> val rdd = sc.parallelize(1 to 100)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[22] at parallelize at <console>:24
scala> rdd.sample(false,0.1,3).collect
res22: Array[Int] = Array(1, 33, 37, 50, 59, 69, 75, 78, 85, 98) 
scala> 
```

###### 1.3.2.3.1.10 `distinct([numTasks]))` Method
- ä½œç”¨ : å¯¹æºRDDè¿›è¡Œå»é‡åè¿”å›ä¸€ä¸ªæ–°çš„RDD,é»˜è®¤æƒ…å†µä¸‹,åªæœ‰8ä¸ªå¹¶è¡Œä»»åŠ¡æ¥æ“ä½œ,ä½†æ˜¯å¯ä»¥ä¼ å…¥ä¸€ä¸ªå¯é€‰çš„numTaskså‚æ•°æ”¹å˜å®ƒ.
- ä½¿ç”¨distinct()å¯¹å…¶å»é‡æ“ä½œ.
```
scala> rdd.distinct(4).collect
res23: Array[Int] = Array(84, 100, 96, 52, 56, 4, 76, 16, 28, 80, 48, 32, 36, 24, 64, 92, 40, 72, 8, 12, 20, 60, 44, 88, 68, 13, 41, 61, 81, 21, 77, 53, 97, 25, 29, 65, 73, 57, 93, 33, 37, 45, 1, 89, 17, 69, 9, 85, 49, 5, 34, 82, 66, 22, 54, 98, 46, 30, 14, 50, 62, 42, 74, 90, 6, 70, 18, 38, 86, 58, 78, 26, 94, 10, 2, 19, 39, 15, 47, 71, 55, 95, 79, 59, 11, 35, 27, 75, 51, 23, 63, 83, 67, 3, 7, 91, 31, 87, 43, 99)
scala> 
```
###### 1.3.2.3.1.11 `coalesce(numPartitions)` Method
- ä½œç”¨ : ç¼©å‡åˆ†åŒºæ•°,ç”¨äºå¤§æ•°æ®é›†è¿‡æ»¤å,æé«˜å°æ•°æ®é›†çš„æ‰§è¡Œæ•ˆç‡.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶ç¼©å‡åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at <console>:24

scala> rdd.partitions.size
res24: Int = 4

scala> val coalesceRDD = rdd.coalesce(3)
coalesceRDD: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[28] at coalesce at <console>:26

scala> coalesceRDD.partitions.size
res25: Int = 3
scala> 
```
###### 1.3.2.3.1.12 `repartition(numPartitions)` Method
- ä½œç”¨ : æ ¹æ®åˆ†åŒºæ•°,é‡æ–°é€šè¿‡ç½‘ç»œéšæœºæ´—ç‰Œæ‰€æœ‰æ•°æ®.
- åˆ›å»º4ä¸ªåˆ†åŒºRDD,å¯¹å…¶é‡æ–°åˆ†åŒº.
- åˆ›å»ºRDD/æŸ¥çœ‹RDDåˆ†åŒºæ•°/å¯¹RDDé‡æ–°åˆ†åŒº/æŸ¥çœ‹æ–°RDDåˆ†åŒºæ•°
```
scala> val rdd = sc.parallelize(1 to 16,4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at <console>:24

scala> rdd.partitions.size
res26: Int = 4

scala> val rerdd = rdd.repartition(2)
rerdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at repartition at <console>:26

scala> rerdd.partitions.size
res27: Int = 2
scala> 
```

###### 1.3.2.3.1.13 `coalesce`ä¸`repartition`åŒºåˆ«
> 1.`coalesce`é‡æ–°åˆ†åŒº,å¯ä»¥é€‰æ‹©æ˜¯å¦è¿›è¡Œshuffleè¿‡ç¨‹,ç”±å‚æ•°`shuffle: Boolean = false/true`å†³å®š.
> 
> 2.`repartition`å®é™…ä¸Šæ˜¯è°ƒç”¨coalesce,è¿›è¡Œshuffleè¿‡ç¨‹,æºç æ¼”ç¤º:
``` scala
def repartition(numpartitions: int)(implicit ord: ordering[t] = null): rdd[t] = withscope {
coalesce(numpartitions, shuffle = true)
}
```
###### 1.3.2.3.1.14 `sortBy(func,[ascending],[numTasks])` Method
- ä½œç”¨ : ä½¿ç”¨funcå…ˆå¯¹æ•°æ®è¿›è¡Œå¤„ç†,æŒ‰ç…§å¤„ç†åçš„æ•°æ®æ¯”è¾ƒç»“æœæ’åº,é»˜è®¤ä¸ºæ­£åº.
- åˆ›å»ºRDD,æŒ‰ç…§ä¸åŒè§„åˆ™è¿›è¡Œæ’åº | æŒ‰ç…§è‡ªèº«å¤§å°æ’åº / æŒ‰ç…§ä¸3ä½™æ•°å¤§å°æ’åº / æŒ‰ç…§å€’åºæ’åº
```
scala>  val rdd = sc.parallelize(List(2,1,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at <console>:24

scala> rdd.sortBy(x => x).collect()
res29: Array[Int] = Array(1, 2, 3, 4)

scala> rdd.sortBy(x => x%3).collect()
res30: Array[Int] = Array(3, 1, 4, 2)

scala> rdd.sortBy(x => x,false).collect()
res31: Array[Int] = Array(4, 3, 2, 1)

scala> 
```

###### 1.3.2.3.1.15 `pipe(command,[envVars])` Method
- ä½œç”¨ : ç®¡é“é’ˆå¯¹æ¯ä¸ªåˆ†åŒº,éƒ½æ‰§è¡Œä¸€ä¸ªshellè„šæœ¬,è¿”å›è¾“å‡ºRDD.
- åˆ›å»ºè„šæœ¬,ä½¿ç”¨ç®¡é“å°†è„šæœ¬ä½œç”¨äºRDDä¸Š
```
[root@systemhub511 ~]# vim /opt/module/spark/input/pipe.sh
[root@systemhub511 ~]# chmod 777 /opt/module/spark/input/pipe.sh
```
- vim `pipe.sh`
``` powershell
#!/bin/
shecho"Start"
while read LINE;do
	echo ">>>" ${LINE}
done
```
```
scala> rdd.pipe("/opt/module/spark/pipe.sh").collect
res18: Array[String] = Array(Start, >>>hi, >>>Hello, >>>how, >>>are, >>>you)
scala> 
```



##### 1.3.2.3.2 åŒValueç±»å‹äº¤äº’

###### 1.3.2.3.2.1 `union(otherDataset)` Method
- ä½œç”¨ : å¯¹æºRDDå’Œå‚æ•°RDDæ±‚å¹¶é›†åè¿”å›ä¸€ä¸ªæ–°RDD | åˆ›å»ºä¸¤ä¸ªRDDè¿›è¡Œå¹¶é›†è®¡ç®—
```
scala> var rdd1 = sc.parallelize(1 to 5)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> var rdd2 = sc.parallelize(5 to 10)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> rdd1.union(rdd2).collect
res0: Array[Int] = Array(1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10)
scala> 
```

###### 1.3.2.3.2.2 `subtract(otherDataset)` Method
- ä½œç”¨ : è®¡ç®—å·®çš„ä¸€ç§å‡½æ•°,å»é™¤ä¸¤ä¸ªRDDä¸­ç›¸åŒå…ƒç´ ,ä¸åŒçš„RDDå°†ä¿ç•™ä¸‹æ¥
```
scala> rdd1.subtract(rdd2).collect
res0: Array[Int] = Array(2, 4, 1, 3)
scala> 
```

###### 1.3.2.3.2.3 `intersection(otherDataset)` Method
- ä½œç”¨ : å¯¹æºRDDå’Œå‚æ•°RDDæ±‚äº¤é›†å,è¿”å›ä¸€ä¸ªæ–°çš„RDD
```
scala> rdd1.intersection(rdd2).collect
res1: Array[Int] = Array(5)
scala> 
```
###### 1.3.2.3.2.4 `cartesian(otherDataset)` Method
- ä½œç”¨ : ç¬›å¡å°”ç§¯ `(å°½é‡é¿å…ä½¿ç”¨)`
```
scala> rdd1.cartesian(rdd2).collect
res2: Array[(Int, Int)] = Array((1,5), (1,6), (1,7), (2,5), (2,6), (2,7), (1,8), (1,9), (1,10), (2,8), (2,9), (2,10), (3,5), (3,6), (3,7), (4,5), (4,6), (4,7), (5,5), (5,6), (5,7), (3,8), (3,9), (3,10), (4,8), (4,9), (4,10), (5,8), (5,9), (5,10))
scala> 
```

###### 1.3.2.3.2.5 `zip(otherDataset)` Method
- ä½œç”¨ : å°†ä¸¤ä¸ªRDDç»„åˆæˆKey/Valueå½¢å¼RDD,é»˜è®¤ä¸¤ä¸ªRDDçš„partitionæ•°é‡ä»¥åŠå…ƒç´ æ•°é‡éƒ½ç›¸åŒ,å¦åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸.
```
scala> rdd1.zip(rdd2).collect
res4: Array[(Int, Int)] = Array((1,6), (2,7), (3,8), (4,9), (5,10))
scala> 
```

##### 1.3.2.3.3 Key-Value ç±»å‹

###### 1.3.2.3.3.1 `partitionBy` Method
- ä½œç”¨ : å¯¹pairRDDè¿›è¡Œåˆ†åŒºæ“ä½œ,å¦‚æœåŸæœ‰çš„partionRDDå’Œç°æœ‰çš„partionRDDæ˜¯ä¸€è‡´çš„è¯å°±ä¸è¿›è¡Œåˆ†åŒº,å¦åˆ™ä¼šç”ŸæˆShuffleRDD,å³ä¼šäº§ç”Ÿshuffleè¿‡ç¨‹.
```
scala> val rdd1 = sc.parallelize(Array((1,"A"),(2,"B"),(3,"C"),(4,"D")),4)
rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> rdd1.mapPartitionsWithIndex((i,t)=>t.map((i,_))).collect
res3: Array[(Int, (Int, String))] = Array((0,(1,A)), (1,(2,B)), (2,(3,C)), (3,(4,D)))

scala> rdd1.partitionBy(new org.apache.spark.HashPartitioner(2))
res5: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[3] at partitionBy at <console>:27

scala> res5.partitions.size
res6: Int = 2
scala> 
```

###### 1.3.2.3.3.2 `reduceByKey(func,[numTasks])` Method
- åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ª(K,V)çš„RDD,ä½¿ç”¨æŒ‡å®šreduceå‡½æ•°,å°†ç›¸åŒkeyå€¼èšåˆåˆ°ä¸€èµ·,reduceä»»åŠ¡ä¸ªæ•°å¯ä»¥é€šè¿‡ç¬¬äºŒä¸ªå¯é€‰å‚æ•°æ¥è®¾ç½®.
```
scala> val rdd = sc.parallelize(List(("female",1),("male",5),("female",5),("male",2)))
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:24

scala> rdd.reduceByKey((x,y)=>x+y).collect
res7: Array[(String, Int)] = Array((female,6), (male,7))
scala> 
```

###### 1.3.2.3.3.3 `groupByKey` Method
- ä½œç”¨ : groupByKeyä¹Ÿæ˜¯å¯¹æ¯ä¸ªkeyè¿›è¡Œæ“ä½œ,ä½†åªç”Ÿæˆä¸€ä¸ªseq.
```
scala> rdd.groupByKey(2).collect
res8: Array[(String, Iterable[Int])] = Array((female,CompactBuffer(5, 1)), (male,CompactBuffer(5, 2)))

scala> 
```

###### 1.3.2.3.3.4 `reduceByKey`ä¸`groupByKey` åŒºåˆ«
> 1.reduceByKey : æŒ‰ç…§keyè¿›è¡Œèšåˆ,åœ¨shuffleä¹‹å‰æœ‰combine(é¢„èšåˆ)æ“ä½œ,è¿”å›ç»“æœæ˜¯RDD[k,v]
> 
> 2.groupByKey : æŒ‰ç…§keyè¿›è¡Œåˆ†ç»„,ç›´æ¥è¿›è¡Œshuffle
> 
> 3.å¼€å‘æŒ‡å¯¼ : reduceByKeyæ¯”groupByKey,å»ºè®®ä½¿ç”¨reduceByKey,ä½†æ˜¯éœ€è¦æ³¨æ„æ˜¯å¦ä¼šå½±å“ä¸šåŠ¡é€»è¾‘.

###### 1.3.2.3.3.5 `aggregateByKey` Method
> å‚æ•° : `(zeroValue:U,[partitioner:Partitioner])(seqOp: (U, V) => U,combOp: (U, U) => U)`
> 
> 1.ä½œç”¨ : åœ¨kvå¯¹çš„RDDä¸­,æŒ‰keyå°†valueè¿›è¡Œåˆ†ç»„åˆå¹¶,åˆå¹¶æ—¶å°†æ¯ä¸ªvalueå’Œåˆå§‹å€¼ä½œä¸ºseqå‡½æ•°å‚æ•°è¿›è¡Œè®¡ç®—,è¿”å›ç»“æœä½œä¸ºä¸€ä¸ªæ–°çš„kvå¯¹,ç„¶åå†å°†ç»“æœæŒ‰ç…§keyè¿›è¡Œåˆå¹¶,æœ€åå°†æ¯ä¸ªåˆ†ç»„çš„valueä¼ é€’ç»™combineå‡½æ•°è¿›è¡Œè®¡ç®—(å…ˆå°†å‰ä¸¤ä¸ªvalueè¿›è¡Œè®¡ç®—,å°†è¿”å›ç»“æœå’Œä¸‹ä¸€ä¸ªvalueä¼ ç»™combineå‡½æ•°,ä»¥æ­¤ç±»æ¨),å°†keyä¸è®¡ç®—ç»“æœä½œä¸ºä¸€ä¸ªæ–°çš„kvå¯¹è¾“å‡º.
> 
> 2.å‚æ•°æè¿° : 
> `zeroValue` : ç»™æ¯ä¸€ä¸ªåˆ†åŒºä¸­çš„æ¯ä¸€ä¸ªkeyä¸€ä¸ªåˆå§‹å€¼.
> `seqOp` : å‡½æ•°ç”¨äºåœ¨æ¯ä¸€ä¸ªåˆ†åŒºä¸­ç”¨åˆå§‹å€¼é€æ­¥è¿­ä»£value
> `combOp` : å‡½æ•°ç”¨äºåˆå¹¶æ¯ä¸ªåˆ†åŒºä¸­çš„ç»“æœ

- åˆ›å»ºä¸€ä¸ªpairRDD,å–å‡ºæ¯ä¸ªåˆ†åŒºç›¸åŒkeyå¯¹åº”å€¼çš„æœ€å¤§å€¼ç„¶åç›¸åŠ .
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_014.jpg)

```
scala> val rdd = sc.parallelize(List(("a",3),("a",2),("c",4),("b",3),("c",6),("c",8)),2)
rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:24

scala> rdd.aggregateByKey(0)(math.max(_,_),_+_).collect
res9: Array[(String, Int)] = Array((b,3), (a,3), (c,12))
scala> 
```
```
scala> rdd.aggregateByKey(0)(_+_,_+_).collect
res10: Array[(String, Int)] = Array((b,3), (a,5), (c,18))

scala> rdd.reduceByKey(_+_).collect
res11: Array[(String, Int)] = Array((b,3), (a,5), (c,18))
scala> 
```

###### 1.3.2.3.3.6 `foldByKey` Method
- å‚æ•° : `(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]`
- ä½œç”¨ : `aggregateByKey`çš„ç®€åŒ–æ“ä½œ,seqopå’Œcombopç›¸åŒ
```
scala> rdd.foldByKey(0)(_+_).collect
res12: Array[(String, Int)] = Array((b,3), (a,5), (c,18))
scala> 
```

###### 1.3.2.3.3.7 `combineByKey[C]` Method
- å‚æ•° : `(createCombiner:V=>C,mergeValue:(C,V)=>C,mergeCombiners:(C,C)=>C) `
- ä½œç”¨ : é’ˆå¯¹ç›¸åŒK,å°†Våˆå¹¶æˆä¸€ä¸ªé›†åˆ.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_015.jpg)

- å‚æ•°æè¿° : 
> 1.`createCombiner:combineByKey()`ä¼šéå†åˆ†åŒºä¸­çš„æ‰€æœ‰å…ƒç´ ,å› æ­¤æ¯ä¸ªå…ƒç´ çš„é”®è¦ä¹ˆè¿˜æ²¡æœ‰é‡åˆ°è¿‡,è¦ä¹ˆå°±å’Œä¹‹å‰çš„æŸä¸ªå…ƒç´ çš„é”®ç›¸åŒã€‚å¦‚æœè¿™æ˜¯ä¸€ä¸ªæ–°çš„å…ƒç´ ,combineByKey()ä¼šä½¿ç”¨ä¸€ä¸ªå«ä½œcreateCombiner()å‡½æ•°æ¥åˆ›å»ºé‚£ä¸ªé”®å¯¹åº”çš„ç´¯åŠ å™¨åˆå§‹å€¼.
> 
> 2.`mergeValue` : å¦‚æœè¿™æ˜¯ä¸€ä¸ªåœ¨å¤„ç†å½“å‰åˆ†åŒºä¹‹å‰å·²ç»é‡åˆ°çš„é”®,å®ƒä¼šä½¿ç”¨mergeValue()æ–¹æ³•å°†è¯¥é”®çš„ç´¯åŠ å™¨å¯¹åº”çš„å½“å‰å€¼ä¸è¿™ä¸ªæ–°çš„å€¼è¿›è¡Œåˆå¹¶.
> 
> 3.`mergeCombiners` : ç”±äºæ¯ä¸ªåˆ†åŒºéƒ½æ˜¯ç‹¬ç«‹å¤„ç†,å› æ­¤å¯¹äºåŒä¸€ä¸ªé”®å¯ä»¥æœ‰å¤šä¸ªç´¯åŠ å™¨,å¦‚æœæœ‰ä¸¤ä¸ªæˆ–è€…æ›´å¤šçš„åˆ†åŒºéƒ½æœ‰å¯¹åº”åŒä¸€ä¸ªé”®çš„ç´¯åŠ å™¨,å°±éœ€è¦ä½¿ç”¨ç”¨æˆ·æä¾›çš„mergeCombiners()æ–¹æ³•å°†å„ä¸ªåˆ†åŒºçš„ç»“æœè¿›è¡Œåˆå¹¶.
```
scala> rdd.combineByKey((_,1),(acc:(Int,Int),v)=>(acc._1+v,acc._2+1),(acc1:(Int,Int),acc2:(Int,Int))=>(acc1._1+acc2._1,acc1._2+acc2._2)).collect
res15: Array[(String, (Int, Int))] = Array((b,(3,1)), (a,(5,2)), (c,(18,3)))    
scala> 
```

###### 1.3.2.3.3.8 `sortByKey([ascending],[numTasks])` Method
- ä½œç”¨ : åœ¨ä¸€ä¸ª(K,V)çš„RDDä¸Šè°ƒç”¨,Kå¿…é¡»å®ç°Orderedæ¥å£,è¿”å›ä¸€ä¸ªæŒ‰ç…§keyè¿›è¡Œæ’åºçš„(K,V)çš„RDD
```
scala>  rdd.sortByKey().collect
res17: Array[(String, Int)] = Array((a,3), (a,2), (b,3), (c,6), (c,8), (c,4))

scala> rdd.sortByKey(false).collect
res19: Array[(String, Int)] = Array((c,4), (c,6), (c,8), (b,3), (a,3), (a,2))
scala> 
```

###### 1.3.2.3.3.9 `mapValues` Method
- é’ˆå¯¹äº(K,V)å½¢å¼çš„ç±»å‹åªå¯¹Vè¿›è¡Œæ“ä½œ
```
scala> rdd.mapValues(_*2).collect
res20: Array[(String, Int)] = Array((a,6), (a,4), (c,8), (b,6), (c,12), (c,16))
scala>
```

###### 1.3.2.3.3.10 `join(otherDataset,[numTasks])` Method
- ä½œç”¨ : åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ªç›¸åŒkeyå¯¹åº”çš„æ‰€æœ‰å…ƒç´ å¯¹åœ¨ä¸€èµ·çš„(K,(V,W))çš„RDD
```
scala> val rdd = sc.parallelize(Array((1,"a"),(2,"b"),(3,"c")))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[20] at parallelize at <console>:24

scala> val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))
rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[21] at parallelize at <console>:24

scala> rdd.join(rdd1).collect
res21: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) 

scala> rdd.leftOuterJoin(rdd1).collect
res22: Array[(Int, (String, Option[Int]))] = Array((1,(a,Some(4))), (2,(b,Some(5))), (3,(c,Some(6))))

scala> rdd.rightOuterJoin(rdd1).collect
res23: Array[(Int, (Option[String], Int))] = Array((1,(Some(a),4)), (2,(Some(b),5)), (3,(Some(c),6)))
scala> 
```

###### 1.3.2.3.3.11 `cogroup(otherDataset,[numTasks])` Method
- ä½œç”¨ : åœ¨ç±»å‹ä¸º(K,V)å’Œ(K,W)çš„RDDä¸Šè°ƒç”¨,è¿”å›ä¸€ä¸ª`(K,(Iterable<V>,Iterable<W>))`ç±»å‹çš„RDD
```
scala> rdd.cogroup(rdd1).collect
res24: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))

scala>
```

##### 1.3.2.4 Action
###### 1.3.2.4.1 `reduce(func)` Method
- ä½œç”¨ : é€šè¿‡funcå‡½æ•°èšé›†RDDä¸­çš„æ‰€æœ‰å…ƒç´ ,å…ˆèšåˆåˆ†åŒºå†…æ•°æ®,å†èšåˆåˆ†åŒºé—´æ•°æ®
```
scala> val rdd = sc.parallelize(1 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at <console>:24

scala> rdd.reduce(_+_)
res25: Int = 55
scala>
```

###### 1.3.2.4.2 `collect()` Method
- ä½œç”¨ : åœ¨é©±åŠ¨ç¨‹åºä¸­,ä»¥æ•°ç»„çš„å½¢å¼è¿”å›æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´ 
```
scala> rdd.collect
res26: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
scala>
```

###### 1.3.2.4.3 `count()` Method
- ä½œç”¨ : è¿”å›RDDä¸­å…ƒç´ çš„ä¸ªæ•°
```
scala> rdd.count
res27: Long = 10
scala> 
```

###### 1.3.2.4.4 `first()` Method
- ä½œç”¨ : è¿”å›RDDä¸­ç¬¬ä¸€ä¸ªå…ƒç´ 
```
scala> rdd.first
res28: Int = 1
scala> 
```

###### 1.3.2.4.5 `take(n)` Method
- ä½œç”¨ : è¿”å›ä¸€ä¸ªç”±RDDå‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„
```
scala> rdd.take(2)
res30: Array[Int] = Array(1, 2)
scala> 
```

###### 1.3.2.4.6 `takeOrdered(n)` Method
- ä½œç”¨ : è¿”å›è¯¥RDDæ’åºåçš„å‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„
```
scala> rdd.takeOrdered(3)
res31: Array[Int] = Array(1, 2, 3) 
scala> 
```

###### 1.3.2.4.7 `aggregate` Method
> å‚æ•° : `(zeroValue: U)(seqOp: (U, T) â‡’U, combOp: (U, U) â‡’U)`
> 
> ä½œç”¨ : aggregateå‡½æ•°å°†æ¯ä¸ªåˆ†åŒºé‡Œé¢çš„å…ƒç´ é€šè¿‡seqOpå’Œåˆå§‹å€¼è¿›è¡Œèšåˆ,ç„¶åç”¨combineå‡½æ•°å°†æ¯ä¸ªåˆ†åŒºçš„ç»“æœå’Œåˆå§‹å€¼(zeroValue)è¿›è¡Œcombineæ“ä½œ,è¿™ä¸ªå‡½æ•°æœ€ç»ˆè¿”å›çš„ç±»å‹ä¸éœ€è¦å’ŒRDDä¸­å…ƒç´ ç±»å‹ä¸€è‡´.
```
scala> rdd.aggregate(0)(_+_,_+_)
res32: Int = 55
scala>
```

###### 1.3.2.4.8 `fold(num)(func)` Method
- ä½œç”¨ : æŠ˜å æ“ä½œ,aggregateçš„ç®€åŒ–æ“ä½œ,seqopå’Œcombopä¸€æ ·
```
scala> rdd.fold(0)(_+_)
res34: Int = 55
scala> 
```

###### 1.3.2.4.9 `saveAsTextFile(path)` Method
- ä½œç”¨ : å°†æ•°æ®é›†å…ƒç´ ä»¥textfileçš„å½¢å¼ä¿å­˜åˆ°HDFSæ–‡ä»¶ç³»ç»Ÿæˆ–è€…å…¶ä»–æ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿ,å¯¹äºæ¯ä¸ªå…ƒç´ ,Sparkå°†ä¼šè°ƒç”¨toStringæ–¹æ³•,å°†å®ƒè£…æ¢ä¸ºæ–‡ä»¶ä¸­çš„æ–‡æœ¬.

###### 1.3.2.4.10 `saveAsSequenceFile(path)` Method
- ä½œç”¨ : å°†æ•°æ®é›†ä¸­çš„å…ƒç´ ä»¥Hadoop sequencefileæ ¼å¼ä¿å­˜åˆ°æŒ‡å®šç›®å½•ä¸‹,å¯ä»¥ä½¿HDFSæˆ–è€…å…¶ä»–Hadoopæ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿ.

###### 1.3.2.4.11 `saveAsObjectFile(path)` Method
- ä½œç”¨ : ç”¨äºå°†RDDä¸­å…ƒç´ åºåˆ—åŒ–æˆå¯¹è±¡,å­˜å‚¨åˆ°æ–‡ä»¶ä¸­.

###### 1.3.2.4.12 `countByKey()` Method
- ä½œç”¨ : é’ˆå¯¹(K,V)ç±»å‹RDD,è¿”å›ä¸€ä¸ª(K,Int)çš„map,è¡¨ç¤ºæ¯ä¸€ä¸ªkeyå¯¹åº”çš„å…ƒç´ ä¸ªæ•°.
```
scala> val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[35] at parallelize at <console>:24

scala> rdd.countByKey
res35: scala.collection.Map[Int,Long] = Map(3 -> 2, 1 -> 3, 2 -> 1)
scala> 
```

###### 1.3.2.4.13 `foreach(func)` Method
- ä½œç”¨ : åœ¨æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Š,è¿è¡Œå‡½æ•°funcè¿›è¡Œæ›´æ–°
```
scala> rdd.foreach(print)
```


##### 1.3.2.5 RDD å‡½æ•°ä¼ é€’
> åœ¨å®é™…å¼€å‘ä¸­å¾€å¾€éœ€è¦å¼€å‘è€…å®šä¹‰ä¸€äº›å¯¹äºRDDæ“ä½œ,é‚£ä¹ˆæ­¤æ—¶éœ€è¦ä¸»è¦çš„æ˜¯,åˆå§‹åŒ–å·¥ä½œæ˜¯åœ¨Driverç«¯è¿›è¡Œ,è€Œå®é™…è¿è¡Œç¨‹åºæ˜¯åœ¨Executorç«¯è¿›è¡Œ,è¿™å°±æ¶‰åŠåˆ°äº†è·¨è¿›ç¨‹é€šä¿¡,è·¨è¿›ç¨‹é€šä¿¡æ˜¯éœ€è¦åºåˆ—åŒ–æ“ä½œ.

###### 1.3.2.5.1 ä¼ é€’æ–¹æ³•
> åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æ‰€è°ƒç”¨çš„æ–¹æ³•`isMatch()`æ˜¯å®šä¹‰åœ¨Searchè¿™ä¸ªç±»ä¸­,å®é™…ä¸Šè°ƒç”¨çš„æ˜¯`this.isMatch()`,`this`è¡¨ç¤ºSearchè¿™ä¸ªç±»çš„å¯¹è±¡,ç¨‹åºåœ¨è¿è¡Œè¿‡ç¨‹ä¸­éœ€è¦å°†Searchå¯¹è±¡åºåˆ—åŒ–ä»¥åä¼ é€’åˆ°Executorç«¯.
###### 1.3.2.5.2 ä¼ é€’å±æ€§
> åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æ‰€è°ƒç”¨çš„æ–¹æ³•`query`æ˜¯å®šä¹‰åœ¨Searchè¿™ä¸ªç±»ä¸­çš„å­—æ®µ,å®é™…ä¸Šè°ƒç”¨çš„æ˜¯`this.query`,thisè¡¨ç¤ºSearchè¿™ä¸ªç±»çš„å¯¹è±¡,ç¨‹åºåœ¨è¿è¡Œè¿‡ç¨‹ä¸­éœ€è¦å°†Searchå¯¹è±¡åºåˆ—åŒ–ä»¥åä¼ é€’åˆ°Executorç«¯.

- Create `Search.scala`
``` scala
package com.geekparkhub.core.spark.application.methods
import org.apache.spark.rdd.RDD

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * Search
  * <p>
  */

class Search(query: String) extends Serializable {

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²æ•°æ®
  def isMatch(s: String): Boolean = {
    s.contains(query)
  }

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²RDD
  def getMatch1(rdd: RDD[String]): RDD[String] = {
    rdd.filter(isMatch)
  }

  // è¿‡æ»¤å‡ºåŒ…å«å­—ç¬¦ä¸²RDD
  def getMatche2(rdd: RDD[String]): RDD[String] = {
    rdd.filter(x => x.contains(query))
  }
}
```

- Create `TransFormAction.scala`
``` scala
package com.geekparkhub.core.spark.application.methods
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * TransFormAction
  * <p>
  */

object TransFormAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("TransFormAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val word: RDD[String] = sc.parallelize(Array("abc", "dcd"))

    // åˆ›å»ºSearchå¯¹è±¡
    val search = new Search("a")

    // è°ƒç”¨æ–¹æ³•
    val searched: RDD[String] = search.getMatch1(word)

    // å¾ªç¯è¾“å‡º
    searched.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

##### 1.3.2.6 RDD ä¾èµ–å…³ç³»
###### 1.3.2.6.1 Lineage
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_016.jpg)

> RDDåªæ”¯æŒç²—ç²’åº¦è½¬æ¢,å³åœ¨å¤§é‡è®°å½•ä¸Šæ‰§è¡Œçš„å•ä¸ªæ“ä½œ,å°†åˆ›å»ºRDDçš„ä¸€ç³»åˆ—Lineage(è¡€ç»Ÿ)è®°å½•ä¸‹æ¥,ä»¥ä¾¿æ¢å¤ä¸¢å¤±çš„åˆ†åŒº,RDDçš„Lineageä¼šè®°å½•RDDçš„å…ƒæ•°æ®ä¿¡æ¯å’Œè½¬æ¢è¡Œä¸º,å½“è¯¥RDDçš„éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶,å®ƒå¯ä»¥æ ¹æ®è¿™äº›ä¿¡æ¯æ¥é‡æ–°è¿ç®—å’Œæ¢å¤ä¸¢å¤±çš„æ•°æ®åˆ†åŒº.

- åˆ›å»ºRDDä¾èµ–å…³ç³»
```
scala> sc.textFile("/core_flow/spark/input/wordcount/wordcount_001.txt")
res0: org.apache.spark.rdd.RDD[String] = /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25

scala> res0.flatMap(_.split(" "))
res2: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:27

scala> res2.map((_,1))
res3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:29

scala> res3.reduceByKey(_+_)
res4: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:31

scala>
```
- åˆ†åˆ«æŸ¥çœ‹å››ä¸ªRDDä¾èµ–å…³ç³»
- res0.toDebugString
```
scala> res0.toDebugString
res5: String =
(2) /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res2.toDebugString
```
scala> res2.toDebugString
res6: String =
(2) MapPartitionsRDD[2] at flatMap at <console>:27 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res3.toDebugString
```
scala> res3.toDebugString
res7: String =
(2) MapPartitionsRDD[3] at map at <console>:29 []
 |  MapPartitionsRDD[2] at flatMap at <console>:27 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
 |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
- res4.toDebugString
```
scala> res4.toDebugString
res8: String =
(2) ShuffledRDD[4] at reduceByKey at <console>:31 []
 +-(2) MapPartitionsRDD[3] at map at <console>:29 []
    |  MapPartitionsRDD[2] at flatMap at <console>:27 []
    |  /core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[1] at textFile at <console>:25 []
    |  /core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[0] at textFile at <console>:25 []
scala> 
```
###### 1.3.2.6.2 çª„ä¾èµ–
- çª„ä¾èµ–æŒ‡çš„æ˜¯æ¯ä¸€ä¸ªçˆ¶RDDçš„Partitionæœ€å¤šè¢«å­RDDçš„ä¸€ä¸ªPartitionä½¿ç”¨.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_017.jpg)


###### 1.3.2.6.3 å®½ä¾èµ–
- å®½ä¾èµ–æŒ‡çš„æ˜¯å¤šä¸ªå­RDDçš„Partitionä¼šä¾èµ–åŒä¸€ä¸ªçˆ¶RDDçš„Partition,ä¼šå¼•èµ·shuffleè¿‡ç¨‹.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_018.jpg)

###### 1.3.2.6.4 DAG
- DAG(Directed Acyclic Graph)å«åšæœ‰å‘æ— ç¯å›¾,åŸå§‹çš„RDDé€šè¿‡ä¸€ç³»åˆ—çš„è½¬æ¢å°±å°±å½¢æˆäº†DAG,æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»çš„ä¸åŒå°†DAGåˆ’åˆ†æˆä¸åŒçš„Stage.
- å¯¹äºçª„ä¾èµ–,partitionçš„è½¬æ¢å¤„ç†åœ¨Stageä¸­å®Œæˆè®¡ç®—,å¯¹äºå®½ä¾èµ–,ç”±äºæœ‰Shuffleçš„å­˜åœ¨,åªèƒ½åœ¨parent RDDå¤„ç†å®Œæˆå,æ‰èƒ½å¼€å§‹æ¥ä¸‹æ¥çš„è®¡ç®—,å› æ­¤å®½ä¾èµ–æ˜¯åˆ’åˆ†Stageä¾æ®.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_019.jpg)


###### 1.3.2.6.5 ä»»åŠ¡åˆ’åˆ†(é‡ç‚¹)
- RDDä»»åŠ¡åˆ‡åˆ†ä¸­é—´åˆ†ä¸º : `Application` / `Job` / `Stage` /  `Task`
- Application : åˆå§‹åŒ–ä¸€ä¸ªSparkContextå³ç”Ÿæˆä¸€ä¸ªApplication.
- Job : ä¸€ä¸ªActionç®—å­å°±ä¼šç”Ÿæˆä¸€ä¸ªJob.
- Stage : æ ¹æ®RDDä¹‹é—´çš„ä¾èµ–å…³ç³»çš„ä¸åŒå°†Jobåˆ’åˆ†æˆä¸åŒçš„Stage,é‡åˆ°ä¸€ä¸ªå®½ä¾èµ–åˆ™åˆ’åˆ†ä¸€ä¸ªStage.
- Task : Stageæ˜¯ä¸€ä¸ªTaskSet,å°†Stageåˆ’åˆ†çš„ç»“æœå‘é€åˆ°ä¸åŒçš„Executoræ‰§è¡Œå³ä¸ºä¸€ä¸ªTask.
- Application`->`Job`->`Stage`->`Task æ¯ä¸€å±‚éƒ½æ˜¯1å¯¹nçš„å…³ç³»

##### 1.3.2.6.7 RDDç¼“å­˜
- RDDé€šè¿‡persistæ–¹æ³•æˆ–cacheæ–¹æ³•å¯ä»¥å°†å‰é¢çš„è®¡ç®—ç»“æœç¼“å­˜,é»˜è®¤æƒ…å†µä¸‹`persist()`ä¼šæŠŠæ•°æ®ä»¥åºåˆ—åŒ–å½¢å¼ç¼“å­˜åœ¨JVM çš„å †ç©ºé—´ä¸­.
- ä½†æ˜¯å¹¶ä¸æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ç«‹å³ç¼“å­˜,è€Œæ˜¯è§¦å‘åé¢çš„actionæ—¶,è¯¥RDDå°†ä¼šè¢«ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹çš„å†…å­˜ä¸­,å¹¶ä¾›åé¢é‡ç”¨
- ç¼“å­˜æœ‰å¯èƒ½ä¸¢å¤±æˆ–è€…å­˜å‚¨å­˜å‚¨äºå†…å­˜çš„æ•°æ®ç”±äºå†…å­˜ä¸è¶³è€Œè¢«åˆ é™¤,RDDçš„ç¼“å­˜å®¹é”™æœºåˆ¶ä¿è¯äº†å³ä½¿ç¼“å­˜ä¸¢å¤±ä¹Ÿèƒ½ä¿è¯è®¡ç®—çš„æ­£ç¡®æ‰§è¡Œ,é€šè¿‡åŸºäºRDDçš„ä¸€ç³»åˆ—è½¬æ¢,ä¸¢å¤±çš„æ•°æ®ä¼šè¢«é‡ç®—,ç”±äºRDDçš„å„ä¸ªPartitionæ˜¯ç›¸å¯¹ç‹¬ç«‹,å› æ­¤åªéœ€è¦è®¡ç®—ä¸¢å¤±çš„éƒ¨åˆ†å³å¯,å¹¶ä¸éœ€è¦é‡ç®—å…¨éƒ¨Partition.

##### 1.3.2.6.8 RDDCheckPoint
- Sparkä¸­å¯¹äºæ•°æ®çš„ä¿å­˜é™¤äº†æŒä¹…åŒ–æ“ä½œä¹‹å¤–,è¿˜æä¾›äº†ä¸€ç§æ£€æŸ¥ç‚¹çš„æœºåˆ¶,æ£€æŸ¥ç‚¹(æœ¬è´¨æ˜¯é€šè¿‡å°†RDDå†™å…¥Diskåšæ£€æŸ¥ç‚¹)æ˜¯ä¸ºäº†é€šè¿‡lineageåšå®¹é”™çš„è¾…åŠ©,lineageè¿‡é•¿ä¼šé€ æˆå®¹é”™æˆæœ¬è¿‡é«˜,è¿™æ ·å°±ä¸å¦‚åœ¨ä¸­é—´é˜¶æ®µåšæ£€æŸ¥ç‚¹å®¹é”™,å¦‚æœä¹‹åæœ‰èŠ‚ç‚¹å‡ºç°é—®é¢˜è€Œä¸¢å¤±åˆ†åŒº,ä»åšæ£€æŸ¥ç‚¹çš„RDDå¼€å§‹é‡åšLineage,å°±ä¼šå‡å°‘å¼€é”€,æ£€æŸ¥ç‚¹é€šè¿‡å°†æ•°æ®å†™å…¥åˆ°HDFSæ–‡ä»¶ç³»ç»Ÿå®ç°äº†RDDçš„æ£€æŸ¥ç‚¹åŠŸèƒ½.
- ä¸ºå½“å‰RDDè®¾ç½®æ£€æŸ¥ç‚¹,è¯¥å‡½æ•°å°†ä¼šåˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶çš„æ–‡ä»¶,å¹¶å­˜å‚¨åˆ°checkpointç›®å½•ä¸­,è¯¥ç›®å½•æ˜¯ç”¨SparkContext.setCheckpointDir()è®¾ç½®çš„,åœ¨checkpointçš„è¿‡ç¨‹ä¸­,è¯¥RDDæ‰€æœ‰ä¾èµ–äºçˆ¶RDDä¸­çš„ä¿¡æ¯å°†å…¨éƒ¨è¢«ç§»é™¤,å¯¹RDDè¿›è¡Œcheckpointæ“ä½œå¹¶ä¸ä¼šé©¬ä¸Šè¢«æ‰§è¡Œ,å¿…é¡»æ‰§è¡ŒActionæ“ä½œæ‰èƒ½è§¦å‘.
- è®¾ç½®æ£€æŸ¥ç‚¹
```
scala> sc.setCheckpointDir("hdfs://systemhub511:9000/core_flow/spark/checkpoint")
```
- åˆ›å»ºRDD
```
scala>  val rdd = sc.parallelize(Array("systemhub511"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[5] at parallelize at <console>:24
```
- å°†RDDè½¬æ¢ä¸ºæºå¸¦å½“å‰æ—¶é—´æˆ³å¹¶åšcheckpoint
```
scala> val check = rdd.map(_+System.currentTimeMillis)
check: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at <console>:26
scala> 
```
- å¤šæ¬¡æ‰“å°ç»“æœ
```
scala> check.collect
res10: Array[String] = Array(systemhub5111559138263898)

scala> check.collect
res11: Array[String] = Array(systemhub5111559138266443)

scala> check.collect
res12: Array[String] = Array(systemhub5111559138267862)

scala> 
```

#### 1.3.3 Key-Value RDD æ•°æ®åˆ†åŒº
- Sparkç›®å‰æ”¯æŒHashåˆ†åŒºå’ŒRangeåˆ†åŒº,å¼€å‘è€…ä¹Ÿå¯ä»¥è‡ªå®šä¹‰åˆ†åŒº,Hashåˆ†åŒºä¸ºå½“å‰é»˜è®¤åˆ†åŒº,Sparkä¸­åˆ†åŒºå™¨ç›´æ¥å†³å®šäº†RDDä¸­åˆ†åŒºçš„ä¸ªæ•°ã€RDDä¸­æ¯æ¡æ•°æ®ç»è¿‡Shuffleè¿‡ç¨‹å±äºå“ªä¸ªåˆ†åŒºå’ŒReduceçš„ä¸ªæ•°.

##### 1.3.3.1 è·å–RDD åˆ†åŒº
- æŸ¥çœ‹RDDåˆ†åŒºå™¨
```
scala> rdd.partitioner
res14: Option[org.apache.spark.Partitioner] = None
```
##### 1.3.3.2 Hash åˆ†åŒº
- HashPartitioneråˆ†åŒºçš„åŸç† : å¯¹äºç»™å®šçš„key,è®¡ç®—å…¶hashCode,å¹¶é™¤ä»¥åˆ†åŒºä¸ªæ•°å–ä½™,å¦‚æœä½™æ•°å°äº0,åˆ™ç”¨ä½™æ•°+åˆ†åŒºçš„ä¸ªæ•°,å¦åˆ™åŠ 0,æœ€åè¿”å›çš„å€¼å°±æ˜¯è¿™ä¸ªkeyæ‰€å±çš„åˆ†åŒºID.
- Hashåˆ†åŒºå®æ“
```
scala> val nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)
nopar: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:25

scala> nopar.mapPartitionsWithIndex((index,iter)=>{Iterator(index.toString+":"+iter.mkString("|"))}).collect
res15: Array[String] = Array(0:, 1:(1,3), 2:(1,2), 3:(2,4), 4:, 5:(2,3), 6:(3,6), 7:(3,8))

scala> val hashpar = nopar.partitionBy(new org.apache.spark.HashPartitioner(7))
hashpar: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[9] at partitionBy at <console>:27

scala> hashpar.count
res20: Long = 6

scala> hashpar.partitioner
res21: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@7)

scala> hashpar.mapPartitions(iter => Iterator(iter.length)).collect()
res22: Array[Int] = Array(0, 2, 2, 2, 0, 0, 0)
scala> 
```

##### 1.3.3.3 Ranger åˆ†åŒº
- HashPartitioneråˆ†åŒº`å¼Šç«¯` : å¯èƒ½å¯¼è‡´æ¯ä¸ªåˆ†åŒºä¸­æ•°æ®é‡ä¸å‡åŒ€,æç«¯æƒ…å†µä¸‹ä¼šå¯¼è‡´æŸäº›åˆ†åŒºæ‹¥æœ‰RDDå…¨éƒ¨æ•°æ®.
- RangePartitionerä½œç”¨ : å°†ä¸€å®šèŒƒå›´å†…æ•°æ˜ å°„åˆ°æŸä¸€ä¸ªåˆ†åŒºå†…,å°½é‡ä¿è¯æ¯ä¸ªåˆ†åŒºä¸­æ•°æ®é‡å‡åŒ€,è€Œä¸”åˆ†åŒºä¸åˆ†åŒºä¹‹é—´æ˜¯æœ‰åº,ä¸€ä¸ªåˆ†åŒºä¸­å…ƒç´ è‚¯å®šéƒ½æ˜¯æ¯”å¦ä¸€ä¸ªåˆ†åŒºå†…å…ƒç´ å°æˆ–è€…å¤§,ä½†æ˜¯åˆ†åŒºå†…å…ƒç´ æ˜¯ä¸èƒ½ä¿è¯é¡ºåº,ç®€å•çš„è¯´å°±æ˜¯å°†ä¸€å®šèŒƒå›´å†…çš„æ•°æ˜ å°„åˆ°æŸä¸€ä¸ªåˆ†åŒºå†….
- å®ç°è¿‡ç¨‹ : 
- 1.å…ˆä»æ•´ä¸ªRDDä¸­æŠ½å–å‡ºæ ·æœ¬æ•°æ®,å°†æ ·æœ¬æ•°æ®æ’åº,è®¡ç®—å‡ºæ¯ä¸ªåˆ†åŒºæœ€å¤§keyå€¼,å½¢æˆä¸€ä¸ª`Array[KEY]`ç±»å‹çš„æ•°ç»„å˜é‡`rangeBounds`.
- 2.åˆ¤æ–­keyåœ¨`rangeBounds`ä¸­æ‰€å¤„çš„èŒƒå›´,ç»™å‡ºè¯¥keyå€¼åœ¨ä¸‹ä¸€ä¸ªRDDä¸­åˆ†åŒºidä¸‹æ ‡,è¯¥åˆ†åŒºå™¨è¦æ±‚RDDä¸­KEYç±»å‹å¿…é¡»æ˜¯å¯æ’åº.


##### 1.3.3.4 è‡ªå®šä¹‰ åˆ†åŒº
- è¦å®ç°è‡ªå®šä¹‰åˆ†åŒºå™¨,éœ€è¦ç»§æ‰¿`org.apache.spark.Partitioner`ç±»å¹¶å®ç°ä¸‹é¢ä¸‰ä¸ªæ–¹æ³•
- 1.`numPartitions: Int` : è¿”å›åˆ›å»ºå‡ºæ¥çš„åˆ†åŒºæ•°
- 2.`getPartition(key: Any): Int` :  è¿”å›ç»™å®šé”®çš„åˆ†åŒºç¼–å·(0åˆ°numPartitions-1)
- 3.`equals()` : Java åˆ¤æ–­ç›¸ç­‰æ€§çš„æ ‡å‡†æ–¹æ³•,è¿™ä¸ªæ–¹æ³•çš„å®ç°éå¸¸é‡è¦,Sparkéœ€è¦ç”¨è¿™ä¸ªæ–¹æ³•æ¥æ£€æŸ¥åˆ†åŒºå™¨å¯¹è±¡æ˜¯å¦å’Œå…¶ä»–åˆ†åŒºå™¨å®ä¾‹ç›¸åŒ,è¿™æ ·Sparkæ‰å¯ä»¥åˆ¤æ–­ä¸¤ä¸ªRDDçš„åˆ†åŒºæ–¹å¼æ˜¯å¦ç›¸åŒ.
- 4.å®šä¹‰è‡ªå®šä¹‰åˆ†åŒºç±» | Create `CustomerPartitioner.scala`
``` scala
package com.geekparkhub.core.spark.application.partitioner

import org.apache.spark.Partitioner

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * CustomerPartitioner
  * <p>
  */

class CustomerPartitioner(partitions: Int) extends Partitioner {
  override def numPartitions: Int = partitions
  override def getPartition(key: Any): Int = {
    0
  }
}
```
- Create `PartitionerAction.scala`
```
package com.geekparkhub.core.spark.application.partitioner

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * PartitionerAction
  * <p>
  */

object PartitionerAction {

  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("TransFormAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val word: RDD[String] = sc.parallelize(Array("abc", "dcd"))

    // å°†å…ƒç´ è½¬æ¢ä¸ºå…ƒç¥–
    val wordAndOne: RDD[(String, Int)] = word.map((_, 1))

    // è‡ªå®šä¹‰åˆ†åŒº
    val partitioned: RDD[(String, Int)] = wordAndOne.partitionBy(new CustomerPartitioner(2))

    // æŸ¥çœ‹åˆ†åŒºååˆ†åŒºç»“æœ
    val indexAndData: RDD[(Int, (String, Int))] = partitioned.mapPartitionsWithIndex((i,t)=>t.map((i,_)))

    // æ‰“å°æ•°æ®
    indexAndData.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
- Log Println
```
(0,(abc,1))
(0,(dcd,1))
```

#### 1.3.4 æ•°æ®è¯»å–&ä¿å­˜
- Sparkæ•°æ®è¯»å–åŠæ•°æ®ä¿å­˜å¯ä»¥ä»ä¸¤ä¸ªç»´åº¦æ¥ä½œåŒºåˆ† : æ–‡ä»¶æ ¼å¼ä»¥åŠæ–‡ä»¶ç³»ç»Ÿ
- æ–‡ä»¶æ ¼å¼åˆ†ä¸º : Textæ–‡ä»¶ / Jsonæ–‡ä»¶ / Csvæ–‡ä»¶ / Sequenceæ–‡ä»¶ä»¥åŠObjectæ–‡ä»¶
- æ–‡ä»¶ç³»ç»Ÿåˆ†ä¸º : æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ / HDFS / HBASEä»¥åŠæ•°æ®åº“

##### 1.3.4.1 æ–‡ä»¶ç±»æ•°æ®è¯»å–&ä¿å­˜
###### 1.3.4.1 Text File
- 1.æ•°æ®è¯»å– : `textFile(String)` 
```
scala> sc.textFile("hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt")
res23: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[12] at textFile at <console>:26

scala> res23.toDebugString
res25: String =
(2) hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[12] at textFile at <console>:26 []
 |  hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt HadoopRDD[11] at textFile at <console>:26 []
scala> 
```
- 2.æ•°æ®ä¿å­˜ : `saveAsTextFile(String)`
```
scala> hdfsFile.saveAsTextFile("/core_flow/spark/output/wordcount/")
```

###### 1.3.4.2 Json File
- å¦‚æœJSONæ–‡ä»¶ä¸­æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªJSONè®°å½•,é‚£ä¹ˆå¯ä»¥é€šè¿‡å°†JSONæ–‡ä»¶å½“åšæ–‡æœ¬æ–‡ä»¶æ¥è¯»å–,ç„¶ååˆ©ç”¨ç›¸å…³çš„JSONåº“å¯¹æ¯ä¸€æ¡æ•°æ®è¿›è¡ŒJSONè§£æ.
- ä½¿ç”¨RDDè¯»å–JSONæ–‡ä»¶å¤„ç†å¾ˆå¤æ‚,åŒæ—¶SparkSQLé›†æˆäº†å¾ˆå¥½çš„å¤„ç†JSONæ–‡ä»¶æ–¹å¼,æ‰€ä»¥åº”ç”¨ä¸­å¤šæ˜¯é‡‡ç”¨SparkSQLå¤„ç†JSONæ–‡ä»¶.
- 1.å¯¼å…¥è§£æjsonæ‰€éœ€åŒ…å
```
scala> import scala.util.parsing.json.JSON
import scala.util.parsing.json.JSON
scala> 
```
- 2.åœ¨HDFSåˆ›å»ºå­˜æ”¾JSONç›®å½•
```
[root@systemhub511 ~]# hadoop fs -mkdir -p /core_flow/spark/json/001
```
- 3.ä¸Šä¼ jsonæ–‡ä»¶åˆ°HDFS
```
[root@systemhub511 ~]# hadoop fs -put /opt/module/spark/examples/src/main/resources/people.json /core_flow/spark/json/001/
```
- 4.è¯»å–æ–‡ä»¶
```
scala> val json = sc.textFile("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
json: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/json/001/people.json MapPartitionsRDD[14] at textFile at <console>:26
scala>
```
- 5.è§£æjsonæ•°æ®
```
scala> val result = json.map(JSON.parseFull)
result: org.apache.spark.rdd.RDD[Option[Any]] = MapPartitionsRDD[15] at map at <console>:28
scala>
```
- 6.æ‰“å°è§£æç»“æœ
```
scala> result.collect
res26: Array[Option[Any]] = Array(Some(Map(name -> Michael)), Some(Map(name -> Andy, age -> 30.0)), Some(Map(name -> Justin, age -> 19.0)))
scala> 
```

###### 1.3.4.3 Sequence File
- SequenceFileæ–‡ä»¶æ˜¯Hadoopç”¨æ¥å­˜å‚¨äºŒè¿›åˆ¶å½¢å¼çš„key-valueå¯¹è€Œè®¾è®¡ä¸€ç§å¹³é¢æ–‡ä»¶(FlatFile).
- Sparkæœ‰ä¸“é—¨ç”¨æ¥è¯»å–SequenceFileæ¥å£,åœ¨SparkContextä¸­,å¯ä»¥è°ƒç”¨`sequenceFile[keyClass, valueClass](path)` | SequenceFileæ–‡ä»¶åªé’ˆå¯¹`PairRDD`
- 1.åˆ›å»ºRDD
```
scala>  val rdd = sc.parallelize(Array((1,2),(3,4),(5,6)))
rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[16] at parallelize at <console>:26
scala> 
```
- 2.å°†RDDä¿å­˜ä¸ºSequenceæ–‡ä»¶
```
scala> rdd.saveAsSequenceFile("file:///opt/module/spark/seqFile")
```
- 3.æŸ¥çœ‹è¯¥æ–‡ä»¶
```
[root@systemhub511 ~]# cd /opt/module/spark/seqFile/
[root@systemhub511 seqFile]# ll -a
æ€»ç”¨é‡ 28
drwxr-xr-x.  2 root          root          4096 5æœˆ  29 23:57 .
drwxr-xr-x. 21 geekdeveloper geekdeveloper 4096 5æœˆ  30 00:05 ..
-rw-r--r--.  1 root          root            92 5æœˆ  29 23:57 part-00000
-rw-r--r--.  1 root          root            12 5æœˆ  29 23:57 .part-00000.crc
-rw-r--r--.  1 root          root           108 5æœˆ  29 23:57 part-00003
-rw-r--r--.  1 root          root            12 5æœˆ  29 23:57 .part-00003.crc
-rw-r--r--.  1 root          root             0 5æœˆ  29 23:57 _SUCCESS
-rw-r--r--.  1 root          root             8 5æœˆ  29 23:57 ._SUCCESS.crc
[root@systemhub511 seqFile]# cat part-00000
SEQ org.apache.hadoop.io.IntWritable org.apache.hadoop.io.IntWritabler[-oï¿½ï¿½ï¿½]hï¿½~uï¿½ï¿½ï¿½
[root@systemhub511 seqFile]#
```
- 4.è¯»å–Sequenceæ–‡ä»¶
```
scala>  val seq = sc.sequenceFile[Int,Int]("file:///opt/module/spark/seqFile")
seq: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at sequenceFile at <console>:26
scala>
```
- 5.æ‰“å°è¯»å–åçš„Sequenceæ–‡ä»¶
```
scala> seq.collect
res14: Array[(Int, Int)] = Array((1,2), (3,4), (5,6))
```

###### 1.3.4.4 ObjectFile
- å¯¹è±¡æ–‡ä»¶æ˜¯å°†å¯¹è±¡åºåˆ—åŒ–åä¿å­˜æ–‡ä»¶,é‡‡ç”¨Javaåºåˆ—åŒ–æœºåˆ¶,å¯ä»¥é€šè¿‡`objectFile[k,v](path)`å‡½æ•°æ¥æ”¶ä¸€ä¸ªè·¯å¾„,è¯»å–å¯¹è±¡æ–‡ä»¶,è¿”å›å¯¹åº”RDD,ä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨`saveAsObjectFile()`å®ç°å¯¹å¯¹è±¡æ–‡ä»¶è¾“å‡º,å› ä¸ºæ˜¯åºåˆ—åŒ–æ‰€ä»¥è¦æŒ‡å®šç±»å‹.
- 1.åˆ›å»ºRDD
```
scala> val rdd = sc.parallelize(Array(1,2,3,4))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at <console>:26
scala> 
```
- 2.å°†RDDä¿å­˜ä¸ºObjectæ–‡ä»¶
```
scala> rdd.saveAsObjectFile("file:///opt/module/spark/objectFile")
```
- 3.æŸ¥çœ‹è¯¥æ–‡ä»¶
```
[root@systemhub511 ~]# cd /opt/module/spark/objectFile/
[root@systemhub511 objectFile]# ll
æ€»ç”¨é‡ 8
-rw-r--r--. 1 root root 138 5æœˆ  30 00:05 part-00000
-rw-r--r--. 1 root root 138 5æœˆ  30 00:05 part-00003
-rw-r--r--. 1 root root   0 5æœˆ  30 00:05 _SUCCESS
[root@systemhub511 objectFile]# cat part-00000
SEQ!org.apache.hadoop.io.NullWritable"org.apache.hadoop.io.BytesWritableï¿½ ï¿½Lï¿½hï¿½l:Tï¿½ï¿½ï¿½#ï¿½ï¿½ur[IMï¿½`&vê²¥xp
[root@systemhub511 objectFile]#
```
- 4.è¯»å–Objectæ–‡ä»¶
```
scala> val objFile = sc.objectFile[(Int)]("file:///opt/module/spark/objectFile")
objFile: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[24] at objectFile at <console>:26
scala>
```
- 5.æ‰“å°è¯»å–åçš„Sequenceæ–‡ä»¶
```
objFile.collect
res19: Array[Int] = Array(1, 2, 3, 4)
```

##### 1.3.4.2 æ–‡ä»¶ç³»ç»Ÿæ•°æ®è¯»å–&ä¿å­˜

###### 1.3.4.1 HDFS
> Sparkæ•´ä¸ªç”Ÿæ€ç³»ç»Ÿä¸Hadoopæ˜¯å®Œå…¨å…¼å®¹,æ‰€ä»¥å¯¹äºHadoopæ‰€æ”¯æŒçš„æ–‡ä»¶ç±»å‹æˆ–è€…æ•°æ®åº“ç±»å‹,Sparkä¹ŸåŒæ ·æ”¯æŒ.
> å¦å¤–ç”±äºHadoopçš„APIæœ‰æ–°æ—§ä¸¤ä¸ªç‰ˆæœ¬,æ‰€ä»¥Sparkä¸ºäº†èƒ½å¤Ÿå…¼å®¹Hadoopæ‰€æœ‰ç‰ˆæœ¬,ä¹Ÿæä¾›äº†ä¸¤å¥—åˆ›å»ºæ“ä½œæ¥å£.
> å¯¹äºå¤–éƒ¨å­˜å‚¨åˆ›å»ºæ“ä½œè€Œè¨€,hadoopRDDå’ŒnewHadoopRDDæ˜¯æœ€ä¸ºæŠ½è±¡çš„ä¸¤ä¸ªå‡½æ•°æ¥å£,ä¸»è¦åŒ…å«ä»¥ä¸‹å››ä¸ªå‚æ•°  : 
> 
> 1.`è¾“å…¥æ ¼å¼(InputFormat)` : åˆ¶å®šæ•°æ®è¾“å…¥ç±»å‹,å¦‚TextInputFormatç­‰,æ–°æ—§ä¸¤ä¸ªç‰ˆæœ¬æ‰€å¼•ç”¨ç‰ˆæœ¬åˆ†åˆ«æ˜¯`org.apache.hadoop.mapred.InputFormat`å’Œ`org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)`
> 2.é”®ç±»å‹ : æŒ‡å®š[K,V]é”®å€¼å¯¹ä¸­Kç±»å‹
> 3.å€¼ç±»å‹: æŒ‡å®š[K,V]é”®å€¼å¯¹ä¸­Vç±»å‹
> 4.åˆ†åŒºå€¼ : æŒ‡å®šç”±å¤–éƒ¨å­˜å‚¨ç”ŸæˆRDDçš„partitionæ•°é‡æœ€å°å€¼,å¦‚æœæ²¡æœ‰æŒ‡å®šç³»ç»Ÿä¼šä½¿ç”¨é»˜è®¤å€¼`defaultMinSplits`.
> 
> å…¶ä»–åˆ›å»ºæ“ä½œAPIæ¥å£éƒ½æ˜¯ä¸ºäº†æ–¹ä¾¿æœ€ç»ˆSparkç¨‹åºå¼€å‘è€…è€Œè®¾ç½®çš„,æ˜¯è¿™ä¸¤ä¸ªæ¥å£é«˜æ•ˆå®ç°ç‰ˆæœ¬,ä¾‹å¦‚å¯¹äºtextFileè€Œè¨€,åªæœ‰pathè¿™ä¸ªæŒ‡å®šæ–‡ä»¶è·¯å¾„å‚æ•°,å…¶ä»–å‚æ•°åœ¨ç³»ç»Ÿå†…éƒ¨æŒ‡å®šäº†é»˜è®¤å€¼.
> 1.åœ¨Hadoopä¸­ä»¥å‹ç¼©å½¢å¼å­˜å‚¨æ•°æ®,ä¸éœ€è¦æŒ‡å®šè§£å‹æ–¹å¼å°±èƒ½å¤Ÿè¿›è¡Œè¯»å–,å› ä¸ºHadoopæœ¬èº«æœ‰ä¸€ä¸ªè§£å‹å™¨ä¼šæ ¹æ®å‹ç¼©æ–‡ä»¶åç¼€æ¨æ–­è§£å‹ç®—æ³•è¿›è¡Œè§£å‹.
> 2.å¦‚æœç”¨Sparkä»Hadoopä¸­è¯»å–æŸç§ç±»å‹æ•°æ®ä¸çŸ¥é“æ€ä¹ˆè¯»å–çš„æ—¶å€™,ä¸Šç½‘æŸ¥æ‰¾ä¸€ä¸ªä½¿ç”¨map-reduceæ—¶å€™æ˜¯æ€ä¹ˆè¯»å–è¿™ç§è¿™ç§æ•°æ®,ç„¶åå†å°†å¯¹åº”çš„è¯»å–æ–¹å¼æ”¹å†™æˆä¸Šé¢çš„hadoopRDDå’ŒnewAPIHadoopRDDä¸¤ä¸ªç±»å³å¯.


###### 1.3.4.2 MySQLæ•°æ®åº“ è¿æ¥
- æ”¯æŒé€šè¿‡JavaJDBCè®¿é—®å…³ç³»å‹æ•°æ®åº“,éœ€è¦é€šè¿‡JdbcRDDè¿›è¡Œ
- 0.æ·»åŠ mysqlä¾èµ–
``` xml
<dependencies>
 <dependency>
  <groupId>mysql</groupId>
  <artifactId>mysql-connector-java</artifactId>
  <version>8.0.15</version>
 </dependency>
</dependencies>
```
- 1.Mysqlè¯»å– | Create `JDBCConnection.scala`
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import java.sql.DriverManager

import org.apache.spark.deploy.worker.DriverWrapper
import org.apache.spark.rdd.JdbcRDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * JDBCConnection
  * <p>
  */

object JDBCConnection {

  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("JDBCConnection")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // å®šä¹‰JDBCè¿æ¥å±æ€§ä¿¡æ¯
    val driver = "com.mysql.jdbc.Driver"
    val url = "jdbc:mysql://systemhub711:3306/company"
    val userName = "root"
    val passWd = "ax04854"

    // åˆ›å»ºJDBC RDD
    val JdbcRDD = new JdbcRDD[(Int, String)](sc, () => {
      Class.forName(driver)
      DriverManager.getConnection(url, userName, passWd)
    }, "select id,name from staff where ? <= id and id <= ?",
      1,
      10,
      1,
      x => {
        (x.getInt(1), x.getString(2))
      }
    )

    // æ‰“å°JdbcRDDç»“æœ
    JdbcRDD.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
- 2.Mysqlå†™å…¥ | Create `JBDCinsertData.scala`
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * JBDCinsertData
  * <p>
  */

object JBDCinsertData {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("JBDCRead")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºæ•°æ®
    val data = sc.parallelize(List("Female", "Male", "Female"))

    // è°ƒç”¨æ·»åŠ æ•°æ®æ–¹æ³•
    data.foreachPartition(insertData)
  }

  // æ·»åŠ æ•°æ®æ–¹æ³•
  def insertData(iterator: Iterator[String]): Unit = {
    Class.forName("com.mysql.jdbc.Driver").newInstance()
    val conn = java.sql.DriverManager.getConnection("jdbc:mysql://systemhub711:3306/company", "root", "000000")
    iterator.foreach(data => {
      val ps = conn.prepareStatement("insert into staff(name) values(?)")
      ps.setString(1, data)
      ps.executeUpdate()
    })
  }
}
```


###### 1.3.4.3 HBase æ•°æ®åº“
- ç”±äº`org.apache.hadoop.hbase.mapreduce.TableInputFormat`ç±»çš„å®ç°,Sparkå¯ä»¥é€šè¿‡Hadoopè¾“å…¥æ ¼å¼è®¿é—®HBase,è¿™ä¸ªè¾“å…¥æ ¼å¼ä¼šè¿”å›é”®å€¼å¯¹æ•°æ®,å…¶ä¸­é”®çš„ç±»å‹ä¸º`org. apache.hadoop.hbase.io.ImmutableBytesWritable`,è€Œå€¼çš„ç±»å‹ä¸º`org.apache.hadoop.hbase.client.Result`.
- 0.æ·»åŠ HBASEä¾èµ–
```xml
<dependency>
  <groupId>org.apache.hbase</groupId>
  <artifactId>hbase-server</artifactId>
  <version>1.3.1</version>
</dependency>
<dependency>
  <groupId>org.apache.hbase</groupId>
  <artifactId>hbase-client</artifactId>
  <version>1.3.1</version>
</dependency>
```
- 1.HBaseè¯»å–æ•°æ®
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.{NewHadoopRDD, RDD}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * HbaseConnection
  * <p>
  */

object HbaseConnection {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("HbaseConnection")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    //æ„å»ºHBaseé…ç½®ä¿¡æ¯
    val conf: Configuration = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", "systemhub511,systemhub611,systemhub711")
    conf.set(TableInputFormat.INPUT_TABLE, "test")

    // è¯»å–HBASEæ•°æ®
    val hbaseRDD = new NewHadoopRDD(sc, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result], conf)

    // è·å–RowKey
    val value: RDD[String] = hbaseRDD.map(x => Bytes.toString(x._2.getRow))

    // è¾“å‡ºæ•°æ®
    value.collect().foreach(println)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

- 2.HBaseå†™å…¥æ•°æ®
``` scala
package com.geekparkhub.core.spark.application.dataconnections

import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * HbaseWrite
  * <p>
  */

object HbaseWrite {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("HbaseWrite")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // åˆ›å»ºRDD
    val initialRDD: RDD[(Int, String, Int)] = sc.parallelize(List((1, "apple", 11), (2, "banana", 12), (3, "pear", 13)))

    // åˆ›å»ºJobConf
    val conf = new JobConf()
    conf.set("hbase.zookeeper.quorum", "systemhub511,systemhub611,systemhub711")
    conf.setOutputFormat(classOf[TableOutputFormat[ImmutableBytesWritable]])
    conf.set(TableOutputFormat.OUTPUT_TABLE, "test")

    // å®šä¹‰ Hbase æ·»åŠ æ•°æ®æ–¹æ³•
    def convert(triple: (Int, String, Int)): (ImmutableBytesWritable, Put) = {
      val put = new Put(Bytes.toBytes(triple._1))
      put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(triple._2))
      put.addImmutable(Bytes.toBytes("info"), Bytes.toBytes("price"), Bytes.toBytes(triple._3))(new ImmutableBytesWritable, put)
    }

    // è½¬æ¢RDD
    val writRDD: RDD[(ImmutableBytesWritable, Put)] = initialRDD.map(convert)

    // å†™å…¥HBASE
    writRDD.saveAsHadoopDataset(conf)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```

#### 1.3.5 RDD ç¼–ç¨‹è¿›é˜¶
##### 1.3.5.1 ç´¯åŠ å™¨
> ç´¯åŠ å™¨ç”¨æ¥å¯¹ä¿¡æ¯è¿›è¡Œèšåˆ,é€šå¸¸åœ¨å‘Sparkä¼ é€’å‡½æ•°æ—¶,æ¯”å¦‚ä½¿ç”¨`map()`å‡½æ•°æˆ–è€…ç”¨`filter()`ä¼ æ¡ä»¶æ—¶,å¯ä»¥ä½¿ç”¨é©±åŠ¨å™¨ç¨‹åºä¸­å®šä¹‰å˜é‡,ä½†æ˜¯é›†ç¾¤ä¸­è¿è¡Œæ¯ä¸ªä»»åŠ¡éƒ½ä¼šå¾—åˆ°è¿™äº›å˜é‡çš„ä¸€ä»½æ–°å‰¯æœ¬,æ›´æ–°è¿™äº›å‰¯æœ¬çš„å€¼ä¹Ÿä¸ä¼šå½±å“é©±åŠ¨å™¨ä¸­çš„å¯¹åº”å˜é‡,å¦‚æœæƒ³å®ç°æ‰€æœ‰åˆ†ç‰‡å¤„ç†æ—¶æ›´æ–°å…±äº«å˜é‡çš„åŠŸèƒ½,é‚£ä¹ˆç´¯åŠ å™¨å¯ä»¥å®ç°æƒ³è¦çš„æ•ˆæœ.

###### 1.3.5.1.1 ç³»ç»Ÿç´¯åŠ å™¨
> é€šè¿‡åœ¨é©±åŠ¨å™¨ä¸­è°ƒç”¨S`parkContext.accumulator(initialValue`)æ–¹æ³•,åˆ›å»ºå‡ºå­˜æœ‰åˆå§‹å€¼çš„ç´¯åŠ å™¨,è¿”å›å€¼ä¸º`org.apache.spark.Accumulator[T]`å¯¹è±¡,å…¶ä¸­Tæ˜¯åˆå§‹å€¼initialValueçš„ç±»å‹,Sparké—­åŒ…é‡Œçš„æ‰§è¡Œå™¨ä»£ç å¯ä»¥ä½¿ç”¨ç´¯åŠ å™¨çš„`+=`æ–¹æ³•(åœ¨Javaä¸­æ˜¯add)å¢åŠ ç´¯åŠ å™¨çš„å€¼,é©±åŠ¨å™¨ç¨‹åºå¯ä»¥è°ƒç”¨ç´¯åŠ å™¨çš„valueå±æ€§(åœ¨Javaä¸­ä½¿ç”¨value()æˆ–setValue())æ¥è®¿é—®ç´¯åŠ å™¨çš„å€¼.
> 
> å·¥ä½œèŠ‚ç‚¹ä¸Šä»»åŠ¡ä¸èƒ½è®¿é—®ç´¯åŠ å™¨å€¼,ä»è¿™äº›ä»»åŠ¡çš„è§’åº¦æ¥çœ‹,ç´¯åŠ å™¨æ˜¯ä¸€ä¸ªåªå†™å˜é‡.
> 
> å¯¹äºè¦åœ¨è¡ŒåŠ¨æ“ä½œä¸­ä½¿ç”¨ç´¯åŠ å™¨,Sparkåªä¼šæŠŠæ¯ä¸ªä»»åŠ¡å¯¹å„ç´¯åŠ å™¨çš„ä¿®æ”¹åº”ç”¨ä¸€æ¬¡,å› æ­¤,å¦‚æœæƒ³è¦ä¸€ä¸ªæ— è®ºåœ¨å¤±è´¥è¿˜æ˜¯é‡å¤è®¡ç®—æ—¶éƒ½ç»å¯¹å¯é çš„ç´¯åŠ å™¨,å¿…é¡»æŠŠå®ƒæ”¾åœ¨foreach()è¿™æ ·çš„è¡ŒåŠ¨æ“ä½œä¸­,è½¬åŒ–æ“ä½œä¸­ç´¯åŠ å™¨å¯èƒ½ä¼šå‘ç”Ÿä¸æ­¢ä¸€æ¬¡æ›´æ–°.
``` scala
package com.geekparkhub.core.spark.application.methods

import org.apache.spark.rdd.RDD
import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AccuAction
  * <p>
  */

object AccuAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSpakConf
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("AccuAction")

    // åˆ›å»ºSC
    val sc = new SparkContext(sparkConf)

    // ç´¯åŠ å™¨
    val sum: LongAccumulator = sc.longAccumulator("sum")

    // åˆ›å»ºRDD
    val value: RDD[Int] = sc.parallelize(Array(1, 2, 3, 4))

    val word: RDD[(Int, Int)] = value.map(x => {
      // æ·»åŠ ç´¯åŠ 
      sum.add(x)
      (x, 1)
    })

    word.collect().foreach(println)

    println(sum.value)

    // å…³é—­èµ„æº
    sc.stop()
  }
}
```
###### 1.3.5.1.2 è‡ªå®šä¹‰ç´¯åŠ å™¨
> è‡ªå®šä¹‰ç´¯åŠ å™¨ç±»å‹åŠŸèƒ½åœ¨1.Xç‰ˆæœ¬ä¸­å°±å·²ç»æä¾›,ä½†æ˜¯ä½¿ç”¨èµ·æ¥æ¯”è¾ƒéº»çƒ¦,åœ¨2.0ç‰ˆæœ¬å,ç´¯åŠ å™¨çš„æ˜“ç”¨æ€§æœ‰äº†è¾ƒå¤§æ”¹è¿›,è€Œä¸”å®˜æ–¹è¿˜æä¾›äº†ä¸€ä¸ªæ–°æŠ½è±¡ç±» : `AccumulatorV2`æ¥æä¾›æ›´åŠ å‹å¥½è‡ªå®šä¹‰ç±»å‹ç´¯åŠ å™¨çš„å®ç°æ–¹å¼,å®ç°è‡ªå®šä¹‰ç±»å‹ç´¯åŠ å™¨éœ€è¦ç»§æ‰¿`AccumulatorV2`å¹¶è‡³å°‘è¦†å†™ä¸‹ä¾‹ä¸­å‡ºç°çš„æ–¹æ³•,
``` scala
package com.geekparkhub.core.spark.application.methods

import org.apache.spark.util.AccumulatorV2

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AccumulatorAction
  * <p>
  */

class AccumulatorAction extends AccumulatorV2[Int,Int]{

  var sum  = 0

  // åˆ¤æ–­æ˜¯å¦ä¸ºç©º
  override def isZero: Boolean = sum == 0

  // å¤åˆ¶æ–¹æ³•
  override def copy(): AccumulatorV2[Int, Int] = {
    val accumulatorAction = new AccumulatorAction
    accumulatorAction.sum = this.sum
    accumulatorAction
  }

  // é‡ç½®æ–¹æ³•
  override def reset(): Unit = 0

  // ç´¯åŠ æ–¹æ³•
  override def add(v: Int): Unit = sum += v

  // åˆå¹¶æ–¹æ³•
  override def merge(other: AccumulatorV2[Int, Int]): Unit = sum += other.value

  // è¿”å›å€¼
  override def value: Int = sum
}
```

##### 1.3.5.2 å¹¿æ’­å˜é‡ (è°ƒä¼˜ç­–ç•¥)
> å¹¿æ’­å˜é‡ç”¨æ¥é«˜æ•ˆåˆ†å‘è¾ƒå¤§å¯¹è±¡,å‘æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»å€¼,ä»¥ä¾›ä¸€ä¸ªæˆ–å¤šä¸ªSparkæ“ä½œä½¿ç”¨.
> 
> æ¯”å¦‚,å¦‚æœåº”ç”¨éœ€è¦å‘æ‰€æœ‰èŠ‚ç‚¹å‘é€ä¸€ä¸ªè¾ƒå¤§çš„åªè¯»æŸ¥è¯¢è¡¨,ç”šè‡³æ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ä¸­çš„ä¸€ä¸ªå¾ˆå¤§çš„ç‰¹å¾å‘é‡,å¹¿æ’­å˜é‡ç”¨èµ·æ¥éƒ½å¾ˆé¡ºæ‰‹,åœ¨å¤šä¸ªå¹¶è¡Œæ“ä½œä¸­ä½¿ç”¨åŒä¸€ä¸ªå˜é‡,ä½†æ˜¯Sparkä¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ†åˆ«å‘é€.
> 
> ä½¿ç”¨å¹¿æ’­å˜é‡è¿‡ç¨‹ : 
> 1.é€šè¿‡å¯¹ä¸€ä¸ªç±»å‹Tçš„å¯¹è±¡è°ƒç”¨`SparkContext.broadcast`åˆ›å»ºå‡º`Broadcast[T]`å¯¹è±¡,ä»»ä½•å¯åºåˆ—åŒ–ç±»å‹éƒ½å¯ä»¥è¿™ä¹ˆå®ç°.
> 2.é€šè¿‡valueå±æ€§è®¿é—®è¯¥å¯¹è±¡å€¼(åœ¨Javaä¸­ä¸º`value()`æ–¹æ³•).
> 3.å˜é‡åªä¼šè¢«å‘åˆ°å„ä¸ªèŠ‚ç‚¹ä¸€æ¬¡,åº”ä½œä¸ºåªè¯»å€¼å¤„ç†(ä¿®æ”¹è¿™ä¸ªå€¼ä¸ä¼šå½±å“åˆ°åˆ«çš„èŠ‚ç‚¹).
```
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)

scala> 
```


### ğŸ”¥ 1.4 Spark SQL ğŸ”¥
#### 1.4.1 Spark SQL æ¦‚è¿°
##### 1.4.1.1 ä»€ä¹ˆæ˜¯ Spark SQL
> Spark SQLæ˜¯Sparkç”¨æ¥å¤„ç†ç»“æ„åŒ–æ•°æ®æ¨¡å—,å®ƒæä¾›äº†2ä¸ªç¼–ç¨‹æŠ½è±¡ : `DataFrame`å’Œ`DataSet`,å¹¶ä¸”ä½œä¸ºåˆ†å¸ƒå¼SQLæŸ¥è¯¢å¼•æ“ä½œç”¨.
> 
> å·²ç»å­¦ä¹ äº†Hive,å®ƒæ˜¯å°†Hive SQLè½¬æ¢æˆMapReduceç„¶åæäº¤åˆ°é›†ç¾¤ä¸Šæ‰§è¡Œ,å¤§å¤§ç®€åŒ–äº†ç¼–å†™MapReducç¨‹åºå¤æ‚æ€§,ç”±äºMapReduceè®¡ç®—æ¨¡å‹æ‰§è¡Œæ•ˆç‡æ¯”è¾ƒæ…¢,æ‰€ä»¥Spark SQLåº”è¿è€Œç”Ÿ,å®ƒæ˜¯å°†Spark SQLè½¬æ¢æˆRDD,ç„¶åæäº¤åˆ°é›†ç¾¤æ‰§è¡Œ,æ‰§è¡Œæ•ˆç‡éå¸¸å¿«.

##### 1.4.1.2 Spark SQL ç‰¹ç‚¹
- æ˜“æ•´åˆ
- ç»Ÿä¸€æ•°æ®è®¿é—®æ–¹å¼
- å…¼å®¹Hive
- æ ‡å‡†æ•°æ®è¿æ¥

##### 1.4.1.3 ä»€ä¹ˆæ˜¯ DataFrame
> ä¸RDDç±»ä¼¼,DataFrameä¹Ÿæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ•°æ®å®¹å™¨,ç„¶è€ŒDataFrameæ›´åƒä¼ ç»Ÿæ•°æ®åº“çš„äºŒç»´è¡¨æ ¼,é™¤äº†æ•°æ®ä»¥å¤–,è¿˜è®°å½•æ•°æ®çš„ç»“æ„ä¿¡æ¯,å³schema,åŒæ—¶ä¸Hiveç±»ä¼¼,DataFrameä¹Ÿæ”¯æŒåµŒå¥—æ•°æ®ç±»å‹(struct / array / map).
> 
> ä»APIæ˜“ç”¨æ€§è§’åº¦ä¸Šçœ‹,DataFrame APIæä¾›æ˜¯ä¸€å¥—é«˜å±‚çš„å…³ç³»æ“ä½œ,æ¯”å‡½æ•°å¼RDD APIè¦æ›´åŠ å‹å¥½,é—¨æ§›æ›´ä½.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_020.jpg)
> ä¸Šå›¾ç›´è§‚åœ°ä½“ç°äº†DataFrameå’ŒRDDåŒºåˆ«,å·¦ä¾§RDD[Person]è™½ç„¶ä»¥Personä¸ºç±»å‹å‚æ•°,ä½†Sparkæ¡†æ¶æœ¬èº«ä¸äº†è§£Personç±»å†…éƒ¨ç»“æ„,è€Œå³ä¾§DataFrameå´æä¾›äº†è¯¦ç»†çš„ç»“æ„ä¿¡æ¯,ä½¿å¾—Spark SQLå¯ä»¥æ¸…æ¥šåœ°çŸ¥é“è¯¥æ•°æ®é›†ä¸­åŒ…å«å“ªäº›åˆ—,æ¯åˆ—åç§°å’Œç±»å‹å„æ˜¯ä»€ä¹ˆ,DataFrameæ˜¯ä¸ºæ•°æ®æä¾›äº†Schemaè§†å›¾,å¯ä»¥æŠŠå®ƒå½“åšæ•°æ®åº“ä¸­ä¸€å¼ æ•°æ®è¡¨.
> 
> DataFrameä¹Ÿæ˜¯æ‡’æ‰§è¡Œ,æ€§èƒ½ä¸Šæ¯”RDDè¦é«˜è¦åŸå›  : ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’,æŸ¥è¯¢è®¡åˆ’é€šè¿‡Spark `catalyst optimiser`è¿›è¡Œä¼˜åŒ–.

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_021.jpg)
> ä¸ºäº†è¯´æ˜æŸ¥è¯¢ä¼˜åŒ–,ä¸Šå›¾å±•ç¤ºçš„äººå£æ•°æ®åˆ†æç¤ºä¾‹,å›¾ä¸­æ„é€ äº†ä¸¤ä¸ªDataFrame,å°†å®ƒä»¬joinä¹‹ååˆåšäº†ä¸€æ¬¡filteræ“ä½œ,å¦‚æœåŸå°ä¸åŠ¨åœ°æ‰§è¡Œè¿™ä¸ªæ‰§è¡Œè®¡åˆ’,æœ€ç»ˆçš„æ‰§è¡Œæ•ˆç‡æ˜¯ä¸é«˜çš„,å› ä¸ºjoinæ˜¯ä¸€ä¸ªä»£ä»·è¾ƒå¤§æ“ä½œ,ä¹Ÿå¯èƒ½ä¼šäº§ç”Ÿä¸€ä¸ªè¾ƒå¤§æ•°æ®é›†,å¦‚æœèƒ½å°†filterä¸‹æ¨åˆ°joinä¸‹æ–¹,å…ˆå¯¹DataFrameè¿›è¡Œè¿‡æ»¤,å†joinè¿‡æ»¤åçš„è¾ƒå°çš„ç»“æœé›†,ä¾¿å¯ä»¥æœ‰æ•ˆç¼©çŸ­æ‰§è¡Œæ—¶é—´.
> è€ŒSpark SQLçš„æŸ¥è¯¢ä¼˜åŒ–å™¨æ­£æ˜¯è¿™æ ·åšçš„,ç®€è€Œè¨€ä¹‹é€»è¾‘æŸ¥è¯¢è®¡åˆ’ä¼˜åŒ–å°±æ˜¯ä¸€ä¸ªåˆ©ç”¨åŸºäºå…³ç³»ä»£æ•°çš„ç­‰ä»·å˜æ¢,å°†é«˜æˆæœ¬çš„æ“ä½œæ›¿æ¢ä¸ºä½æˆæœ¬æ“ä½œçš„è¿‡ç¨‹.

##### 1.4.1.4 ä»€ä¹ˆæ˜¯ DataSet
> 1.DataSetæ˜¯DataframeAPIæ‰©å±•,æ˜¯SparkSQLæœ€æ–°æ•°æ®æŠ½è±¡.
> 
> 2.å‹å¥½APIé£æ ¼,æ—¢å…·æœ‰ç±»å‹å®‰å…¨æ£€æŸ¥ä¹Ÿå…·æœ‰Dataframeçš„æŸ¥è¯¢ä¼˜åŒ–ç‰¹æ€§.
> 
> 3.Datasetæ”¯æŒç¼–è§£ç å™¨,å½“éœ€è¦è®¿é—®éå †ä¸Šçš„æ•°æ®æ—¶å¯ä»¥é¿å…ååºåˆ—åŒ–æ•´ä¸ªå¯¹è±¡,æé«˜äº†æ•ˆç‡.
> 
> 4.æ ·ä¾‹ç±»è¢«ç”¨æ¥åœ¨Datasetä¸­å®šä¹‰æ•°æ®ç»“æ„ä¿¡æ¯,æ ·ä¾‹ç±»ä¸­æ¯ä¸ªå±æ€§çš„åç§°ç›´æ¥æ˜ å°„åˆ°DataSetä¸­çš„å­—æ®µåç§°.
> 
> 5.Dataframeæ˜¯Datasetçš„ç‰¹åˆ—,`DataFrame=Dataset[Row]`,æ‰€ä»¥å¯ä»¥é€šè¿‡asæ–¹æ³•å°†Dataframeè½¬æ¢ä¸ºDataset,Rowæ˜¯ä¸€ä¸ªç±»å‹,è·ŸCar / Personè¿™äº›ç±»å‹ä¸€æ ·,æ‰€æœ‰è¡¨ç»“æ„ä¿¡æ¯éƒ½ç”¨Rowæ¥è¡¨ç¤º.
> 
> 6.DataSetæ˜¯å¼ºç±»å‹,æ¯”å¦‚å¯ä»¥æœ‰`Dataset[Car]`,`Dataset[Person]`.
> 
> 7.DataFrameåªæ˜¯çŸ¥é“å­—æ®µ,ä½†æ˜¯ä¸çŸ¥é“å­—æ®µç±»å‹,æ‰€ä»¥åœ¨æ‰§è¡Œè¿™äº›æ“ä½œæ—¶æ˜¯æ²¡åŠæ³•åœ¨ç¼–è¯‘çš„æ—¶å€™æ£€æŸ¥æ˜¯å¦ç±»å‹å¤±è´¥,æ¯”å¦‚å¯ä»¥å¯¹ä¸€ä¸ªStringè¿›è¡Œå‡æ³•æ“ä½œ,åœ¨æ‰§è¡Œæ—¶æ‰æŠ¥é”™,è€ŒDataSetä¸ä»…ä»…çŸ¥é“å­—æ®µ,è€Œä¸”çŸ¥é“å­—æ®µç±»å‹,æ‰€ä»¥æœ‰æ›´ä¸¥æ ¼çš„é”™è¯¯æ£€æŸ¥,å°±è·ŸJSONå¯¹è±¡å’Œç±»å¯¹è±¡ä¹‹é—´çš„ç±»æ¯”.



#### 1.4.2 Spark SQL ç¼–ç¨‹
##### 1.4.2.1 SparkSession æ–°èµ·å§‹ç‚¹
> åœ¨è€ç‰ˆæœ¬ä¸­,SparkSQLæä¾›ä¸¤ç§SQLæŸ¥è¯¢èµ·å§‹ç‚¹ : 
> SQLContext : ç”¨äºSparkæä¾›SQLæŸ¥è¯¢.
> HiveContext : ç”¨äºè¿æ¥HiveæŸ¥è¯¢.
> 
> SparkSessionæ˜¯Sparkæœ€æ–°SQLæŸ¥è¯¢èµ·å§‹ç‚¹,å®è´¨ä¸Šæ˜¯SQLContextå’ŒHiveContextç»„åˆ,æ‰€ä»¥åœ¨SQLContextå’ŒHiveContextä¸Šå¯ç”¨APIåœ¨SparkSessionä¸ŠåŒæ ·æ˜¯å¯ä»¥ä½¿ç”¨,SparkSessionå†…éƒ¨å°è£…äº†`sparkContext`,æ‰€ä»¥è®¡ç®—å®é™…ä¸Šæ˜¯ç”±sparkContextå®Œæˆ.

##### 1.4.2.2 DataFrame
###### 1.4.2.2.1 åˆ›å»º
> åœ¨SparkSQLä¸­`SparkSession`æ˜¯åˆ›å»ºDataFrameå’Œæ‰§è¡ŒSQLå…¥å£.
> åˆ›å»ºDataFrameæœ‰ä¸‰ç§æ–¹å¼ : 
> 1.é€šè¿‡Sparkæ•°æ®æºè¿›è¡Œåˆ›å»º.
> 2.ä»å·²å­˜åœ¨çš„RDDè¿›è¡Œè½¬æ¢.
> 3.ä»Hive Tableè¿›è¡ŒæŸ¥è¯¢è¿”å›.

- 1.ä»Sparkæ•°æ®æºè¿›è¡Œåˆ›å»º
- æŸ¥çœ‹Sparkæ•°æ®æºè¿›è¡Œåˆ›å»ºæ–‡ä»¶æ ¼å¼
```
scala> spark.read.
csv      jdbc   load     options   parquet   table   textFile      
format   json   option   orc       schema    text 
scala> spark.read.
```
- 2.è¯»å–jsonæ–‡ä»¶åˆ›å»ºDataFrameå±•ç¤ºç»“æœ
```
scala> val jsonflow = spark.read.json("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
jsonflow: org.apache.spark.sql.DataFrame = [age: bigint, name: string]=

scala> jsonflow.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> 
```
- 3.RDDè¿›è¡Œè½¬æ¢ | è½»è½»1.4.2.5
- 4.Hive Tableè¿›è¡ŒæŸ¥è¯¢è¿”å› | 

###### 1.4.2.2.2 SQLé£æ ¼è¯­æ³•(ä¸»è¦)
- å¯¹DataFrameåˆ›å»ºä¸´æ—¶è¡¨
- ä¸´æ—¶è¡¨æ˜¯SessionèŒƒå›´å†…,Sessioné€€å‡ºå,è¡¨å°±ä¼šå¤±æ•ˆ,å¦‚æœæƒ³åº”ç”¨èŒƒå›´å†…æœ‰æ•ˆ,å¯ä»¥ä½¿ç”¨å…¨å±€è¡¨,æ³¨æ„ä½¿ç”¨å…¨å±€è¡¨æ—¶éœ€è¦å…¨è·¯å¾„è®¿é—®,å¦‚ : `global_temp.people`
```
scala> jsonflow.createTempView("people")
```
- é€šè¿‡SQLè¯­å¥å®ç°æŸ¥è¯¢å…¨è¡¨ç»“æœå±•ç¤º
```
scala> val sqlDF = spark.sql("SELECT * FROM people")
sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> sqlDF.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> 
```
- å¯¹äºDataFrameåˆ›å»ºå…¨å±€è¡¨
```
scala> jsonflow.createGlobalTempView("peoples")
```
- é€šè¿‡SQLè¯­å¥å®ç°æŸ¥è¯¢å…¨è¡¨ç»“æœå±•ç¤º
```
scala> spark.sql("SELECT * FROM global_temp.peoples").show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala> spark.newSession().sql("SELECT * FROM global_temp.peoples").show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala>
```
###### 1.4.2.2.3 DSLé£æ ¼è¯­æ³•(æ¬¡è¦)
- æŸ¥çœ‹DataFrame Schemaä¿¡æ¯
```
scala> jsonflow.printSchema
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

scala> 
```
- åªæŸ¥çœ‹nameåˆ—æ•°æ®
```
scala> jsonflow.select("name").show
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

scala> 
```
- æŸ¥çœ‹nameåˆ—æ•°æ®ä»¥åŠage+1æ•°æ®
```
scala> jsonflow.select($"name",$"age" + 1).show()
+-------+---------+
|   name|(age + 1)|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+

scala>
```
- æŸ¥çœ‹ageå¤§äº21æ•°æ®
```
scala> jsonflow.filter($"age" > 21).show()
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

scala> 
```
- æŒ‰ç…§ageåˆ†ç»„,æŸ¥çœ‹æ•°æ®æ¡æ•°
```
scala> jsonflow.groupBy("age").count().show()
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+

scala> 
```
###### 1.4.2.2.4 RDDè½¬æ¢ä¸ºDateFrame
> å¦‚æœéœ€è¦RDDä¸DFæˆ–è€…DSä¹‹é—´æ“ä½œ,éœ€è¦å¼•å…¥`import spark.implicits._`
> sparkå¹¶ä¸æ˜¯åŒ…å,è€Œæ˜¯sparkSessionå¯¹è±¡åç§°.

- å¯¼å…¥éšå¼è½¬æ¢å¹¶åˆ›å»ºRDD
```
scala> import spark.implicits._
import spark.implicits._

scala> val peopleRDD = sc.textFile("hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt")
peopleRDD: org.apache.spark.rdd.RDD[String] = hdfs://systemhub511:9000/core_flow/spark/input/wordcount/wordcount_001.txt MapPartitionsRDD[30] at textFile at <console>:27

scala>
```
- 1.é€šè¿‡æ‰‹åŠ¨è½¬æ¢
```
scala> peopleRDD.map{x=>{val split = x.split(",");(split(0),split(1).trim)}}.toDF("name","age")
res11: org.apache.spark.sql.DataFrame = [name: string, age: string]
scala> 
```
- 2.é€šè¿‡åå°„è½¬æ¢ (éœ€è¦ç”¨åˆ°æ ·ä¾‹ç±») 
- åˆ›å»ºæ ·ä¾‹ç±»,æ ¹æ®æ ·ä¾‹ç±»å°†RDDè½¬æ¢ä¸ºDataFrame
```
scala> case class People(name:String, age:Int)
defined class People

scala> peopleRDD.map{x=>{val split = x.split(",");People(split(0),split(1).trim.toInt)}}.toDF
res17: org.apache.spark.sql.DataFrame = [name: string, age: int]

scala> res17.toDF
res18: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala> 
```
- 3.é€šè¿‡ç¼–ç¨‹æ–¹å¼è½¬æ¢
- å¯¼å…¥æ‰€éœ€ç±»å‹
```
scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._
scala> 
```
- åˆ›å»ºSchema
```
scala> val structType: StructType = StructType(StructField("name",StringType) :: StructField("age",IntegerType) :: Nil)
structType: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true))
scala> 
```
- å¯¼å…¥æ‰€éœ€ç±»å‹
```
scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row
scala> 
```
- æ ¹æ®æŒ‡å®šç±»å‹åˆ›å»ºäºŒå…ƒç»„RDD
```
scala> val data = peopleRDD.map{x => val para = x.split(",");Row(para(0),para(1).trim.toInt)}
scala> 
```
- æ ¹æ®æ•°æ®åŠæŒ‡å®šschemaåˆ›å»ºDataFrame
``` 
scala> val dataFrame = spark.createDataFrame(data, structType)
```
- Create SqlAction.scala
``` scala
package com.geekparkhub.core.spark.application.sparksql

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{IntegerType, StructField, StructType}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * SqlAction
  * <p>
  */

object SqlAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("SqlAction").getOrCreate()

    // åˆ›å»ºSC
    val sc: SparkContext = sparkSession.sparkContext

    // åˆ›å»º RDD
    val rdd: RDD[Int] = sc.parallelize(Array(1,2,3,4,5))

    // å°†Intç±»å‹RDDè½¬æ¢ä¸ºRowç±»å‹RDD
    val rowRDD: RDD[Row] = rdd.map(x => {Row(x)})

    // æ•°æ®è¾“å‡º
    rowRDD.collect().foreach(println)

    // åˆ›å»ºå…ƒæ•°æ®ä¿¡æ¯
    val structType = new StructType
    val structTypes: StructType = structType.add(StructField("id", IntegerType))
    val dataFrame: DataFrame = sparkSession.createDataFrame(rowRDD,structTypes)

    // å¯¼å…¥éšå¼è½¬æ¢
    import sparkSession.implicits._

    // DSLé£æ ¼ æ•°æ®æŸ¥è¯¢
    dataFrame.select("id").show()
    
    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```
###### 1.4.2.2.5 DateFrameè½¬æ¢ä¸ºRDD
- ç›´æ¥è°ƒç”¨rddå³å¯.
- åˆ›å»ºDataFrame
```
scala> val df = spark.read.json("/core_flow/spark/json/001/people.json")df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]                
scala> 
```
- å°†DataFrameè½¬æ¢ä¸ºRDD
```
scala> val dfToRDD = df.rdd
dfToRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[6] at rdd at <console>:29
scala>
```
- æ‰“å°RDD
```
scala> dfToRDD.collect
res0: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])
scala>
```

##### 1.4.2.3 DataSet
- Datasetæ˜¯å…·æœ‰å¼ºç±»å‹çš„æ•°æ®é›†åˆ,éœ€è¦æä¾›å¯¹åº”ç±»å‹ä¿¡æ¯.
###### 1.4.2.3.1 åˆ›å»º
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- åˆ›å»ºDataSet
```
scala> val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- æŸ¥çœ‹ç»“æœ
```
scala> caseClassDS.show
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+

scala> 
```

###### 1.4.2.3.2 RDDè½¬æ¢ä¸ºDataSet
- SparkSQLèƒ½å¤Ÿè‡ªåŠ¨å°†åŒ…å«æœ‰caseç±»RDDè½¬æ¢æˆDataFrame,caseç±»å®šä¹‰äº†tableç»“æ„,caseç±»å±æ€§é€šè¿‡åå°„å˜æˆäº†è¡¨åˆ—å,Caseç±»å¯ä»¥åŒ…å«è¯¸å¦‚Seqsæˆ–è€…Arrayç­‰å¤æ‚ç»“æ„.
- åˆ›å»ºRDD
```
scala> val peopleRDD = sc.textFile("examples/src/main/resources/people.txt")
peopleRDD: org.apache.spark.rdd.RDD[String] = examples/src/main/resources/people.txt MapPartitionsRDD[8] at textFile at <console>:28

scala>
```
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- å°†RDDè½¬åŒ–ä¸ºDataSet
```
scala> peopleRDD.map(line => {val para = line.split(",");Person(para(0),para(1).trim.toInt)}).toDS
res2: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```

###### 1.4.2.3.3 DataSetè½¬æ¢ä¸ºRDD
- è°ƒç”¨rddæ–¹æ³•å³å¯.
- åˆ›å»ºä¸€ä¸ªDataSet
```
scala> val DS= Seq(Person("Andy", 32)).toDS()
DS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- å°†DataSetè½¬æ¢ä¸ºRDD
```
scala> DS.rdd
res3: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[12] at rdd at <console>:28

scala> res3.collect
res4: Array[Person] = Array(Person(Andy,32))
scala> 
```

##### 1.4.2.4 DataFrameä¸DataSetç›¸äº’æ“ä½œ
###### 1.4.2.4.1 DataFrameè½¬Dataset
- æ­¤æ–¹æ³•å°±æ˜¯åœ¨ç»™å‡ºæ¯ä¸€åˆ—ç±»å‹å,ä½¿ç”¨asæ–¹æ³•è½¬æˆDataset,è¿™åœ¨æ•°æ®ç±»å‹æ˜¯DataFrameåˆéœ€è¦é’ˆå¯¹å„ä¸ªå­—æ®µå¤„ç†æ—¶æä¸ºæ–¹ä¾¿,åœ¨ä½¿ç”¨ä¸€äº›ç‰¹æ®Šçš„æ“ä½œæ—¶,ä¸€å®šè¦åŠ ä¸Š`import spark.implicits._`ä¸ç„¶toDFã€toDSæ— æ³•ä½¿ç”¨.
- åˆ›å»ºDateFrame
```
scala> val df = spark.read.json("./examples/src/main/resources/people.json")
```
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala> 
```
- å°†DateFrameè½¬åŒ–ä¸ºDataSet
```
scala> df.as[Person]
res14:  org.apache.spark.sql.Dataset[Person]  =  [age:  bigint,  name: string]
scala> 
```

###### 1.4.2.4.2 Datasetè½¬DataFrame
- åˆ›å»ºæ ·ä¾‹ç±»
```
scala> case class Person(name: String, age: Long)
defined class Person
scala>
```
- åˆ›å»ºDataSet
```
scala> val ds = Seq(Person("Andy", 32)).toDS()
ds: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]
scala> 
```
- å°†DataSetè½¬åŒ–ä¸ºDataFrameå¹¶å±•ç¤ºç»“æœ
```
scala> val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: bigint]

scala> df.show
+----+---+
|name|age|
+----+---+
|Andy| 32|
+----+---+
scala> 
```


##### 1.4.2.5 RDD / DataFrame / DataSet
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/spark/start_023.jpg)
> åœ¨SparkSQLä¸­Sparkä¸ºæä¾›äº†ä¸¤ä¸ªæ–°æŠ½è±¡,åˆ†åˆ«æ˜¯`DataFrame`å’Œ`DataSet`.
> ä»–ä»¬å’ŒRDDæœ‰ä»€ä¹ˆåŒºåˆ«? é¦–å…ˆä»ç‰ˆæœ¬çš„äº§ç”Ÿä¸Šæ¥çœ‹ : 
```
RDD (Spark1.0) â€”> Dataframe(Spark1.3) â€”> Dataset(Spark1.6)
```
> å¦‚æœåŒæ ·æ•°æ®éƒ½ç»™åˆ°è¿™ä¸‰ä¸ªæ•°æ®ç»“æ„,ä»–ä»¬åˆ†åˆ«è®¡ç®—ä¹‹å,éƒ½ä¼šç»™å‡ºç›¸åŒç»“æœ,ä¸åŒæ˜¯æ‰§è¡Œæ•ˆç‡å’Œæ‰§è¡Œæ–¹å¼.
> åœ¨åæœŸSparkç‰ˆæœ¬ä¸­,DataSetä¼šé€æ­¥å–ä»£RDDå’ŒDataFrameæˆä¸ºå”¯ä¸€çš„APIæ¥å£.

###### 1.4.2.5.1 ä¸‰è€…å…±æ€§
- 1.RDD / DataFrame / Datasetå…¨éƒ½æ˜¯sparkå¹³å°ä¸‹åˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†,ä¸ºå¤„ç†è¶…å¤§å‹æ•°æ®æä¾›ä¾¿åˆ©.
- 2.ä¸‰è€…éƒ½æœ‰æƒ°æ€§æœºåˆ¶,åœ¨è¿›è¡Œåˆ›å»º / è½¬æ¢,å¦‚mapæ–¹æ³•æ—¶ä¸ä¼šç«‹å³æ‰§è¡Œ,åªæœ‰åœ¨é‡åˆ°Actionå¦‚foreachæ—¶,ä¸‰è€…æ‰ä¼šå¼€å§‹éå†è¿ç®—.
- 3.ä¸‰è€…éƒ½ä¼šæ ¹æ®sparkå†…å­˜æƒ…å†µè‡ªåŠ¨ç¼“å­˜è¿ç®—,è¿™æ ·å³ä½¿æ•°æ®é‡å¾ˆå¤§,ä¹Ÿä¸ç”¨æ‹…å¿ƒä¼šå†…å­˜æº¢å‡º.
- 4.ä¸‰è€…éƒ½æœ‰partitionæ¦‚å¿µ.
- 5.ä¸‰è€…æœ‰è®¸å¤šå…±åŒå‡½æ•°,å¦‚filter,æ’åºç­‰.
- 6.åœ¨å¯¹DataFrameå’ŒDatasetè¿›è¡Œæ“ä½œè®¸å¤šæ“ä½œéƒ½éœ€è¦è¿™ä¸ªåŒ…è¿›è¡Œæ”¯æŒ`importspark.implicits._`
- 7.DataFrameå’ŒDatasetå‡å¯ä½¿ç”¨æ¨¡å¼åŒ¹é…è·å–å„ä¸ªå­—æ®µå€¼å’Œç±»å‹.
- DataFrame : 
```
DF.map{
 caseRow(col1:String,col2:Int)=>
  println(col1);println(col2)
   col1
    case_=> ""
}
```
- Dataset : 
```
// å®šä¹‰å­—æ®µåå’Œç±»å‹
caseclassColtest(col1:String,col2:Int)extendsSerializable
DS.map{
 caseColtest(col1:String,col2:Int)=>
  println(col1);println(col2)
   col1
    case_=> ""
}
```

###### 1.4.2.5.2 ä¸‰è€…åŒºåˆ«
- `1. RDD` :
- RDDä¸€èˆ¬å’Œspark mlibåŒæ—¶ä½¿ç”¨
- RDDä¸æ”¯æŒsparksqlæ“ä½œ
- `2. DataFrame`
- ä¸RDDå’ŒDatasetä¸åŒ,DataFrameæ¯ä¸€è¡Œç±»å‹å›ºå®šä¸ºRow,æ¯ä¸€åˆ—çš„å€¼æ²¡æ³•ç›´æ¥è®¿é—®,åªæœ‰é€šè¿‡è§£ææ‰èƒ½è·å–å„ä¸ªå­—æ®µå€¼.
```
DF.foreach{
 line=>
  valcol1=line.getAs[String]("col1")
  valcol2=line.getAs[String]("col2")
}
```
- DataFrameä¸Datasetä¸€èˆ¬ä¸ä¸spark mlibåŒæ—¶ä½¿ç”¨
- DataFrameä¸Datasetå‡æ”¯æŒsparksqlæ“ä½œ,æ¯”å¦‚select,groupby,è¿˜èƒ½æ³¨å†Œä¸´æ—¶è¡¨/è§†çª—,è¿›è¡Œsqlè¯­å¥æ“ä½œ.
- DataFrameä¸Datasetæ”¯æŒä¸€äº›ç‰¹åˆ«æ–¹ä¾¿ä¿å­˜æ–¹å¼,æ¯”å¦‚ä¿å­˜æˆcsv,å¯ä»¥å¸¦ä¸Šè¡¨å¤´,è¿™æ ·æ¯ä¸€åˆ—å­—æ®µåä¸€ç›®äº†ç„¶.
- `3.`Dataset`
- Datasetå’ŒDataFrameæ‹¥æœ‰å®Œå…¨ç›¸åŒçš„æˆå‘˜å‡½æ•°,åŒºåˆ«åªæ˜¯æ¯ä¸€è¡Œæ•°æ®ç±»å‹ä¸åŒ.
- DataFrameä¹Ÿå¯ä»¥å«`Dataset[Row]`,æ¯ä¸€è¡Œçš„ç±»å‹æ˜¯Row,ä¸è§£ææ¯ä¸€è¡Œç©¶ç«Ÿæœ‰å“ªäº›å­—æ®µ,å„ä¸ªå­—æ®µåˆæ˜¯ä»€ä¹ˆç±»å‹éƒ½æ— ä»å¾—çŸ¥,åªèƒ½ç”¨ä¸Šé¢æåˆ°`getAS`æ–¹æ³•æˆ–è€…å…±æ€§ä¸­çš„ç¬¬ä¸ƒæ¡æåˆ°çš„æ¨¡å¼åŒ¹é…æ‹¿å‡ºç‰¹å®šå­—æ®µ,è€ŒDatasetä¸­,æ¯ä¸€è¡Œæ˜¯ä»€ä¹ˆç±»å‹æ˜¯ä¸ä¸€å®š,åœ¨è‡ªå®šä¹‰äº†case classä¹‹åå¯ä»¥å¾ˆè‡ªç”±è·å¾—æ¯ä¸€è¡Œä¿¡æ¯.
- Datasetåœ¨éœ€è¦è®¿é—®åˆ—ä¸­æŸä¸ªå­—æ®µæ—¶æ˜¯éå¸¸æ–¹ä¾¿,ç„¶è€Œå¦‚æœè¦å†™ä¸€äº›é€‚é…æ€§å¾ˆå¼ºå‡½æ•°æ—¶,å¦‚æœä½¿ç”¨Dataset,è¡Œç±»å‹åˆä¸ç¡®å®š,å¯èƒ½æ˜¯å„ç§case class,æ— æ³•å®ç°é€‚é…,è¿™æ—¶å€™ç”¨DataFrameå³Dataset[Row]å°±èƒ½æ¯”è¾ƒå¥½è§£å†³é—®é¢˜.

##### 1.4.2.6 SparkSQL Application
- SQL & DSLé£æ ¼æ•°æ®æŸ¥è¯¢
``` scala
package com.geekparkhub.core.spark.application.sparksql

import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * SqlAction
  * <p>
  */

object SqlAction {
  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("SqlAction").getOrCreate()

    // å¯¼å…¥éšå¼è½¬æ¢
    import sparkSession.implicits._

    // åˆ›å»ºDF
    val df: DataFrame = sparkSession.read.json("/Volumes/GEEK-SYSTEM/Technical_Framework/spark/projects/spark_server/spark-sql/data/people.json")

    // SQLé£æ ¼ æ•°æ®æŸ¥è¯¢ | åˆ›å»ºä¸´æ—¶è¡¨
    df.createTempView("PEOPLE")
    sparkSession.sql("SELECT * FROM PEOPLE").show()

    // DSLé£æ ¼ æ•°æ®æŸ¥è¯¢
    df.select("name").show()

    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```

##### 1.4.2.7 è‡ªå®šä¹‰å‡½æ•°
- åœ¨Shellçª—å£ä¸­å¯é€šè¿‡`spark.udf`åŠŸèƒ½è‡ªå®šä¹‰å‡½æ•°.

###### 1.4.2.7.1 è‡ªå®šä¹‰UDFå‡½æ•°
- åˆ›å»ºDF
```
scala> val df = spark.read.json("hdfs://systemhub511:9000/core_flow/spark/json/001/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.show()
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

scala>
```
- æ³¨å†ŒUDF
```
scala> spark.udf.register("addName",(x:String) => "Name:" + x)
res1: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))

scala>
```
- åˆ›å»ºæ•°æ®è¡¨
```
scala> df.createOrReplaceTempView("people")
```
- æŸ¥è¯¢æ•°æ®è¡¨
```
scala> spark.sql("Select addName(name),age from people").show()
+-----------------+----+
|UDF:addName(name)| age|
+-----------------+----+
|     Name:Michael|null|
|        Name:Andy|  30|
|      Name:Justin|  19|
+-----------------+----+
scala>
```
###### 1.4.2.7.2 è‡ªå®šä¹‰èšåˆå‡½æ•°
- å¼ºç±»å‹Datasetå’Œå¼±ç±»å‹DataFrameéƒ½æä¾›äº†ç›¸å…³èšåˆå‡½æ•°,å¦‚count(),countDistinct(),avg(),max(),min(),é™¤æ­¤ä¹‹å¤–è¿˜å¯ä»¥è®¾å®šè‡ªå®šä¹‰èšåˆå‡½æ•°.
- å¼±ç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•° : é€šè¿‡ç»§æ‰¿`UserDefinedAggregateFunction`æ¥å®ç°è‡ªå®šä¹‰èšåˆå‡½æ•°.
- ä¸‹é¢å±•ç¤º æ±‚å¹³å‡å·¥èµ„è‡ªå®šä¹‰èšåˆå‡½æ•°
- Create `AvgAction.scala`
``` scala
package com.geekparkhub.core.spark.application.aggregation

import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DoubleType, LongType, StructField, StructType}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * AvgAction
  * <p>
  */

object AvgAction extends UserDefinedAggregateFunction {

  // å®šä¹‰è¾“å…¥æ•°æ®ç±»å‹
  override def inputSchema: StructType = StructType(StructField("input", LongType) :: Nil)

  // ç¼“å­˜ä¸­é—´å€¼ç±»å‹
  override def bufferSchema: StructType = StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)

  // å®šä¹‰è¾“å‡ºæ•°æ®ç±»å‹
  override def dataType: DataType = DoubleType

  // å‡½æ•°ç¨³å®šå‚æ•°
  override def deterministic: Boolean = true

  // åˆå§‹åŒ–ç¼“å­˜æ•°æ®
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }

  // åœ¨æ‰§è¡Œå™¨ä¹‹å†…æ›´æ–°
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    buffer(0) = buffer.getLong(0) + input.getLong(0)
    buffer(1) = buffer.getLong(1) + 1L
  }

  // åœ¨æ‰§è¡Œå™¨ä¹‹å¤–åˆå¹¶
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)

  }

  // æ‰§è¡Œæ•°æ®è®¡ç®—
  override def evaluate(buffer: Row): Any = buffer.getLong(0).toDouble / buffer.getLong(1)
}
```
- Create `UdafAction.scala`
```
package com.geekparkhub.core.spark.application.aggregation

import org.apache.spark.sql.{DataFrame, SparkSession}

/**
  * Geek International Park | æå®¢å›½é™…å…¬å›­
  * GeekParkHub | æå®¢å®éªŒå®¤
  * Website | https://www.geekparkhub.com/
  * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
  * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
  * Website | https://www.hackerparkhub.org/
  * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
  * GeekDeveloper : JEEP-711
  *
  * @author system
  * <p>
  * UdafAction
  * <p>
  */

object UdafAction {
  def main(args: Array[String]): Unit = {

    // åˆ›å»ºSparkSession
    val sparkSession: SparkSession = SparkSession
      .builder().master("local[*]").appName("UdafAction").getOrCreate()

    // åˆ›å»ºDF
    val df: DataFrame = sparkSession.read.json("/Volumes/GEEK-SYSTEM/Technical_Framework/spark/projects/spark_server/spark-sql/data/people.json")

    // SQLé£æ ¼ æ•°æ®æŸ¥è¯¢ | åˆ›å»ºä¸´æ—¶è¡¨
    df.createTempView("PEOPLE")

    // æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
    sparkSession.udf.register("AvgAction", AvgAction)

    // ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
    sparkSession.sql("SELECT AvgAction(age) FROM PEOPLE").show()

    // å…³é—­èµ„æº
    sparkSession.stop()
  }
}
```

## ğŸ”’ å°šæœªè§£é” æ­£åœ¨æ¢ç´¢ä¸­... å°½æƒ…æœŸå¾… Blogæ›´æ–°! ğŸ”’
- å¼ºç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•° : é€šè¿‡ç»§æ‰¿`Aggregator`æ¥å®ç°å¼ºç±»å‹è‡ªå®šä¹‰èšåˆå‡½æ•°,åŒæ ·æ˜¯æ±‚å¹³å‡å·¥èµ„.


#### 1.4.3 Spark SQL æ•°æ®æº
#####1.4.3.1 é€šç”¨åŠ è½½ / ä¿å­˜æ–¹æ³•
#####1.4.3.2 JSONæ–‡ä»¶
#####1.4.3.3 Parquetæ–‡ä»¶
#####1.4.3.4 JDBC
#####1.4.3.5 Hive DataBase


#### 1.4.4 Spark SQL å®ä¾‹



### ğŸ”¥ 1.5 Spark Streaming ğŸ”¥
#### 1.5.1 Spark Streaming æ¦‚è¿°
#### 1.5.2 Spark Streaming Program
#### 1.5.3 DataStream æ¦‚è¿°
#### 1.5.4 DataStream è¾“å…¥
#### 1.5.5 DataStream è½¬æ¢
#### 1.5.6 DataStream è¾“å‡º
#### 1.5.7 7*24hourè¿è¡Œ
#### 1.5.8 Spark Streaming å®ä¾‹


## ğŸ”¥ 2. Spark é«˜é˜¶ ğŸ”¥
### 2.1 å†…æ ¸æœºåˆ¶
### 2.1 æ€§èƒ½è°ƒä¼˜







## 3. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------