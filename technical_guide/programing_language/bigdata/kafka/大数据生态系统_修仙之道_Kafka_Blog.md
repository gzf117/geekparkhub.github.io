# å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿ ä¿®ä»™ä¹‹é“ Kafka Blog

@(2019-04-01)[ Docs Language:ç®€ä½“ä¸­æ–‡ & English|Programing Language:Kafka|Website:[www.geekparkhub.com](https://www.geekparkhub.com/)|![OpenSource](https://img.shields.io/badge/Open%20Source-%E2%9D%A4-brightgreen.svg) | ![GitHub repo size in bytes](https://img.shields.io/github/repo-size/geekparkhub/geekparkhub.github.io.svg) | GeekDeveloper:[JEEP-711](https://github.com/jeep711)|Github:[github.com/geekparkhub](https://github.com/geekparkhub)|Gitee:[gitee.com/geekparkhub](https://gitee.com/geekparkhub) ]

##  ğŸ˜ Kafka Technology ä¿®ä»™ä¹‹é“ ç‚¼è™šåˆé“ ğŸ˜

![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/kafka.jpg)

- **æå®¢å®éªŒå®¤æ˜¯æå®¢å›½é™…å…¬å›­æ——ä¸‹ä¸ºæœªæ¥è€Œæ„å»ºçš„æå®¢ç¤¾åŒº;**
- **æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€ä¸ªæ´»è·ƒçš„å°ä¼—ç¤¾åŒº,æ±‡èšä¼—å¤šä¼˜ç§€å¼€å‘è€…ä¸è®¾è®¡å¸ˆ;**
- **å…³æ³¨æå…·åˆ›æ–°ç²¾ç¥çš„å‰æ²¿æŠ€æœ¯&åˆ†äº«äº¤æµ&é¡¹ç›®åˆä½œæœºä¼šç­‰äº’è”ç½‘è¡Œä¸šæœåŠ¡;**
- **Openå¼€æ”¾ `Â·` Creationåˆ›æƒ³ `|` OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§!**
- **Future Vision : Establishment of the Geek Foundation;**
- **GeekParkHub GithubHome:**<https://github.com/geekparkhub>
- **GeekParkHub GiteeHome:**<https://gitee.com/geekparkhub>
- **æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`**
- ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>


-------------------



[TOC]



## 1. Kafka æ¦‚è¿°
### 1.1 æ¶ˆæ¯é˜Ÿåˆ—

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_002.jpg)
> `ç‚¹å¯¹ç‚¹æ¨¡å¼` (ä¸€å¯¹ä¸€,æ¶ˆè´¹è€…ä¸»åŠ¨æ‹‰å–æ•°æ®,æ¶ˆæ¯æ”¶åˆ°åæ¶ˆæ¯æ¸…é™¤)ç‚¹å¯¹ç‚¹æ¨¡å‹é€šå¸¸æ˜¯ä¸€ä¸ªåŸºäºæ‹‰å–æˆ–è€…è½®è¯¢çš„æ¶ˆæ¯ä¼ é€æ¨¡å‹,è¿™ç§æ¨¡å‹ä»é˜Ÿåˆ—ä¸­è¯·æ±‚ä¿¡æ¯,è€Œä¸æ˜¯å°†æ¶ˆæ¯æ¨é€åˆ°å®¢æˆ·ç«¯.è¿™ä¸ªæ¨¡å‹çš„ç‰¹ç‚¹æ˜¯å‘é€åˆ°é˜Ÿåˆ—çš„æ¶ˆæ¯è¢«ä¸€ä¸ªä¸”åªæœ‰ä¸€ä¸ªæ¥æ”¶è€…æ¥æ”¶å¤„ç†,å³ä½¿æœ‰å¤šä¸ªæ¶ˆæ¯ç›‘å¬è€…ä¹Ÿæ˜¯å¦‚æ­¤.
> 
> `å‘å¸ƒ/è®¢é˜…æ¨¡å¼` (ä¸€å¯¹å¤š,æ•°æ®ç”Ÿäº§å,æ¨é€ç»™æ‰€æœ‰è®¢é˜…è€…)
> å‘å¸ƒè®¢é˜…æ¨¡å‹åˆ™æ˜¯ä¸€ä¸ªåŸºäºæ¨é€çš„æ¶ˆæ¯ä¼ é€æ¨¡å‹,å‘å¸ƒè®¢é˜…æ¨¡å‹å¯ä»¥æœ‰å¤šç§ä¸åŒçš„è®¢é˜…è€…,ä¸´æ—¶è®¢é˜…è€…åªåœ¨ä¸»åŠ¨ç›‘å¬ä¸»é¢˜æ—¶æ‰æ¥æ”¶æ¶ˆæ¯,è€ŒæŒä¹…è®¢é˜…è€…åˆ™ç›‘å¬ä¸»é¢˜çš„æ‰€æœ‰æ¶ˆæ¯,å³ä½¿å½“å‰è®¢é˜…è€…ä¸å¯ç”¨,å¤„äºç¦»çº¿çŠ¶æ€.
### 1.2 ä¸ºä»€ä¹ˆéœ€è¦æ¶ˆæ¯é˜Ÿåˆ—
> `è§£è€¦` : å…è®¸ä½ ç‹¬ç«‹çš„æ‰©å±•æˆ–ä¿®æ”¹ä¸¤è¾¹çš„å¤„ç†è¿‡ç¨‹,åªè¦ç¡®ä¿å®ƒä»¬éµå®ˆåŒæ ·çš„æ¥å£çº¦æŸ.
> 
> `å†—ä½™` : æ¶ˆæ¯é˜Ÿåˆ—æŠŠæ•°æ®è¿›è¡ŒæŒä¹…åŒ–ç›´åˆ°å®ƒä»¬å·²ç»è¢«å®Œå…¨å¤„ç†,é€šè¿‡è¿™ä¸€æ–¹å¼è§„é¿äº†æ•°æ®ä¸¢å¤±é£é™©,è®¸å¤šæ¶ˆæ¯é˜Ÿåˆ—æ‰€é‡‡ç”¨çš„"æ’å…¥-è·å–-åˆ é™¤"èŒƒå¼ä¸­,åœ¨æŠŠä¸€ä¸ªæ¶ˆæ¯ä»é˜Ÿåˆ—ä¸­åˆ é™¤ä¹‹å‰,éœ€è¦ä½ çš„å¤„ç†ç³»ç»Ÿæ˜ç¡®çš„æŒ‡å‡ºè¯¥æ¶ˆæ¯å·²ç»è¢«å¤„ç†å®Œæ¯•,ä»è€Œç¡®ä¿ä½ çš„æ•°æ®è¢«å®‰å…¨çš„ä¿å­˜ç›´åˆ°ä½ ä½¿ç”¨å®Œæ¯•.
> 
> `æ‰©å±•æ€§` : å› ä¸ºæ¶ˆæ¯é˜Ÿåˆ—è§£è€¦äº†ä½ çš„å¤„ç†è¿‡ç¨‹,æ‰€ä»¥å¢å¤§æ¶ˆæ¯å…¥é˜Ÿå’Œå¤„ç†çš„é¢‘ç‡æ˜¯å¾ˆå®¹æ˜“çš„,åªè¦å¦å¤–å¢åŠ å¤„ç†è¿‡ç¨‹å³å¯.
> 
> `çµæ´»æ€§&å³°å€¼å¤„ç†èƒ½åŠ›` : åœ¨è®¿é—®é‡å‰§å¢çš„æƒ…å†µä¸‹,åº”ç”¨ä»ç„¶éœ€è¦ç»§ç»­å‘æŒ¥ä½œç”¨,ä½†æ˜¯è¿™æ ·çš„çªå‘æµé‡å¹¶ä¸å¸¸è§,å¦‚æœä¸ºä»¥èƒ½å¤„ç†è¿™ç±»å³°å€¼è®¿é—®ä¸ºæ ‡å‡†æ¥æŠ•å…¥èµ„æºéšæ—¶å¾…å‘½æ— ç–‘æ˜¯å·¨å¤§çš„æµªè´¹,ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—èƒ½å¤Ÿä½¿å…³é”®ç»„ä»¶é¡¶ä½çªå‘çš„è®¿é—®å‹åŠ›,è€Œä¸ä¼šå› ä¸ºçªå‘çš„è¶…è´Ÿè·çš„è¯·æ±‚è€Œå®Œå…¨å´©æºƒ.
> 
> `å¯æ¢å¤æ€§` : ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ç»„ä»¶å¤±æ•ˆæ—¶,ä¸ä¼šå½±å“åˆ°æ•´ä¸ªç³»ç»Ÿ,æ¶ˆæ¯é˜Ÿåˆ—é™ä½äº†è¿›ç¨‹é—´çš„è€¦åˆåº¦,æ‰€ä»¥å³ä½¿ä¸€ä¸ªå¤„ç†æ¶ˆæ¯çš„è¿›ç¨‹æŒ‚æ‰,åŠ å…¥é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯ä»ç„¶å¯ä»¥åœ¨ç³»ç»Ÿæ¢å¤åè¢«å¤„ç†.
> 
> `é¡ºåºä¿è¯` : åœ¨å¤§å¤šä½¿ç”¨åœºæ™¯ä¸‹,æ•°æ®å¤„ç†çš„é¡ºåºéƒ½å¾ˆé‡è¦,å¤§éƒ¨åˆ†æ¶ˆæ¯é˜Ÿåˆ—æœ¬æ¥å°±æ˜¯æ’åºçš„,å¹¶ä¸”èƒ½ä¿è¯æ•°æ®ä¼šæŒ‰ç…§ç‰¹å®šçš„é¡ºåºæ¥å¤„ç†,(Kafkaä¿è¯ä¸€ä¸ªPartitionå†…çš„æ¶ˆæ¯çš„æœ‰åºæ€§).
> 
> `ç¼“å†²` : æœ‰åŠ©äºæ§åˆ¶å’Œä¼˜åŒ–æ•°æ®æµç»è¿‡ç³»ç»Ÿçš„é€Ÿåº¦,è§£å†³ç”Ÿäº§æ¶ˆæ¯å’Œæ¶ˆè´¹æ¶ˆæ¯çš„å¤„ç†é€Ÿåº¦ä¸ä¸€è‡´çš„æƒ…å†µ.
> 
> `å¼‚æ­¥é€šä¿¡` : å¾ˆå¤šæ—¶å€™ç”¨æˆ·ä¸æƒ³ä¹Ÿä¸éœ€è¦ç«‹å³å¤„ç†æ¶ˆæ¯,æ¶ˆæ¯é˜Ÿåˆ—æä¾›äº†å¼‚æ­¥å¤„ç†æœºåˆ¶,å…è®¸ç”¨æˆ·æŠŠä¸€ä¸ªæ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—,ä½†å¹¶ä¸ç«‹å³å¤„ç†å®ƒ,æƒ³å‘é˜Ÿåˆ—ä¸­æ”¾å…¥å¤šå°‘æ¶ˆæ¯å°±æ”¾å¤šå°‘,ç„¶ååœ¨éœ€è¦çš„æ—¶å€™å†å»å¤„ç†å®ƒä»¬.

### 1.3 Kafka ç®€ä»‹
> åœ¨æµå¼è®¡ç®—ä¸­,Kafkaä¸€èˆ¬ç”¨æ¥ç¼“å­˜æ•°æ®,Stormé€šè¿‡æ¶ˆè´¹Kafkaæ•°æ®è¿›è¡Œè®¡ç®—.
> 
> Apache Kafkaæ˜¯ä¸€ä¸ªå¼€æºæ¶ˆæ¯ç³»ç»Ÿ,æ˜¯ç”±Scalaç¼–ç¨‹è¯­è¨€ç¼–å†™,Apacheè½¯ä»¶åŸºé‡‘ä¼šå¼€å‘ä¸€ä¸ªå¼€æºæ¶ˆæ¯ç³»ç»Ÿé¡¹ç›®.
> 
> Kafkaæœ€åˆæ˜¯ç”±LinkedInå…¬å¸å¼€å‘,å¹¶äº2011å¹´åˆå¼€æº,2012å¹´10æœˆä»Apache Incubatoræ¯•ä¸š,è¯¥é¡¹ç›®çš„ç›®æ ‡æ˜¯ä¸ºå¤„ç†å®æ—¶æ•°æ®æä¾›ä¸€ä¸ªç»Ÿä¸€ / é«˜é€šé‡ / ä½ç­‰å¾…å¹³å°.
> 
> Kafkaæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—,Kafkaå¯¹æ¶ˆæ¯ä¿å­˜æ—¶æ ¹æ®Topicè¿›è¡Œå½’ç±»,å‘é€æ¶ˆæ¯è€…ç§°ä¸ºProducer,æ¶ˆæ¯æ¥å—è€…ç§°ä¸ºConsumer,æ­¤å¤–Kafkaé›†ç¾¤æœ‰å¤šä¸ªKafkaå®ä¾‹ç»„æˆ,æ¯ä¸ªå®ä¾‹(Server)æˆä¸ºBroker.
> 
> æ— è®ºæ˜¯Kafkaé›†ç¾¤,è¿˜æ˜¯Producerå’Œconsumeréƒ½ä¾èµ–äºZookeeperé›†ç¾¤ä¿å­˜ä¸€äº›Metaä¿¡æ¯æ¥ä¿è¯ç³»ç»Ÿå¯ç”¨æ€§.

### 1.4 Kafka æ¶æ„

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_003.jpg)

> `Producer` : æ¶ˆæ¯ç”Ÿäº§è€…,å‘Kafka Brokerå‘æ¶ˆæ¯çš„å®¢æˆ·ç«¯.
> 
> `Consumer` : æ¶ˆæ¯æ¶ˆè´¹è€…,å‘Kafka Brokeræ‹‰å–æ¶ˆæ¯çš„å®¢æˆ·ç«¯.
> 
> `Topic` : å¯ä»¥ç†è§£ä¸ºä¸€ä¸ªé˜Ÿåˆ—.
> 
> `Consumer Group`(CG) : è¿™æ˜¯Kafkaç”¨æ¥å®ç°Topicæ¶ˆæ¯å¹¿æ’­(å‘ç»™æ‰€æœ‰Consumer)å•æ’­(å‘ç»™ä»»æ„ä¸€ä¸ªConsumer)æ‰‹æ®µ,ä¸€ä¸ªTopicå¯ä»¥æœ‰å¤šä¸ªCG,Topicæ¶ˆæ¯ä¼šå¤åˆ¶(ä»…ä»…åªæ˜¯æ¦‚å¿µä¸Šçš„å¤åˆ¶)æ‰€æœ‰CG,ä½†æ¯ä¸ªPartionåªä¼šæŠŠæ¶ˆæ¯å‘ç»™è¯¥CGä¸­ä¸€ä¸ªConsumer.
> å¦‚æœéœ€è¦å®ç°å¹¿æ’­,åªè¦æ¯ä¸ªConsumeræœ‰ä¸€ä¸ªç‹¬ç«‹CGå°±å¯ä»¥äº†,è¦å®ç°å•æ’­åªè¦æ‰€æœ‰Consumeråœ¨åŒä¸€ä¸ªCG,ç”¨CGè¿˜å¯ä»¥å°†Consumerè¿›è¡Œè‡ªç”±åˆ†ç»„è€Œä¸éœ€è¦å¤šæ¬¡å‘é€æ¶ˆæ¯åˆ°ä¸åŒçš„Topic.
> 
> `Broker` : ä¸€å°KafkaæœåŠ¡å™¨å°±æ˜¯ä¸€ä¸ªBroker,ä¸€ä¸ªé›†ç¾¤ç”±å¤šä¸ªBrokerç»„æˆ,ä¸€ä¸ªBrokerå¯ä»¥å®¹çº³å¤šä¸ªTopic.
> 
> `Partition` : ä¸ºäº†å®ç°æ‰©å±•æ€§,ä¸€ä¸ªéå¸¸å¤§çš„Topicå¯ä»¥åˆ†å¸ƒåˆ°å¤šä¸ªBroker(å³æ˜¯æœåŠ¡å™¨),ä¸€ä¸ªTopicå¯ä»¥åˆ†ä¸ºå¤šä¸ªPartition,æ¯ä¸ªPartitionæ˜¯ä¸€ä¸ªæœ‰åºé˜Ÿåˆ—,Partitionä¸­æ¯æ¡æ¶ˆæ¯éƒ½ä¼šè¢«åˆ†é…ä¸€ä¸ªæœ‰åºId(Offset),Kafkaåªä¿è¯æŒ‰ä¸€ä¸ªPartitionä¸­é¡ºåºå°†æ¶ˆæ¯å‘ç»™Consumer,ä¸ä¿è¯ä¸€ä¸ªTopicæ•´ä½“(å¤šä¸ªPartitioné—´)é¡ºåº.
> 
> `Offset` : Kafkaå­˜å‚¨æ–‡ä»¶éƒ½æ˜¯æŒ‰ç…§offset.kafkaæ¥å‘½å,ç”¨Offsetä½œä¸ºåç§°çš„å¥½å¤„æ˜¯æ–¹ä¾¿æŸ¥æ‰¾,ä¾‹å¦‚æƒ³æ‰¾ä½äº2049ä½ç½®,åªè¦æ‰¾åˆ°2048.kafkaæ–‡ä»¶å³å¯,å½“ç„¶`the first offset`å°±æ˜¯00000000000.kafka.

## 2. Kafka é›†ç¾¤éƒ¨ç½²
### 2.1 ç¯å¢ƒå‡†å¤‡
#### 2.1.1 é›†ç¾¤è§„åˆ’
| Server Node | Zookooper Server | Kafka Server |
| :--------: | :--------:| :------: |
| systemhub511 ğŸ–¥ï¸ | zookeeper âœ… | kafka âœ…  |
| systemhub611 ğŸ–¥ï¸ | zookeeper âœ… | kafka âœ…  |
| systemhub711 ğŸ–¥ï¸ | zookeeper âœ… | kafka âœ…  |

#### 2.1.2 Download
- `Download Zookeeper` : [archive.apache.org/dist/zookeeper](https://archive.apache.org/dist/zookeeper/)
- `Download Kafka` : [kafka.apache.org/downloads](http://kafka.apache.org/downloads.html)

#### 2.1.3 å®‰è£… Zookeeper
##### 2.1.3.1 è§£å‹ zookeeper
```
[root@systemhub511 ~]#  cd /opt/software/
[root@systemhub511 software]# ll
total 512064
-rwxrwxrwx. 1 root root  35042811 Jan 17 19:18 zookeeper-3.4.10.tar.gz
[root@systemhub511 software]# tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/
zookeeper-3.4.10/
zookeeper-3.4.10/LICENSE.txt
zookeeper-3.4.10/lib/
zookeeper-3.4.10/lib/log4j-1.2.16.LICENSE.txt
[root@systemhub511 software]#
```
##### 2.1.3.2 é‡å‘½å zookeeper
```
[root@systemhub511 module]# mv zookeeper-3.4.10 zookeeper
[root@systemhub511 module]# ll
total 20
drwxr-xr-x.  9 root  root  4096 Feb 24 21:55 apache-tomcat
drwxr-xr-x. 10 root  root  4096 Apr 11 17:02 flume
drwxr-xr-x. 12 10011 10011 4096 Mar  3 00:42 hadoop
drwxr-xr-x.  8 uucp    143 4096 Dec 20  2017 jdk1.8.0_162
drwxr-xr-x. 10  1001  1001 4096 Mar 23  2017 zookeeper
```
##### 2.1.3.3 åˆ›å»º zkDataç›®å½•
```
[root@systemhub511 zookeeper]# mkdir zkData
```
##### 2.1.3.4 é‡å‘½åé…ç½®æ–‡ä»¶
```
[root@systemhub511 conf]# mv zoo_sample.cfg zoo.cfg
[root@systemhub511 conf]# ll
total 12
-rw-rw-r--. 1 1001 1001  535 Mar 23  2017 configuration.xsl
-rw-rw-r--. 1 1001 1001 2161 Mar 23  2017 log4j.properties
-rw-rw-r--. 1 1001 1001  922 Mar 23  2017 zoo.cfg
[root@systemhub511 conf]# 
```
##### 2.1.3.5 é…ç½® zoo.cfgæ–‡ä»¶
> é…ç½®æ•°æ®ç¼“å­˜è·¯å¾„
```
[root@systemhub511 zkData]# pwd
/opt/module/zookeeper/zkData
[root@systemhub511 zookeeper]# cd conf/
[root@systemhub511 conf]# vim zoo.cfg
```
> ä¿®æ”¹é…ç½®ä¿¡æ¯
> 
> é…ç½®å‚æ•°è§£è¯» : `Server.A = B:C:D`
> Aè¡¨ç¤º æ ‡è¯†æœåŠ¡å™¨èŠ‚ç‚¹ID.
> Bè¡¨ç¤º æ ‡è¯†æœåŠ¡å™¨èŠ‚ç‚¹åç§°.
> Cè¡¨ç¤º æ ‡è¯†æœåŠ¡å™¨ä¸é›†ç¾¤ä¸­LeaderæœåŠ¡å™¨äº¤æ¢ä¿¡æ¯ç«¯å£.
> Dè¡¨ç¤º å¦‚é›†ç¾¤ä¸­LeaderæœåŠ¡å™¨å®•æœºæ—¶,éœ€è¦ä¸€ä¸ªç«¯å£æ¥é‡æ–°è¿›è¡Œé€‰ä¸¾,å¹¶é€‰å‡ºæ–°çš„Leader,è€Œè¿™ä¸ªç«¯å£å°±æ˜¯ç”¨æ¥æ‰§è¡Œé€‰ä¸¾æ—¶æœåŠ¡å™¨ç›¸äº’é€šä¿¡ç«¯å£.
``` dsconfig
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/opt/module/zookeeper/zkData
# the port at which the clients will connect
clientPort=2181

################### Cluster ######################
server.1=systemhub511:2888:3888
server.2=systemhub611:2888:3888
server.3=systemhub711:2888:3888
```

##### 2.1.3.6 Zookeeper é›†ç¾¤
> é›†ç¾¤æ¨¡å¼ä¸‹éœ€é…ç½®myidæ–‡ä»¶,æ­¤æ–‡ä»¶æ˜¯åœ¨dataDirç›®å½•ä¸‹,æ­¤æ–‡ä»¶æ•°æ®ä¸­å°±æ˜¯Aå€¼,Zookeeperå¯åŠ¨æ—¶è¯»å–æ­¤æ–‡ä»¶,å¾—åˆ°çš„æ•°æ®ä¸zoo.cfgä¸­é…ç½®ä¿¡æ¯æ¯”è¾ƒä»è€Œåˆ¤æ–­å“ªä¸ªServer.
> 
> åˆ›å»ºmyid
```
[root@systemhub511 zookeeper]# cd zkData/
[root@systemhub511 zkData]# touch myid
[root@systemhub511 zkData]# vim myid
```
> æ ¹æ®zoo.cfgæœåŠ¡èŠ‚ç‚¹é…ç½®å¯¹åº”id,åœ¨å½“å‰systemhub511æœåŠ¡å™¨,idå¦‚1
```
1
```
> é›†ç¾¤åˆ†å‘
```
[root@systemhub511 module]# scp -r zookeeper/ root@systemhub611:/opt/module/zookeeper/
README.txt 100% 1585     1.6KB/s   00:00 
zookeeper-3.4.10-recipes-election.jar 100%   13KB  13.4KB/s   00:00    
[root@systemhub511 module]# 
[root@systemhub511 module]# scp -r zookeeper/ root@systemhub711:/opt/module/zookeeper/
README.txt 100% 1585     1.6KB/s   00:00 
zookeeper-3.4.10-recipes-election.jar 100%   13KB  13.4KB/s   00:00    
[root@systemhub511 module]#  
```
> åˆ†åˆ«é…ç½®myidæ–‡ä»¶
```
[root@systemhub611 module]# cd zookeeper/zkData/
[root@systemhub611 zkData]# vim myid
```
```
2
```
```
[root@systemhub711 module]# cd zookeeper/zkData/
[root@systemhub711 zkData]# vim myid
```
```
3
```
> å¯åŠ¨ Zookeeper Server é›†ç¾¤
> 
> åº”äº‹å…ˆå…³é—­é›†ç¾¤é˜²ç«å¢™
> 
>  Start systemhub511 Server Node
```
[root@systemhub511 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... already running as process 31221.
[root@systemhub511 zookeeper]#
```
>  Start systemhub611 Server Node
```
[root@systemhub611 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... already running as process 29605.
[root@systemhub611 zookeeper]# 
```
>  Start systemhub711 Server Node
```
[root@systemhub711 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... already running as process 29650.
[root@systemhub711 zookeeper]#
```
> æŸ¥çœ‹ Zookeeper Server é›†ç¾¤çŠ¶æ€
> 
> systemhub511 Server Node Info
```
[root@systemhub511 zookeeper]# bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Mode: follower
[root@systemhub511 zookeeper]#
```
> systemhub611 Server Node Info
```
[root@systemhub611 zookeeper]# bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Mode: leader
[root@systemhub611 zookeeper]# 
```
> systemhub711 Server Node Info
```
[root@systemhub711 zookeeper]# bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Mode: follower
[root@systemhub711 zookeeper]#
```


### 2.2 Kafka é›†ç¾¤éƒ¨ç½²
#### 2.2.1 è§£å‹ kafka
```
[root@systemhub511 software]# tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/
kafka_2.11-0.11.0.0/
kafka_2.11-0.11.0.0/LICENSE
kafka_2.11-0.11.0.0/NOTICE
kafka_2.11-0.11.0.0/bin/
[root@systemhub511 software]#
```
#### 2.2.2 é‡å‘½åæ–‡ä»¶åç§°
```
[root@systemhub511 module]# mv kafka_2.11-0.11.0.0 kafka
[root@systemhub511 module]# ll
drwxr-xr-x.  6 root  root  4096 Jun 23  2017 kafka
[root@systemhub511 module]# 
```
#### 2.2.3 åˆ›å»ºlogsç›®å½•
```
[root@systemhub511 module]# cd kafka/
[root@systemhub511 kafka]# mkdir logs
```
#### 2.2.4 ä¿®æ”¹é…ç½®æ–‡ä»¶
```
[root@systemhub511 kafka]# cd config/
[root@systemhub511 config]# vim server.properties
```
> 
```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# see kafka.server.KafkaConfig for additional details and defaults

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

# Switch to enable topic deletion or not, default value is false
delete.topic.enable=true

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from 
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092

# Hostname and port the broker will advertise to producers and consumers. If not set, 
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://your.host.name:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs=/opt/module/kafka/logss

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=systemhub511:2181,systemhub611:2181,systemhub711:2181

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
```
#### 2.2.5 é…ç½®ç¯å¢ƒå˜é‡
```
[root@systemhub511 kafka]# pwd
/opt/module/kafka
[root@systemhub511 kafka]# vim /etc/profile
```
> é…ç½®ä¿¡æ¯
```
## SET KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin	
```
> æ›´æ–°é…ç½®æ–‡ä»¶
```
[root@systemhub511 kafka]# source /etc/profile
[root@systemhub511 kafka]# echo $KAFKA_HOME
/opt/module/kafka
```

#### 2.2.6 åˆ†å‘ kafkaé›†ç¾¤
```
[root@systemhub511 module]# scp -r kafka/ root@systemhub611:/opt/module/kafka/
connect-file-source.properties 100%  881     0.9KB/s   00:00    
.server.properties.swp 100%   16KB  16.0KB/s   00:00    
connect-file-sink.properties 100%  883     0.9KB/s   00:00
[root@systemhub511 module]#
```
```
[root@systemhub511 module]# scp -r kafka/ root@systemhub711:/opt/module/kafka/
connect-file-source.properties 100%  881     0.9KB/s   00:00    
.server.properties.swp 100%   16KB  16.0KB/s   00:00 
[root@systemhub511 module]#
```
> åˆ†åˆ«ä¿®æ”¹é…ç½®æ–‡ä»¶,broker.idä¸å¾—é‡å¤
```
[root@systemhub611 ~]# cd /opt/module/kafka/config/
[root@systemhub611 config]# vim server.properties
```
```
# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1
```
```
[root@systemhub711 ~]# cd /opt/module/kafka/config/
[root@systemhub711 config]# vim server.properties
```
```
# The id of the broker. This must be set to a unique integer for each broker.
broker.id=2
```

#### 2.2.7 å¯åŠ¨ kafkaé›†ç¾¤
> å› KafkaServerä¾èµ–äºZookeeperServer,æ‰€ä»¥è¦äº‹å…ˆå¼€å¯ZookeeperServer.
```
[root@systemhub511 kafka]#  bin/kafka-server-start.sh config/server.properties
[1] 23152
```

```
[root@systemhub611 kafka]#  bin/kafka-server-start.sh config/server.properties
[1] 22647
```

```
[root@systemhub711 kafka]# bin/kafka-server-start.sh config/server.properties &
[1] 22736
```
#### 2.2.8 å…³é—­ kafkaé›†ç¾¤
```
[root@systemhub511 kafka]# bin/kafka-server-stop.sh stop
```
```
[root@systemhub611 kafka]# bin/kafka-server-stop.sh stop
```
```
[root@systemhub711 kafka]# bin/kafka-server-stop.sh stop
```


### 2.3 Kafka æŒ‡ä»¤
#### 2.3.1 åˆ›å»ºTopic
> å‚æ•°è¯´æ˜ : 
> `--topic` å®šä¹‰topicåç§°
> `--replication-factor` å®šä¹‰å‰¯æœ¬æ•°é‡
> `--partitions`   å®šä¹‰åˆ†åŒºæ•°é‡
```
[root@systemhub511 kafka]# bin/kafka-topics.sh --create --zookeeper systemhub511:2181 -partitions 2 --replication-factor 2 --topic topic001
Created topic "topic001".
[root@systemhub511 kafka]# 
```
#### 2.3.2 æŸ¥çœ‹å½“å‰æœåŠ¡ä¸­æ‰€æœ‰Topic
```
[root@systemhub511 kafka]# bin/kafka-topics.sh --list --zookeeper systemhub511:2181
topic001
[root@systemhub511 kafka]# 
```

#### 2.3.3 ç”Ÿäº§è€…
```
[root@systemhub511 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>hello kafka
```
#### 2.3.4 æ¶ˆè´¹è€…
> è¿‡æ—¶ç‰ˆæœ¬è¯­æ³•
```
[root@systemhub711 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello kafka
```
> æ–°ç‰ˆæœ¬è¯­æ³•
``` dsconfig
[root@systemhub711 kafka]# bin/kafka-console-consumer.sh --bootstrap-server systemhub511:9092 --topic topic001 --from-beginning

hello kafka
```
#### 2.3.5 æŸ¥çœ‹Topicè¯¦æƒ…
```
[root@systemhub511 kafka]# bin/kafka-topics.sh --describe --zookeeper systemhub511 --topic topic001
Topic:topic001  PartitionCount:2        ReplicationFactor:2     Configs:
        Topic: topic001 Partition: 0    Leader: 1       Replicas: 1,0   Isr: 0,1
        Topic: topic001 Partition: 1    Leader: 2       Replicas: 2,1   Isr: 1,2
[root@systemhub511 kafka]# 
```
#### 2.3.6 åˆ é™¤Topic
> éœ€è¦åœ¨server.propertiesé…ç½®æ–‡ä»¶ä¸­è®¾ç½®`delete.topic.enable=true`
> å¦åˆ™åªæ˜¯æ ‡è®°åˆ é™¤æˆ–è€…ç›´æ¥é‡å¯
```
[root@systemhub511 kafka]# bin/kafka-topics.sh --delete --zookeeper systemhub511:2181 --topic topic001
Topic topic001 is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
[root@systemhub511 kafka]# 
```

### 2.4 Kafka é…ç½®ä¿¡æ¯
#### 2.4.1 Broker é…ç½®ä¿¡æ¯

| å±æ€§        |     é»˜è®¤å€¼ |   æè¿°   |
| :--------: | :--------:| :------: |
| broker.id     |   0/1/2/3/... |  å¿…å¡«å‚æ•°,brokerå”¯ä¸€æ ‡è¯†.  |
| log.dirs     |   /tmp/kafka-logs |  Kafkaæ•°æ®å­˜æ”¾çš„ç›®å½•,å¯ä»¥æŒ‡å®šå¤šä¸ªç›®å½•,ä¸­é—´ç”¨é€—å·åˆ†éš”,å½“æ–°partitionè¢«åˆ›å»ºæ—¶ä¼šè¢«å­˜æ”¾åˆ°å½“å‰å­˜æ”¾partitionæœ€å°‘çš„ç›®å½•.  |
| port     |   9092 |  BrokerServeræ¥å—å®¢æˆ·ç«¯è¿æ¥ç«¯å£å·  |
| zookeeper.connect     |   null |  Zookeeperè¿æ¥ä¸²æ ¼å¼ä¸º:`hostname1:port1,hostname2:port2,hostname3:port3`,æ³¨æ„,æ­¤é…ç½®å…è®¸æŒ‡å®šä¸€ä¸ªzookeeperè·¯å¾„æ¥å­˜æ”¾æ­¤kafkaé›†ç¾¤æ‰€æœ‰æ•°æ®,ä¸ºäº†ä¸å…¶ä»–åº”ç”¨é›†ç¾¤åŒºåˆ†å¼€,å»ºè®®åœ¨æ­¤é…ç½®ä¸­æŒ‡å®šæœ¬é›†ç¾¤å­˜æ”¾ç›®å½•æ ¼å¼ä¸º:`hostname1:port1,hostname2:port2,hostname3:port3/chroot/path`,è¦æ³¨æ„çš„æ˜¯,æ¶ˆè´¹è€…å‚æ•°è¦å’Œæ­¤å‚æ•°ä¸€è‡´.  |
| message.max.bytes     |   1000000 |  æœåŠ¡å™¨å¯ä»¥æ¥æ”¶åˆ°æœ€å¤§æ¶ˆæ¯çš„å¤§å°,æ³¨æ„æ­¤å‚æ•°è¦å’Œconsumerçš„`maximum.message.size`å€¼å¤§å°ä¸€è‡´,å¦åˆ™ä¼šå› ä¸ºç”Ÿäº§è€…ç”Ÿäº§æ¶ˆæ¯å¤ªå¤§å¯¼è‡´æ¶ˆè´¹è€…æ— æ³•æ¶ˆè´¹.  |
| num.io.threads     |   8 |  æœåŠ¡å™¨ç”¨æ¥æ‰§è¡Œè¯»å†™è¯·æ±‚IOçº¿ç¨‹æ•°,æ­¤å‚æ•°æ•°é‡è‡³å°‘è¦ç­‰äºæœåŠ¡å™¨ä¸Šç£ç›˜æ•°é‡.  |
| queued.max.requests     |   500 |  I/Oçº¿ç¨‹å¯ä»¥å¤„ç†è¯·æ±‚é˜Ÿåˆ—å¤§å°,è‹¥å®é™…è¯·æ±‚æ•°è¶…è¿‡æ­¤å¤§å°,ç½‘ç»œçº¿ç¨‹å°†åœæ­¢æ¥æ”¶æ–°è¯·æ±‚.  |
| socket.send.buffer.bytes     |   100 * 1024 |  The SO_SNDBUFF buffer    the server prefers for socket connections  |
| socket.receive.buffer.bytes.     |   field2 |  field3  |
| field1     |   100 * 1024 |  The SO_RCVBUFF buffer the server prefers for socket connections.  |
| socket.request.max.bytes     |   100 * 1024 * 1024 |  æœåŠ¡å™¨å…è®¸è¯·æ±‚æœ€å¤§å€¼,ç”¨æ¥é˜²æ­¢å†…å­˜æº¢å‡º,å…¶å€¼åº”è¯¥å°äºJava heap size.  |
| num.partitions     |   1 |  é»˜è®¤partitionæ•°é‡,å¦‚æœtopicåœ¨åˆ›å»ºæ—¶æ²¡æœ‰æŒ‡å®špartitionæ•°é‡,é»˜è®¤ä½¿ç”¨æ­¤å€¼,å»ºè®®æ”¹ä¸º5.  |
| log.segment.bytes     |   1024 * 1024 * 1024 |  Segmentæ–‡ä»¶å¤§å°,è¶…è¿‡æ­¤å€¼å°†ä¼šè‡ªåŠ¨æ–°å»ºä¸€ä¸ªsegment,æ­¤å€¼å¯ä»¥è¢«topicçº§åˆ«å‚æ•°è¦†ç›–.  |
| log.roll.{ms,hours}     |   24 * 7 hours |  æ–°å»ºsegmentæ–‡ä»¶æ—¶é—´,æ­¤å€¼å¯ä»¥è¢«topicçº§åˆ«å‚æ•°è¦†ç›–.  |
| log.retention.{ms,minutes,hours}     |   7 days |  Kafka segment logä¿å­˜å‘¨æœŸ,ä¿å­˜å‘¨æœŸè¶…è¿‡æ­¤æ—¶é—´æ—¥å¿—å°±ä¼šè¢«åˆ é™¤,æ­¤å‚æ•°å¯ä»¥è¢«topicçº§åˆ«å‚æ•°è¦†ç›–,æ•°æ®é‡å¤§æ—¶å»ºè®®å‡å°æ­¤å€¼.  |
| log.retention.bytes     |   -1 |  æ¯ä¸ªpartitionæœ€å¤§å®¹é‡,è‹¥æ•°æ®é‡è¶…è¿‡æ­¤å€¼,partitionæ•°æ®å°†ä¼šè¢«åˆ é™¤,æ³¨æ„è¿™ä¸ªå‚æ•°æ§åˆ¶æ¯ä¸ªpartitionè€Œä¸æ˜¯topic,æ­¤å‚æ•°å¯ä»¥è¢«logçº§åˆ«å‚æ•°è¦†ç›–.  |
| log.retention.check.interval.ms     |   5 minutes |  åˆ é™¤ç­–ç•¥çš„æ£€æŸ¥å‘¨æœŸ  |
| auto.create.topics.enable     |   true |  è‡ªåŠ¨åˆ›å»ºtopicå‚æ•°,å»ºè®®æ­¤å€¼è®¾ç½®ä¸ºfalse,ä¸¥æ ¼æ§åˆ¶topicç®¡ç†,é˜²æ­¢ç”Ÿäº§è€…é”™å†™topic.  |
| default.replication.factor     |   1 |  é»˜è®¤å‰¯æœ¬æ•°é‡,å»ºè®®æ”¹ä¸º2  |
| replica.lag.time.max.ms     |   10000 |  åœ¨æ­¤çª—å£æ—¶é—´å†…æ²¡æœ‰æ”¶åˆ°follower fetchè¯·æ±‚,leaderä¼šå°†å…¶ä»ISR(in-syncreplicas)ä¸­ç§»é™¤.  |
| replica.lag.max.messages     |   4000 |  å¦‚æœreplicaèŠ‚ç‚¹è½åleaderèŠ‚ç‚¹æ­¤å€¼å¤§å°æ¶ˆæ¯æ•°é‡,leaderèŠ‚ç‚¹å°±ä¼šå°†å…¶ä»ISRä¸­ç§»é™¤.  |
| replica.socket.timeout.ms     |   30 * 1000 |  replicaå‘leaderå‘é€è¯·æ±‚çš„è¶…æ—¶æ—¶é—´.  |
| replica.socket.receive.buffer.bytes     |   64 * 1024 |  The socket  receive buffer for network requests tothe leader for replicating data.  |
| replica.fetch.max.bytes     |   1024 * 1024 |  The number of byes   of messages to attempt to fetch for each partition in the fetch requests  the replicas send to the leader.  |
| replica.fetch.wait.max.ms     |   500 |  The maximum amount of time  towait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.  |
| num.replica.fetchers     |   1 |  Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of  I/O parallelism in thefollower broker  |
| fetch.purgatory.purge.interval.requests     |   1000 |  The purge    interval (in number of requests) of the fetch request purgatory.  |
| zookeeper.session.timeout.ms     |   6000 |  ZooKeeper sessionè¶…æ—¶æ—¶é—´,å¦‚æœåœ¨æ­¤æ—¶é—´å†…serveræ²¡æœ‰å‘zookeeperå‘é€å¿ƒè·³,zookeeperå°±ä¼šè®¤ä¸ºæ­¤èŠ‚ç‚¹å·²æŒ‚æ‰,æ­¤å€¼å¤ªä½å¯¼è‡´èŠ‚ç‚¹å®¹æ˜“è¢«æ ‡è®°æ­»äº¡,è‹¥å¤ªé«˜ä¼šå¯¼è‡´å¤ªè¿Ÿå‘ç°èŠ‚ç‚¹æ­»äº¡. |
| zookeeper.connection.timeout.ms     |   6000 |  å®¢æˆ·ç«¯è¿æ¥zookeeperè¶…æ—¶æ—¶é—´.  |
| controlled.shutdown.enable     |   true |  å…è®¸broker shutdown,å¦‚æœå¯ç”¨brokeråœ¨å…³é—­ä¹‹å‰ä¼šæŠŠå®ƒä¸Šé¢æ‰€æœ‰leadersè½¬ç§»åˆ°å…¶å®ƒbrokersä¸Š,å»ºè®®å¯ç”¨å¢åŠ é›†ç¾¤ç¨³å®šæ€§.  |
| auto.leader.rebalance.enable     |   true |  If this is enabled the   controller will automatically try to balance leadership for partitions   among the brokers by periodically returning leadership to the â€œpreferredâ€  replica for each partition if it is available.  |
| leader.imbalance.per.broker.percentage     |   10 |  The percentage   of leader imbalance allowed per broker. The controller will rebalance  leadership if this ratio goes above the configured value per broker  |
| delete.topic.enable     |   false |  å¯ç”¨deletetopicå‚æ•°,å»ºè®®è®¾ç½®ä¸ºtrue.  |


#### 2.4.2 Producer é…ç½®ä¿¡æ¯

| å±æ€§        |     é»˜è®¤å€¼ |   æè¿°   |
| :--------: | :--------:| :------: |
| metadata.broker.list     |   |  å¯åŠ¨æ—¶produceræŸ¥è¯¢brokersåˆ—è¡¨,å¯ä»¥æ˜¯é›†ç¾¤ä¸­æ‰€æœ‰brokersä¸€ä¸ªå­é›†,æ³¨æ„è¿™ä¸ªå‚æ•°åªæ˜¯ç”¨æ¥è·å–topicå…ƒä¿¡æ¯,producerä¼šä»å…ƒä¿¡æ¯ä¸­æŒ‘é€‰åˆé€‚çš„brokerå¹¶ä¸ä¹‹å»ºç«‹socketè¿æ¥,æ ¼å¼ä¸º:`host1:port1,host2:port2`  |
| request.timeout.ms     |   10000 |  Brokerç­‰å¾…ackè¶…æ—¶æ—¶é—´,è‹¥ç­‰å¾…æ—¶é—´è¶…è¿‡æ­¤å€¼,ä¼šè¿”å›å®¢æˆ·ç«¯é”™è¯¯ä¿¡æ¯.  |
| producer.type     |   sync |  åŒæ­¥å¼‚æ­¥æ¨¡å¼,asyncè¡¨ç¤ºå¼‚æ­¥,syncè¡¨ç¤ºåŒæ­¥,å¦‚æœè®¾ç½®æˆå¼‚æ­¥æ¨¡å¼,å¯ä»¥å…è®¸ç”Ÿäº§è€…ä»¥batchå½¢å¼pushæ•°æ®,è¿™æ ·ä¼šæå¤§æé«˜brokeræ€§èƒ½,æ¨èè®¾ç½®ä¸ºå¼‚æ­¥.  |
| serializer.class     |   kafka.serializer.DefaultEncoder |  åºåˆ—å·ç±»,é»˜è®¤åºåˆ—åŒ–ç±»å‹ä¸ºbyte[]  |
| key.serializer.class     |   |  Keyåºåˆ—åŒ–ç±»,é»˜è®¤åŒä¸Š  |
| partitioner.class     |   kafka.producer.DefaultPartitioner |  Partitionç±»,é»˜è®¤å¯¹keyè¿›è¡Œhash.  |
| compression.codec     |   none |  æŒ‡å®šproduceræ¶ˆæ¯å‹ç¼©æ ¼å¼,å¯é€‰å‚æ•°ä¸ºï¼šnone / gzip / snappy  |
| compressed.topics     |   null |  å¯ç”¨å‹ç¼©topicåç§°,è‹¥ä¸Šé¢å‚æ•°é€‰æ‹©äº†ä¸€ä¸ªå‹ç¼©æ ¼å¼,é‚£ä¹ˆå‹ç¼©ä»…å¯¹æœ¬å‚æ•°æŒ‡å®šçš„topicæœ‰æ•ˆ,è‹¥æœ¬å‚æ•°ä¸ºç©ºåˆ™å¯¹æ‰€æœ‰topicæœ‰æ•ˆ.  |
| message.send.max.retries     |   3 |  Producerå‘é€å¤±è´¥æ—¶é‡è¯•æ¬¡æ•°,è‹¥ç½‘ç»œå‡ºç°é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´ä¸æ–­é‡è¯•.  |
| queue.buffering.max.ms     |   5000 |  å¯ç”¨å¼‚æ­¥æ¨¡å¼æ—¶,producerç¼“å­˜æ¶ˆæ¯æ—¶é—´,æ¯”å¦‚è®¾ç½®æˆ1000æ—¶,å®ƒä¼šç¼“å­˜1ç§’æ•°æ®å†ä¸€æ¬¡å‘é€å‡ºå»,è¿™æ ·å¯ä»¥æå¤§å¢åŠ brokerååé‡,ä½†ä¹Ÿä¼šé€ æˆæ—¶æ•ˆæ€§é™ä½.  |
| queue.buffering.max.messages     |   10000 |  é‡‡ç”¨å¼‚æ­¥æ¨¡å¼æ—¶producer  bufferé˜Ÿåˆ—é‡Œæœ€å¤§ç¼“å­˜æ¶ˆæ¯æ•°é‡,å¦‚æœè¶…è¿‡è¿™ä¸ªæ•°å€¼,producerå°±ä¼šé˜»å¡æˆ–è€…ä¸¢æ‰æ¶ˆæ¯.  |
| queue.enqueue.timeout.ms     |   -1 |  å½“è¾¾åˆ°ä¸Šé¢å‚æ•°å€¼æ—¶produceré˜»å¡ç­‰å¾…æ—¶é—´,å¦‚æœå€¼è®¾ç½®ä¸º0,bufferé˜Ÿåˆ—æ»¡æ—¶producerä¸ä¼šé˜»å¡,æ¶ˆæ¯ç›´æ¥è¢«ä¸¢æ‰,è‹¥å€¼è®¾ç½®ä¸º-1,producerä¼šè¢«é˜»å¡ä¸ä¼šä¸¢æ¶ˆæ¯.  |
| batch.num.messages     |   200 |  ç”¨å¼‚æ­¥æ¨¡å¼æ—¶,ä¸€ä¸ªbatchç¼“å­˜æ¶ˆæ¯æ•°é‡,è¾¾åˆ°è¿™ä¸ªæ•°é‡å€¼æ—¶produceræ‰ä¼šå‘é€æ¶ˆæ¯.  |
| send.buffer.bytes     |   100 * 1024 |  Socket write buffer size  |


#### 2.4.3 Consumer é…ç½®ä¿¡æ¯
| å±æ€§        |     é»˜è®¤å€¼ |   æè¿°   |
| :--------: | :--------:| :------: |
| group.id     |   |  Consumerç»„ID,ç›¸åŒgoup.idçš„consumerå±äºåŒä¸€ä¸ªç»„.  |
| zookeeper.connect     |   |  Consumerçš„zookeeperè¿æ¥ä¸²,è¦å’Œbrokerçš„é…ç½®ä¸€è‡´. |
| consumer.id     |   null |  å¦‚æœä¸è®¾ç½®ä¼šè‡ªåŠ¨ç”Ÿæˆ.  |
| socket.timeout.ms     |   30 * 1000 |  ç½‘ç»œè¯·æ±‚socketè¶…æ—¶æ—¶é—´,å®é™…è¶…æ—¶æ—¶é—´ç”±`max.fetch.wait` + `socket.timeout.ms` ç¡®å®š. |
| fetch.message.max.bytes     |   1024 * 1024 |  æŸ¥è¯¢topic-partitionæ—¶å…è®¸çš„æœ€å¤§æ¶ˆæ¯å¤§å°,consumerä¼šä¸ºæ¯ä¸ªpartitionç¼“å­˜æ­¤å¤§å°æ¶ˆæ¯åˆ°å†…å­˜,å› æ­¤è¿™ä¸ªå‚æ•°å¯ä»¥æ§åˆ¶consumerå†…å­˜ä½¿ç”¨é‡,è¿™ä¸ªå€¼åº”è¯¥è‡³å°‘æ¯”serverå…è®¸æœ€å¤§æ¶ˆæ¯å¤§å°å¤§,ä»¥å…producerå‘é€æ¶ˆæ¯å¤§äºconsumerå…è®¸æ¶ˆæ¯.  |
| auto.commit.enable     |   true |  å¦‚æœæ­¤å€¼è®¾ç½®ä¸ºtrue,consumerä¼šå‘¨æœŸæ€§æŠŠå½“å‰æ¶ˆè´¹offsetå€¼ä¿å­˜åˆ°zookeeper,å½“consumerå¤±è´¥é‡å¯ä¹‹åå°†ä¼šä½¿ç”¨æ­¤å€¼ä½œä¸ºæ–°å¼€å§‹æ¶ˆè´¹çš„å€¼.  |
| auto.commit.interval.ms     |   60 * 1000 |  Consumeræäº¤offsetå€¼åˆ°zookeeperå‘¨æœŸ.  |
| queued.max.message.chunks     |   2 |  ç”¨æ¥è¢«consumeræ¶ˆè´¹message chunks æ•°é‡,æ¯ä¸ªchunkå¯ä»¥ç¼“å­˜`fetch.message.max.bytes`å¤§å°æ•°æ®é‡.  |
| auto.commit.interval.ms     |   60 * 1000 |  Consumeræäº¤offsetå€¼åˆ°zookeeperå‘¨æœŸ.  |
| queued.max.message.chunks     |   2 |  ç”¨æ¥è¢«consumeræ¶ˆè´¹message chunks æ•°é‡,æ¯ä¸ªchunkå¯ä»¥ç¼“å­˜`fetch.message.max.bytes`å¤§å°æ•°æ®é‡.  |
| consumer.timeout.ms     |   -1 |  è‹¥åœ¨æŒ‡å®šæ—¶é—´å†…æ²¡æœ‰æ¶ˆæ¯æ¶ˆè´¹,consumerå°†ä¼šæŠ›å‡ºå¼‚å¸¸.  |


## 3. Kafka å·¥ä½œæµåˆ†æ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_003.jpg)

### 3.1 Kafka ç”Ÿäº§è¿‡ç¨‹
#### 3.1.1 å†™å…¥æ–¹å¼
> produceré‡‡ç”¨æ¨(push)æ¨¡å¼å°†æ¶ˆæ¯å‘å¸ƒåˆ°broker,æ¯æ¡æ¶ˆæ¯éƒ½è¢«è¿½åŠ (append)åˆ°åˆ†åŒº(patition)ä¸­,å±äºé¡ºåºå†™ç£ç›˜(é¡ºåºå†™ç£ç›˜æ•ˆç‡æ¯”éšæœºå†™å†…å­˜è¦é«˜,ä¿éšœkafkaååç‡).

#### 3.1.2 åˆ†åŒº (Partition)
> æ¶ˆæ¯å‘é€æ—¶éƒ½è¢«å‘é€åˆ°ä¸€ä¸ªtopic,å…¶æœ¬è´¨å°±æ˜¯ä¸€ä¸ªç›®å½•,è€Œtopicæ˜¯ç”±ä¸€äº›Partition Logs(åˆ†åŒºæ—¥å¿—)ç»„æˆ,å…¶ç»„ç»‡ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º

![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_004.jpg)
> æ¯ä¸ªPartitionä¸­çš„æ¶ˆæ¯éƒ½æ˜¯æœ‰åº,ç”Ÿäº§æ¶ˆæ¯è¢«ä¸æ–­è¿½åŠ åˆ°Partition logä¸Š,å…¶ä¸­æ¯ä¸€ä¸ªæ¶ˆæ¯éƒ½è¢«èµ‹äºˆäº†ä¸€ä¸ªå”¯ä¸€çš„offsetå€¼.
> 
> `åˆ†åŒºåŸå› `
> 1.æ–¹ä¾¿åœ¨é›†ç¾¤ä¸­æ‰©å±•,æ¯ä¸ªPartitionå¯ä»¥é€šè¿‡è°ƒæ•´ä»¥é€‚åº”å®ƒæ‰€åœ¨æœºå™¨,è€Œä¸€ä¸ªtopicåˆå¯ä»¥æœ‰å¤šä¸ªPartitionç»„æˆ,å› æ­¤æ•´ä¸ªé›†ç¾¤å°±å¯ä»¥é€‚åº”ä»»æ„å¤§å°æ•°æ®äº†.
> 
> 2.å› ä¸ºä»¥Partitionä¸ºå•ä½è¯»å†™,æ‰€ä»¥å¯ä»¥æé«˜å¹¶å‘.
> 
> `åˆ†åŒºè§„åˆ™`
> æŒ‡å®špatition,åˆ™ç›´æ¥ä½¿ç”¨.
> æœªæŒ‡å®špatitionä½†æŒ‡å®škey,é€šè¿‡å¯¹keyçš„valueè¿›è¡Œhashå‡ºä¸€ä¸ªpatition.
> patitionå’Œkeyéƒ½æœªæŒ‡å®š,ä½¿ç”¨è½®è¯¢é€‰å‡ºä¸€ä¸ªpatition.
> 
> DefaultPartitionerç±»
``` java
    public int partition(String topic, Object key, byte[] keyBytes,
        Object value, byte[] valueBytes, Cluster cluster) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        intnumPartitions = partitions.size();

        if (keyBytes == null) {
            intnextValue = nextValue(topic);

            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);

            if (availablePartitions.size() > 0) {
                intpart = Utils.toPositive(nextValue) % availablePartitions.size();

                return availablePartitions.get(part).partition();
            } else {
                // no partitions are available, give a non-available partition
                return Utils.toPositive(nextValue) % numPartitions;
            }
        } else {
            // hash the keyBytes to choose a partition
            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
        }
    }
```

#### 3.1.3 å‰¯æœ¬ (Replication)
> åŒä¸€ä¸ªpartitionå¯èƒ½ä¼šæœ‰å¤šä¸ªreplication(å¯¹åº”server.propertiesé…ç½®ä¸­çš„`default.replication.factor=N`)
> 
> æ²¡æœ‰replicationæƒ…å†µä¸‹,ä¸€æ—¦brokerå®•æœº,å…¶ä¸Šæ‰€æœ‰patitionæ•°æ®éƒ½ä¸å¯è¢«æ¶ˆè´¹.
> 
> åŒæ—¶producerä¹Ÿä¸èƒ½å†å°†æ•°æ®å­˜äºå…¶ä¸Šçš„patition,å¼•å…¥replicationä¹‹å,åŒä¸€ä¸ªpartitionå¯èƒ½ä¼šæœ‰å¤šä¸ªreplication,è€Œè¿™æ—¶éœ€è¦åœ¨è¿™äº›replicationä¹‹é—´é€‰å‡ºä¸€ä¸ªleader,producerå’Œconsumeråªä¸è¿™ä¸ªleaderäº¤äº’,å…¶å®ƒreplicationä½œä¸ºfollowerä»leaderä¸­å¤åˆ¶æ•°æ®.


#### 3.1.4 å†™å…¥æµç¨‹
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_005.jpg)
> 1.producerå…ˆä»zookeeper `/brokers/.../state`èŠ‚ç‚¹æ‰¾åˆ°è¯¥partitionçš„leader.
> 
> 2.producerå°†æ¶ˆæ¯å‘é€ç»™è¯¥leader.
> 
> 3.leaderå°†æ¶ˆæ¯å†™å…¥æœ¬åœ°log.
> 
> 4.followersä»leader pullæ¶ˆæ¯,å†™å…¥æœ¬åœ°logåå‘leaderå‘é€ACK.
> 
> 5.leaderæ”¶åˆ°æ‰€æœ‰ISRä¸­çš„replicationçš„ACKå,å¢åŠ HW(high watermark,æœ€åcommit çš„offset)å‘producerå‘é€ACK


### 3.2 Broker å­˜å‚¨è¿‡ç¨‹
#### 3.2.1 å­˜å‚¨æ–¹å¼
> ç‰©ç†ä¸ŠæŠŠtopicåˆ†æˆä¸€ä¸ªæˆ–å¤šä¸ªpatition(å¯¹åº”server.propertiesä¸­`num.partitions=3`é…ç½®),æ¯ä¸ªpatitionç‰©ç†ä¸Šå¯¹åº”ä¸€ä¸ªæ–‡ä»¶å¤¹(è¯¥æ–‡ä»¶å¤¹å­˜å‚¨è¯¥patitionçš„æ‰€æœ‰æ¶ˆæ¯å’Œç´¢å¼•æ–‡ä»¶).
```
[root@systemhub711 logss]# ll
total 88
-rw-r--r-- 1 root root    4 Apr 17 22:37 cleaner-offset-checkpoint
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-11
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-14
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-17
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-2
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-20
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-23
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-26
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-29
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-32
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-35
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-38
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-41
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-44
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-47
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-5
drwxr-xr-x 2 root root 4096 Apr 20 15:23 __consumer_offsets-8
-rw-r--r-- 1 root root    4 Apr 20 18:59 log-start-offset-checkpoint
-rw-r--r-- 1 root root   54 Apr 17 14:16 meta.properties
-rw-r--r-- 1 root root  399 Apr 20 18:59 recovery-point-offset-checkpoint
-rw-r--r-- 1 root root  399 Apr 20 19:00 replication-offset-checkpoint
drwxr-xr-x 2 root root 4096 Apr 20 15:28 topic001-0
[root@systemhub711 logss]# cd topic001-0/
[root@systemhub711 topic001-0]# ll
total 8
-rw-r--r-- 1 root root 10485760 Apr 20 15:23 00000000000000000000.index
-rw-r--r-- 1 root root       80 Apr 20 15:28 00000000000000000000.log
-rw-r--r-- 1 root root 10485756 Apr 20 15:23 00000000000000000000.timeindex
-rw-r--r-- 1 root root        8 Apr 20 15:28 leader-epoch-checkpoint
[root@systemhub711 topic001-0]# 
```
#### 3.2.2 å­˜å‚¨ç­–ç•¥
> æ— è®ºæ¶ˆæ¯æ˜¯å¦è¢«æ¶ˆè´¹,kafkaéƒ½ä¼šä¿ç•™æ‰€æœ‰æ¶ˆæ¯,æœ‰ä¸¤ç§ç­–ç•¥å¯ä»¥åˆ é™¤æ—§æ•°æ®:
> 1.åŸºäºæ—¶é—´ : `log.retention.hours=168`
> 2.åŸºäºå¤§å° : `log.retention.bytes=1073741824`
> éœ€è¦æ³¨æ„çš„æ˜¯,å› ä¸ºKafkaè¯»å–ç‰¹å®šæ¶ˆæ¯çš„æ—¶é—´å¤æ‚åº¦ä¸ºO(1),å³ä¸æ–‡ä»¶å¤§å°æ— å…³,æ‰€ä»¥è¿™é‡Œåˆ é™¤è¿‡æœŸæ–‡ä»¶ä¸æé«˜Kafkaæ€§èƒ½æ— å…³.

#### 3.2.3 Zookeeper å­˜å‚¨ç»“æ„
> æ³¨æ„ï¼šproducerä¸åœ¨zkä¸­æ³¨å†Œ,è€Œæ¶ˆè´¹è€…åœ¨zkä¸­æ³¨å†Œ.
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_006.jpg)


### 3.3 Kafka æ¶ˆè´¹è¿‡ç¨‹
> Kafkaæä¾›ä¸¤å¥—Consumer API : é«˜çº§Consumer API å’Œ ä½çº§ API
#### 3.3.1 é«˜çº§ API
> `é«˜çº§API ä¼˜ç‚¹`
- é«˜çº§APIç¼–å†™èµ·æ¥éå¸¸ç®€å•. 
- ä¸éœ€è¦è‡ªè¡Œç®¡ç†offset,ç³»ç»Ÿé€šè¿‡zookeeperè‡ªè¡Œç®¡ç†.
- ä¸éœ€è¦ç®¡ç†åˆ†åŒº,å‰¯æœ¬ç­‰æƒ…å†µ,ç³»ç»Ÿè‡ªåŠ¨ç®¡ç†.
- æ¶ˆè´¹è€…æ–­çº¿ä¼šè‡ªåŠ¨æ ¹æ®ä¸Šä¸€æ¬¡è®°å½•åœ¨zookeeperä¸­çš„offsetè·å–æ•°æ®(é»˜è®¤è®¾ç½®1åˆ†é’Ÿæ›´æ–°ä¸€ä¸‹zookeeperä¸­å­˜çš„offset).
- å¯ä»¥ä½¿ç”¨groupæ¥åŒºåˆ†å¯¹åŒä¸€ä¸ªtopicä¸åŒç¨‹åºè®¿é—®åˆ†ç¦»å¼€æ¥(ä¸åŒçš„groupè®°å½•ä¸åŒçš„offset,è¿™æ ·ä¸åŒç¨‹åºè¯»å–åŒä¸€ä¸ªtopicæ‰ä¸ä¼šå› ä¸ºoffsetäº’ç›¸å½±å“).

> `é«˜çº§API ç¼ºç‚¹`
- å¯¹äºæŸäº›ç‰¹æ®Šéœ€æ±‚æ¥è¯´,ä¸èƒ½è‡ªè¡Œæ§åˆ¶offset.
- ä¸èƒ½ç»†åŒ–æ§åˆ¶å¦‚åˆ†åŒºã€å‰¯æœ¬ã€zkç­‰.

#### 3.3.2 ä½çº§ API
> `ä½çº§API ä¼˜ç‚¹`
- èƒ½è®©å¼€å‘è€…è‡ªæ§offset.
- è‡ªè¡Œæ§åˆ¶è¿æ¥åˆ†åŒº,å¯¹åˆ†åŒºè‡ªå®šä¹‰è¿›è¡Œè´Ÿè½½å‡è¡¡.
- å¯¹zookeeperä¾èµ–æ€§é™ä½ (å¦‚ : offsetä¸ä¸€å®šè¦é zkå­˜å‚¨,è‡ªè¡Œå­˜å‚¨offsetå³å¯,æ¯”å¦‚å­˜åœ¨æ–‡ä»¶æˆ–è€…å†…å­˜ä¸­).

> `ä½çº§API ç¼ºç‚¹`
- æ“ä½œå¤ªè¿‡å¤æ‚,éœ€è¦è‡ªè¡Œæ§åˆ¶offset,è¿æ¥åˆ†åŒº,æ‰¾åˆ°åˆ†åŒºleaderç­‰.


#### 3.3.3 æ¶ˆè´¹è€…ç»„
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_007.jpg)
> æ¶ˆè´¹è€…æ˜¯ä»¥consumer groupæ¶ˆè´¹è€…ç»„çš„æ–¹å¼å·¥ä½œ,ç”±ä¸€ä¸ªæˆ–è€…å¤šä¸ªæ¶ˆè´¹è€…ç»„æˆä¸€ä¸ªç»„ï¼Œå…±åŒæ¶ˆè´¹ä¸€ä¸ªtopic.
> 
> æ¯ä¸ªåˆ†åŒºåœ¨åŒä¸€æ—¶é—´åªèƒ½ç”±groupä¸­çš„ä¸€ä¸ªæ¶ˆè´¹è€…è¯»å–,ä½†æ˜¯å¤šä¸ªgroupå¯ä»¥åŒæ—¶æ¶ˆè´¹è¿™ä¸ªpartition.
> 
> å¦‚å›¾æ‰€ç¤º,ä¸€ä¸ªç”±ä¸‰ä¸ªæ¶ˆè´¹è€…ç»„æˆçš„group,æœ‰ä¸€ä¸ªæ¶ˆè´¹è€…è¯»å–ä¸»é¢˜ä¸­ä¸¤ä¸ªåˆ†åŒº,å¦å¤–ä¸¤ä¸ªåˆ†åˆ«è¯»å–ä¸€ä¸ªåˆ†åŒº,æŸä¸ªæ¶ˆè´¹è€…è¯»å–æŸä¸ªåˆ†åŒº,ä¹Ÿå¯ä»¥å«åšæŸä¸ªæ¶ˆè´¹è€…æ˜¯æŸä¸ªåˆ†åŒºçš„æ‹¥æœ‰è€….
> 
> åœ¨è¿™ç§æƒ…å†µä¸‹,æ¶ˆè´¹è€…å¯ä»¥é€šè¿‡æ°´å¹³æ‰©å±•æ–¹å¼åŒæ—¶è¯»å–å¤§é‡æ¶ˆæ¯,å¦å¤–,å¦‚æœä¸€ä¸ªæ¶ˆè´¹è€…å¤±è´¥äº†,é‚£ä¹ˆå…¶ä»–groupæˆå‘˜ä¼šè‡ªåŠ¨è´Ÿè½½å‡è¡¡è¯»å–ä¹‹å‰å¤±è´¥æ¶ˆè´¹è€…è¯»å–åˆ†åŒº.


#### 3.3.4 æ¶ˆè´¹æ–¹å¼
> consumeré‡‡ç”¨pull(æ‹‰)æ¨¡å¼ä»brokerä¸­è¯»å–æ•°æ®.
> 
> push(æ¨)æ¨¡å¼å¾ˆéš¾é€‚åº”æ¶ˆè´¹é€Ÿç‡ä¸åŒæ¶ˆè´¹è€…,å› ä¸ºæ¶ˆæ¯å‘é€é€Ÿç‡æ˜¯ç”±brokerå†³å®š.
> å®ƒç›®æ ‡æ˜¯å°½å¯èƒ½ä»¥æœ€å¿«é€Ÿåº¦ä¼ é€’æ¶ˆæ¯,ä½†æ˜¯è¿™æ ·å¾ˆå®¹æ˜“é€ æˆconsumeræ¥ä¸åŠå¤„ç†æ¶ˆæ¯,å…¸å‹è¡¨ç°å°±æ˜¯æ‹’ç»æœåŠ¡ä»¥åŠç½‘ç»œæ‹¥å¡,è€Œpullæ¨¡å¼åˆ™å¯ä»¥æ ¹æ®consumeræ¶ˆè´¹èƒ½åŠ›ä»¥é€‚å½“é€Ÿç‡æ¶ˆè´¹æ¶ˆæ¯.
> 
> å¯¹äºKafkaè€Œè¨€,pullæ¨¡å¼æ›´åˆé€‚,å®ƒå¯ç®€åŒ–brokerçš„è®¾è®¡,consumerå¯è‡ªä¸»æ§åˆ¶æ¶ˆè´¹æ¶ˆæ¯é€Ÿç‡,åŒæ—¶consumerå¯ä»¥è‡ªå·±æ§åˆ¶æ¶ˆè´¹æ–¹å¼â€”â€”å³å¯æ‰¹é‡æ¶ˆè´¹ä¹Ÿå¯é€æ¡æ¶ˆè´¹,åŒæ—¶è¿˜èƒ½é€‰æ‹©ä¸åŒæäº¤æ–¹å¼ä»è€Œå®ç°ä¸åŒä¼ è¾“è¯­ä¹‰.
> 
> pullæ¨¡å¼ä¸è¶³ä¹‹å¤„æ˜¯,å¦‚æœkafkaæ²¡æœ‰æ•°æ®,æ¶ˆè´¹è€…å¯èƒ½ä¼šé™·å…¥å¾ªç¯ä¸­,ä¸€ç›´ç­‰å¾…æ•°æ®åˆ°è¾¾.
> 
> ä¸ºäº†é¿å…è¿™ç§æƒ…å†µ,åœ¨æ‹‰è¯·æ±‚ä¸­æœ‰å‚æ•°,å…è®¸æ¶ˆè´¹è€…è¯·æ±‚åœ¨ç­‰å¾…æ•°æ®åˆ°è¾¾â€œé•¿è½®è¯¢â€ä¸­è¿›è¡Œé˜»å¡(å¹¶ä¸”å¯é€‰åœ°ç­‰å¾…åˆ°ç»™å®šçš„å­—èŠ‚æ•°,ä»¥ç¡®ä¿å¤§çš„ä¼ è¾“å¤§å°)

#### 3.3.5 æ¶ˆè´¹è€…ç»„æ¡ˆä¾‹
> æµ‹è¯•åŒä¸€ä¸ªæ¶ˆè´¹è€…ç»„ä¸­çš„æ¶ˆè´¹è€…,åŒä¸€æ—¶åˆ»åªèƒ½æœ‰ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹.
> 
> åœ¨systemhub511ã€systemhub611æœåŠ¡å™¨ä¸­ä¿®æ”¹`/opt/module/kafka/config/consumer.properties`é…ç½®æ–‡ä»¶ä¸­çš„`group.id`å±æ€§ä¸ºä»»æ„ç»„å.
```
[root@systemhub511 module]# cd kafka/
[root@systemhub511 kafka]# vim config/consumer.properties
```
```
#consumer group id
group.id=systemhub611
```
```
[root@systemhub611 module]# cd kafka/
[root@systemhub611 kafka]# vim config/consumer.properties
```
```
#consumer group id
group.id=systemhub611
```
> åˆ†åˆ«å¯åŠ¨æ¶ˆè´¹è€…
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001 --consumer.config config/consumer.properties
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
[067] WARN [systemhub611_systemhub611-1555744110617-1d4d406d], no brokers found when trying to rebalance. (kafka.consumer.ZookeeperConsumerConnector)
```
```
[root@systemhub611 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001 --consumer.config config/consumer.properties
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
```
> åœ¨systemhub711æœåŠ¡å™¨ å¯åŠ¨ç”Ÿäº§è€…
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>hello kafka!
```
> æŸ¥çœ‹systemhub511å’Œsystemhub611 æ¥æ”¶è€…
> åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªæ¶ˆè´¹è€…æ¥æ”¶åˆ°æ¶ˆæ¯.
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001 --consumer.config config/consumer.properties
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
[067] WARN [systemhub611_systemhub611-1555744110617-1d4d406d], no brokers found when trying to rebalance. (kafka.consumer.ZookeeperConsumerConnector)
```
```
[root@systemhub611 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001 --consumer.config config/consumer.properties
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello kafka!
```


## 4. Kafka API
### 4.1 ç¯å¢ƒå‡†å¤‡
- JetBrains IntelliJ IDEA New Maven Project | æ­¤è¿‡ç¨‹çœç•¥
- Maven pom.xml
``` xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.geekparkhub.core.kafka</groupId>
    <artifactId>kafka_server</artifactId>
    <version>1.0-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>3.8.1</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.11.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka_2.11</artifactId>
            <version>0.11.0.0</version>
        </dependency>
    </dependencies>
</project>
```

### 4.2 Kafka ç”Ÿäº§è€… Java API
#### 4.2.1 Create (è¿‡æ—¶API)ç”Ÿäº§è€…
> Create OldProduce.class
``` java
package com.geekparkhub.core.kafka.producer;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * OldProduce
 * <p>
 */

public class OldProduce {
    @SuppressWarnings("deprecation")
    public static void main(String[] args) {
        /**
         * Configuration information
         * é…ç½®ä¿¡æ¯
         */
        Properties props = new Properties();

        /**
         * Kafka configuration information
         * Kafkaé…ç½®ä¿¡æ¯
         */
        props.put("metadata.broker.list", "systemhub511:9092");

        /**
         * Response level
         * åº”ç­”çº§åˆ«
         */
        props.put("request.required.acks", "1");

        /**
         * K value serialization
         * Kå€¼ åºåˆ—åŒ–
         */
        props.put("serializer.class", "kafka.serializer.StringEncoder");

        /**
         * Instantiate producer object
         * å®ä¾‹åŒ– ç”Ÿäº§è€…å¯¹è±¡
         */
        Producer<Integer, String> producer = new Producer<Integer, String>(new ProducerConfig(props));

        /**
         * Send data
         * å‘é€æ•°æ®
         */
        KeyedMessage<Integer, String> message = new KeyedMessage<Integer, String>("topic001", "Hello,World");
        producer.send(message);

    }
}
```
#### 4.2.2 Create (æ–°API)ç”Ÿäº§è€…
> Create NewProduce.class
``` java
package com.geekparkhub.core.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * NewProduce
 * <p>
 */

public class NewProduce {
    public static void main(String[] args) {

        /**
         * Configuration information
         * é…ç½®ä¿¡æ¯
         */
        Properties props = new Properties();

        /**
         * Kafka configuration information
         * Kafkaé…ç½®ä¿¡æ¯
         */
        props.put("bootstrap.servers", "systemhub511:9092");

        /**
         * Response level
         * åº”ç­”çº§åˆ«
         */
        props.put("acks", "all");

        /**
         * number of retries
         * é‡è¯•æ¬¡æ•°
         */
        props.put("retries", 0);

        /**
         * Cache size
         * ç¼“å­˜å¤§å°
         */
        props.put("batch.size", 16384);

        /**
         * Submission delay
         * æäº¤å»¶æ—¶
         */
        props.put("linger.ms", 1);

        /**
         * Cache mode
         * ç¼“å­˜æ–¹å¼
         */
        props.put("buffer.memory", 33554432);

        /**
         * K value serialization
         * Kå€¼ åºåˆ—åŒ–
         */
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * V value serialization
         * Vå€¼ åºåˆ—åŒ–
         */
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * Instantiate producer object
         * å®ä¾‹åŒ– ç”Ÿäº§è€…å¯¹è±¡
         */
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        /**
         * Cycling data
         * å¾ªç¯å‘é€æ•°æ®
         */
        for (int i = 0; i < 10; i++) {
            producer.send(new ProducerRecord<String, String>("topic001", String.valueOf(i)));
            System.out.println("Result is = " + producer.toString());
        }

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        producer.close();

    }
}
```
- å¯åŠ¨zkå’Œkafkaé›†ç¾¤,åœ¨kafkaé›†ç¾¤ä¸­å¼€å¯æ¶ˆè´¹è€…æœåŠ¡,å¹¶æŸ¥çœ‹æ•°æ®.
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
0
2
4
6
8
1
3
5
7
9
```
#### 4.2.3 Create (æ–°API)ç”Ÿäº§è€…å›è°ƒå‡½æ•°
> Create CallBackProduce.class
``` java
package com.geekparkhub.core.kafka.producer;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * CallBackProduce
 * <p>
 */

public class CallBackProduce {
    public static void main(String[] args) {
        /**
         * Configuration information
         * é…ç½®ä¿¡æ¯
         */
        Properties props = new Properties();

        /**
         * Kafka configuration information
         * Kafkaé…ç½®ä¿¡æ¯
         */
        props.put("bootstrap.servers", "systemhub511:9092");

        /**
         * Response level
         * åº”ç­”çº§åˆ«
         */
        props.put("acks", "all");

        /**
         * number of retries
         * é‡è¯•æ¬¡æ•°
         */
        props.put("retries", 0);

        /**
         * Cache size
         * ç¼“å­˜å¤§å°
         */
        props.put("batch.size", 16384);

        /**
         * Submission delay
         * æäº¤å»¶æ—¶
         */
        props.put("linger.ms", 1);

        /**
         * Cache mode
         * ç¼“å­˜æ–¹å¼
         */
        props.put("buffer.memory", 33554432);

        /**
         * K value serialization
         * Kå€¼ åºåˆ—åŒ–
         */
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * V value serialization
         * Vå€¼ åºåˆ—åŒ–
         */
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * Instantiate producer object
         * å®ä¾‹åŒ– ç”Ÿäº§è€…å¯¹è±¡
         */
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        /**
         * Cycling data
         * å¾ªç¯å‘é€æ•°æ®
         */
        for (int i = 0; i < 10; i++) {
            producer.send(new ProducerRecord<String, String>("topic001", String.valueOf(i)), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    /**
                     * Determine if the exception is empty
                     * åˆ¤æ–­exceptionæ˜¯å¦ä¸ºç©º
                     */
                    if (exception == null) {
                        System.out.println("Data Sent Successfully !");
                        /**
                         * Get metadata information : offset & partition
                         * è·å–å…ƒæ•°æ®ä¿¡æ¯ : offset & partition
                         */
                        long offset = metadata.offset();
                        int partition = metadata.partition();
                        System.out.println("Offset is = " + offset + " -- & -- Partition is = " + partition);
                    } else {
                        System.out.println("Data Transmission Failed !");
                    }
                }
            });
        }

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        producer.close();
    }
}
```
- æŸ¥çœ‹ç»“æœ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_008.jpg)
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
0
2
4
6
8
1
3
5
7
9
```
#### 4.2.4 è‡ªå®šä¹‰åˆ†åŒºç”Ÿäº§è€…
- å°†æ‰€æœ‰æ•°æ®å­˜å‚¨åˆ°topicç¬¬0å·åˆ†åŒº.
- è‡ªå®šä¹‰åˆ†åŒºç±»,å®ç°Partitioneræ¥å£,é‡å†™åˆ†åŒºæ–¹æ³•.
- Create CustomPartitioner.class
``` java
package com.geekparkhub.core.kafka.producer;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

import java.util.Map;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * CustomPartitioner
 * <p>
 */

public class CustomPartitioner implements Partitioner {

    /**
     * Empty reference constructor
     * ç©ºå‚æ„é€ å™¨
     */
    public CustomPartitioner() {
        super();
    }

    /**
     * å¤å†™åˆ†åŒºæ–¹æ³•
     *
     * @param topic
     * @param key
     * @param keyBytes
     * @param value
     * @param valueBytes
     * @param cluster
     * @return
     */
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        return 0;
    }

    /**
     * Close resource
     * å…³é—­èµ„æº
     */
    @Override
    public void close() {
    }

    /**
     * Configuration information
     * é…ç½®ä¿¡æ¯
     *
     * @param configs
     */
    @Override
    public void configure(Map<String, ?> configs) {
    }
}

```
- Create CustomProducePartitioner.class
``` java
package com.geekparkhub.core.kafka.producer;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * CustomProducePartitioner
 * <p>
 */

public class CustomProducePartitioner {
    public static void main(String[] args) {
        /**
         * Configuration information
         * é…ç½®ä¿¡æ¯
         */
        Properties props = new Properties();

        /**
         * Kafka configuration information
         * Kafkaé…ç½®ä¿¡æ¯
         */
        props.put("bootstrap.servers", "systemhub511:9092");

        /**
         * Response level
         * åº”ç­”çº§åˆ«
         */
        props.put("acks", "all");

        /**
         * number of retries
         * é‡è¯•æ¬¡æ•°
         */
        props.put("retries", 0);

        /**
         * Cache size
         * ç¼“å­˜å¤§å°
         */
        props.put("batch.size", 16384);

        /**
         * Submission delay
         * æäº¤å»¶æ—¶
         */
        props.put("linger.ms", 1);

        /**
         * Cache mode
         * ç¼“å­˜æ–¹å¼
         */
        props.put("buffer.memory", 33554432);

        /**
         * K value serialization
         * Kå€¼ åºåˆ—åŒ–
         */
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * V value serialization
         * Vå€¼ åºåˆ—åŒ–
         */
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        /**
         * Custom Partition
         * è‡ªå®šä¹‰åˆ†åŒº
         */
        props.put("partitioner.class", "com.geekparkhub.core.kafka.producer.CustomPartitioner");

        /**
         * Instantiate producer object
         * å®ä¾‹åŒ– ç”Ÿäº§è€…å¯¹è±¡
         */
        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        /**
         * Cycling data
         * å¾ªç¯å‘é€æ•°æ®
         */
        for (int i = 0; i < 10; i++) {
            producer.send(new ProducerRecord<String, String>("topic001", String.valueOf(i)), new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    /**
                     * Determine if the exception is empty
                     * åˆ¤æ–­exceptionæ˜¯å¦ä¸ºç©º
                     */
                    if (exception == null) {
                        System.out.println("Data Sent Successfully !");
                        /**
                         * Get metadata information : offset & partition
                         * è·å–å…ƒæ•°æ®ä¿¡æ¯ : offset & partition
                         */
                        long offset = metadata.offset();
                        int partition = metadata.partition();
                        System.out.println("Offset is = " + offset + " -- & -- Partition is = " + partition);
                    } else {
                        System.out.println("Data Transmission Failed !");
                    }
                }
            });
        }

        /**
         * Close resource
         * å…³é—­èµ„æº
         */
        producer.close();
    }
}
```
- æŸ¥çœ‹ç»“æœ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_009.jpg)
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic001
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
0
1
2
3
4
5
6
7
8
9
```

### 4.3 Kafka æ¶ˆè´¹è€… Java API

#### 4.3.1 é«˜çº§ API
- Create CustomConsumer.class
``` java
package com.geekparkhub.core.kafka.consumer;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.Arrays;
import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * CustomConsumer
 * <p>
 */

public class CustomConsumer {

    public static void main(String[] args) {

        /**
         * Configuration information
         * é…ç½®ä¿¡æ¯
         */
        Properties props = new Properties();

        /**
         * Kafka configuration information
         * Kafkaé…ç½®ä¿¡æ¯
         */
        props.put("bootstrap.servers", "systemhub511:9092");

        /**
         * Consumer group ID
         * æ¶ˆè´¹è€…ç»„ID
         */
        props.put("group.id", "test");

        /**
         * Set auto-submit offset
         * è®¾ç½®è‡ªåŠ¨æäº¤offset
         */
        props.put("enable.auto.commit", "true");

        /**
         * Submission delay
         * æäº¤å»¶æ—¶
         */
        props.put("auto.commit.interval.ms", "1000");

        /**
         * K value Deserialization
         * Kå€¼ ååºåˆ—åŒ–
         */
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        /**
         * V value Deserialization
         * Vå€¼ ååºåˆ—åŒ–
         */
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        /**
         * Instantiation Consumer Object
         * å®ä¾‹åŒ– æ¶ˆè´¹è€…å¯¹è±¡
         */
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        /**
         * Specify Topic
         * æŒ‡å®šTopic
         */
        consumer.subscribe(Arrays.asList("topic001", "topic002", "topic003"));

        while (true) {
            /**
             * retrieve data
             * è·å–æ•°æ®
             */
            ConsumerRecords<String, String> consumerRecords = consumer.poll(100);

            /**
             * Loop output
             * å¾ªç¯è¾“å‡º
             */
            for (ConsumerRecord<String, String> records : consumerRecords) {
                String topic = records.topic();
                int partition = records.partition();
                String value = records.value();
                System.out.println("Topic is = " + topic + " -- & -- Partition is = " + partition + " -- & -- Value is = " + value);
            }
        }
    }
}
```
- åˆ†åˆ«å¼€å¯ç”Ÿäº§è€…æœåŠ¡
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
```
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic002
```
```
root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic003
```
- æ‰§è¡ŒCustomConsumer.class

- ç”Ÿäº§è€…å‘é€æ¶ˆæ¯
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>hello topic001
```
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic002
>hello topic002
```
```
[root@systemhub711 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic003
>hello topic003
```

- æŸ¥çœ‹ç»“æœ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_010.jpg)


#### 4.3.2 ä½çº§ API
- å®ç°ä½¿ç”¨ä½çº§APIè¯»å–æŒ‡å®šTopic / æŒ‡å®šPartition / æŒ‡å®šoffsetæ•°æ®.
- æ¶ˆè´¹è€…ä½¿ç”¨ä½çº§APIä¸»è¦æ­¥éª¤ : 
| æ­¥éª¤      |     ä¸»è¦å·¥ä½œ |
| :--------: | :-------:|
| 1    |   æ ¹æ®æŒ‡å®šåˆ†åŒºä»ä¸»é¢˜å…ƒæ•°æ®ä¸­æ‰¾åˆ°ä¸»å‰¯æœ¬. |
| 2    |   è·å–åˆ†åŒºæœ€æ–°æ¶ˆè´¹è¿›åº¦. |
| 3    |   ä»ä¸»å‰¯æœ¬æ‹‰å–åˆ†åŒºæ¶ˆæ¯. |
| 4    |   è¯†åˆ«ä¸»å‰¯æœ¬å˜åŒ–. |
- æ–¹æ³•æè¿° : 
| æ–¹æ³•å      |     æè¿° |
| :--------: | :--------:|
| findLeader()    |   å®¢æˆ·ç«¯å‘ç§å­èŠ‚ç‚¹å‘é€ä¸»é¢˜å…ƒæ•°æ®,å°†å‰¯æœ¬é›†åŠ å…¥å¤‡ç”¨èŠ‚ç‚¹. |
| getLastOffset()    |   æ¶ˆè´¹è€…å®¢æˆ·ç«¯å‘é€åç§»é‡è¯·æ±‚,è·å–åˆ†åŒºæœ€è¿‘åç§»é‡. |
| run()    |   æ¶ˆè´¹è€…ä½çº§APIæ‹‰å–æ¶ˆæ¯ä¸»è¦æ–¹æ³• |
| findNewLeader()    |   å½“åˆ†åŒºä¸»å‰¯æœ¬èŠ‚ç‚¹å‘é€æ•…éšœ,å®¢æˆ·å°†è¦æ‰¾å‡ºæ–°çš„ä¸»å‰¯æœ¬. |

- Create LowerConsumer.class
``` java
package com.geekparkhub.core.kafka.consumer;

import kafka.api.FetchRequestBuilder;
import kafka.cluster.BrokerEndPoint;
import kafka.javaapi.*;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.javaapi.message.ByteBufferMessageSet;
import kafka.message.MessageAndOffset;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * LowerConsumer
 * <p>
 */

public class LowerConsumer {
    public static void main(String[] args) {

        /**
         * Define related parameters
         * å®šä¹‰ç›¸å…³å‚æ•°
         */

        /**
         * Kafka cluster
         * Kafka é›†ç¾¤
         */
        ArrayList<String> brokers = new ArrayList<>();
        brokers.add("systemhub511");
        brokers.add("systemhub611");
        brokers.add("systemhub711");

        /**
         * port number
         * ç«¯å£å·
         */
        int port = 9092;

        /**
         * Theme Topic
         * ä¸»é¢˜ Topic
         */
        String topic = "topic002";

        /**
         * Partition
         * åˆ†åŒº Partition
         */
        int partition = 0;

        /**
         * Offset
         */
        long offset = 2;

        /**
         * Instantiate the LowerConsumer object
         * å®ä¾‹åŒ– LowerConsumerå¯¹è±¡
         */
        LowerConsumer lowerConsumer = new LowerConsumer();
        lowerConsumer.getData(brokers,port,topic,partition,offset);
    }

    /**
     * Discover partition leader
     * å‘ç°åˆ†åŒºLeader
     *
     * @return
     */
    private BrokerEndPoint findLeader(List<String> brokers, int port, String topic, int partition) {

        /**
         * Instantiation Partition Leader Consumer Object
         * å®ä¾‹åŒ– åˆ†åŒºLeaderæ¶ˆè´¹è€…å¯¹è±¡
         */
        for (String broker : brokers) {
            SimpleConsumer getLeader = new SimpleConsumer(broker, port, 1000, 1024 * 4, "getLeader");

            /**
             * Create topic metadata information request
             * åˆ›å»ºä¸»é¢˜å…ƒæ•°æ®ä¿¡æ¯è¯·æ±‚
             */
            TopicMetadataRequest topicMetadataRequest = new TopicMetadataRequest(Collections.singletonList(topic));

            /**
             * Get theme metadata return value
             * è·å–ä¸»é¢˜å…ƒæ•°æ®è¿”å›å€¼
             */
            TopicMetadataResponse metadataResponse = getLeader.send(topicMetadataRequest);

            /**
             * Parse the metadata return value
             * è§£æå…ƒå…ƒæ•°æ®è¿”å›å€¼
             */
            List<TopicMetadata> topicsMetadata = metadataResponse.topicsMetadata();

            /**
             * Loop theme metadata
             * å¾ªç¯ ä¸»é¢˜å…ƒæ•°æ®
             */
            for (TopicMetadata topicMetadatum : topicsMetadata) {

                /**
                 * Get multiple partition metadata information
                 * è·å–å¤šä¸ªåˆ†åŒºå…ƒæ•°æ®ä¿¡æ¯
                 */
                List<PartitionMetadata> partitionsMetadata = topicMetadatum.partitionsMetadata();

                /**
                 * Loop multi-partition metadata
                 * å¾ªç¯ å¤šåˆ†åŒºå…ƒæ•°æ®
                 */
                for (PartitionMetadata partitionMetadata : partitionsMetadata) {
                    /**
                     * Returns the leader metadata information if the partition number is equal to 0
                     * å¦‚æœåˆ†åŒºå·ç­‰äº0,åˆ™è¿”å›leaderå…ƒæ•°æ®ä¿¡æ¯
                     */
                    if (partition == partitionMetadata.partitionId()) {
                        return partitionMetadata.leader();
                    }
                }
            }
        }
        return null;
    }

    /**
     * retrieve data
     * è·å–æ•°æ®
     */
    private void getData(List<String> brokers, int port, String topic, int partition, long offset) {

        /**
         * Get Partition leader
         * è·å–åˆ†åŒºleader
         */
        BrokerEndPoint leader = findLeader(brokers, port, topic, partition);
        if (leader == null) {
            return;
        }

        String leaderHost = leader.host();

        /**
         * Get data consumer object
         * è·å–æ•°æ®æ¶ˆè´¹è€…å¯¹è±¡
         */
        SimpleConsumer getData = new SimpleConsumer(leaderHost, port, 1000, 1024 * 4, "getData");

        /**
         * Instantiation get data object
         * å®ä¾‹åŒ– è·å–æ•°æ®å¯¹è±¡
         */
        kafka.api.FetchRequest fetchRequest = new FetchRequestBuilder().addFetch(topic, partition, offset, 100).build();

        /**
         * Get data return value
         * è·å–æ•°æ®è¿”å›å€¼
         */
        FetchResponse fetchResponse = getData.fetch(fetchRequest);

        /**
         * Parse the return value
         * è§£æè¿”å›å€¼
         */
        ByteBufferMessageSet messageAndOffsets = fetchResponse.messageSet(topic, partition);
        for (MessageAndOffset messageAndOffset : messageAndOffsets) {
            long offset1 = messageAndOffset.offset();
            ByteBuffer payload = messageAndOffset.message().payload();
            byte[] bytes = new byte[payload.limit()];
            payload.get(bytes);
            System.out.println("Offset is = " + offset1 + " -- & -- Message is = " + new String(bytes));
        }
    }
}
```

- å¼€å¯ Zookeeperé›†ç¾¤æœåŠ¡ & Kafkaé›†ç¾¤æœåŠ¡
```
[root@systemhub511 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@systemhub511 zookeeper]#
```
```
[root@systemhub611 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@systemhub611 zookeeper]#
```
```
[root@systemhub711 zookeeper]# bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /opt/module/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@systemhub711 zookeeper]#
```
```
root@systemhub511 kafka]# bin/kafka-server-start.sh config/server.properties &
[1] 23017
```
```
[root@systemhub611 kafka]# bin/kafka-server-start.sh config/server.properties &
[1] 23716
```
```
[root@systemhub711 kafka]# bin/kafka-server-start.sh config/server.properties &
[1] 24134
```

- æ‰§è¡ŒLowerConsumer.class
- æŸ¥çœ‹ç»“æœ
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_011.jpg)


## 5. Kafka Produceræ‹¦æˆªå™¨
### 5.1 æ‹¦æˆªå™¨åŸç†
> Produceræ‹¦æˆªå™¨(interceptor)æ˜¯åœ¨Kafka 0.10ç‰ˆæœ¬è¢«å¼•å…¥,ä¸»è¦ç”¨äºå®ç°Clientså®šåˆ¶åŒ–æ§åˆ¶é€»è¾‘.
> 
> å¯¹äºProducerè€Œè¨€,interceptorä½¿å¾—ç”¨æˆ·åœ¨æ¶ˆæ¯å‘é€å‰ä»¥åŠProducerå›è°ƒé€»è¾‘å‰æœ‰æœºä¼šå¯¹æ¶ˆæ¯åšä¸€äº›å®šåˆ¶åŒ–éœ€æ±‚,æ¯”å¦‚ä¿®æ”¹æ¶ˆæ¯ç­‰,åŒæ—¶Producerå…è®¸ç”¨æˆ·æŒ‡å®šå¤šä¸ªinterceptoræŒ‰åºä½œç”¨äºåŒä¸€æ¡æ¶ˆæ¯ä»è€Œå½¢æˆä¸€ä¸ªæ‹¦æˆªé“¾(interceptor Chain).
> 
> Intercetporå®ç°æ¥å£æ˜¯`org.apache.kafka.clients.producer.ProducerInterceptor`,å…¶å®šä¹‰çš„æ–¹æ³•åŒ…æ‹¬:
- `configure(configs)` è·å–é…ç½®ä¿¡æ¯å’Œåˆå§‹åŒ–æ•°æ®æ—¶è°ƒç”¨.
- `onSend(ProducerRecord)` è¯¥æ–¹æ³•å°è£…è¿›KafkaProducer.sendæ–¹æ³•ä¸­,å³å®ƒè¿è¡Œåœ¨ç”¨æˆ·ä¸»çº¿ç¨‹ä¸­,Producerç¡®ä¿åœ¨æ¶ˆæ¯è¢«åºåˆ—åŒ–ä»¥åŠè®¡ç®—åˆ†åŒºå‰è°ƒç”¨è¯¥æ–¹æ³•,ç”¨æˆ·å¯ä»¥åœ¨è¯¥æ–¹æ³•ä¸­å¯¹æ¶ˆæ¯åšä»»ä½•æ“ä½œ,ä½†æœ€å¥½ä¿è¯ä¸è¦ä¿®æ”¹æ¶ˆæ¯æ‰€å±çš„topicå’Œåˆ†åŒº,å¦åˆ™ä¼šå½±å“ç›®æ ‡åˆ†åŒºè®¡ç®—.
- `nAcknowledgement(RecordMetadata, Exception)` è¯¥æ–¹æ³•ä¼šåœ¨æ¶ˆæ¯è¢«åº”ç­”æˆ–æ¶ˆæ¯å‘é€å¤±è´¥æ—¶è°ƒç”¨,å¹¶ä¸”é€šå¸¸éƒ½æ˜¯åœ¨producerå›è°ƒé€»è¾‘è§¦å‘ä¹‹å‰,onAcknowledgementè¿è¡Œåœ¨producerçš„IOçº¿ç¨‹ä¸­,å› æ­¤ä¸è¦åœ¨è¯¥æ–¹æ³•ä¸­æ”¾å…¥å¾ˆé‡é€»è¾‘,å¦åˆ™ä¼šæ‹–æ…¢Producerçš„æ¶ˆæ¯å‘é€æ•ˆç‡.
- `close` å…³é—­interceptor,ä¸»è¦ç”¨äºæ‰§è¡Œä¸€äº›èµ„æºæ¸…ç†å·¥ä½œå¦‚å‰æ‰€è¿°,interceptorå¯èƒ½è¢«è¿è¡Œåœ¨å¤šä¸ªçº¿ç¨‹ä¸­,å› æ­¤åœ¨å…·ä½“å®ç°æ—¶ç”¨æˆ·éœ€è¦è‡ªè¡Œç¡®ä¿çº¿ç¨‹å®‰å…¨,å¦å¤–å€˜è‹¥æŒ‡å®šäº†å¤šä¸ªinterceptor,åˆ™producerå°†æŒ‰ç…§æŒ‡å®šé¡ºåºè°ƒç”¨å®ƒä»¬,å¹¶ä»…ä»…æ˜¯æ•è·æ¯ä¸ªinterceptorå¯èƒ½æŠ›å‡ºçš„å¼‚å¸¸è®°å½•åˆ°é”™è¯¯æ—¥å¿—ä¸­è€Œéåœ¨å‘ä¸Šä¼ é€’.

### 5.2 æ‹¦æˆªå™¨æ¡ˆä¾‹
- å®ç°ä¸€ä¸ªç®€å•åŒinterceptorç»„æˆæ‹¦æˆªé“¾.
- ç¬¬ä¸€ä¸ªinterceptorä¼šåœ¨æ¶ˆæ¯å‘é€å‰å°†æ—¶é—´æˆ³ä¿¡æ¯åŠ è½½åˆ°æ¶ˆæ¯valueæœ€å‰éƒ¨.
- ç¬¬äºŒä¸ªinterceptorä¼šåœ¨æ¶ˆæ¯å‘é€åæ›´æ–°æˆåŠŸå‘é€æ¶ˆæ¯æ•°æˆ–å¤±è´¥å‘é€æ¶ˆæ¯æ•°.
- Create Timeinterceptor.class
``` java
package com.geekparkhub.core.kafka.interceptor;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

import java.util.Map;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * Timeinterceptor
 * <p>
 */

public class Timeinterceptor implements ProducerInterceptor<String, String> {


    @Override
    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
        return new ProducerRecord<String, String>(record.topic(), record.key(), System.currentTimeMillis() + " - - " + record.value());
    }

    @Override
    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {

    }

    @Override
    public void close() {

    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}
```
- Create Countinterceptor.class
``` java
package com.geekparkhub.core.kafka.interceptor;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

import java.util.Map;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * Countinterceptor
 * <p>
 */

public class Countinterceptor implements ProducerInterceptor<String, String> {

    private int successCount = 0;
    private int errorCount = 0;

    @Override
    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
        return record;
    }

    @Override
    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {

        if (exception == null) {
            successCount++;
        } else {
            errorCount++;
        }
    }

    @Override
    public void close() {
        System.out.println("Success : " + successCount + "Article Data !");
        System.out.println("Failure : " + errorCount + "Article Data !");
    }

    @Override
    public void configure(Map<String, ?> configs) {

    }
}
```
- å¼€å¯æ¶ˆè´¹è€…æœåŠ¡
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic002
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
```
- æ‰§è¡Œç”Ÿäº§è€…å¹¶æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯
![enter image description here](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/kafka/start_012.jpg)

## 6. Kafka Streams
### 6.1 æ¦‚è¿°
#### 6.1.1 Kafka Streams
- Kafka Streams,Apache Kafkaå¼€æºé¡¹ç›®ä¸€ä¸ªç»„æˆéƒ¨åˆ†,æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§,æ˜“äºä½¿ç”¨åº“,ç”¨äºåœ¨Kafkaä¸Šæ„å»ºé«˜å¯åˆ†å¸ƒå¼ã€æ‹“å±•æ€§,å®¹é”™çš„åº”ç”¨ç¨‹åº.
#### 6.1.2 Kafka Streams ç‰¹ç‚¹
- åŠŸèƒ½å¼ºå¤§ : é«˜æ‰©å±•æ€§,å¼¹æ€§,å®¹é”™
- è½»é‡çº§ : æ— éœ€ä¸“é—¨é›†ç¾¤ä¸€ä¸ªåº“,è€Œä¸æ˜¯æ¡†æ¶
- å®Œå…¨é›†æˆ : 100%Kafka 0.10.0ç‰ˆæœ¬å…¼å®¹,æ˜“äºé›†æˆåˆ°ç°æœ‰çš„åº”ç”¨ç¨‹åº
- å®æ—¶æ€§ : æ¯«ç§’çº§å»¶è¿Ÿ/å¹¶éå¾®æ‰¹å¤„ç†/çª—å£å…è®¸ä¹±åºæ•°æ®/å…è®¸è¿Ÿåˆ°æ•°æ®
#### 6.1.3 ä¸ºä»€ä¹ˆè¦æœ‰Kafka Stream
> å½“å‰å·²ç»æœ‰éå¸¸å¤šçš„æµå¼å¤„ç†ç³»ç»Ÿ,æœ€çŸ¥åä¸”åº”ç”¨æœ€å¤šå¼€æºæµå¼å¤„ç†ç³»ç»Ÿæœ‰Spark Streamingå’ŒApache Storm,Apache Stormå‘å±•å¤šå¹´åº”ç”¨å¹¿æ³›,æä¾›è®°å½•çº§åˆ«çš„å¤„ç†èƒ½åŠ›,å½“å‰ä¹Ÿæ”¯æŒSQL on Stream,è€ŒSpark StreamingåŸºäºApache Spark,å¯ä»¥éå¸¸æ–¹ä¾¿ä¸å›¾è®¡ç®—,SQLå¤„ç†ç­‰é›†æˆ,åŠŸèƒ½å¼ºå¤§,å¯¹äºç†Ÿæ‚‰å…¶å®ƒSparkåº”ç”¨å¼€å‘ç”¨æˆ·è€Œè¨€ä½¿ç”¨é—¨æ§›ä½,å¦å¤–,ç›®å‰ä¸»æµçš„Hadoopå‘è¡Œç‰ˆ,å¦‚Clouderaå’ŒHortonworks,éƒ½é›†æˆäº†Apache Stormå’ŒApache Spark,ä½¿å¾—éƒ¨ç½²æ›´å®¹æ˜“.
> 
> ç¬¬ä¸€,Sparkå’ŒStorméƒ½æ˜¯æµå¼å¤„ç†æ¡†æ¶,è€ŒKafka Streamæä¾›æ˜¯ä¸€ä¸ªåŸºäºKafkaæµå¼å¤„ç†ç±»åº“,æ¡†æ¶è¦æ±‚å¼€å‘è€…æŒ‰ç…§ç‰¹å®šçš„æ–¹å¼å»å¼€å‘é€»è¾‘éƒ¨åˆ†,ä¾›æ¡†æ¶è°ƒç”¨,å¼€å‘è€…å¾ˆéš¾äº†è§£æ¡†æ¶å…·ä½“è¿è¡Œæ–¹å¼,ä»è€Œä½¿å¾—è°ƒè¯•æˆæœ¬é«˜,å¹¶ä¸”ä½¿ç”¨å—é™,è€ŒKafka Streamä½œä¸ºæµå¼å¤„ç†ç±»åº“,ç›´æ¥æä¾›å…·ä½“çš„ç±»ç»™å¼€å‘è€…è°ƒç”¨,æ•´ä¸ªåº”ç”¨è¿è¡Œæ–¹å¼ä¸»è¦ç”±å¼€å‘è€…æ§åˆ¶,æ–¹ä¾¿ä½¿ç”¨å’Œè°ƒè¯•.
> 
> ç¬¬äºŒ,è™½ç„¶Clouderaä¸Hortonworksæ–¹ä¾¿äº†Stormå’ŒSparkéƒ¨ç½²,ä½†æ˜¯è¿™äº›æ¡†æ¶çš„éƒ¨ç½²ä»ç„¶ç›¸å¯¹å¤æ‚,è€ŒKafka Streamä½œä¸ºç±»åº“,å¯ä»¥éå¸¸æ–¹ä¾¿åµŒå…¥åº”ç”¨ç¨‹åºä¸­,å®ƒå¯¹åº”ç”¨æ‰“åŒ…å’Œéƒ¨ç½²åŸºæœ¬æ²¡æœ‰ä»»ä½•è¦æ±‚.
> 
> ç¬¬ä¸‰.å°±æµå¼å¤„ç†ç³»ç»Ÿè€Œè¨€.åŸºæœ¬éƒ½æ”¯æŒKafkaä½œä¸ºæ•°æ®æº.ä¾‹å¦‚Stormå…·æœ‰ä¸“é—¨kafka-Spout.è€ŒSparkä¹Ÿæä¾›ä¸“é—¨Spark-Streaming-Kafkaæ¨¡å—.äº‹å®ä¸ŠKafkaåŸºæœ¬ä¸Šæ˜¯ä¸»æµçš„æµå¼å¤„ç†ç³»ç»Ÿçš„æ ‡å‡†æ•°æ®æº,æ¢è¨€ä¹‹å¤§éƒ¨åˆ†æµå¼ç³»ç»Ÿä¸­éƒ½å·²éƒ¨ç½²äº†Kafka,æ­¤æ—¶ä½¿ç”¨Kafka Streamçš„æˆæœ¬éå¸¸ä½.
> 
> ç¬¬å››,ä½¿ç”¨Stormæˆ–SparkStreamingæ—¶,éœ€è¦ä¸ºæ¡†æ¶æœ¬èº«çš„è¿›ç¨‹é¢„ç•™èµ„æº,å¦‚Stormçš„Supervisorå’ŒSpark on YARNçš„NodeManager,å³ä½¿å¯¹äºåº”ç”¨å®ä¾‹è€Œè¨€,æ¡†æ¶æœ¬èº«ä¹Ÿä¼šå ç”¨éƒ¨åˆ†èµ„æº,å¦‚Spark Streamingéœ€è¦ä¸ºShuffleå’ŒStorageé¢„ç•™å†…å­˜,ä½†æ˜¯Kafkaä½œä¸ºç±»åº“ä¸å ç”¨ç³»ç»Ÿèµ„æº.
> 
> ç¬¬äº”,ç”±äºKafkaæœ¬èº«æä¾›æ•°æ®æŒä¹…åŒ–,å› æ­¤Kafka Streamæä¾›æ»šåŠ¨éƒ¨ç½²æ»šåŠ¨å‡çº§ä»¥åŠé‡æ–°è®¡ç®—èƒ½åŠ›.
> 
> ç¬¬å…­,ç”±äºKafka Consumer Rebalanceæœºåˆ¶,Kafka Streamå¯ä»¥åœ¨çº¿åŠ¨æ€è°ƒæ•´å¹¶è¡Œåº¦.


### 6.2 Kafka Stream æ•°æ®æ¸…æ´—
- å®æ—¶å¤„ç†å•è¯å¸¦æœ‰`>>>`å‰ç¼€å†…å®¹.
- ä¾‹å¦‚è¾“å…¥hello>>>kafka!æœ€ç»ˆå¤„ç†æˆhello_kafka!
- UpDate pom.xml
``` xml
 <dependency>
  <groupId>org.apache.kafka</groupId>
  <artifactId>kafka-streams</artifactId>
  <version>0.11.0.0</version>
 </dependency>
```
- Create KafkaStream.class
``` java
package com.geekparkhub.core.kafka.streams;

import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.processor.Processor;
import org.apache.kafka.streams.processor.ProcessorSupplier;
import org.apache.kafka.streams.processor.TopologyBuilder;
import java.util.Properties;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * KafkaStream
 * <p>
 */

public class KafkaStream {
    public static void main(String[] args) {

        /**
         * Instantiate topology objects
         * å®ä¾‹åŒ– æ‹“æ‰‘å¯¹è±¡
         */
        TopologyBuilder builder = new TopologyBuilder();

        /**
         * Instantiation configuration file
         * å®ä¾‹åŒ– é…ç½®æ–‡ä»¶
         */
        Properties properties = new Properties();
        properties.put("bootstrap.servers", "systemhub511:9092");
        properties.put("application.id", "KafkaStream");

        /**
         * Build topology
         * æ„å»ºæ‹“æ‰‘ç»“æ„
         */
        builder.addSource("SOURCE", "topic001")
                .addProcessor("PROCESSOR", new ProcessorSupplier() {
                    @Override
                    public Processor get() {
                        return new LogProcessor() {
                        };
                    }
                }, "SOURCE")
                .addSink("SINK", "topic002", "PROCESSOR");

        /**
         * Instantiate KafkaStreams objects
         * å®ä¾‹åŒ– KafkaStreamså¯¹è±¡
         */
        KafkaStreams kafkaStreams = new KafkaStreams(builder, properties);
        kafkaStreams.start();
    }
}
```
- Create LogProcessor.class
``` java
package com.geekparkhub.core.kafka.streams;

import org.apache.kafka.streams.processor.Processor;
import org.apache.kafka.streams.processor.ProcessorContext;

/**
 * Geek International Park | æå®¢å›½é™…å…¬å›­
 * GeekParkHub | æå®¢å®éªŒå®¤
 * Website | https://www.geekparkhub.com/
 * Description | Openå¼€æ”¾ Â· Creationåˆ›æƒ³ | OpenSourceå¼€æ”¾æˆå°±æ¢¦æƒ³ GeekParkHubå…±å»ºå‰æ‰€æœªè§
 * HackerParkHub | é»‘å®¢å…¬å›­æ¢çº½
 * Website | https://www.hackerparkhub.com/
 * Description | ä»¥æ— æ‰€ç•æƒ§çš„æ¢ç´¢ç²¾ç¥ å¼€åˆ›æœªçŸ¥æŠ€æœ¯ä¸å¯¹æŠ€æœ¯çš„å´‡æ‹œ
 * GeekDeveloper : JEEP-711
 *
 * @author system
 * <p>
 * LogProcessor
 * <p>
 */

public class LogProcessor implements Processor<byte[], byte[]> {
    private ProcessorContext context;

    @Override
    public void init(ProcessorContext processorContext) {
        context = processorContext;
    }

    @Override
    public void process(byte[] key, byte[] value) {

        /**
         * Get a row of data
         * è·å–ä¸€è¡Œæ•°æ®
         */
        String line = new String(value);

        /**
         * Remove dirty data
         * å»é™¤è„æ•°æ®
         */
        line = line.replaceAll(">>>", "_");

        value = line.getBytes();

        context.forward(key, value);
    }

    @Override
    public void punctuate(long l) {

    }

    @Override
    public void close() {

    }
}
```
- æ‰§è¡ŒKafkaStream.class

- å¼€å¯ç”Ÿäº§è€…æœåŠ¡ å‘é€æ¶ˆæ¯
```
[root@systemhub511 kafka]# bin/kafka-console-producer.sh --broker-list systemhub511:9092 --topic topic001
>hello
>hello>>>kafka!
```

- å¼€å¯æ¶ˆè´¹è€…æœåŠ¡ æ¥æ”¶æ¶ˆæ¯
```
[root@systemhub511 kafka]# bin/kafka-console-consumer.sh --zookeeper systemhub511:2181 --topic topic002
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello
hello_kafka!
```
## 7. Kafka æ‰©å±•
### 7.1 Kafkaä¸Flumeæ¯”è¾ƒ
- `Flume` : é€‚åˆå¤šä¸ªç”Ÿäº§è€…,é€‚åˆä¸‹æ¸¸æ•°æ®æ¶ˆè´¹è€…ä¸å¤šçš„æƒ…å†µ,é€‚åˆæ•°æ®å®‰å…¨æ€§è¦ä¸é«˜çš„æ“ä½œ,é€‚åˆä¸Hadoopç”Ÿæ€ä½“ç³»å¯¹æ¥æ“ä½œ.
- `Kafka` : é€‚åˆæ•°æ®ä¸‹æ¸¸æ¶ˆè´¹è€…ä¼—å¤šçš„æƒ…å†µ,é€‚åˆæ•°æ®å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„æ“ä½œ,æ”¯æŒreplication.
- å¸¸ç”¨æ¨¡å‹ : Online Data â¡ï¸ Flume â¡ï¸ Kafka â¡ï¸ HDFS



## 8. ä¿®ä»™ä¹‹é“ æŠ€æœ¯æ¶æ„è¿­ä»£ ç™»å³°é€ æä¹‹åŠ¿
![Alt text](https://raw.githubusercontent.com/geekparkhub/geekparkhub.github.io/master/technical_guide/assets/media/main/technical_framework.jpg)


-----

## ğŸ’¡å¦‚ä½•å¯¹è¯¥å¼€æºæ–‡æ¡£è¿›è¡Œè´¡çŒ®ğŸ’¡

1. Blogå†…å®¹å¤§å¤šæ˜¯æ‰‹æ•²,æ‰€ä»¥éš¾å…ä¼šæœ‰ç¬”è¯¯,ä½ å¯ä»¥å¸®æˆ‘æ‰¾é”™åˆ«å­—ã€‚
2. å¾ˆå¤šçŸ¥è¯†ç‚¹æˆ‘å¯èƒ½æ²¡æœ‰æ¶‰åŠåˆ°,æ‰€ä»¥ä½ å¯ä»¥å¯¹å…¶ä»–çŸ¥è¯†ç‚¹è¿›è¡Œè¡¥å……ã€‚
3. ç°æœ‰çš„çŸ¥è¯†ç‚¹éš¾å…å­˜åœ¨ä¸å®Œå–„æˆ–è€…é”™è¯¯,æ‰€ä»¥ä½ å¯ä»¥å¯¹å·²æœ‰çŸ¥è¯†ç‚¹çš„ä¿®æ”¹/è¡¥å……ã€‚
4. ğŸ’¡æ¬¢è¿è´¡çŒ®`å„é¢†åŸŸå¼€æºé‡ç”ŸBlog`&`ç¬”è®°`&`æ–‡ç« `&`ç‰‡æ®µ`&`åˆ†äº«`&`åˆ›æƒ³`&`OpenSource Project`&`Code`&`Code Review`
5. ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ issues: [geekparkhub.github.io/issues](https://github.com/geekparkhub/geekparkhub.github.io/issues) ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ

### å¸Œæœ›æ¯ä¸€ç¯‡æ–‡ç« éƒ½èƒ½å¤Ÿå¯¹è¯»è€…ä»¬æä¾›å¸®åŠ©ä¸æå‡,è¿™ä¹ƒæ˜¯æ¯ä¸€ä½ç¬”è€…çš„åˆè¡·                          


-----


## ğŸ’Œæ„Ÿè°¢æ‚¨çš„é˜…è¯» æ¬¢è¿æ‚¨çš„ç•™è¨€ä¸å»ºè®®ğŸ’Œ

- FaceBookï¼š[JEEP SevenEleven](https://www.facebook.com/profile.php?id=100018099483403)
- Twitterï¼š[@JEEP7ll](https://twitter.com/JEEP7ll)
- Sina Weibo: [@JEEP-711](https://weibo.com/JEEP511)
- GeekParkHub GithubHomeï¼š<https://github.com/geekparkhub>
- GeekParkHub GiteeHomeï¼š<https://gitee.com/geekparkhub>
- Blog GardenHomeï¼š<http://www.cnblogs.com/JEEP711/>
- W3C/BlogHomeï¼š<https://www.w3cschool.cn/jeep711blog/>
- CSDN/BlogHomeï¼š<http://blog.csdn.net/jeep911>
- 51CTO/BlogHomeï¼š<http://jeep711.blog.51cto.com/>
- **`Official Public Email`**
- Group Emailï¼š<geekparkhub@outlook.com> â€”â€” <hackerparkhub@outlook.com> â€”â€” <hackerpark@hotmail.com>
- User Emailï¼š<jeep711.home.@gmail.com> â€”â€” <jeep-711@outlook.com>
- System Emailï¼š<systemhub-711@outlook.com>
- Service Emailï¼š<servicehub-711@outlook.com>



### æåŠ© é¡¹ç›®çš„å‘å±•ç¦»ä¸å¼€ä½ çš„æ”¯æŒ,è¯·å¼€å‘è€…å–æ¯â˜•Coffeeâ˜•å§!
![enter image description here](https://www.geekparkhub.com/docs/images/pay.jpg)

#### `è‡´è°¢`ï¼š
**æåŠ©æ—¶è¯·å¤‡æ³¨ UserName**
| ID| UserName | Donation | Money | Consume |
|:-| :-------- | --------:| :--: |:--: |
|1 | Object | WeChatPay |  5RMB | ä¸€æ¯å¯ä¹ | 
|2| æ³°è¿ªç†Šçœ‹æœˆäº®  | AliPay |  20RMB  | ä¸€æ¯å’–å•¡ | 
|3| ä¿®ä»™é“é•¿  | WeChatPay |  10RMB | ä¸¤æ¯å¯ä¹ | 


## License å¼€æºåè®®
[Apache License Version 2.0](https://github.com/geekparkhub/geekparkhub.github.io/blob/master/LICENSE)

---------